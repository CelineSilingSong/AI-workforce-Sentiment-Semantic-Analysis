URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,url,image,author,publisher,headline,datePublished,dateModified,articleSection,name,isAccessibleForFree,itemListElement,article:section,article:summary,article text,sameAs,@graph,potentialAction,thumbnailUrl,creator,mainEntityOfPAge,inLanguage,text,articleBody,@id,logo,contactPoint,mainEntityOfPage,dateCreated,entry-title,published,updated,isBasedOn,isPartOf,alternativeHeadline,claimReviewed,reviewRating,itemReviewed,issn,hasPart
https://news.google.com/rss/articles/CBMifmh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvam9lbWNrZW5kcmljay8yMDE4LzEyLzE5L2hvdy1mYXN0LWlzLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWdyb3dpbmctbG9vay1hdC10aGUta2V5LWJlbGx3ZXRoZXJzL9IBAA?oc=5,How Fast Is Artificial Intelligence Growing? Look At The Key Bellwethers - Forbes,2018-12-19,Forbes,https://www.forbes.com,"Stanford researchers document AI growth across many indicators, including startups, venture capital, job openings and academic programs.",,"Stanford researchers document AI growth across many indicators, including startups, venture capital, job openings and academic programs.","Stanford researchers document AI growth across many indicators, including startups, venture capital, job openings and academic programs.",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/joemckendrick/2018/12/19/how-fast-is-artificial-intelligence-growing-look-at-the-key-bellwethers/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/blogs-images/joemckendrick/files/2016/11/National-Gallery-of-Art-cropped-Washington-DC-July-2016-photo-by-Joe-McKendrick-300x275.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Joe McKendrick', 'url': 'https://www.forbes.com/sites/joemckendrick/', 'description': 'I am an author, independent researcher and speaker exploring innovation, information technology trends and markets. I served as co-chair of the 2023 AI Summit in New York, as well as the 2021 and 2022 Summits. I regularly contribute to Harvard Business Review on AI topics. My column on service orientation appears on CNET, covering topics shaping business and technology careers. I am also a co-author of the SOA Manifesto, which outlines the values and guiding principles of service orientation in business and IT. Much of my research work is in conjunction with Forbes Insights and Unisphere Research/ Information Today, Inc., covering topics such as artificial intelligence, cloud computing, digital transformation, and big data analytics. In a previous life, I served as communications and research manager of the Administrative Management Society (AMS), an international professional association dedicated to advancing knowledge within the IT and business management fields. I am a graduate of Temple University.', 'sameAs': ['https://www.twitter.com/joemckendrick', 'Joe McKendrick']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",How Fast Is Artificial Intelligence Growing? Look At The Key Bellwethers,2018-12-19T00:26:00-05:00,2018-12-19T11:03:01-05:00,AI & Big Data,How Fast Is Artificial Intelligence Growing? Look At The Key Bellwethers,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Enterprise Tech', 'item': 'https://www.forbes.com/enterprise-tech/'}]",AI & Big Data,N/A,"Edit StoryInnovationEnterprise TechHow Fast Is Artificial Intelligence Growing? Look At The Key BellwethersJoe McKendrickSenior ContributorOpinions expressed by Forbes Contributors are their own.I track how technology innovations move markets and careersFollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got itDec 19, 2018,12:26am ESTUpdated Dec 19, 2018, 11:03am ESTThis article is more than 5 years old.Share to FacebookShare to TwitterShare to LinkedinOne can intuitively surmise artificial intelligence (AI) is today's hot commodity, gaining traction in businesses, academia and government in recent years. Now, there is data -- all in one place -- that documents growth across many indicators, including startups, venture capital, job openings and academic programs. These bellwethers were captured in the AI Index, produced under the auspices of was conceived within Stanford University's Human-Centered AI Institute and the One Hundred Year Study on AI (AI100).








AI in the mainstream
Photo: Joe McKendrick






One key measure of AI development is startups and venture capital funding. From January 2015 to January 2018, active AI startups increased 2.1x, while all  active startups increased 1.3x, the report states. ""For the most part, growth in all active startups has remained relatively steady, while the number of AI startups has seen exponential growth,"" the report's authors add. The trickle of venture capital into AI startups, another bellwether, also turned into a torrent. VC funding for AI startups in the US increased 4.5x from 2013 to 2017. Meanwhile, VC funding for all active startups increased 2.08x.
Another key measure, job openings, accelerated in AI. While machine learning is the largest skill cited as a requirement, deep learning is growing at the fastest rate — from 2015 to 2017 the number of job openings requiring deep learning increased 35x, the report's authors state.
PROMOTED
The AI Index also cited McKinsey data that demonstrated the types of AI solutions being deployed in organizations. In North American organizations, the main forms of AI include the following:

Robotic process automation 23%
Machine learning 23%
Conversational interfaces 20%
Computer vision 20%









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



Natural language text understanding 17%
Natural language speech understanding 16%
Natural language generation 11%

Another interesting bellwether, downloads from AI-oriented open source solutions, is way up. The number of robot operating system (ROS) binary packages downloaded from ROS.org, an open source software stack for robotics.  Since 2014, total downloads and unique downloads have increased by 352% and 567%, respectively. ""This represents an increased interest in both robotics and the use of robot systems,"" the report's authors conclude. ""Because the number of unique downloads is growing at a faster rate than the total number of downloads, we can infer that there are more ROS users, not just that ROS is more frequently used.""
Finally, another telling AI bellwether is AI course enrollment. The percentage of undergraduate students enrolled in introductory AI and machine learning courses has grown. While introductory AI courses tend to have a slightly larger proportion of undergraduate students than introductory machine learning courses (an average of 5.2% in AI versus 4.4% in ML), the number of undergraduate students in introductory machine learning courses are growing at a faster rate.  Introductory AI enrollment was 3.4x larger in 2017 than it was in 2012, while introductory machine learning course enrollment was 5x larger than it was in 2012.  ""This depicts the growing importance of machine learning as a subfield of AI,"" the report states.Joe McKendrickFollowingFollowI am an author, independent researcher and speaker exploring innovation, information technology trends and markets. I served as co-chair of the... Read MoreEditorial StandardsPrintReprints & Permissions",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiSWh0dHBzOi8vd3d3LnB1YmxpY3NlY3RvcmV4ZWN1dGl2ZS5jb20vSW50ZXJ2aWV3cy90aGUtZGV2aWwtaXMtaW4tdGhlLWRhdGHSAQA?oc=5,Artificial intelligence: the devil is in the data - Public Sector Executive,2018-12-17,Public Sector Executive,https://www.publicsectorexecutive.com,N/A,N/A,N/A,N/A,,,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiRGh0dHBzOi8vd3d3LmluZy5jb20vTmV3c3Jvb20vTmV3cy9Vc2luZy1BSS10by1hc3Nlc3MtY3JlZGl0LXJpc2suaHRt0gEA?oc=5,Using AI to assess credit risk | ING - ING.com,2018-12-17,ING.com,https://www.ing.com,17 December 2018,"Financial Services, ING",ING teams up with Google and PWC in the area of artificial intelligence.,ING teams up with Google and PWC in the area of artificial intelligence.,https://schema.org,Organization,https://www.ing.com,,,,,,,,ING,,,N/A,N/A,"


Using AI to assess credit risk



17 December 2018
 1 min read



 17 December 2018 
Next to blockchain, artificial intelligence (AI) is the cool kid on the technology block. The financial services industry especially is reaping the benefits of AI with ING no stranger to the game.
 Fresh from bringing in chatbots to process customer requests faster than ever, using predictive analytics in bond trading, and working with Google on digital voice assistants, ING is putting artificial intelligence to work in Risk Management. We’ve teamed up with Google and PwC to develop an early warning system (EWS) that helps credit risk analysts make quicker and more informed decisions. 


Anand Autar, ING’s early warning system project leader.

 The EWS is an AI-powered application that collects and analyses large amounts of data to identify whether clients are exposed to potential risks, a task currently performed manually by risk analysts. “Speed is of the essence in credit risk management. The earlier we detect any risk, the quicker and better we can serve clients to prevent losses,” said Anand Autar, project leader at ING.  “Through machine learning, the EWS scans financial and non-financial information, such as news items from all over the world.”Credit risk analysts set their own warning criteria. For example, a client’s share price falls by more than a pre-set percentage, or a client’s media coverage is negative based on sentiment analysis. Processing up to 80,000 articles every day, the EWS is ‘fed’ real-time market data from Refinitiv  (former Thomson Reuters) and news from public sources. It uses Google’s natural language processing and translation services for articles published in local media outlets.“The system learns from experience, so in time it will become better at identifying the sentiment of news and developments in the market,” said Görkem Köseoğlu, head of AI and Robotics at ING.ING aims to add predictive capabilities to the application in the near future. “This ambition requires further refinement of algorithms, and we’ll get there,” said Görkem. “Customers expect more predictive capabilities in their products and services, so for us meeting that customer demand is important.”Read what PwC says about the collaboration.





Share


Twitter
LinkedIn
E-mail
Whatsapp








","['https://www.facebook.com/ing', 'https://www.twitter.com/ING_news', 'https://www.youtube.com/ING', 'https://www.linkedin.com/company/ing', 'https://www.slideshare.net/ING/', 'https://www.flickr.com/photos/inggroup/']",,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMicWh0dHBzOi8vbmV3cy5taWNyb3NvZnQuY29tL2FwYWMvZmVhdHVyZXMvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtYW5kLXJlbmV3YWJsZXMtYS1wZWVrLWludG8tdGhlLWZ1dHVyZS1vZi1lbmVyZ3kv0gEA?oc=5,Artificial intelligence and renewables: A peek into the future of energy - Microsoft Stories Asia - Microsoft,2018-12-20,Microsoft,https://news.microsoft.com,N/A,N/A,N/A,N/A,https://schema.org,,,,,,,,,,,,,N/A,N/A,N/A,,"[{'@type': 'WebPage', '@id': 'https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/', 'url': 'https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/', 'name': 'Artificial intelligence and renewables: A peek into the future of energy - Microsoft Stories Asia', 'isPartOf': {'@id': 'https://news.microsoft.com/apac/#website'}, 'primaryImageOfPage': {'@id': 'https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/#primaryimage'}, 'image': {'@id': 'https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/#primaryimage'}, 'thumbnailUrl': 'https://news.microsoft.com/wp-content/uploads/prod/sites/43/2018/12/envision-header.jpg', 'datePublished': '2018-12-20T10:25:02+00:00', 'dateModified': '2022-11-30T22:26:28+00:00', 'breadcrumb': {'@id': 'https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/#primaryimage', 'url': 'https://news.microsoft.com/wp-content/uploads/prod/sites/43/2018/12/envision-header.jpg', 'contentUrl': 'https://news.microsoft.com/wp-content/uploads/prod/sites/43/2018/12/envision-header.jpg', 'width': 2557, 'height': 1354}, {'@type': 'BreadcrumbList', '@id': 'https://news.microsoft.com/apac/features/artificial-intelligence-and-renewables-a-peek-into-the-future-of-energy/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://news.microsoft.com/apac/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Features', 'item': 'https://news.microsoft.com/apac/features/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Artificial intelligence and renewables: A peek into the future of energy'}]}, {'@type': 'WebSite', '@id': 'https://news.microsoft.com/apac/#website', 'url': 'https://news.microsoft.com/apac/', 'name': 'Asia News Center', 'description': 'Microsoft Stories Asia', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://news.microsoft.com/apac/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}]",,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vaGJyLm9yZy8yMDE4LzEyLzUtcXVlc3Rpb25zLXdlLXNob3VsZC1iZS1hc2tpbmctYWJvdXQtYXV0b21hdGlvbi1hbmQtam9ic9IBAA?oc=5,5 Questions We Should Be Asking About Automation and Jobs - Harvard Business Review,2018-12-19,Harvard Business Review,https://hbr.org,"We simply don’t know for sure whether automation, algorithms, and AI will ultimately create more jobs than they destroy. But this uncertainty should not blind or distract us from other pressing questions about automation that we’re sure to face regardless of whether automation adds to or subtracts from the total number of jobs. Here are five important, overlooked questions about automation and jobs: Will workers whose jobs are automated be able to transition to new jobs? Who will bear the burden of automation? How will automation affect the supply of labor? How will automation affect wages, and how will wages affect automation? And, how will automation change job searching?",N/A,"We simply don’t know for sure whether automation, algorithms, and AI will ultimately create more jobs than they destroy. But this uncertainty should not blind or distract us from other pressing questions about automation that we’re sure to face regardless of whether automation adds to or subtracts from the total number of jobs. Here are five important, overlooked questions about automation and jobs: Will workers whose jobs are automated be able to transition to new jobs? Who will bear the burden of automation? How will automation affect the supply of labor? How will automation affect wages, and how will wages affect automation? And, how will automation change job searching?",N/A,https://schema.org,WebSite,https://hbr.org/,,,,,,,,,,,Economics,N/A,N/A,,,"{'@type': 'SearchAction', 'target': 'https://hbr.org/search?term={search_term_string}', 'query-input': 'required name=search_term_string'}",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiZmh0dHBzOi8vd3d3LnRoZWFydG5ld3NwYXBlci5jb20vMjAxOC8xMi8xNy93ZS1tdXN0LW5vdC1sZXQtdGhlLWFydC1tYXJrZXQtaG9vZHdpbmstdXMtaW4tdGhlLWFpLWRlYmF0ZdIBAA?oc=5,We must not let the art market hoodwink us in the AI debate - Art Newspaper,2018-12-17,Art Newspaper,https://www.theartnewspaper.com,"The AI work that was sold at Christie's is profound in its conservatism, but others reflect how the technology can impact on art in fascinating ways","['Auction houses', ""Christie's"", 'Technology', 'Artificial intelligence']","The AI work that was sold at Christie's is profound in its conservatism, but others reflect how the technology can impact on art in fascinating ways","The AI work that was sold at Christie's is profound in its conservatism, but others reflect how the technology can impact on art in fascinating ways",http://schema.org,NewsArticle,https://www.theartnewspaper.com/2018/12/17/we-must-not-let-the-art-market-hoodwink-us-in-the-ai-debate,"https://cdn.sanity.io/images/cxgd3urn/production/18b6ad6743c082ec359cbb05f914dc819ec32fde-910x476.jpg?rect=59,0,793,476&w=1200&h=720&q=85&fit=crop&auto=format","[{'@type': 'Person', 'name': 'Ben Luke', 'url': 'https://www.theartnewspaper.com/authors/ben-luke'}]","{'@type': 'Organization', 'name': 'The Art Newspaper - International art news and events', 'logo': {'@type': 'ImageObject', 'url': 'https://cdn.sanity.io/images/cxgd3urn/production/3241e32fac3321c3bdffacdbe1fabc51852fe343-828x315.jpg?rect=152,0,525,315&w=1200&h=720&q=85&fit=crop&auto=format'}}",We must not let the art market hoodwink us in the AI debate,2018-12-17T10:10:14.937Z,,comment,,,,N/A,N/A,N/A,,,,"https://cdn.sanity.io/images/cxgd3urn/production/18b6ad6743c082ec359cbb05f914dc819ec32fde-910x476.jpg?rect=59,0,793,476&w=1200&h=720&q=85&fit=crop&auto=format","[{'@type': 'Person', 'name': 'Ben Luke', 'url': 'https://www.theartnewspaper.com/authors/ben-luke'}]",,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMidmh0dHBzOi8vd3d3LnByaW5jZXRvbi5lZHUvbmV3cy8yMDE4LzEyLzE4L2dvb2dsZS1vcGVuLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWxhYi1wcmluY2V0b24tYW5kLWNvbGxhYm9yYXRlLXVuaXZlcnNpdHnSAQA?oc=5,Google to open artificial intelligence lab in Princeton and collaborate with University researchers - Princeton University,2018-12-18,Princeton University,https://www.princeton.edu,Princeton University computer science professors Elad Hazan and Yoram Singer will lead a new Google artificial intelligence lab opening in January in the town of Princeton. The lab is expected to expand New Jersey’s burgeoning innovation ecosystem by building a collaborative effort to advance research in artificial intelligence.,N/A,Princeton University computer science professors Elad Hazan and Yoram Singer will lead a new Google artificial intelligence lab opening in January in the town of Princeton. The lab is expected to expand New Jersey’s burgeoning innovation ecosystem by building a collaborative effort to advance research in artificial intelligence.,Princeton University computer science professors Elad Hazan and Yoram Singer will lead a new Google artificial intelligence lab opening in January in the town of Princeton. The lab is expected to expand New Jersey’s burgeoning innovation ecosystem by building a collaborative effort to advance research in artificial intelligence.,,,,,,,,,,,,,,N/A,N/A,"






Google to open artificial intelligence lab in Princeton and collaborate with University researchers





Share on Facebook
Share on Twitter
Share on LinkedIn
Email
Print


By 

Steven Schultz, Office of Engineering Communications

    on 
            Dec. 18, 2018, 3:04 p.m.
       

Google will open an artificial intelligence laboratory in January at 1 Palmer Square in the town of Princeton. This is a view of the main entrance to the lab, which will be led by Princeton computer science professors Elad Hazan and Yoram Singer.Photo byDenise Applewhite, Office of Communications



Two Princeton University computer science(link is external) professors will lead a new Google AI lab opening in January in the town of Princeton. The lab is expected to expand New Jersey’s burgeoning innovation ecosystem by building a collaborative effort to advance research in artificial intelligence.
The lab, at 1 Palmer Square, will start with a small number of faculty members, graduate and undergraduate student researchers, recent graduates and software engineers. The lab builds on several years of close collaboration between Google and professors Elad Hazan and Yoram Singer, who will split their time working for Google and Princeton.
The work in the lab will focus on a discipline within artificial intelligence known as machine learning(link is external), in which computers learn from existing information and develop the ability to draw conclusions and make decisions in new situations that were not in the original data. Examples include speech recognition systems that transcribe a wide spectrum of voices, and self-driving cars that process complex visual cues. In particular, the work will build on recent advances by Hazan, Singer and colleagues in optimization methods for machine learning to improve their speed and accuracy while reducing the required computing power.
“We feel it’s a great opportunity, both for machine learning theorists at Princeton to benefit from exposure to real-world computing problems, and for Google to benefit from long-term, unconstrained academic research that Google may incorporate into future products,” said Singer. 
Hazan said Princeton has longstanding strength in the mathematics and theory behind machine learning, optimization and computing in general. “As academics we try to think about theory for solving problems that are, many times, in the abstract, and it’s very helpful for us to be in touch with real-world problems,” he said.
Inside the Google lab, illustrations showing iconic Princeton campus structures Blair Arch and Whig Hall adorn an interior wall.Photo byDenise Applewhite, Office of Communications
“A primary focus of the group is developing efficient methods for faster training of learning machines,” said Hazan. One of the most popular methods to train deep neural networks, a powerful current approach to machine learning, is an algorithm called AdaGrad, co-developed by Hazan and Singer with their colleague Stanford University professor John Duchi. “The study of efficient mathematical optimization has deep roots in Princeton” said Hazan, “starting from the work of John von Neumann,” who was a visiting faculty member at the University before moving to the neighboring Institute for Advanced Study. 
Von Neumann was also the founder of game theory, which is of great relevance to creating optimization algorithms that cope effectively with various types of noise, or spurious information in data, said Hazan. In the field of mathematical optimization, such robust algorithms are said to attain “no regret guarantees.” 
“Computing started at Princeton more than 80 years ago when alumnus Alan Turing first introduced a theory for how machines could calculate,” said Emily Carter, dean of the School of Engineering and Applied Science. “This collaboration is another excellent example of how fundamental insights in mathematics and theoretical computer science drive new technologies with benefits far beyond the original domain of the work.”
Jennifer Rexford, chair of the Department of Computer Science, said the new venture comes at a time of significant growth in computer science and related areas of data science at Princeton. “The work with Google will complement all three pillars of excellence that make data science at Princeton strong today: a foundation in the theory and math behind computing; collaborations that are accelerating discovery across fields such as genomics, neuroscience, chemistry, psychology and sociology; and leadership, through our Center for Information Technology Policy(link is external), in the broader societal implications of computing such as bias and ethics in AI, privacy and security,” Rexford said.
“It’s an exciting opportunity to work with a leading company while also maintaining the strong academic independence and freedom that is essential to Princeton,” Rexford said. 
The decision to open a lab in Princeton reflects Google’s longstanding openness to collaborating with academic researchers, supporting the open-source community and publishing results in peer-reviewed conferences and journals, said Andrew Pierson, a Google program manager. On a practical level, Google’s enormous computing resources give researchers the ability to run experiments that would otherwise be difficult as they optimize algorithms that deal with millions of variables and perform trillions of calculations, Pierson said.
But a bottom-line motivation for collaborating with Princeton, said Amy McDonald Sandjideh, a technical program manager at Google, is talent. Because the community of artificial intelligence researchers is small, she said, continued progress requires new sources of inspiration and collaboration. 
“We specifically chose a location very close to the University to promote such collaborations,” McDonald Sandjideh said. “Particularly having access to graduate students and even undergrads can provide a lot of inspiration. Sometimes you learn the most from teaching and helping younger people understand what you’ve been working on and that can really push you in new directions. That is a great benefit for Google in working more closely with universities like Princeton that have really excellent minds.”


",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiU2h0dHBzOi8vd3d3LmVmaW5hbmNpYWxjYXJlZXJzLmNvbS9uZXdzLzIwMTgvMTIvYnJldmFuLWhvd2FyZC1haS1rYXJpbS1wYXRyaWNrLWtoaWFy0gEA?oc=5,Brevan Howard takes bigger leap into AI with key appointment - eFinancialCareers,2018-12-17,eFinancialCareers,https://www.efinancialcareers.com,Brevan Howard just promoted one of its own as it seeks to tap into the power of AI.,N/A,Brevan Howard just promoted one of its own as it seeks to tap into the power of AI.,N/A,http://schema.org,NewsArticle,,https://cdn.filestackcontent.com/jbmbWG2TRzaaoQM66rur,"{'@type': 'Person', 'name': 'Beecher Tuttle'}","{'@type': 'Organization', 'name': 'eFinancialCareers', 'logo': 'https://cdn.filestackcontent.com/output=format:png/STeC4XocSHe1udufYKjO'}",Brevan Howard takes bigger leap into AI with key appointment,2018-12-17T17:03:37.000Z,2018-12-17T17:03:37.000Z,News,Brevan Howard takes bigger leap into AI with key appointment,,,dhiefc:analytics,N/A,N/A,,,,,,https://www.efinancialcareers.com/news/2018/12/brevan-howard-ai-karim-patrick-khiar,english,"Over the last two years in particular, quantitative hedge funds have begun integrating artificial intelligence and machine learning as part of their strategies. Two Sigma, Citadel, AQR and Man Group come to mind as some of the more notable early adopters. Now Brevan Howard appears to be making a bigger push.

 The hedge fund giant has promoted Karim-Patrick Khiar to the head of artificial intelligence in New York, where he’ll work on driven systematic investments. Khiar started with Brevan Howard in 2017 as its head data strategist but moved into his new role last month.

 The move doesn’t mean that Brevan Howard is only now incorporating AI and machine learning technologies. Founder Alan Howard https://www.fnlondon.com/articles/alan-howard-backs-artificial-intelligence-data-venture-20180705 back in July. He said at the time that Brevan Howard was a current user of Quant Insight’s service, which “helps [the fund] untangle complex markets and identify what is driving asset prices.” However, the hedge fund has even deeper ties to Quant Insight, /2017/02/quant-insights, a former partner and macro portfolio manager at Brevan Howard. The startup /2017/03/amit-khanna-quant-insight in 2017, but clearly there is no bad blood between the two firms. Even so, with Khiar now in his new role, Brevan Howard appears to be making more of a direct investment in AI itself.

 Khiar has one of the most eclectic backgrounds you’ll find. He spent roughly eight years as an aerospace engineer in Paris before moving into finance, according to LinkedIn. He later ended up at BlackRock as the global head of structuring for fixed income and, eventually, a director of risk and quantitative analysis and research. He began delving into AI as a managing director at Morgan Stanley in 2015. He has both an MBA and a master’s in financial engineering from the University of California, Berkeley Haas School of Management. He has also done master’s-level work in artificial engineering and aerospace engineering.

 Have a confidential story, tip, or comment you’d like to share? Contact: mailto:btuttle@efinancialcareers.com

 Bear with us if you leave a comment at the bottom of this article: all our comments are moderated by actual human beings. Sometimes these humans might be asleep, or away from their desks, so it may take a while for your comment to appear. Eventually it will – unless it’s offensive or libelous (in which case it won’t). ",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifmh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDE4LzEyLzE3LzE4MTQ0MzU2L2FpLWltYWdlLWdlbmVyYXRpb24tZmFrZS1mYWNlcy1wZW9wbGUtbnZpZGlhLWdlbmVyYXRpdmUtYWR2ZXJzYXJpYWwtbmV0d29ya3MtZ2Fuc9IBAA?oc=5,These faces show how far AI image generation has advanced in just four years - The Verge,2018-12-17,The Verge,https://www.theverge.com,Artificial intelligence has become incredibly good at creating fake AI faces. Just look at this latest research from Nvidia as proof. But what problems will these AI fakes cause in the future? Will society be able to trust its eyes?,N/A,Can you spot the fake AI faces? ,N/A,http://schema.org/,NewsArticle,https://www.theverge.com/2018/12/17/18144356/ai-image-generation-fake-faces-people-nvidia-generative-adversarial-networks-gans,"[{'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/3N5ntVSJcBFJqMe4lv-mMjl2UXw=/0x0:920x613/1400x788/filters:focal(460x307:461x308)/cdn.vox-cdn.com/uploads/chorus_asset/file/13631853/ai_face_generation.jpg', 'width': 1400, 'height': 788}, {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/0yXQNjSoawaWwHN-MUcbsW0vYmw=/0x0:920x613/1400x1050/filters:focal(460x307:461x308)/cdn.vox-cdn.com/uploads/chorus_asset/file/13631853/ai_face_generation.jpg', 'width': 1400, 'height': 1050}, {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/cIhhsTmqT0X17R9qpDovc1q9sfI=/0x0:920x613/1400x1400/filters:focal(460x307:461x308)/cdn.vox-cdn.com/uploads/chorus_asset/file/13631853/ai_face_generation.jpg', 'width': 1400, 'height': 1400}]","[{'@type': 'Person', 'name': 'James Vincent', 'url': 'https://www.theverge.com/authors/james-vincent'}]","{'@type': 'Organization', 'name': 'The Verge', 'logo': {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/uploads/chorus_asset/file/24015294/verge_duet_google_news.png', 'width': 250, 'height': 50}}",These faces show how far AI image generation has advanced in just four years,2018-12-17T16:49:06.000Z,2018-12-17T16:49:06.000Z,,,,,N/A,N/A,"Tech/Artificial IntelligenceThese faces show how far AI image generation has advanced in just four yearsThese faces show how far AI image generation has advanced in just four years / Those people on the right aren’t real; they’re the product of machine learningBy  James Vincent, a senior reporter who has covered AI, robotics, and more for eight years at The Verge. Dec 17, 2018, 11:49 AM ESTShare this story0 Comments / 0 NewThe faces on the left were created by AI in 2014; on the right are ones made by AI in 2018.  Image: Goodfellow et al; Karras, Laine, Aila / NvidiaDevelopments in artificial intelligence move at a startling pace — so much so that it’s often difficult to keep track. But one area where progress is as plain as the nose on your AI-generated face is the use of neural networks to create fake images. In brief: we’re getting scarily good at it. In the image above you can see what four years of progress in AI image generation looks like. The crude black-and-white faces on the left are from 2014, published as part of a landmark paper that introduced the AI tool known as the generative adversarial network (GAN). The color faces on the right come from a paper published earlier this month, which uses the same basic method but is clearly a world apart in terms of image quality. These realistic faces are the work of researchers from Nvidia. In their paper, shared publicly last week, they describe modifying the basic GAN architecture to create these images. Take a look at the pictures below. If you didn’t know they were fake, could you tell the difference? Some of Nvidia’s AI-generated faces. Image: Karras, Laine, AilaWhat’s particularly interesting is that these fake faces can also be easily customized. Nvidia’s engineers incorporated a method known as style transfer into their work, in which the characteristics of one image are blended with another. You might recognize the term from various image filters that are popular on apps like Prisma and Facebook in recent years, which can make your selfies look like an impressionist painting or a cubist work of art.Applying style transfer to face generation allowed Nvidia’s researchers to customize faces to an impressive degree. In the grid below, you can see this in action. A source image of a real person (the top row) has the facial characteristics of another person (right-hand column) imposed onto it. Traits like skin and hair color are blended together, creating what looks like to be an entirely new person in the process.Style transfer allows you to blend facial characteristics from different people.  Image: Karras, Laine, AilaOf course, the ability to create realistic AI faces raises troubling questions. (Not least of all, how long until stock photo models go out of work?) Experts have been raising the alarm for the past couple of years about how AI fakery might impact society. These tools could be used for misinformation and propaganda and might erode public trust in pictorial evidence, a trend that could damage the justice system as well as politics. (Sadly, these issues aren’t discussed in Nvidia’s paper, and when we reached out to the company, it said it couldn’t talk about the work until it had been properly peer-reviewed.) These warnings shouldn’t be ignored. As we’ve seen with the use of deepfakes to create non-consensual pornography, there are always people who are willing to use these tools in questionable ways. But at the same time, despite what the doomsayers say, the information apocalypse is not quite nigh. For one, the ability to generate faces has received special attention in the AI community; you can’t doctor any image in any way you like with the same fidelity. There are also serious constraints when it comes to expertise and time. It took Nvidia’s researchers a week training their model on eight Tesla GPUs to create these faces.There are also clues we can look for to spot fakes. In a recent blog post, artist and coder Kyle McDonald listed a number of tells. Hair, for example, is very difficult to fake. It often looks too regular, like it’s been painted on with a brush, or too blurry, blending into someone’s face. Similarly, AI generators don’t quite understand human facial symmetry. They often place ears at different levels or make eyes different colors. They’re also not very good at generating text or numbers, which just come out as illegible blobs. Some examples of AI-generated faces with obvious asymmetrical features.  Image by Kyle McDonaldIf you read the beginning of this post, though, these hints probably aren’t a huge consolation. After all, Nvidia’s work shows just how fast AI in this domain is progressing, and it won’t be long until researchers create algorithms that can avoid these tells. Thankfully, experts are already thinking about new ways to authenticate digital pictures. Some solutions have already been launched, like camera apps that stamp pictures with geocodes to verify when and where they were taken, for example. Clearly, there is going to be a running battle between AI fakery and image authentication for decades to come. And at the moment, AI is charging decisively into the lead. Comments0 Comments / 0 NewFeatured Videos From The VergeSamsung Galaxy Watch Ultra: ring any bells?
1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGEThe $649.99 Samsung Galaxy Watch Ultra doesn’t hide where it got its inspiration from, but it is the first to have FDA-cleared sleep apnea detection.Most PopularMost PopularIt’s never been easier for the cops to break into your phoneThe FBI says it has ‘gained access’ to the Trump rally shooter’s phoneGoogle is reportedly planning its biggest startup acquisition everThe Google Pixel 9 just leaked againAmazon’s press-to-order Dash buttons are officially discontinuedVerge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly.Email (required)Sign upBy submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.From our sponsorAdvertiser Content From",,,,https://cdn.vox-cdn.com/thumbor/3N5ntVSJcBFJqMe4lv-mMjl2UXw=/0x0:920x613/1400x788/filters:focal(460x307:461x308)/cdn.vox-cdn.com/uploads/chorus_asset/file/13631853/ai_face_generation.jpg,,,,,"Developments in artificial intelligence move at a startling pace — so much so that it’s often difficult to keep track. But one area where progress is as plain as the nose on your AI-generated face is the use of neural networks to create fake images. In brief: we’re getting scarily good at it. 

In the image above you can see what four years of progress in AI image generation looks like. The crude black-and-white faces on the left are from 2014, published as part of a landmark paper that introduced the AI tool known as the generative adversarial network (GAN). The color faces on the right come from a paper published earlier this month, which uses the same basic method but is clearly a world apart in terms of image quality. 

These realistic faces are the work of researchers from Nvidia. In their paper, shared publicly last week, they describe modifying the basic GAN architecture to create these images. Take a look at the pictures below. If you didn’t know they were fake, could you tell the difference? 

[Image: Some of Nvidia’s AI-generated faces. https://cdn.vox-cdn.com/thumbor/ZoxdDK4Es5JoiZ4aItSeGOB6F6Q=/0x0:1114x556/1114x556/filters:focal(557x278:558x279)/cdn.vox-cdn.com/uploads/chorus_asset/file/13631823/Screen_Shot_2018_12_17_at_3.25.35_PM.png]

What’s particularly interesting is that these fake faces can also be easily customized. Nvidia’s engineers incorporated a method known as style transfer into their work, in which the characteristics of one image are blended with another. You might recognize the term from various image filters that are popular on apps like Prisma and Facebook in recent years, which can make your selfies look like an impressionist painting or a cubist work of art.

Applying style transfer to face generation allowed Nvidia’s researchers to customize faces to an impressive degree. In the grid below, you can see this in action. A source image of a real person (the top row) has the facial characteristics of another person (right-hand column) imposed onto it. Traits like skin and hair color are blended together, creating what looks like to be an entirely new person in the process.

[Image: Style transfer allows you to blend facial characteristics from different people. https://cdn.vox-cdn.com/thumbor/NsvqcRRUgul0nycoQ_zOeUaC7NA=/0x0:1011x650/1011x650/filters:focal(506x325:507x326)/cdn.vox-cdn.com/uploads/chorus_asset/file/13631832/Screen_Shot_2018_12_17_at_3.32.40_PM.png]

Of course, the ability to create realistic AI faces raises troubling questions. (Not least of all, how long until stock photo models go out of work?) Experts have been raising the alarm for the past couple of years about how AI fakery might impact society. These tools could be used for misinformation and propaganda and might erode public trust in pictorial evidence, a trend that could damage the justice system as well as politics. (Sadly, these issues aren’t discussed in Nvidia’s paper, and when we reached out to the company, it said it couldn’t talk about the work until it had been properly peer-reviewed.) 

These warnings shouldn’t be ignored. As we’ve seen with the use of deepfakes to create non-consensual pornography, there are always people who are willing to use these tools in questionable ways. But at the same time, despite what the doomsayers say, the information apocalypse is not quite nigh. For one, the ability to generate faces has received special attention in the AI community; you can’t doctor any image in any way you like with the same fidelity. There are also serious constraints when it comes to expertise and time. It took Nvidia’s researchers a week training their model on eight Tesla GPUs to create these faces.

There are also clues we can look for to spot fakes. In a recent blog post, artist and coder Kyle McDonald listed a number of tells. Hair, for example, is very difficult to fake. It often looks too regular, like it’s been painted on with a brush, or too blurry, blending into someone’s face. Similarly, AI generators don’t quite understand human facial symmetry. They often place ears at different levels or make eyes different colors. They’re also not very good at generating text or numbers, which just come out as illegible blobs. 

[Image: Some examples of AI-generated faces with obvious asymmetrical features. https://cdn.vox-cdn.com/thumbor/u3o27PLhma47CewKZp8NoFt1Q50=/0x0:800x266/800x266/filters:focal(400x133:401x134)/cdn.vox-cdn.com/uploads/chorus_asset/file/13631847/1_fQEPjVkihxmK4iRm8dyVcA.png]

If you read the beginning of this post, though, these hints probably aren’t a huge consolation. After all, Nvidia’s work shows just how fast AI in this domain is progressing, and it won’t be long until researchers create algorithms that can avoid these tells. 

Thankfully, experts are already thinking about new ways to authenticate digital pictures. Some solutions have already been launched, like camera apps that stamp pictures with geocodes to verify when and where they were taken, for example. Clearly, there is going to be a running battle between AI fakery and image authentication for decades to come. And at the moment, AI is charging decisively into the lead. 
",,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiSGh0dHBzOi8vdGhlZ2xvYmVwb3N0LmNvbS8yMDE4LzEyLzE4L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWh1bWFuLWxhYm9yL9IBTGh0dHBzOi8vdGhlZ2xvYmVwb3N0LmNvbS8yMDE4LzEyLzE4L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWh1bWFuLWxhYm9yL2FtcC8?oc=5,Germany’s Artificial Intelligence Initiative: Last Hurrah or New Hope? - The Globe Post,2018-12-18,The Globe Post,https://theglobepost.com,Germany’s investment in artificial intelligence is step in the right direction but redistributing resources is needed as AI could eliminate human labor.,N/A,Germany’s investment in artificial intelligence is step in the right direction but redistributing resources is needed as AI could eliminate human labor.,N/A,http://schema.org,BreadcrumbList,https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/,"{'@type': 'ImageObject', 'url': 'https://theglobepost.com/wp-content/uploads/2018/12/Merkel-AI.jpg', 'width': 940, 'height': 529}","{'@type': 'Person', 'name': 'Arend Hintze', 'url': 'https://theglobepost.com/author/arendhintze/'}","{'@type': 'Organization', 'name': 'The Globe Post', 'url': 'https://theglobepost.com', 'logo': {'@type': 'ImageObject', 'url': 'https://theglobepost.com/wp-content/uploads/2018/12/globe-post-logo-1.png'}, 'sameAs': ['https://www.facebook.com/tglobepost/', 'https://twitter.com/tglobepost']}",Germany&#8217;s Artificial Intelligence Initiative: Last Hurrah or New Hope?,2018-12-18 05:05:18,2018-12-18 23:51:35,['Opinion'],Germany&#8217;s Artificial Intelligence Initiative: Last Hurrah or New Hope?,,"[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://theglobepost.com', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://theglobepost.com/category/opinion/', 'name': 'Opinion'}}, {'@type': 'ListItem', 'position': 3, 'item': {'@id': 'https://theglobepost.com', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 4, 'item': {'@id': 'https://theglobepost.com/category/opinion/', 'name': 'Opinion'}}]",N/A,N/A,"







World



AI Images of White Faces Are Now ‘Hyper-Real’: Study

by Staff Writer November 13, 2023


","['https://www.facebook.com/tglobepost/', 'https://twitter.com/tglobepost']","[{'@type': 'Article', '@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/#article', 'isPartOf': {'@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/'}, 'author': {'name': 'Arend Hintze', '@id': 'https://theglobepost.com/#/schema/person/dd1a433d9f2bcab8926b43586304fd14'}, 'headline': 'Germany&#8217;s Artificial Intelligence Initiative: Last Hurrah or New Hope?', 'datePublished': '2018-12-18T10:05:18+00:00', 'dateModified': '2018-12-18T23:51:35+00:00', 'mainEntityOfPage': {'@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/'}, 'wordCount': 1151, 'publisher': {'@id': 'https://theglobepost.com/#organization'}, 'image': {'@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/#primaryimage'}, 'thumbnailUrl': 'https://theglobepost.com/wp-content/uploads/2018/12/Merkel-AI.jpg', 'keywords': ['Angela Merkel', 'artificial intelligence', 'Germany'], 'articleSection': ['Opinion'], 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/', 'url': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/', 'name': ""Germany's Artificial Intelligence Initiative: Last Hurrah or New Hope?"", 'isPartOf': {'@id': 'https://theglobepost.com/#website'}, 'primaryImageOfPage': {'@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/#primaryimage'}, 'image': {'@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/#primaryimage'}, 'thumbnailUrl': 'https://theglobepost.com/wp-content/uploads/2018/12/Merkel-AI.jpg', 'datePublished': '2018-12-18T10:05:18+00:00', 'dateModified': '2018-12-18T23:51:35+00:00', 'description': 'Germany’s investment in artificial intelligence is step in the right direction but redistributing resources is needed as AI could eliminate human labor.', 'breadcrumb': {'@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/#primaryimage', 'url': 'https://theglobepost.com/wp-content/uploads/2018/12/Merkel-AI.jpg', 'contentUrl': 'https://theglobepost.com/wp-content/uploads/2018/12/Merkel-AI.jpg', 'width': 940, 'height': 529, 'caption': 'German Chancellor Angela Merkel looking at a robot during a trade fair for industrial technology. Photo: Tobias Schwarz, AFP'}, {'@type': 'BreadcrumbList', '@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://theglobepost.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Germany&#8217;s Artificial Intelligence Initiative: Last Hurrah or New Hope?'}]}, {'@type': 'WebSite', '@id': 'https://theglobepost.com/#website', 'url': 'https://theglobepost.com/', 'name': 'The Globe Post', 'description': 'Daily and breaking news, analysis, opinion, and features from around the world.', 'publisher': {'@id': 'https://theglobepost.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://theglobepost.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://theglobepost.com/#organization', 'name': 'The Globe Post', 'url': 'https://theglobepost.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://theglobepost.com/#/schema/logo/image/', 'url': 'https://theglobepost.com/wp-content/uploads/2017/07/logo-the-globe-post-mobile.png', 'contentUrl': 'https://theglobepost.com/wp-content/uploads/2017/07/logo-the-globe-post-mobile.png', 'width': 140, 'height': 48, 'caption': 'The Globe Post'}, 'image': {'@id': 'https://theglobepost.com/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/tglobepost/', 'https://twitter.com/tglobepost', 'https://www.instagram.com/tglobepost/', 'https://www.youtube.com/channel/UC6fmebWgQEQ_0B6INdyKoMA']}, {'@type': 'Person', '@id': 'https://theglobepost.com/#/schema/person/dd1a433d9f2bcab8926b43586304fd14', 'name': 'Arend Hintze', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://theglobepost.com/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/63ebde423e83d823aa3724d31419bcbc?s=96&d=blank&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/63ebde423e83d823aa3724d31419bcbc?s=96&d=blank&r=g', 'caption': 'Arend Hintze'}, 'description': 'Assistant Professor for Integrative Biology &amp; Computer science and Engineering, Michigan State University', 'url': 'https://theglobepost.com/author/arendhintze/'}]","{'@type': 'SearchAction', 'target': 'https://theglobepost.com/?s={search_term_string}', 'query-input': 'required name=search_term_string'}",,,,,,"<p style=""font-weight: 400;"">Germany's government is in trouble. The nationalist AfD party united <a href=""https://www.theguardian.com/world/ng-interactive/2017/sep/24/german-elections-2017-latest-results-live-merkel-bundestag-afd"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://www.theguardian.com/world/ng-interactive/2017/sep/24/german-elections-2017-latest-results-live-merkel-bundestag-afd&amp;source=gmail&amp;ust=1545091781622000&amp;usg=AFQjCNGnVoTLT_Lk8s8wmA5z6e4ZHxxqKg"">12.6 percent</a> of the public voters behind them during the <a href=""https://theglobepost.com/2017/09/27/germany-election-merkel-afd/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://theglobepost.com/2017/09/27/germany-election-merkel-afd/&amp;source=gmail&amp;ust=1545091781622000&amp;usg=AFQjCNHP8xsjc06_TrbQVQe9AkVXwEDPjA"">2017 elections</a>, for a simple reason: Chancellor <strong>Angela Merkel</strong> took the immigration crisis head-on, allowing <a href=""https://www.vox.com/world/2018/6/18/17474908/germany-immigration-migration-angela-merkel-donald-trump"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://www.vox.com/world/2018/6/18/17474908/germany-immigration-migration-angela-merkel-donald-trump&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNF5Inatc9GzoWG8PRI97OkeNCzpLA"">about 1.4 million refugees</a> to migrate to Germany. This made many people afraid of losing their jobs. The <a href=""https://theglobepost.com/2017/12/02/german-far-right-new-leaders/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://theglobepost.com/2017/12/02/german-far-right-new-leaders/&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNGXcUJOAkqyGDYk_gvWNxrM75T8QA"">right-wing demagogues</a> jumped on that opportunity and aggravated the discontent of those who have little income, little education, and low support of the government by pointing out that every German euro that goes towards immigrants could have gone to them instead.</p>
<p style=""font-weight: 400;"">People see their financial existence and jobs at peril because of others who have less. But it is not those who have less that get the jobs. It is automation and <a href=""https://theglobepost.com/2018/10/05/ai-racial-bias/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://theglobepost.com/2018/10/05/ai-racial-bias/&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNGPd1xRFtnOQtAi9Zv-MXlvYCNVVg"">artificial intelligence</a> (AI) that remove the necessity of human labor.</p>
During the <a href=""https://www.history.com/topics/industrial-revolution"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://www.history.com/topics/industrial-revolution&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNHnEA0eKqTEt8GvTIVLA_62m0N_gQ"">industrial revolution</a>, many people who did physical labor lost their jobs to machines, and the job market pushed workers to more cognitive jobs and offices. Instead of muscles and skills, the one thing that machines couldn't do - thinking - became in demand. But with the advent of AI, this last niche for employment will be in jeopardy soon.
<h2><strong>Human Labor and Artificial Intelligence</strong></h2>
Until now, production costs depended on resources, energy, cost of human labor, and the cost of development. We cannot remove energy and resources from this equation, but with AI and <a href=""https://www.zdnet.com/article/what-is-artificial-general-intelligence/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://www.zdnet.com/article/what-is-artificial-general-intelligence/&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNH6GSPrapoNz2wxiMZ9BEsfeM2qCw"">artificial general intelligence</a> – machines that have the same cognitive abilities as human beings – human labor can be eliminated.

https://www.youtube.com/watch?v=zjeBGkS4LAA

There is, however, one big problem with this approach: the consumers of the products that are created by these new machines will no longer have jobs or an income that allows them to purchase these products. So, how do we prevent this perfect storm of first making everyone unemployed and then consequently crashing the economy altogether?
<p style=""font-weight: 400;"">Avoiding this future by banning AI technology, for example, would only work if everyone is going along. We know how easy it is for politicians to agree on something as obvious as <a href=""https://theglobepost.com/2018/06/27/climate-change-global-warming-pollution/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://theglobepost.com/2018/06/27/climate-change-global-warming-pollution/&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNGaq6aHf6NSQ4RN1JzLC8r54Y3jHw"">climate change</a>, right? So, the answer is that instead of preventing that future, we should embrace it.</p>


[caption id=""attachment_16176"" align=""alignright"" width=""350""]<a href=""https://theglobepost.com/wp-content/uploads/2018/10/AfD-protest-Germany.jpg""><img class=""wp-image-16176"" src=""https://theglobepost.com/wp-content/uploads/2018/10/AfD-protest-Germany.jpg"" alt=""Supporters of Germany's AfD show German flags and display posters saying 'Merkel must go'"" width=""350"" height=""197"" /></a> AfD-supporters in Berlin. Photo: AFP[/caption]
<p style=""font-weight: 400;"">Ironically, it is the socialist idea of an <a href=""https://basicincome.org/basic-income/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://basicincome.org/basic-income/&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNEs910GInyXlafVRnW3HkuvzfTZvA"">unconditional basic income</a> that might solve this self-inflicted capitalist nightmare. Distribute resources equally and let the fully automatized free market deal with the question about what needs to be produced. This is exactly what those <a href=""https://theglobepost.com/2018/02/19/germany-far-right/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://theglobepost.com/2018/02/19/germany-far-right/&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNHgyglypsVG-bVXb0lq9qhS-v3rkg"">German nationalist voters</a> demanded in the first place: “We want to make a living, and we fear for competition.”</p>
Let me address another elephant in the room. We keep hearing that competition and a monetary incentive drive innovation, and therefore, we cannot get rid of money or capitalism. I work on artificial general intelligence not because I get a stellar salary, but because I am an idealist. Redistributing wealth will not get rid of idealists, but allows more people to become one. Outsourcing thinking to the machines also involved outsourcing creativity and innovation. You see, I don't need money to incentivize AI to innovate, I just need AI.
<h2><strong>Germany’s AI Strategy</strong></h2>
I think the German government fears that the country will be outcompeted by others who have an advantage in AI research, but I don't think they are aware of the other socio-economic implications that I made above.

Regardless, the government pledged a <a href=""https://theglobepost.com/2018/12/11/germany-artificial-intelligence/"" target=""_blank"" rel=""noopener"" data-saferedirecturl=""https://www.google.com/url?q=https://theglobepost.com/2018/12/11/germany-artificial-intelligence/&amp;source=gmail&amp;ust=1545091781623000&amp;usg=AFQjCNHV8w3iGSt0N171wp1XWN0U8f1Aug"">€3 billion ($3.4 billion) investment in artificial intelligence</a> to employ more professors and professionals in the domain of AI research.

[caption id=""attachment_17260"" align=""aligncenter"" width=""768""]<a href=""https://theglobepost.com/wp-content/uploads/2018/12/Merkel-Digital-Gipfel.jpg""><img class=""size-full wp-image-17260"" src=""https://theglobepost.com/wp-content/uploads/2018/12/Merkel-Digital-Gipfel.jpg"" alt=""German Chancellor Angela Merkel speaking about artificial intelligence at the Digital Summit in Nuremberg"" width=""768"" height=""481"" /></a> German Chancellor Angela Merkel speaking at the Digital Summit in Nuremberg. Photo: Chrisof Stache, AFP[/caption]
<p style=""font-weight: 400;"">Current job offers seek people who know how to do machine learning (systems that can improve from experience without being explicitly programmed) and AI in general, and because of their competitive salaries attract everyone who knows how to apply these technologies and who doesn’t think that writing grants, publications, and teaching are the most rewarding ways of spending time. As a result, it becomes harder and harder to find professors and postdocs, our academic youth.</p>
University job offers, on the other hand, are often overly focused on finding researchers in deep learning specifically instead of artificial intelligence in general. Current AI technology is dominated by deep learning: a technology that trains complex <a href=""https://www.digitaltrends.com/cool-tech/what-is-an-artificial-neural-network/"" target=""_blank"" rel=""noopener"">artificial neural networks (ANNs)</a> to recognize situations and act accordingly. For example, you keep showing the ANN images of cats and dogs and positively or negatively reinforce the network, until it can tell the difference.

[caption id=""attachment_17509"" align=""alignleft"" width=""250""]<a href=""https://theglobepost.com/wp-content/uploads/2018/12/Siri-e1545015382815.jpg""><img class=""wp-image-17509"" src=""https://theglobepost.com/wp-content/uploads/2018/12/Siri-e1545015382815.jpg"" alt=""Mobile phone showing Siri"" width=""250"" height=""282"" /></a> Photo: AFP[/caption]
<p style=""font-weight: 400;"">ANNs were invented in 1958, but with the extreme computing power we now have and new mathematical and computational tricks, training these ANNs becomes tractable. Remember, the hype we experience from products like Cortana, Google Assistant, and Siri is not coming from great academic advancements in the field, but from the <a href=""http://fortune.com/ai-artificial-intelligence-deep-machine-learning/"" target=""_blank"" rel=""noopener"">ability to translate it into products due to better computational power</a>. The next breakthrough or paradigm shift will not come from deep learning, but another field within the domain of artificial intelligence research.</p>
<p style=""font-weight: 400;"">I am obviously biased and think that neuro-evolution is the ticket to general purpose AI. Regardless, the best hiring strategy in this situation, given that <a href=""https://www.ft.com/content/fe1f9194-e8e3-11e8-a34c-663b3f553b35"" target=""_blank"" rel=""noopener"">Germany is now providing sufficient resources</a>, is bet hedging. My advice would be to advance research in as many directions as possible, by hiring in the general field of AI and not just in deep learning. It is like buying lottery tickets: the more different numbers you have, the bigger your chances to win.</p>
<p style=""font-weight: 400;"">At the same time, the industry is translating academic advancements into products and desperately seeks people with deep learning expertise. While these technologies become much more accessible and easier to use, it is also implied that the new professors to be hired need to be able to teach deep learning and data science.</p>
<p style=""font-weight: 400;"">Right now, job ads for academic positions focus too narrowly on deep learning, and it is exactly that category of people who are also sought in the industry - a competition that makes hiring professors almost impossible. Academia should lead the innovation, and thus an investment in academia should be for future technology, as long as these academics can teach the skills needed in the industry.</p>
<p style=""font-weight: 400;""><a href=""https://www.dw.com/en/germany-launches-digital-strategy-to-become-artificial-intelligence-leader/a-46298494"" target=""_blank"" rel=""noopener"">Germany’s €3 billion investment</a> is a step in the right direction, and 0.1 percent of total GDP is an excellent starting point. Germany, pump as much money into this as possible and hire as diverse as possible! This is not the race to the moon, it is the race for the artificial mind, and this endeavor has an immediate impact on the economy and will change our way of living in the most profound way imaginable. It will make the need to work for money obsolete and requires us to find a sustainable way to redistribute resources.</p>",https://theglobepost.com/#website,"{'@type': 'ImageObject', 'url': 'https://theglobepost.com/wp-content/uploads/2018/12/globe-post-logo-1.png'}","{'@type': 'ContactPoint', 'telephone': '2027538899', 'contactType': 'customer service', 'areaServed': ['World']}","{'@type': 'WebPage', '@id': 'https://theglobepost.com/2018/12/18/artificial-intelligence-human-labor/'}",2018-12-18 05:05:18,Germany&#8217;s Artificial Intelligence Initiative: Last Hurrah or New Hope?,2018-12-18 05:05:18,2018-12-18 23:51:35,,,,,,,,
https://news.google.com/rss/articles/CBMifmh0dHBzOi8vcHJlc3NyZWxlYXNlcy5yZXNwb25zZXNvdXJjZS5jb20vbmV3cy85NjgyNy9ncmVlbnZhbi1vZmZlcnMtZWFzaWVyLWdyZWVuZXItcmVtb3ZhbHMtd2l0aC1hdXRvbWF0ZWQtaG91ci1ib29raW5nLXVzaW5nL9IBAA?oc=5,"GreenVan offers easier, greener removals with automated 24 hour booking using Artificial Intelligence - ResponseSource",2018-12-17,ResponseSource,https://pressreleases.responsesource.com,"Customers looking to move home - or just a few items - can now book in minutes instead of hours using a very simple chat booking service from GreenVan. Customers get the lowest price promise, free ins...","Health,Home & Garden,Women's Interest & Beauty,Environment & Nature,Consumer Technology,Business & Finance,Public Sector, Third Sector & Legal,Computing & Telecoms,Transport & Logistics,Construction & Property","Customers looking to move home - or just a few items - can now book in minutes instead of hours using a very simple chat booking service from GreenVan. Customers get the lowest price promise, free ins...","Customers looking to move home - or just a few items - can now book in minutes instead of hours using a very simple chat booking service from GreenVan. Customers get the lowest price promise, free ins...",http://schema.org,Article,,"{'@type': 'ImageObject', 'url': 'https://mediaserver.responsesource.com/press-release/115546/GreenVan+Image+17+Dec+18.png', 'thumbnail': {'@type': 'ImageObject', 'url': 'https://mediaserver.responsesource.com/press-release/tb_lrg/115546/GreenVan+Image+17+Dec+18.png'}}","{'@type': 'Organization', 'logo': {'@type': 'ImageObject', 'url': 'https://mediaserver.responsesource.com/newsroom-logo/tb_lrg/115462/greenvan.png'}, 'name': 'GreenVan'}","{'@type': 'Organization', 'name': 'ResponseSource', 'logo': {'@type': 'ImageObject', 'url': 'https://pressreleases.responsesource.com/img/logo/RS-LOGO-600.jpg'}}","GreenVan offers easier, greener removals with automated 24 hour booking using Artificial Intelligence ",2018-12-17T10:40:00+00:00,,,,,,N/A,N/A,"



GreenVan offers easier, greener removals with automated 24 hour booking using Artificial Intelligence
Monday 17 December 2018 PDF Print




















Customers looking to move home - or just a few items - can now book in minutes instead of hours using a very simple chat booking service from GreenVan. Customers get the lowest price promise, free insurance, a 24 hour manager to help them, as well as being able to specify their own pick up times, free changes and cancellations.

The service – with over a thousand satisfied customers already - uses a mixture of AI, algorithms and automation to provide a revolutionary service for the removals industry, saving money and increasing sustainability for both consumers and drivers.

Anuj Gupta founder of Greenvan says: “Our bot and and human team members are available 24 hours to make it easy for our customers – it takes a couple of minutes on www.GreenVan.eu  to book a job and receive an instant confirmation.”

GreenVan’s Driver Partners who provide the removals services also benefit - they can double their disposable income, get a ready made job sheet, and reduce the need for an office, marketing or their own customer support because GreenVan takes care of all this. Drivers have the opportunity to become their own boss by working on days they want and get more family time. 

This magic equation of customers experiencing lower prices on one hand and the mover drivers doubling their disposable income on the other due to increased volumes makes it an exciting self feeding loop. This greater demand for movers creates more jobs in the economy.

The algorithms optimise the routes and keep the vehicles as regionalised as possible reducing the fuel consumption and carbon emission, making the environment much healthier.The company has moved a thousand customers who seem delighted with the service as many have given a 10 on 10 rating.

GreenVan is supported by the Department of International Trade (DIT) UK and is yet another high growth startup tech company which will play a vital role in reshaping the logistics sector and also feed into the government’s goals of a Greener London. 

The founders have received a number of awards including BEST TECHNOLOGY IN THE WORLD BY MICROSOFT and ERNST & YOUNG ENTREPRENEUR OF THE YEAR NEW YORK for previous successful ventures.

The company’s high profitability at scale makes it a unique value proposition for investors. In the future the company also intends to move into other services including electricians and plumbers where the founders have identified a lot of both customer and vendor pain.

www.Greenvan.eu 
Media or investor enquiries can be sent to:
sara@greenvan.eu
 0207 193 5403


Summary of the business
A PLATFORM WHERE THE USER CAN FIND VENDORS OFFERING VARIOUS HOME SERVICES
THE FIRST SERVICE ADDRESSED IS HOME REMOVALS
THE PLATFORM IS A UNIQUE COMBINATION OF AI, AUTOMATION, BLOCKCHAIN & ALGORITHM
THE PLATFORM MAKES IT AFFORDABLE, EASY, RELIABLE FOR CUSTOMERS TO MOVE
DRIVERS DOUBLE THEIR DISPOSABLE INCOME, WITH NO NEED FOR AN OFFICE, MARKETING ETC
THOUSANDS OF NEW JOBS DUE TO GREATER DEMAND
REDUCTION IN FUEL CONSUMPTION MAKES THE ENVIRONMENT MUCH HEALTHIER https://greenvan.eu/green-mission
THE INDUSTRY TODAY HAS EITHER SMALL PLAYERS WHO CANNOT SCALE BECAUSE OF THE LACK OF SUCH A TECHNOLOGY PLATFORM OR VERY LARGE PLAYERS WHO ARE TOO EXPENSIVE FOR THE GENERAL CONSUMER
THE TEAM IS OF EXPERIENCED ENTREPRENEURS - WINNERS OF BEST TECHNOLOGY IN THE WORLD BY MICROSOFT, HOLDERS OF PATENTS, E & Y AWARD, NEW YORK.
TECHNOLOGY ENABLES THE BUSINESS TO SCALE GLOBALLY WITHOUT MORE MAN POWER
THE BUSINESS HOLDS STRONG POTENTIAL TO BE A UNICORN IN THE NEXT COUPLE OF YEARS




This press release was distributed by ResponseSource Press Release Wire on behalf of GreenVan in the following categories:
			                Health, 			                Home & Garden, 			                Women's Interest & Beauty, 			                Environment & Nature, 			                Consumer Technology, 			                Business & Finance, 			                Public Sector, Third Sector & Legal, 			                Computing & Telecoms, 			                Transport & Logistics, 			                Construction & Property, 			 for more information visit https://pressreleasewire.responsesource.com/about.
",,,,,,,en,,,,,,"{'@type': 'WebPage', '@id': 'https://pressreleases.responsesource.com/news/96827/greenvan-offers-easier-greener-removals-with-automated-hour-booking-using/'}",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9pbnNpZGUtdGhlLXBlbnRhZ29ucy1wbGFuLXRvLXdpbi1vdmVyLXNpbGljb24tdmFsbGV5cy1haS1leHBlcnRzL9IBAA?oc=5,"In Project Maven's Wake, the Pentagon Seeks AI Tech Talent - WIRED",2018-12-21,WIRED,https://www.wired.com,"The Defense Department wants to use AI in warfare. In the aftermath of Project Maven, it still needs Big Tech’s help.","['the big story', 'security', 'ai', 'project maven', 'web']","The Defense Department wants to use AI in warfare. In the aftermath of Project Maven, it still needs Big Tech’s help.","The Defense Department wants to use AI in warfare. In the aftermath of Project Maven, it still needs Big Tech’s help.",https://schema.org/,BreadcrumbList,https://www.wired.com/story/inside-the-pentagons-plan-to-win-over-silicon-valleys-ai-experts/,"['https://media.wired.com/photos/5c1c383529ff200b2600066b/16:9/w_992,h_558,c_limit/ai-pentagon-elena-lacey-wired.gif', 'https://media.wired.com/photos/5c1c383529ff200b2600066b/4:3/w_1000,h_750,c_limit/ai-pentagon-elena-lacey-wired.gif', 'https://media.wired.com/photos/5c1c383529ff200b2600066b/1:1/w_750,h_750,c_limit/ai-pentagon-elena-lacey-wired.gif']","[{'@type': 'Person', 'name': 'Zachary Fryer-Biggs', 'sameAs': 'https://www.wired.com/author/zachary-fryer-biggs/'}]","{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}","In Project Maven's Wake, the Pentagon Seeks AI Tech Talent",2018-12-21T07:26:14.934-05:00,2018-12-21T07:26:14.934-05:00,the big story,,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'The Big Story', 'item': 'https://www.wired.com/big-story/'}, {'@type': 'ListItem', 'position': 2, 'name': ""Inside the Pentagon’s Plan to Win Over Silicon Valley's AI Experts""}]",tags,N/A,"Zachary Fryer-BiggsThe Big StoryDec 21, 2018 7:26 AMInside the Pentagon’s Plan to Win Over Silicon Valley's AI ExpertsThe Defense Department wants to use AI in warfare. In the aftermath of Project Maven, it still needs Big Tech’s help.Play/Pause ButtonPauseElena Lacey; Getty ImagesSave this storySaveSave this storySaveThe American military is desperately trying to get a leg up in the field of artificial intelligence, which top officials are convinced will deliver victory in future warfare. But internal Pentagon documents and interviews with senior officials make clear that the Defense Department is reeling from being spurned by a tech giant and struggling to develop a plan that might work in a new sort of battle—for hearts and minds in Silicon Valley.The battle began with an unexpected loss. In June, Google announced it was pulling out of a Pentagon program—the much-discussed Project Maven—that used the tech giant’s artificial intelligence software. Thousands of the company’s employees had signed a petition two months earlier calling for an end to its work on the project, an effort to create algorithms that could help intelligence analysts pick out military targets from video footage.Inside the Pentagon, Google’s withdrawal brought a combination of frustration and distress—even anger—that has percolated ever since, according to five sources familiar with internal discussions on Maven, the military’s first big effort to utilize AI in warfare.About This StoryThis article was produced in partnership with the Center for Public Integrity, a nonprofit, nonpartisan news organization.“We have stumbled unprepared into a contest over the strategic narrative,” said an internal Pentagon memo circulated to roughly 50 defense officials on June 28. The memo depicted a department caught flat-footed and newly at risk of alienating experts critical to the military’s artificial intelligence development plans.Trending NowHow a Son Made a Chatbot of His Dying Dad“We will not compete effectively against our adversaries if we do not win the ‘hearts and minds’ of the key supporters,” it warned.Maven was actually far from complete and cost only about $70 million in 2017, a molecule of water in the Pentagon’s oceanic $600 billion budget that year. But Google’s announcement exemplified a larger public relations and scientific challenge the department is still wrestling with. It has responded so far by trying to create a new public image for its AI work and by seeking a review of the department’s AI policy by an advisory board of top executives from tech companies.The reason for the Pentagon’s anxiety is clear: It wants a smooth path to use artificial intelligence in weaponry of the future, a desire already backed by the promise of several billion dollars to try to ensure such systems are trusted and accepted by military commanders, plus billions more in expenditures on the technologies themselves.AdvertisementThe exact role that AI will wind up playing in warfare remains unclear. Many weapons with AI will not involve decision-making by machine algorithms, but the potential for them to do so will exist. As a Pentagon strategy document said in August: “Technologies underpinning unmanned systems would make it possible to develop and deploy autonomous systems that could independently select and attack targets with lethal force.”Developing artificial intelligence, officials say, is unlike creating other military technologies. While the military can easily turn to big defense contractors for cutting-edge work on fighter jets and bombs, the heart of innovation in AI and machine learning resides among the non-defense tech giants of Silicon Valley. Without their help, officials worry, they could lose an escalating global arms race in which AI will play an increasingly important role, something top officials say they are unwilling to accept.“If you decide not to work on Maven, you’re not actually having a discussion on if artificial intelligence or machine learning are going to be used for military operations,” Chris Lynch, a former tech entrepreneur who now runs the Pentagon’s Defense Digital Service, said in an interview. AI is coming to warfare, he says, so the question is, which American technologists are going to engineer it?Most PopularCultureThe Real Reason Will Smith Broke Twitch’s Biggest Streaming RecordBy Adam BumasGearThe Top New Features Coming to Apple’s iOS 18 and iPadOS 18By Julian ChokkattuPoliticsJ.D. Vance Is Trump’s Pick for Vice PresidentBy Vittoria ElliottSecurityUS Senators Secretly Work to Block Safeguards Against Surveillance AbuseBy Dell CameronLynch, who recruits technical experts to spend several years working on Pentagon problems before returning to the private sector, said that AI technology is too important, and that the agency will proceed even if it has to rely on lesser experts. But without the help of the industry’s best minds, Lynch added, “we’re going to pay somebody who is far less capable to go build a far less capable product that may put young men and women in dangerous positions, and there may be mistakes because of it.”Google isn’t likely to shift gears soon. Less than a week after announcing that the company would not seek to renew the Maven contract in June, Google released a set of AI principles which specified that the company would not use AI for “weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.”Some defense officials have complained since then that Google was being unpatriotic, noting that the company was still pursuing work with the Chinese government, the top US competitor in artificial intelligence technology.“I have a hard time with companies that are working very hard to engage in the market inside of China, and engaging in projects where intellectual property is shared with the Chinese, which is synonymous with sharing it with the Chinese military, and then don't want to work for the US military,” General Joe Dunford, chairman of the Joint Chiefs of Staff, commented while speaking at a conference in November.In December testimony before congress, Google CEO Sundar Pichai acknowledged that Google had experimented with a program involving China, Project Dragonfly, aimed at developing a model of what government-censored search results would look like in China. However, Pichai testified that Google currently “has no plans to launch in China.”Project Maven’s aim was to simplify work for intelligence analysts by tagging object types in video footage from drones and other platforms, helping analysts gather information and narrow their focus on potential targets, according to sources familiar with the partly classified program. But the algorithms did not select the targets or order strikes, a longtime fear of those worried about the intersection of advanced computing and new forms of lethal violence.Many at Google nonetheless saw the program in alarming terms.“They immediately heard drones and then they thought machine learning and automatic target recognition, and I think it escalated for them pretty quickly about enabling targeted killing, enabling targeted warfare,” said a former Google employee familiar with the internal discussions.Google is just one of the tech giants that the Pentagon has sought to enlist in its effort to inject AI into modern warfare technology. Among the others: Microsoft and Amazon. After Google’s announcement in June more than a dozen large defense firms approached defense officials, offering to take over the work, according to current and former Pentagon officials.But Silicon Valley activists also say the industry cannot easily ignore the ethical qualms of tech workers. “There’s a division between those who answer to shareholders, who want to get access to Defense Department contracts worth multimillions of dollars, and the rank and file who have to build the things and who feel morally complicit for things they don’t agree with,” the former Google employee said.Most PopularCultureThe Real Reason Will Smith Broke Twitch’s Biggest Streaming RecordBy Adam BumasGearThe Top New Features Coming to Apple’s iOS 18 and iPadOS 18By Julian ChokkattuPoliticsJ.D. Vance Is Trump’s Pick for Vice PresidentBy Vittoria ElliottSecurityUS Senators Secretly Work to Block Safeguards Against Surveillance AbuseBy Dell CameronIn an effort to bridge this gulf and dampen hard-edged opposition from AI engineers, the Defense Department has so far undertaken two initiatives.The first, formally begun in late June, was to create a Joint Artificial Intelligence Center meant to oversee and manage all of the military’s AI efforts, with an initial focus on PR-friendly humanitarian missions. It’s set to be run by Lieutenant General Jack Shanahan, whose last major assignment was running Project Maven. In a politically shrewd decision, its first major initiative is to figure out a way to use AI to help organize the military’s search and rescue response to natural disasters.“Our goal is to save lives,” Brendan McCord, one of the chief architects of the Pentagon’s AI strategy, said while speaking at a technical conference in October. “Our military’s fundamental role, its mission, is to keep the peace. It is to deter war and protect our country. It is to improve global stability, and it’s to ultimately protect the set of values that came out of the Enlightenment.”The second initiative is to order a new review of AI ethics by an advisory panel of tech experts, the Defense Innovation Board, which includes former Google CEO Eric Schmidt and LinkedIn cofounder Reid Hoffman.That review, designed to develop principles for the use of AI by the military, is being managed by Joshua Marcuse, a former adviser to the secretary of defense on innovation issues who is now executive director of the board. Set to take about nine months, the advisory panel will hold public meetings with AI experts, while an internal Pentagon group also considers questions. Then it will forward recommendations to secretary of defense James Mattis about the ways that AI should or should not be injected into weapons programs.“This has got to be about actually looking in the mirror and being willing to impose some constraints on what we will do, on what we won’t do, knowing what the boundaries are,” Marcuse said in an interview.To make sure the debate is robust, Marcuse said that the board is seeking out critics of the military’s role in AI.“They have a set of concerns, I think really valid and legitimate concerns, about how the Department of Defense is going to apply these technologies, because we have legal authority to invade people’s privacy in certain circumstances, we have legal authority to commit violence, we have legal authority to wage war,” he said.Resolving those concerns is critical, officials say, because of the difference in how Washington and Beijing manage AI talent. China can conscript experts to work on military problems, whereas the United States has to find a way to interest and attract outside experts.“They have to choose to work with us, so we need to offer them a meaningful, verifiable commitment that there are real opportunities to work with us where they can feel confident that they’re the good guys,” Marcuse said.Despite his willingness to discuss potential future constraints on AI usage, Marcuse said he didn’t think the board would try to change the Pentagon’s existing policy on autonomous weapons that depend on AI, which was put in place by the Obama administration in 2012.Most PopularCultureThe Real Reason Will Smith Broke Twitch’s Biggest Streaming RecordBy Adam BumasGearThe Top New Features Coming to Apple’s iOS 18 and iPadOS 18By Julian ChokkattuPoliticsJ.D. Vance Is Trump’s Pick for Vice PresidentBy Vittoria ElliottSecurityUS Senators Secretly Work to Block Safeguards Against Surveillance AbuseBy Dell CameronThat policy, which underwent a minor technical revision by the Trump administration in May 2017, doesn’t prevent the military from using artificial intelligence in any of its weapons systems. It mandates that commanders have “appropriate levels of human judgment” over any AI-infused weapons systems, although the phrase isn’t further defined and remains a source of confusion within the Pentagon, according to multiple officials there.It does, however, require that before a computer could be programmed to initiate deadly action, the weapons system that contains it must undergo special review by three senior Pentagon officials—in advance of its purchase. To date that special review hasn’t been undertaken.In late 2016, during the waning days of the Obama administration, the Pentagon took a new look at the 2012 policy and decided in a classified report that no major change was needed, according to a former defense official familiar with the details. “There was nothing that was held up, there was no one who thought, ‘Oh we have to update the directives,’” the former official said.The Trump administration nonetheless has internally discussed making it clearer to weapons engineers within the military—who it fears have been reluctant to inject AI into their designs—that the policy doesn’t ban the use of autonomy in weapons systems. The contretemps in Silicon Valley over Project Maven at least temporarily halted that discussion, prompting the department’s leaders to try first to win the support of the Defense Innovation Board.But one way or another, the Pentagon intends to integrate more AI into its weaponry. “We’re not going to sit on the sidelines as a new technology revolutionizes the battlefield,” Marcuse said. “It’s not fair to the American people, it’s not fair to our service members who we send into harm’s way, and it’s not fair to our allies who depend on us.”The Center for Public Integrity is a nonprofit, nonpartisan, investigative newsroom in Washington, DC. More of its national security reporting can be found here.Enter your email to get the Wired newsletterclose dialogRecommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyPlease enter abovesign upUsed consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.Recommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyYou're signed up!Used consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.close dialogMore Great WIRED StoriesAlexa grew up this year, mostly because we talked to it8 sci-fi writers imagine the bold and new future of workThe mad scramble for the world's most coveted meteoriteGalileo, krypton, and how the true meter came to beEverything you want to know about the promise of 5G👀 Looking for the latest gadgets? Check out our picks, gift guides, and best deals all year round📩 Get even more of our inside scoops with our weekly Backchannel newsletter",,,,"https://media.wired.com/photos/5c1c383529ff200b2600066b/1:1/w_750,h_750,c_limit/ai-pentagon-elena-lacey-wired.gif",,,,,"The battle began with an unexpected loss. In June, Google announced it was pulling out of a Pentagon program—the much-discussed Project Maven—that used the tech giant’s artificial intelligence software. Thousands of the company’s employees had signed a petition two months earlier calling for an end to its work on the project, an effort to create algorithms that could help intelligence analysts pick out military targets from video footage.
Inside the Pentagon, Google’s withdrawal brought a combination of frustration and distress—even anger—that has percolated ever since, according to five sources familiar with internal discussions on Maven, the military’s first big effort to utilize AI in warfare.
“We have stumbled unprepared into a contest over the strategic narrative,” said an internal Pentagon memo circulated to roughly 50 defense officials on June 28. The memo depicted a department caught flat-footed and newly at risk of alienating experts critical to the military’s artificial intelligence development plans.
“We will not compete effectively against our adversaries if we do not win the ‘hearts and minds’ of the key supporters,” it warned.
Maven was actually far from complete and cost only about $70 million in 2017, a molecule of water in the Pentagon’s oceanic $600 billion budget that year. But Google’s announcement exemplified a larger public relations and scientific challenge the department is still wrestling with. It has responded so far by trying to create a new public image for its AI work and by seeking a review of the department’s AI policy by an advisory board of top executives from tech companies.
The reason for the Pentagon’s anxiety is clear: It wants a smooth path to use artificial intelligence in weaponry of the future, a desire already backed by the promise of several billion dollars to try to ensure such systems are trusted and accepted by military commanders, plus billions more in expenditures on the technologies themselves.
Developing artificial intelligence, officials say, is unlike creating other military technologies. While the military can easily turn to big defense contractors for cutting-edge work on fighter jets and bombs, the heart of innovation in AI and machine learning resides among the non-defense tech giants of Silicon Valley. Without their help, officials worry, they could lose an escalating global arms race in which AI will play an increasingly important role, something top officials say they are unwilling to accept.
“If you decide not to work on Maven, you’re not actually having a discussion on if artificial intelligence or machine learning are going to be used for military operations,” Chris Lynch, a former tech entrepreneur who now runs the Pentagon’s Defense Digital Service, said in an interview. AI is coming to warfare, he says, so the question is, which American technologists are going to engineer it?
Lynch, who recruits technical experts to spend several years working on Pentagon problems before returning to the private sector, said that AI technology is too important, and that the agency will proceed even if it has to rely on lesser experts. But without the help of the industry’s best minds, Lynch added, “we’re going to pay somebody who is far less capable to go build a far less capable product that may put young men and women in dangerous positions, and there may be mistakes because of it.”
Google isn’t likely to shift gears soon. Less than a week after announcing that the company would not seek to renew the Maven contract in June, Google released a set of AI principles which specified that the company would not use AI for “weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.”
Some defense officials have complained since then that Google was being unpatriotic, noting that the company was still pursuing work with the Chinese government, the top US competitor in artificial intelligence technology.
“I have a hard time with companies that are working very hard to engage in the market inside of China, and engaging in projects where intellectual property is shared with the Chinese, which is synonymous with sharing it with the Chinese military, and then don't want to work for the US military,” General Joe Dunford, chairman of the Joint Chiefs of Staff, commented while speaking at a conference in November.
In December testimony before congress, Google CEO Sundar Pichai acknowledged that Google had experimented with a program involving China, Project Dragonfly, aimed at developing a model of what government-censored search results would look like in China. However, Pichai testified that Google currently “has no plans to launch in China.”
Project Maven’s aim was to simplify work for intelligence analysts by tagging object types in video footage from drones and other platforms, helping analysts gather information and narrow their focus on potential targets, according to sources familiar with the partly classified program. But the algorithms did not select the targets or order strikes, a longtime fear of those worried about the intersection of advanced computing and new forms of lethal violence.
Many at Google nonetheless saw the program in alarming terms.
“They immediately heard drones and then they thought machine learning and automatic target recognition, and I think it escalated for them pretty quickly about enabling targeted killing, enabling targeted warfare,” said a former Google employee familiar with the internal discussions.
Google is just one of the tech giants that the Pentagon has sought to enlist in its effort to inject AI into modern warfare technology. Among the others: Microsoft and Amazon. After Google’s announcement in June more than a dozen large defense firms approached defense officials, offering to take over the work, according to current and former Pentagon officials.
But Silicon Valley activists also say the industry cannot easily ignore the ethical qualms of tech workers. “There’s a division between those who answer to shareholders, who want to get access to Defense Department contracts worth multimillions of dollars, and the rank and file who have to build the things and who feel morally complicit for things they don’t agree with,” the former Google employee said.
The first, formally begun in late June, was to create a Joint Artificial Intelligence Center meant to oversee and manage all of the military’s AI efforts, with an initial focus on PR-friendly humanitarian missions. It’s set to be run by Lieutenant General Jack Shanahan, whose last major assignment was running Project Maven. In a politically shrewd decision, its first major initiative is to figure out a way to use AI to help organize the military’s search and rescue response to natural disasters.
“Our goal is to save lives,” Brendan McCord, one of the chief architects of the Pentagon’s AI strategy, said while speaking at a technical conference in October. “Our military’s fundamental role, its mission, is to keep the peace. It is to deter war and protect our country. It is to improve global stability, and it’s to ultimately protect the set of values that came out of the Enlightenment.”
The second initiative is to order a new review of AI ethics by an advisory panel of tech experts, the Defense Innovation Board, which includes former Google CEO Eric Schmidt and LinkedIn cofounder Reid Hoffman.
That review, designed to develop principles for the use of AI by the military, is being managed by Joshua Marcuse, a former adviser to the secretary of defense on innovation issues who is now executive director of the board. Set to take about nine months, the advisory panel will hold public meetings with AI experts, while an internal Pentagon group also considers questions. Then it will forward recommendations to secretary of defense James Mattis about the ways that AI should or should not be injected into weapons programs.
“This has got to be about actually looking in the mirror and being willing to impose some constraints on what we will do, on what we won’t do, knowing what the boundaries are,” Marcuse said in an interview.
To make sure the debate is robust, Marcuse said that the board is seeking out critics of the military’s role in AI.
“They have a set of concerns, I think really valid and legitimate concerns, about how the Department of Defense is going to apply these technologies, because we have legal authority to invade people’s privacy in certain circumstances, we have legal authority to commit violence, we have legal authority to wage war,” he said.
Resolving those concerns is critical, officials say, because of the difference in how Washington and Beijing manage AI talent. China can conscript experts to work on military problems, whereas the United States has to find a way to interest and attract outside experts.
“They have to choose to work with us, so we need to offer them a meaningful, verifiable commitment that there are real opportunities to work with us where they can feel confident that they’re the good guys,” Marcuse said.
Despite his willingness to discuss potential future constraints on AI usage, Marcuse said he didn’t think the board would try to change the Pentagon’s existing policy on autonomous weapons that depend on AI, which was put in place by the Obama administration in 2012.
That policy, which underwent a minor technical revision by the Trump administration in May 2017, doesn’t prevent the military from using artificial intelligence in any of its weapons systems. It mandates that commanders have “appropriate levels of human judgment” over any AI-infused weapons systems, although the phrase isn’t further defined and remains a source of confusion within the Pentagon, according to multiple officials there.
It does, however, require that before a computer could be programmed to initiate deadly action, the weapons system that contains it must undergo special review by three senior Pentagon officials—in advance of its purchase. To date that special review hasn’t been undertaken.
In late 2016, during the waning days of the Obama administration, the Pentagon took a new look at the 2012 policy and decided in a classified report that no major change was needed, according to a former defense official familiar with the details. “There was nothing that was held up, there was no one who thought, ‘Oh we have to update the directives,’” the former official said.
The Trump administration nonetheless has internally discussed making it clearer to weapons engineers within the military—who it fears have been reluctant to inject AI into their designs—that the policy doesn’t ban the use of autonomy in weapons systems. The contretemps in Silicon Valley over Project Maven at least temporarily halted that discussion, prompting the department’s leaders to try first to win the support of the Defense Innovation Board.
But one way or another, the Pentagon intends to integrate more AI into its weaponry. “We’re not going to sit on the sidelines as a new technology revolutionizes the battlefield,” Marcuse said. “It’s not fair to the American people, it’s not fair to our service members who we send into harm’s way, and it’s not fair to our allies who depend on us.”

The Center for Public Integrity is a nonprofit, nonpartisan, investigative newsroom in Washington, DC. More of its national security reporting can be found here.

More Great WIRED Stories

Alexa grew up this year, mostly because we talked to it
8 sci-fi writers imagine the bold and new future of work
The mad scramble for the world's most coveted meteorite
Galileo, krypton, and how the true meter came to be
Everything you want to know about the promise of 5G
👀 Looking for the latest gadgets? Check out our picks, gift guides, and best deals all year round
📩 Get even more of our inside scoops with our weekly Backchannel newsletter",,,,"{'@type': 'WebPage', '@id': 'https://www.wired.com/story/inside-the-pentagons-plan-to-win-over-silicon-valleys-ai-experts/'}",,,,,,"{'@type': 'CreativeWork', 'name': 'WIRED'}","The Defense Department wants to use AI in warfare. In the aftermath of Project Maven, it still needs Big Tech’s help.",,,,,
https://news.google.com/rss/articles/CBMiWmh0dHBzOi8vd3d3LmtkbnVnZ2V0cy5jb20vMjAxOC8xMi9tYWNoaW5lLWxlYXJuaW5nLWV4cGxhaW5hYmlsaXR5LWludGVycHJldGFiaWxpdHktYWkuaHRtbNIBAA?oc=5,Machine Learning Explainability vs Interpretability: Two concepts that could help restore trust in AI - KDnuggets - KDnuggets,2018-12-20,KDnuggets,https://www.kdnuggets.com,"We explain the key differences between explainability and interpretability and why they're so important for machine learning and AI, before taking a look at several techniques and methods for improving machine learning interpretability.",N/A,"We explain the key differences between explainability and interpretability and why they're so important for machine learning and AI, before taking a look at several techniques and methods for improving machine learning interpretability.","We explain the key differences between explainability and interpretability and why they're so important for machine learning and AI, before taking a look at several techniques and methods for improving machine learning interpretability.",,,,,,,,,,,,,,2018 Dec Opinions,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiV2h0dHBzOi8vd3d3LnZpY2UuY29tL2VuL2FydGljbGUvN3h5YWJiL2NoaW5hLWFpLWRvbWluYW5jZS1yZWxpZXMtb24teW91bmctZGF0YS1sYWJlbGVyc9IBAA?oc=5,China Is Achieving AI Dominance by Relying on Young Blue-Collar Workers - VICE,2018-12-21,VICE,https://www.vice.com,"To remain the world leader in artificial intelligence, China relies on young “data labelers” who work eight hours a day processing massive amounts of data to make computers smart.",N/A,"To remain the world leader in artificial intelligence, China relies on young “data labelers” who work eight hours a day processing massive amounts of data to make computers smart.","To remain the world leader in artificial intelligence, China relies on young “data labelers” who work eight hours a day processing massive amounts of data to make computers smart.",https://schema.org,,,,,,,,,,,,,N/A,N/A,N/A,,"[{'@type': 'BreadcrumbList', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.vice.com/en'}, {'@type': 'ListItem', 'position': 2, 'name': 'Tech', 'item': 'https://www.vice.com/en/section/tech'}]}, {'@context': 'https://schema.org', '@type': 'NewsArticle', 'mainEntityOfPage': {'@type': 'WebPage', '@id': 'https://www.vice.com/en/article/7xyabb/china-ai-dominance-relies-on-young-data-labelers'}, 'headline': 'China Is Achieving AI Dominance by Relying on Young Blue-Collar Workers', 'image': ['https://video-images.vice.com/articles/5c1be0f6edf7c400064f1b1c/lede/1545332276147-ai_11.jpeg?crop=1xw:0.8433382137628112xh;center,center&resize=1200:*'], 'datePublished': '2018-12-21T14:00:00.000Z', 'dateModified': '2018-12-21T14:00:00.000Z', 'author': {'@type': 'Person', 'name': 'Huizhong Wu'}, 'publisher': {'@type': 'Organization', 'name': 'VICE', 'logo': {'@type': 'ImageObject', 'url': 'https://vice-web-statics-cdn.vice.com/images/vice-og.png'}}}, {'@context': 'https://schema.org', '@type': 'ItemList', 'name': 'China Is Achieving AI Dominance by Relying on Young Blue-Collar Workers', 'itemListElement': []}, []]",,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiR2h0dHBzOi8vY3Jvc3NjdXQuY29tLzIwMTgvMTIvY291bGQtd2FzaGluZ3Rvbi1zdGF0ZS1iZS1uZXh0LWFpLWZyb250aWVy0gEA?oc=5,Could Washington state be the next AI frontier? - Crosscut,2018-12-21,Crosscut,https://crosscut.com,"As the state Legislature contemplates how it can help the emergent industry, the University of Washington lays the groundwork for a future workforce.",N/A,"As the state Legislature contemplates how it can help the emergent industry, the University of Washington lays the groundwork for a future workforce.","As the state Legislature contemplates how it can help the emergent industry, the University of Washington lays the groundwork for a future workforce.",https://schema.org,,,,,,,,,,,,,N/A,N/A,"






Share




Share on Facebook


Share on Twitter


Share via E-mail


Print






Politics

Could Washington state be the next AI frontier?


As the state Legislature contemplates how it can help the emerging industry, the University of Washington lays the groundwork for a future workforce.


by 
      
John Stang


 / 
December 21, 2018









 


Advances in artificial intelligence are at the heart of the Amazon Go stores, where checkers have been replaced by sensors. (Photo by Matt M. McKnight/Crosscut)






Advertisement











An Israeli company claims it can figure out someone’s personality just by having its machine-learning programs study that person’s face. Meanwhile, in China, a machine-learning program picks criminals from noncriminal just from looking at their photos. The program relies on data based on conclusions made by Chinese judges. The criminals looked more masculine, the judges had concluded, had smaller eyes and were more likely to frown than the noncriminals.









But is this a fair way to label someone a criminal?
This is the kind of question that students grapple with in the University of Washington’s new course in artificial intelligence ethics, which wrapped up earlier this month. Developed by professors Adrienne Fairhall and Blaise Aguera y Arcas, the course asks thorny questions about bias, privacy and surveillance in technology that has infiltrated our modern lives, from the checkout counter to self-driving cars.
Another lesson in artificial intelligence has been happening about 60 miles down I-5, where lawmakers have been contemplating the role that government should play in regulating how artificial intelligence  — of which “machine learning” is a subset — is developed and deployed, as well as what to do about its potential impact on labor.



Next:
Four candidates are vying to fill the open WA Supreme Court seat



In recent weeks, the Washington House Technology & Economic Development Committee has received two briefings in Olympia concerning Washington state’s place as a center for artificial intelligence — an attempt to “get our mind around the subject,” said Rep, Jeff Morris, D-Mount Vernon and outgoing chairman of the technology committee. At issue is how the industry can be improved and, if necessary, regulated. 
“What should we be looking for?  What should we be doing?” said Rep. Terry Nealey, R-Dayton. 
The Legislature is acting now because the world of artificial intelligence is booming. 
This month, a report by experts from several universities and corporations — dubbed the “AI Index 2018” — noted that while there were no artificial intelligence startup ventures in 1995, almost 5,000 such companies existed by 2015 and slightly more than 10,000 exist today. Another indication of this boom is that 2017 produced nine times the academic papers on artificial intelligence than were written in 2000. 



Next:
Seattle City Council sends $1.55B transportation tax to the ballot



Washington is a player in this emerging market. The state is currently home to almost 200 ventures dealing with artificial intelligence — from Microsoft and Amazon to the Allen Institute for Artificial Intelligence and dozens of tiny startups. Meanwhile, regional companies, including Expedia, Starbucks, Boeing and T-Mobile, are all working with artificial intelligence.
“We are Ground Zero of an amazing AI cluster,” said Joseph Williams, director of tech industry economic development at the Washington Department of Commerce.
Yet, despite its position in the AI market, Washington has had trouble attracting venture capital — the private investment money needed for startup firms to survive their initial years. Washington has about 100 venture capital deals involving AI in play, said Williams. By comparisons, three venture capital firms in the San Francisco area each have about 100 such deals in play. New York — because it is a banking center — has about 235 in motion.
Other investments are on the rise. The University of Washington, for instance, is increasing its AI footprint.
The number of students at the University of Washington studying some form of artificial intelligence has jumped from about 200 in 2008 to almost 1,000 today. The number of artificial intelligence and machine-learning instructors at the university’s Paul Allen School of Computer Science & Engineering has increased to 20 from eight in the same period.
Naturally, the number of courses at the school for both major and nonmajors has grown as well, to 10 today from four in 2008, and include natural language processing, robotics and deep learning. These do not include artificial intelligence-related classes taught elsewhere at the university, including the new ethics course.
“I think it would be great if we can expand more rapidly,” said Dan Weld, who teaches artificial Intelligence courses at the University of Washington. “I’m not sure we’re meeting the demand.”
“It’s always hard to predict whether we’re at the beginning of huge growth or if we’re at a point where the bubble will pop,” he added, noting that he personally believes the field is facing a huge growth spurt.
Williams said the Seattle area now accounts for 5.9 percent of the job posting for artificial intelligence jobs in the nation — sixth best in the United States, behind New York City at 11.6 percent, San Francisco at 9.6 percent, San Jose at 9.2 percent, Washington, D.C., at 7.9 percent and Boston at 6.1 percent.
“We have way better talent than New York on AI,” Williams said. “(Washington state) is just not on the radar screen.”
Washington state has roughly 6,400 people working in the artificial intelligence industry, although not all are actual AI researchers or scientists, whose jobs come with salaries of $300,000 to $500,000 a year. The competition for these workers is fierce. “We get our data scientists poached all the time,” Sacha Fontaine, energy solutions director for Siemens.
One way the government could help the industry is to increase the flow of homegrown talent, said Weld, the UW instructor. He believes the Legislature should focus on efforts by the state’s school districts to prepare students for the field. “It’s important to make sure computer science is taught at the (elementary) and the high school levels,” he said  
Roughly a third of Washington’s high schools teach advanced computer science, and the state does not have a plan to provide computer science in the rest of the schools, especially for those that currently do not have the money to do so, said Ed Lazowska, who holds the Bill & Melinda Gates Chair at the UW's computer science and engineering school.
“Computer science needs to be available in every school in our state,” he added. 
As far as regulation goes, Dan Grossman, deputy director of UW’s computer science and engineering school, said he believes the Legislature should pick its path carefully in tackling artificial intelligence. “The best regulation is not about a particular technology, but about a particular problem,” he said. “Where autonomous vehicles are allowed and not allowed to drive — that is the role of government.”






      Please support independent local news for all.
    
We rely on donations from readers like you to sustain Crosscut's in-depth reporting on issues critical to the PNW.
Donate









Advertisement




Advertisement












Recent









Politics



The Cascade PBS 2024 Washington Statewide Voter Guide is here





 We partnered with newsrooms around WA to bring you profiles on every candidate, from the governor's race to your local legislative district.




by 
      
Cascade PBS Newsroom Staff


 / 
July 11













News



Dancers struggle to find work as Eastern WA’s last strip club closes





 “Clubs shutting down may not immediately lead to trafficking,” says one advocate. “But it immediately leads to all sorts of other vulnerable situations.”




by 
      
Erin Sellers


 / 
June 28












Advertisement




Advertisement











Topics:

Education,
Innovation,
Labor,
Washington State




Share




Share on Facebook


Share on Twitter


Share via E-mail


Print







",,"[{'@type': 'NewsArticle', 'headline': 'Could Washington state be the next AI frontier?', 'name': 'Could Washington state be the next AI frontier?', 'description': 'As the state Legislature contemplates how\xa0it can help the emergent industry, the University of Washington lays the groundwork for a future workforce.', 'image': {'@type': 'ImageObject', 'representativeOfPage': 'True', 'url': 'https://crosscut.com/sites/default/files/styles/max_2000x2000/public/images/articles/mcknight_amazon_0263_0.jpg?itok=vuG0BXxN', 'width': '2000', 'height': '1333'}, 'datePublished': '2018-12-21T13:00:00Z', 'dateModified': '2018-12-21T05:00:00-0800', 'author': {'@type': 'Person', 'name': 'John Stang'}, 'publisher': {'@type': 'Organization', '@id': 'https://crosscut.com/', 'name': 'Cascade PBS News', 'url': 'https://crosscut.com/', 'logo': {'@type': 'ImageObject', 'url': 'https://crosscut.com/themes/custom/crosscut/assets/images/logos/cpbs_logo.png', 'width': '95', 'height': '51'}}, 'articleSection': 'Politics', 'keywords': ['Education', 'Innovation', 'Labor', 'Washington State'], 'url': 'https://crosscut.com/2018/12/could-washington-state-be-next-ai-frontier', 'thumbnailUrl': 'https://crosscut.com/sites/default/files/styles/thumbnail/public/images/articles/mcknight_amazon_0263_0.jpg?itok=Gdk5lV5h', 'creator': {'@type': 'Person', 'name': 'John Stang'}, 'mainEntityOfPage': 'https://crosscut.com/2018/12/could-washington-state-be-next-ai-frontier'}]",,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiPGh0dHBzOi8vd3d3LnNub3Blcy5jb20vZmFjdC1jaGVjay9haS1yb2JvdHMta2lsbC1zY2llbnRpc3RzL9IBAA?oc=5,Did Four AI Robots Kill 29 Scientists in Japan? - Snopes.com,2018-12-19,Snopes.com,https://www.snopes.com,Four AI-controlled robots killed 29 scientists in Japan in August 2017. ,"robots, Artificial Intelligence","The claim came from a UFOlogist -- and, yes, it does sound like something from a movie.",,https://schema.org,BreadcrumbList,https://www.snopes.com,"{'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'url': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'caption': 'Snopes.com'}","{'@context': 'http://schema.org/', '@type': 'Organization', 'url': 'https://www.snopes.com', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'url': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'caption': 'Snopes.com'}, 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'url': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'caption': 'Snopes.com'}, 'sameAs': ['https://www.facebook.com/snopes', 'https://twitter.com/snopes', 'https://www.instagram.com/snopesdotcom/', 'https://www.linkedin.com/company/snopes.com', 'https://www.youtube.com/channel/UCHQAmn49BObyOsPHCnKRC4w', 'https://www.pinterest.com/snopesdotcom/', 'https://en.wikipedia.org/wiki/Snopes']}","{'@type': 'Organization', 'name': 'Snopes.com', 'logo': {'@type': 'ImageObject', 'url': 'https://media.snopes.com/2017/09/snopes-logo-black.png'}}",Did Four AI Robots Kill 29 Scientists in Japan?,2018-12-19T03:52:58Z,2018-12-19T03:52:58Z,Junk News,,,"[{'@type': 'ListItem', 'name': 'Homepage', 'item': 'https://www.snopes.com', 'position': 1}, {'@type': 'ListItem', 'name': 'Fact Check', 'item': 'https://www.snopes.com/fact-check/', 'position': 2}]",,N/A,"



Claim:

											Four AI-controlled robots killed 29 scientists in Japan in August 2017. 										


Rating:





													False
													
About this rating 










In December 2018, social media users began circulating a short and blurry video clip showing a woman relaying a story about 29 scientists who were reportedly killed by artificial intelligence-controlled robots in Japan (""4 robots kill 29 scientists""). One iteration of this footage, which asserted that the AI robot massacre had occurred in South Korea, not Japan, can be glimpsed below:

Advertisement:This clip was taken from an hour-long presentation delivered by conspiracy theorist and UFOlogist Linda Moulton Howe at the Conscious of Life Expo in Los Angeles in February 2018. Howe's presentation was largely about the dangers of artificial intelligence, but it was also entwined with stories about alien encounters, abductions, and alternate dimensions.

Howe kicked off her speech with a story about four AI robots killing 29 scientists in Japan that she reportedly heard about from a former Marine who had been doing contract work for government agencies such as the CIA, the NSA, and the DIA (Defense Intelligence Agency):
Advertisement:
On Saturday August 26 2017, not very long ago, I received a phone call from a whistle blower in the Intel world I've known for about a year and a half. He is an honorably discharged marine, but he continues to work on contracts with the CIA, NSA, DIA agencies. I always keep notebooks all over my house, my office, my car, everywhere so that I can write down a phone call that I can't record or that I'm not in my studio to record.
So I wrote this down almost word for word.
At a top robotics company in Japan this week four robots being developed for military applications killed 29 humans in the lab. And they did it by shooting what he called metal bullets. I didn't know there was any other kind.
The scariest part is that lab workers deactivated two of the robots, took apart the third, but the fourth began restoring itself and somehow connected to an orbiting satellite to download information about how it could rebuild itself even more strongly than before.
And this next sentence, this is a quote, I'm writing this down. I've been doing this for years.
This is serious shit Linda. but you're never going to hear about this in the news. The robotics company has too much to lose, and the government wants AI robot soldiers. Close quote.
Howe's story was suspiciously void of specifics. She didn't identify the source of her information, the name of the factory where the alleged massacre occurred, or the names of any of the scientists who were reportedly killed. And while Howe claimed that reports of this robot uprising were suppressed by the government, one would expect some coverage about the deaths or disappearances of dozens of Japan's top scientists to have hit the news, even if the robot aspect of the story was obscured. Yet we found no such reports.
Advertisement:It was also odd that this tale served as little more than a footnote in Howe's overall presentation. In fact, this outlandish story seemed to function more as an attention-grabbing anecdote and less as a retelling of a genuine incident. After opening with this alleged robot massacre report, Howe transitioned to the dangers of artificial intelligence, spent most of her time focused on alien encounters, and then concluded by saying that humans may actually be the artificially intelligent creations of an alien race: ""Is it fair to ponder that those first humans were artificial intelligence for those who made us from manipulating genetics like robotic lab scientists are now doing on earth today? ... Are we humans actually someone else's androids?""
Linda Moulton Howe's full presentation can be viewed below:

Advertisement:Howe has told this story at least one other time, as part of an interview with the ""Mysterious Outpost Radio"" at the ""Ozark Mountain UFO Conference."" During that telling, Howe added that her Marine source would only communicate with her in short texts or 30-second phone calls because his phone had been tapped, and that she made no claims of having handwritten notes of those conversations. Howe also maintained that her source didn't actually witness any such robot massacre, but that he was relaying information he had reportedly come across during his contract work:

At best, the claim that 29 scientists were killed by AI robots in Japan is based on third-hand information unsupported by any actual evidence. At worst, this rumor was made up out of whole cloth as an attention-grabbing anecdote for a speech about how human beings are merely the artificially intelligent creations of an alien race.
Advertisement:","['https://www.facebook.com/snopes', 'https://twitter.com/snopes', 'https://www.instagram.com/snopesdotcom/', 'https://www.linkedin.com/company/snopes.com', 'https://www.youtube.com/channel/UCHQAmn49BObyOsPHCnKRC4w', 'https://www.pinterest.com/snopesdotcom/', 'https://en.wikipedia.org/wiki/Snopes']",,,https://mediaproxy.snopes.com/width/1200/height/675/https://media.snopes.com/2018/12/shutterstock_638342005.jpg,,,,,,,"{'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'url': 'https://www.snopes.com/design/images/logo-s-crop-on.svg', 'caption': 'Snopes.com'}",,"{'@type': 'WebPage', '@id': 'https://www.snopes.com//fact-check/ai-robots-kill-scientists/'}",2018-12-19T03:52:58Z,,,,,,"The claim came from a UFOlogist -- and, yes, it does sound like something from a movie.",Four AI-controlled robots killed 29 scientists in Japan in August 2017. ,"{'@type': 'Rating', 'alternateName': 'False'}",{'@type': 'CreativeWork'},,
https://news.google.com/rss/articles/CBMiZWh0dHBzOi8vd3d3LnBicy5vcmcvbmV3c2hvdXIvc2NpZW5jZS9ob3ctYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utc3BvdHRlZC1ldmVyeS1zb2xhci1wYW5lbC1pbi10aGUtdS1z0gFpaHR0cHM6Ly93d3cucGJzLm9yZy9uZXdzaG91ci9hbXAvc2NpZW5jZS9ob3ctYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utc3BvdHRlZC1ldmVyeS1zb2xhci1wYW5lbC1pbi10aGUtdS1z?oc=5,How artificial intelligence spotted every solar panel in the U.S. - PBS NewsHour,2018-12-19,PBS NewsHour,https://www.pbs.org,Curious onlookers can check out the open-access maps. Publicly available information on this scale could help revolutionize the future of energy.,N/A,Curious onlookers can check out the open-access maps. Publicly available information on this scale could help revolutionize the future of energy.,Curious onlookers can check out the open-access maps. Publicly available information on this scale could help revolutionize the future of energy.,,,,,,,,,,,,,,Science,N/A,"






Full Episode










Monday, Jul 15


",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVGh0dHBzOi8vcG9saWN5b3B0aW9ucy5pcnBwLm9yZy9tYWdhemluZXMvZGVjZW1iZXItMjAxOC9mdXR1cmUtd29ya2Vycy1yaWdodHMtYWktYWdlL9IBAA?oc=5,The future of workers' rights in the AI age - Policy Options,2018-12-17,Policy Options,https://policyoptions.irpp.org,"When thinking about protecting basic rights for citizens in the AI age, we should ask how technology contributes to general welfare and good society.",N/A,"When thinking about protecting basic rights for citizens in the AI age, we should ask how technology contributes to general welfare and good society.",N/A,https://schema.org,,,,,,,,,,,,,N/A,N/A,"







EconomySocial Policy 


				How to help older Canadians continue to work 
			

by  Rosanna Tamburri
July 2, 2024 

",,"[{'@type': 'WebSite', '@id': 'https://policyoptions.irpp.org/#website', 'url': 'https://policyoptions.irpp.org/', 'name': 'Policy Options', 'description': 'Institute for Research on Public Policy', 'potentialAction': [{'@type': 'SearchAction', 'target': 'https://policyoptions.irpp.org/?s={search_term_string}', 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'ImageObject', '@id': 'https://policyoptions.irpp.org/magazines/december-2018/future-workers-rights-ai-age/#primaryimage', 'inLanguage': 'en-US', 'url': 'https://policyoptions.irpp.org/wp-content/uploads/sites/2/2018/12/Wp-kai.jpg', 'width': 2000, 'height': 700}, {'@type': 'WebPage', '@id': 'https://policyoptions.irpp.org/magazines/december-2018/future-workers-rights-ai-age/#webpage', 'url': 'https://policyoptions.irpp.org/magazines/december-2018/future-workers-rights-ai-age/', 'name': 'The future of workers’ rights in the AI age', 'isPartOf': {'@id': 'https://policyoptions.irpp.org/#website'}, 'primaryImageOfPage': {'@id': 'https://policyoptions.irpp.org/magazines/december-2018/future-workers-rights-ai-age/#primaryimage'}, 'datePublished': '2018-12-17T11:31:14+00:00', 'dateModified': '2021-04-02T12:36:26+00:00', 'description': 'When thinking about protecting basic rights for citizens in the AI age, we should ask how technology contributes to general welfare and good society.', 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://policyoptions.irpp.org/magazines/december-2018/future-workers-rights-ai-age/']}]}, {'@type': 'Person', '@id': ''}]",,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiigFodHRwczovL3d3dy5mb3JiZXMuY29tL3NpdGVzL2NvZ25pdGl2ZXdvcmxkLzIwMTgvMTIvMjAvZ2VvZmYtaGludG9uLWRpc21pc3NlZC10aGUtbmVlZC1mb3ItZXhwbGFpbmFibGUtYWktOC1leHBlcnRzLWV4cGxhaW4td2h5LWhlcy13cm9uZy_SAQA?oc=5,Geoff Hinton Dismissed The Need For Explainable AI: 8 Experts Explain Why He's Wrong - Forbes,2018-12-20,Forbes,https://www.forbes.com,"Geoffrey Hinton, Godfather of AI and Head of Google Brain dismissed the need for Explainable AI. His justification has set off a discourse among AI/ML practitioners in industry, academia and government, who were eager to refute his arguments.",,"Geoffrey Hinton, Godfather of AI and Head of Google Brain dismissed the need for Explainable AI. His justification has set off a discourse among AI/ML practitioners in industry, academia and government, who were eager to refute his arguments.","Geoffrey Hinton, Godfather of AI and Head of Google Brain dismissed the need for Explainable AI. His justification has set off a discourse among AI/ML practitioners in industry, academia and government, who were eager to refute his arguments.",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/cognitiveworld/2018/12/20/geoff-hinton-dismissed-the-need-for-explainable-ai-8-experts-explain-why-hes-wrong/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/blogs-images/cognitiveworld/files/2018/12/QuestionMarksStack-1200x800.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Hessie Jones', 'url': 'https://www.forbes.com/sites/hessiejones/', 'description': 'Hessie Jones is an Author, Strategist, Investor and Data Privacy Practitioner, advocating for human-centred AI, education and the ethical distribution of AI in this era of transformation. She is a member of the cofounding team of the Personally identifiable Information Standards Architecture (PIISA), developing an open model to detect and remediate PII in AI models; is also a cofounding member of MyData Canada, is on the Board of Women in AI Ethics and is the Innovation Manager with Altitude Accelerator. In her work, she aims to advance the quality of human-computer experiences through values-based innovation prioritizing human-centred design. As a seasoned digital strategist, author, tech geek and data junkie, she has spent the last 18 years on the internet at Yahoo!, Aegis Media, CIBC, and Citi, as well as tech startups including Cerebri and OverlayTV. Hessie saw things change rapidly when search and social started to change the game for advertising and decided to figure out the way new market dynamics would change corporate environments forever: in process, in culture and in mindset. She launched ArCompany in social intelligence, AI readiness and research. Through the weekly think tank discussions her team curated, she surfaced the generational divide in this changing technology landscape across a multitude of topics. She was highlighted in 2019 in the 100 Brilliant Women in AI Ethics. She is also a board member with Technology for Good Canada and a contributor/editor to GritDaily, and Forbes, as well as a former editor for Towards Data Science.', 'sameAs': ['https://www.linkedin.com/in/hessiejones1/', 'https://www.twitter.com/hessiejones', 'https://muckrack.com/hessie-jones', 'hessie.jones']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",Geoff Hinton Dismissed The Need For Explainable AI: 8 Experts Explain Why He's Wrong,2018-12-20T22:47:00-05:00,2018-12-20T22:47:01-05:00,AI & Big Data,Geoff Hinton Dismissed The Need For Explainable AI: 8 Experts Explain Why He's Wrong,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'AI', 'item': 'https://www.forbes.com/ai/'}]",AI & Big Data,N/A,"More From ForbesJun 17, 2024,04:42am EDTProtecting Human Rights In The Age Of Intelligent MachinesApr 24, 2024,05:00am EDTWhat AI Can Tell Us About OzempicFeb 22, 2024,03:42am ESTUnlocking The Power Of Social Data For Clinical Trial DesignJan 25, 2024,04:55am ESTCPG Product Innovation Should Be A Science, Not An ArtAug 7, 2023,08:57am EDTListening Before We Talk: Why Social Data Is Key To Successful NPD In 2023Mar 28, 2023,10:07am EDTHow Technology Is Empowering CPGs To Do More With LessFeb 21, 2023,05:24am ESTAre Humans And Collaboration Still At The Heart Of Innovation?Edit StoryForbesInnovationAIGeoff Hinton Dismissed The Need For Explainable AI: 8 Experts Explain Why He's WrongHessie JonesContributorCOGNITIVE WORLDContributor GroupOpinions expressed by Forbes Contributors are their own.FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got itDec 20, 2018,10:47pm ESTUpdated Dec 20, 2018, 10:47pm ESTThis article is more than 5 years old.Share to FacebookShare to TwitterShare to Linkedin







A heap of questions..
Depositphotos enhanced by CogWorld





If the expectation is that automation will be ubiquitous in the next decade, reliance on human judgement will diminish. The promises of Artificial Intelligence are met with cautious optimism as the technology evolves and the environment around it attempts to keep pace. In this nascent period, industry, academia and lawmakers are grappling with the outcomes of these technologies and their impact on our social norms. The domino effect here created by AI will alter all facets of policy, technology and society.

In a recent interview with Wired Geoff Hinton, distinguished computer scientist, Head of Google Brain, and renowned for his work with Artificial Neural Networks, stated this when prompted about AI's eventual role in decision-making:

I’m an expert on trying to get the technology to work, not an expert on social policy. One place where I do have technical expertise that’s relevant is [whether] regulators should insist that you can explain how your AI system works. I think that would be a complete disaster.
PROMOTED
People can’t explain how they work, for most of the things they do. When you hire somebody, the decision is based on all sorts of things you can quantify, and then all sorts of gut feelings. People have no idea how they do that. If you ask them to explain their decision, you are forcing them to make up a story. 
Neural nets have a similar problem. When you train a neural net, it will learn a billion numbers that represent the knowledge it has extracted from the training data. If you put in an image, out comes the right decision, say, whether this was a pedestrian or not. But if you ask “Why did it think that?” well if there were any simple rules for deciding whether an image contains a pedestrian or not, it would have been a solved problem ages ago.
... [In response to how do we trust systems?] You should regulate them based on how they perform. You run the experiments to see if the thing’s biased, or if it is likely to kill fewer people than a person. With self-driving cars, I think people kind of accept that now. That even if you don’t quite know how a self-driving car does it all, if it has a lot fewer accidents than a person-driven car then it’s a good thing. I think we’re going to have to do it like you would for people: You just see how they perform, and if they repeatedly run into difficulties then you say they’re not so good.









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            




I reached out to notable practitioners in AI/ML industry, government and academia to weigh in on his arguments:  Ann Cavoukian, Rumman Chowdhury, Joy Buolamwini, Karen Bennet, Tim Miller, Heather Roff, Alejandro Saucedo and David Gunning. This article curates this discourse:
Let's break this down:
Hinton: ""I’m an expert on trying to get the technology to work, not an expert on social policy. One place where I do have technical expertise that’s relevant is [whether] regulators should insist that you can explain how your AI system works. I think that would be a complete disaster.""
For sometime, debate's ensued to determine who is liable for societal or individual harms resulting from algorithmic flaws: The data scientist, the machine or the company? For Dr. Heather Roff, Associate Fellow from the Leverhulme Centre for the Future of Intelligence, University of Cambridge, the responsibility needs to be broadened. She asserts,

This is a dangerous position to take. An expert on technology who feels themselves divorced from social or policy implications does not understand that technology is not value neutral, and that their decisions—even seemingly basic ones on how many gradient descents to take in a system — have socio-political implications. If one thinks they are only Scientists doing Science, but then simultaneously think that regulators should take an interest has fundamentally misunderstood their role as scientists engaging in socially and morally important questions. If your work requires legislation then you should think about that at the design stage… period.

Dr. Rumman Chowdhury, Managing Director and Global Lead for Responsible AI at Accenture agrees with Roff and addresses the very harms that today's AI experimentation exhibit. The fallout is societal. The solution needs to be a holistic collaboration between technology and policy.


We cannot divorce 'making things work' and 'impact on society' when it comes to applied artificial intelligence. Frankly, your AI does not ""work"" if it is biased, perpetuates social inequality and discrimination, or reinforces unequal power structures. Setting up that delineation is not only dangerous, it sets up a false dichotomy of ""tech innovators"" versus ""regulators."" Regulation, whether in the form of social norms, guidelines, or enforceable law, is intended to enable trust and ease adoption of technology in a way that is beneficial to society. Safe innovation is enabled with well designed regulation. 


Hinton: ""People can’t explain how they work, for most of the things they do... People have no idea how they do that. If you ask them to explain their decision, you are forcing them to make up a story.""
 

Timothy Miller, Associate Professor in computer science at the University of Melbourne, Australia, whose specializes in explainable AI and human-agent collaboration, disputes Hinton's claim on the limitations of human explanation: 

His quoted paragraph is itself an explanation: an explanation of why he has reached the decision that explainability for AI would be a disaster. Is he making up a story about this? I imagine he would claim that he is not and that it is based on careful reasoning. But in reality, it is based on neurons in his brain firing in a particular way that nobody understands. The ability to communicate his reasons to others is a strength of the human brain. Philosopher Daniel Dennett claims that consciousness itself is simply our brain creating an `edited digest’ of our brains inner workers for precisely the purpose of communicating our thoughts and intentions (including explanations) to others. 

Hinton: ""Neural nets have a similar problem. When you train a neural net, it will learn a billion numbers that represent the knowledge it has extracted from the training data. If you put in an image, out comes the right decision, say, whether this was a pedestrian or not. But if you ask “Why did it think that?” well if there were any simple rules for deciding whether an image contains a pedestrian or not, it would have been a solved problem ages ago.""
So if humans are to slowly cede control to autonomous algorithms, it will increasingly become difficult to understand what led to those decisions. Deep Learning requires minimal supervision and with enough training data can identify patterns from the data it accesses. As Hinton notes, eventually it becomes more difficult, even for the architects of this algorithm to understand the specific reasons for those outcomes. Dr. Ann Cavoukian, Distinguished Expert-in-Residence, leading the Privacy by Design Centre of Excellence at Ryerson University and former three-term Privacy Commissioner of Ontario, agrees with this.... somewhat:

...humans can externalize features that define a dog... however with current deep learning algorithms, although they may initially decompose an image into relevant features, and then recompose those features back into an image for categorization, these features are implicit to the algorithm, buried in the myriad numbers of parameter values. Current algorithms cannot externalize these features and use them to explain a decision. What is needed are algorithms that construct “wholes” from previously learned “parts/features” such that the features are also external to the algorithm that is making the decision.
...there is a second process taking place: there is a meta-algorithm in the brain that is able to view the process of decision-making and collect the sequence of features that were involved in the decision, and based on those, output the explanation. Again, this cannot be done with existing deep learning because the features are implicit– in the parameter values. Moreover, any one parameter value may affect features associated with categories other than “dog,” in the above example.

For Heather Roff, comparing humans to neural nets is not a ""true equivalence"":

...it is false equivalence. We can interrogate and probe human beings as to why they did X or Y. We even claim that we have AI based “lie detectors” to use micro facial expressions to show when someone is being untruthful. So why should we think that AI can save us from our worst selves but also accept that we cannot as humans figure these same things out? Designers must figure out what to measure, what data is important or relevant and the like, and AIs right now are not able to do that themselves. So to claim that humans are inherently opaque and non-transparent and that justifies us using other intelligence that are actually more opaque and inherently nonhuman-like in their reasoning as a justified argument is a false equivalence. Humans have a theory of mind. AIs right now do not. I don’t have a sense of what another being like me may think, if I’m an AI. I DO have that as a human being. And this excuse — as a an attempted justification at using tech that we don’t understand fully — is a red herring.

For the DOD, where precision in aspects of  war require investigation and justification, David Gunning introduces the work being done on explainable ML that will allow future warfighters to ""understand, appropriately trust and manage an emerging generation of AI Machine partners"" :

There are techniques to explain deep nets: DARPA’s Explainable AI (XAI) program, and a growing community of researchers, are developing techniques that can be used to explain, at least partially, deep nets: (1) there are techniques that can select the training examples that were most influential in a decision; (2) there are techniques to identify the most salient input features used in a decision; (3) there are network dissection techniques that can identify meaning features inside the layers of a deep net that can be used for explanation; and: (4) there are deep learning researchers who are developing deep learning techniques to generate explanations. 

Alejandro Saucedo, Chief Scientist, The Institute for Ethical AI & Machine Learning, partially agrees with Hinton in that it's not possible to open complex systems or models and provide a thorough explanation, however only focusing on the algorithm itself and trying to understand the value of each weight and its interaction with the outcomes is short-sighted.


AI explainability cannot be addressed by solely looking at it as a technological challenge. It requires consideration of the processes, infrastructure and even humans (yes, humans) operating the algorithms themselves.
It is possible to reach a reasonable level of explainability and accountability by ensuring the right touchpoints with domain experts are in place throughout the development and operation of AI systems. Sometimes this may involve a trade-off between explainability and accuracy, but it may be required depending on the critical nature of the project. A reasonable level of explainability can only be achieved through cross-functional collaboration across technology experts, industry practitioners and policy-makers.
At the Institute for Ethical AI, we are empowering industry practitioners to find this reasonable level of explainability with the AI Procurement Framework we released this year. It provides professionals with the tools to evaluate the maturity of their machine learning systems through a checklist which highlights red flags around processes and infrastructure.


Hinton: ""You should regulate them based on how they perform. You run the experiments to see if the thing’s biased, or if it is likely to kill fewer people than a person. With self-driving cars, I think people kind of accept that now. That even if you don’t quite know how a self-driving car does it all, if it has a lot fewer accidents than a person-driven car then it’s a good thing. I think we’re going to have to do it like you would for people: You just see how they perform, and if they repeatedly run into difficulties then you say they’re not so good.""
Reactive legislation cannot and should not be the panacea for technology going forward. Today, rampant harms as a result of existing bias in systems, unproven technology raise ethical concerns about their deployment. Recidivism, self-driving cars, social bots and facial recognition technologies released into the ether have created a growing movement to determine thresholds for accuracy with a check on legal and societal acceptances before they are commercialized. Joy Buolamwini, MIT Media Lab, Graduate Researcher and Founder, Algorithmic Justice League, who has extensive experience in facial recognition technology has been a strong proponent for government action:


Not only do we need to look at biased outcomes we need to look at bias from design, development, deployment, and governance of AI. Even systems that show decreased of harmful technical bias can be deployed in ways that breach civil liberties and human rights. Take the example of facial analysis systems which my research has shown to have substantial racial and gender bias. Companies have been working to reduce this technical bias, but there still needs to be accountability about how the technology is used and who it is sold to. Given everything we know about facial analysis technology failures it should not be used for lethal applications. Yet companies like Microsoft and Amazon appear poised to apply the technology for fatal military use if left unchecked. Even though the social impact of facial recognition technology is ill understood, companies are equipping law enforcement departments with this technology with no legal oversight or meaningful accountability. We need explanations for irresponsible use of facial analysis technology and a moratorium on life or liberty threatening applications. This is why we launched the Safe Face Pledge to prevent the lethal use and mitigate abuse of facial analysis technology. 



In business, justification behind decisions is a standard practice. In the fallout of Enron, the genesis of laws like Sarbanes Oxley were formed for the purpose of protecting the public and the business from fraud or errors. Explainability has been the business norm. It's our system of checks and balances. Ann Cavoukian, who served in policy for three consecutive terms as Privacy Commissioner disagrees with regulation as a result of performance:

I don’t share Geoff’s view of after-the-fact regulation. Technology is simply moving far too fast, and regulations, in this day and age, are a lagging remedy. We must be pre-emptive and proactively build-in explainability. However, to implement useful explainability will require different artificial architectures from the existing ones.



 Rumman Chowdhury goes a step further beyond explainability and towards understandable AI for law, business and society:



In talking about ""explainability"" as a false fix, Hinton is raising a discussion those of us in the ethics and AI space have addressed already. We agree, that explaining the activation functions of nodes, or a technical explanation of decisions, is not useful. What we seek is actionable understanding. Depending on the implementation (whether human or algorithmic), there are decisions that need an explanation, and we work to create understandable and actionable explanations to AI outcomes. This means, not only creating systems of explanations that are useful to humans, but creating systems of addressing and redressing potential issues and harms.


Karen Bennet, Principle at ArCompany (disclosure) and former senior engineering executive at Yahoo!, Redhat and IBM provides a pragmatic perspective that supports Chowdhury's understandable AI, and builds more accountability within data science and engineering:

In all the organizations that I have been working with, there is a disconnect between the people who are building predictive models and those who know how to best serve the organization's objectives. There are laws, standards or regulations that an organization must adhere to, so we do need to solve the problem of making an intelligence system accountable, so it can be audited and trusted to do the right thing. In the financial industry, e.g., before software is deployed live, there is an exhaust checklist that must be adhered to get approval; and one of them is explaining to a non-techie what the software does. It is not enough to just say look at the results. As incidents happen, the judicial system will also require these explanations. I suggest we look at Intelligent systems in (4) categories: Research/Human in Loop; Applied/Human in Loop; Research/Autonomous and Applied/Autonomous, as each has different requirements.

AI adoption comes with trust in our systems, with the belief that humanity's interests are given full consideration in AI's decisions. We're not there yet. In retrospect, this discourse is necessary to bring the issues of technology and all its impacts to the forefront. The division that exists between research and regulation will quickly dissipate as AI slowly wields itself into all facets of our lives.Hessie JonesFollowingFollowHessie Jones is an Author, Strategist, Investor and Data Privacy Practitioner, advocating for human-centred AI, education and the ethical... Read MoreEditorial StandardsPrintReprints & Permissions",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiiwFodHRwczovL3d3dy5uZXh0Z292LmNvbS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS8yMDE4LzEyL2FpLXlvdXItbmV4dC1qb2ItY291bGQtYmUtZmx5aW5nLWNhci1kZXZlbG9wZXItb3ItY3liZXItY2FsYW1pdHktZm9yZWNhc3Rlci8xNTM2NDAv0gEA?oc=5,"With AI, Your Next Job Could Be 'Flying Car Developer' Or 'Cyber Calamity Forecaster' - Nextgov/FCW",2018-12-18,Nextgov/FCW,https://www.nextgov.com,Can man and machine can co-exist at work?,,Can man and machine can co-exist at work?,Can man and machine can co-exist at work?,http://schema.org,Article,https://www.nextgov.com,"{'url': 'https://cdn.nextgov.com/media/img/cd/2018/12/18/121818aiNG/route-fifty-lead-image.jpg?1627467788', 'width': 1200, '@type': 'ImageObject', 'height': 550}","{'url': '/voices/nupur-anand/14322/', '@type': 'Person', 'name': 'Nupur Anand'}","{'@type': 'Organization', 'name': 'Quartz'}","With AI, Your Next Job Could Be 'Flying Car Developer' Or 'Cyber Calamity Forecaster'",2018-12-18T15:00:00,2023-07-10T17:33:02,,Nextgov/FCW,,,N/A,N/A,"

With AI, Your Next Job Could Be 'Flying Car Developer' Or 'Cyber Calamity Forecaster'
                    CNStock/Shutterstock.com
                  
Sponsor Message

Sponsor Message


      
  Get the latest federal technology news delivered to your inbox.

    emailStay Connected
Sponsor Message

Sponsor Message

Featured eBooks
  Cyber Workforce
            Read Now
              Law Enforcement TechRead NowAI in the WorkplaceRead Now





            
              
                
                  




  


By

Nupur Anand,Quartz







































            
              
                
  



  By



        Nupur Anand
      ,Quartz

|

             December 18, 2018
            

Can man and machine can co-exist at work?






                  Artificial Intelligence
                









                  Career Advice
                









                  Workforce
                












































Have you wondered what new job roles the rise of artificial intelligence and data analytics will throw up in the near future?Cognizant, IT services major, tried to figure it out and here’s what it has found. In a study it published Monday, the IT major listed 21 roles that will emerge in the next 10 years and that will be central to the future of work. Some of these are:“…even as work is changing with the emergence of AI, humans have never been more integral to the future of work,” the study said. “These jobs are both plausible and futuristic—and represent important work that humans will continue to need to do.”Cognizant believes man and machine can co-exist in these roles. These futuristic jobs fall under the same theme. However, it is extremely important for techies to upgrade their skills to remain relevant in the job market.“With the future of work evolving quickly, skills too must evolve to keep pace. Frequent skill upgrades are needed to ensure that people have the relevant skills to be–and stay–employable,” added Cognizant.This should serve as a wake-up call for graduates in India who are anyway in dire need for re-skilling. As per some estimates, currently, not even 60 percent of the graduates from management, technical, and engineering institutions get placed, Satya Pal Singh, the union minister of state for human resources development, told parliament Monday.“To encourage 100% placements to the graduates, All India Council for Technical Education (AICTE) has launched outcome-based model curriculum for UG (undergraduation) and PG (postgraduation) level courses in engineering and management programmes to make it industry oriented. Institutions have now been made responsible for arranging internships of their students so as to enhance their employability,” he added.However, clearly, a lot more needs to be done.








Share This:



NEXT STORY:

              Homeland Security Delegation Visits Asia for Emerging Tech Ideas
            













Human operators must be held accountable for AI’s use in conflicts, Air Force secretary says







Why NIST is prioritizing creating a dictionary of AI development







SSA restructures tech shop to center on the CIO







How a push to the cloud helped a Ukrainian bank keep faith with customers amid war







The people problem behind the government’s AI ambitions






sponsor content

Optimize Field Service Operations with Trusted Solutions for Government








Human operators must be held accountable for AI’s use in conflicts, Air Force secretary says






Why NIST is prioritizing creating a dictionary of AI development






SSA restructures tech shop to center on the CIO






How a push to the cloud helped a Ukrainian bank keep faith with customers amid war






The people problem behind the government’s AI ambitions





sponsor content

Optimize Field Service Operations with Trusted Solutions for Government






","['https://www.facebook.com/NextgovFCW/', 'https://twitter.com/NextgovFCW', 'https://www.linkedin.com/company/nextgovfcw/']",,,,,,,,,,,,https://qz.com/india/1499168/the-new-tech-jobs-ai-big-data-will-create-as-per-cognizant/,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMicGh0dHBzOi8vd3d3LnRoZWF0bGFudGljLmNvbS90ZWNobm9sb2d5L2FyY2hpdmUvMjAxOC8xMi83LWFyZ3VtZW50cy1hZ2FpbnN0LXRoZS1hdXRvbm9tb3VzLXZlaGljbGUtdXRvcGlhLzU3ODYzOC_SAQA?oc=5,Seven Arguments Against the Autonomous-Vehicle Utopia - The Atlantic,2018-12-20,The Atlantic,https://www.theatlantic.com,All the ways the self-driving future &lt;em&gt;won’t&lt;/em&gt; come to pass,"Self-driving cars, self-driving-car service, old-school car manufacturers, self-driving works, driverless cars, autonomous future, Google’s sister company Waymo, human intelligence, different disciplines, Such algorithms, materialize.Bear Case, lifetimes of many people, Proponents of autonomous cars, self-driving-car security specialist, various levels of artificial intelligence, electric cars, likely outcome, banality of the Equifax breachThe transportation reporter, whole lot of other things, University of Washington, Carnegie Mellon University, early days, Human driving, Public transportation, lot of perceptual challenges, duo of essays, big-data hacks, different bear case, individual choice, Consumer vehicles, identity theft, Bear Case, Electric Vehicles, unusual circumstances, safe operation, fleet of robo-cars, affordable housing, physical danger, self-driving-car safety, individual tasks, sheer number, different things, current transportation-service companies, physical reality, question of calibrating, artificial-intelligence researcher, seminal event, MIT Computer Science, vehicles, self-driving systems",All the ways the self-driving future won’t come to pass,N/A,https://schema.org,NewsArticle,https://www.theatlantic.com/technology/archive/2018/12/7-arguments-against-the-autonomous-vehicle-utopia/578638/,"[{'@type': 'ImageObject', 'width': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 720}, 'height': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 405}, 'url': 'https://cdn.theatlantic.com/thumbor/9brOwkirzkUT593ffcUU1_-V6rY=/0x388:5383x3416/720x405/media/img/mt/2018/12/RTX6G49X/original.jpg'}, {'@type': 'ImageObject', 'width': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 1080}, 'height': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 1080}, 'url': 'https://cdn.theatlantic.com/thumbor/ifbMIkSBanvsazkE9EdUY6c6CA0=/1012x0:4601x3589/1080x1080/media/img/mt/2018/12/RTX6G49X/original.jpg'}, {'@type': 'ImageObject', 'width': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 1200}, 'height': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 900}, 'url': 'https://cdn.theatlantic.com/thumbor/Yk5SCqPRjl5YZ7gFeEVCuVDua8U=/297x0:5082x3589/1200x900/media/img/mt/2018/12/RTX6G49X/original.jpg'}, {'@type': 'ImageObject', 'width': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 1600}, 'height': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 900}, 'url': 'https://cdn.theatlantic.com/thumbor/_KkQYWCSUwfKW6S_47uyz_vjaJM=/0x388:5383x3416/1600x900/media/img/mt/2018/12/RTX6G49X/original.jpg'}, {'@type': 'ImageObject', 'width': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 960}, 'height': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 540}, 'url': 'https://cdn.theatlantic.com/thumbor/GoWV0SR7-FujzmYO_SnPiCTKStc=/0x388:5383x3416/960x540/media/img/mt/2018/12/RTX6G49X/original.jpg'}, {'@type': 'ImageObject', 'width': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 540}, 'height': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 540}, 'url': 'https://cdn.theatlantic.com/thumbor/jDPgJeFagh4TjlCRR5R9WNpwrXs=/1012x0:4601x3589/540x540/media/img/mt/2018/12/RTX6G49X/original.jpg'}]","[{'@type': 'Person', 'name': 'Alexis C. Madrigal', 'sameAs': 'https://www.theatlantic.com/author/alexis-madrigal/'}]",{'@id': 'https://www.theatlantic.com/#publisher'},Seven Arguments Against the Autonomous-Vehicle Utopia,2018-12-20T17:14:56Z,2023-08-08T22:57:16Z,Technology,The Atlantic,False,,Technology,N/A,"TechnologySeven Arguments Against the Autonomous-Vehicle UtopiaAll the ways the self-driving future won’t come to passBy Alexis C. MadrigalA Neolix self-driving vehicle, seen at the IEEV New Energy Vehicles Exhibition in Beijing (Thomas Peter / Reuters)December 20, 2018ShareSave Self-driving cars are coming. Tech giants such as Uber and Alphabet have bet on it, as have old-school car manufacturers such as Ford and General Motors. But even as Google’s sister company Waymo prepares to launch its self-driving-car service and automakers prototype vehicles with various levels of artificial intelligence, there are some who believe that the autonomous future has been oversold—that even if driverless cars are coming, it won’t be as fast, or as smooth, as we’ve been led to think. The skeptics come from different disciplines inside and out of the technology and automotive industries, and each has a different bear case against self-driving cars. Add them up and you have a guide to all the ways our autonomous future might not materialize.Bear Case 1: They Won’t Work Until Cars Are as Smart as HumansComputers have nowhere near human intelligence. On individual tasks, such as playing Go or identifying some objects in a picture, they can outperform humans, but that skill does not generalize. Proponents of autonomous cars tend to see driving as more like Go: a task that can be accomplished with a far-lower-than-human understanding of the world. But in a duo of essays in 2017, Rodney Brooks, a legendary roboticist and artificial-intelligence researcher who directed the MIT Computer Science and Artificial Intelligence Laboratory for a decade, argued against the short-term viability of self-driving cars based on the sheer number of “edge cases,” i.e., unusual circumstances, they’d have to handle.To read this story, Sign in or start a subscription.CloseNever miss a story. Start your subscription.Uncompromising quality. Enduring impact. Your support ensures a bright future for independent journalism.Get StartedAlready have an account? Sign inAlexis Madrigal is a contributing writer at The Atlantic and the host of KQED’s Forum.","['https://www.facebook.com/TheAtlantic', 'https://twitter.com/theatlantic']",,"{'@type': 'SearchAction', 'target': 'https://www.theatlantic.com/search/?q={q}', 'query-input': 'required name=q'}",,,,en-US,,,https://www.theatlantic.com/#publisher,"{'@type': 'ImageObject', 'width': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 224}, 'height': {'@type': 'QuantitativeValue', 'unitCode': 'E37', 'value': 224}, 'url': 'https://cdn.theatlantic.com/assets/media/files/atlantic-logo--224x224.png'}",,"{'@type': 'WebPage', '@id': 'https://www.theatlantic.com/technology/archive/2018/12/7-arguments-against-the-autonomous-vehicle-utopia/578638/'}",,,,,,,Seven Arguments Against the Autonomous-Vehicle Utopia,,,,1072-7825,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.article-content-body'}"
https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vd3d3LmFybXkubWlsL2FydGljbGUvMjE1MjkxL2JsYWNrX2hhd2tfaGVsaWNvcHRlcl9waWxvdF9pbnRlcm5zX3dpdGhfYXJteV9yZXNlYXJjaGVyc9IBYWh0dHBzOi8vd3d3LmFybXkubWlsL2FydGljbGUtYW1wLzIxNTI5MS9ibGFja19oYXdrX2hlbGljb3B0ZXJfcGlsb3RfaW50ZXJuc193aXRoX2FybXlfcmVzZWFyY2hlcnM?oc=5,Black Hawk helicopter pilot interns with Army researchers - United States Army,2018-12-19,United States Army,https://www.army.mil,"Researchers from the U.S. Army Research, Development, and Engineering Command Research Laboratory, the Army's corporate research laboratory, recently partnered with Texas A&M University to work on artificial intelligence and machine learning as appli...",N/A,"Researchers from the U.S. Army Research, Development, and Engineering Command Research Laboratory, the Army's corporate research laboratory, recently partnered with Texas A&M University to work on artificial intelligence and machine learning as appli...","Researchers from the U.S. Army Research, Development, and Engineering Command Research Laboratory, the Army's corporate research laboratory, recently partnered with Texas A&M University to work on artificial intelligence and machine learning as appli...",,,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,
