URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,article:section,article:summary,article text,@type,@context,name,url,logo,sameAs,mainEntityOfPage,headline,datePublished,dateModified,thumbnailUrl,articleSection,publisher,author,image,itemListElement,isAccessibleForFree,@graph,creator,dateCreated,hasPart,speakable
https://news.google.com/rss/articles/CBMigwFodHRwczovL2Jsb2dzLmxzZS5hYy51ay9idXNpbmVzc3Jldmlldy8yMDE5LzA1LzEwL2h1bWFuLXBsdXMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtdGhlLWZ1dHVyZS1vZi13b3JrLWluLXRoZS1pbnZlc3RtZW50LWluZHVzdHJ5L9IBAA?oc=5,'Human plus artificial' intelligence: the future of work in the investment industry - LSE Home,2019-05-10,LSE Home,https://blogs.lse.ac.uk,N/A,lse authors,"What is the future of work in the investment industry? How do we, as providers of human capital prepare for the evolution of this profession? What does it mean for employers seeking to engage and m…",N/A,N/A,N/A,"






Lutfey Siddiqi
May 10th, 2019

‘Human plus artificial’ intelligence: the future of work in the investment industry


0 comments
          | 3 shares
      
Estimated reading time: 5 minutes







Lutfey Siddiqi
May 10th, 2019


‘Human plus artificial’ intelligence: the future of work in the investment industry


0 comments
          | 3 shares
      
Estimated reading time: 5 minutes












































      3
      Shares






What is the future of work in the investment industry? How do we, as providers of human capital prepare for the evolution of this profession? What does it mean for employers seeking to engage and motivate staff over the long term?
It is widely accepted that, like most other industries, the financial services industry is in a state of flux. Unlike other episodes of change in recent decades, the current context is often characterised as an “industrial revolution” creating continual disruption of a structural nature.
Prepare not predict
On that backdrop, the prudent approach is to prepare, not predict. Instead of making point-predictions about the future state of the industry, a handful of thematic narratives might be useful as navigational guide-posts.

For example, it is safe to assume that technology disruption (“fintech”) will challenge virtually every aspect of the existing service delivery model.
Secondly, changes in the delivery model will likely take place at an uneven pace across the geographies of developed and emerging markets.
Thirdly, interest rates are likely to remain lower for longer. The benchmark ten-year US treasury rate, which used to average close to six per cent before the global financial crisis, now struggles to sustain above three per cent.
And finally, there is a secular drive towards a more purposeful capitalism, requiring the investment profession to be demonstrably more client-centric and proficient about sustainability factors.

Who do we mean when we refer to the investment professional?
There are at least three categories of roles in the value-chain. There are “asset owners” who act as principal fiduciary investors of capital. Secondly, there are “asset managers” who are fiduciary investors as agents. Thirdly, there are specialist intermediaries such as investment bankers, traders and sell-side analysts who provide investment products and services. It helps to zoom out and think of almost every role at this level of generality as the specific bundle of tasks and boundaries between them will shift. Ultimately, everyone plays a part in either asset allocation or execution.
Increasingly competitive
Looking to the future, the first observation is that the job environment is getting competitive at an increasingly rapid pace. The pool of candidates for any job — many of them armed with credentials such as a CFA charter — is now truly global. CFA exam candidates have grown at an annual rate of 18 per cent over the past three years (faster in Asia-Pacific) with a current stock of candidates at over 300,000. At the same time, the required headcount for existing roles in existing firms will likely shrink as a result of technology.
In addition, there is a relentless downward pressure on fees. Consolidation by firms attempting to lower costs may negatively impact headcount. Fifty four per cent of respondents to a CFA survey last year cited “fee pressure” or “switching to lower-fee products” as the number one issue facing investment firms in the next five to ten years. As a result, individuals have to choose between competing in intensely contested “red oceans” or prepare for less contested “blue oceans”.
HI + AI: ‘human plus artificial’ intelligence 
These blue ocean roles are likely to straddle human plus artificial intelligence. In these “HI + AI” setups, human heuristics would benefit from the multiplier effect of data analytics. Within firms, investment and technology teams (that previously worked in silos) would collaborate to enhance performance.
In the early days of electronic foreign exchange dealing, online trading venues were seen as an alternative to the phone. Instead of calling her dealer, the asset manager could hit a button to execute her trade. The trade was still processed and risk-managed by a traditional (human) trader. In recent years, online trading venues have become an alternative to the trader. Once the asset manager hits the button to execute, the trade is processed largely by algorithms. The exact boundary between human and machine evolved gradually and through trial and error over almost a decade. Unfortunately, the current context of cost pressure and rapid technological progress will not allow the luxury of finding the right handover point at an organic pace. Today, the drive for organisational efficiency leaves less room for experimentation, and optionality to see how things pan out.
This will place the burden of adaptation disproportionately on the employee.
The growth of machine learning, AI methods and use of alternative data for portfolio construction will increasingly require ambidextrous skills – technical and traditional – to be deployed side by side. The question for the investment professional is not whether she can hand over tasks to robots but whether she can interact with robots through feedback loops. Traders of illiquid asset-classes will need to combine intuitive intervention with auto-pilot technology. The human elements of ethical orientation, communication, empathy, tacit knowledge etc. will remain relevant. However, the traditional bifurcation of “relationship roles” from “technical roles” will probably not survive.
A ‘Marina Bay Sands’-shaped skills profile
We cannot rely on any one skill for ever. Gone are the days when a person who started as a trader of precious metals could retire in the same role thirty years later. What is required is a T-shaped profile of skills where core expertise in one domain is supplemented by a breadth of knowledge over a wide range. Perhaps more than a T-shape, what is required is a “Marina Bay Sands” -shaped skills profile that rests on more than one core domain!
Marina Bay Sands (Singapore) is an integrated resort, casino, shopping mall and museum designed by Israeli-Canadian architect Moshe Safdie. Photo by Michaela Loheit, under a CC-BY-ND-2.0 licence
Skills acquisition will need to be approached in a similar manner to strategic asset allocation across a basket of skills with the ability to tactically adjust between them. The range and combination of possible skills is vast: from soft skills, design-thinking and systems-thinking to data interpretation, data visualisation, factor investing, portfolio risk optimisation, sustainability, governance issues and alternative investments.
Connectors, communicators and collaborators will command a premium. Those who are able to bridge traditional divides of offline versus online, quantitative versus relationship savvy, public versus private markets, profitability metrics versus sustainability and stewardship goals, will differentiate themselves.
T-shaped professionals will also enable firms to benefit from diversity. Diverse teams not only facilitate the uncovering of novel sources of value-creation, they also help thwart risk events lurking in potential blind-spots.
However, diversity can remain dormant and un-utilised if inclusion is not actively pursued through organisational design. It is the depth and range of abilities amongst professional staff that determine the extent to which diversity can translate into collective, firm-level intelligence. This calls for more T-shaped professionals who have the ability to forge connections, gain understanding and form perspective. They will also need to deal with complexity, ambiguity and incessant change without getting motion sickness.
From the employer’s perspective, it is important to deliberately foster a culture of collaboration. Teams of IT specialists and relationship managers may have different styles of work, terminology and rhythm. In the absence of deliberate processes that facilitate collaboration, including the tone at the top, firms can easily end up with coordination failures. This can be particularly challenging if the legacy culture relied on “star performers” and compensation plans incentivised lone wolfs. It is often seen that an accent on diversity – gender, ethnic and age diversity – can become a source of positive organisational culture overall.
A career flywheel
From an employee’s perspective, career management requires an explicit strategy for skills management. It is helpful to think of it as a “career flywheel”. Popularised by his book “Good to Great”, Jim Collins describes the flywheel effect as a process of incremental transition as opposed to a single defining action. That process resembles relentlessly pushing a giant heavy flywheel turn upon turn, building momentum towards points of breakthrough and beyond. The investment professional of the future will need to plan her career like the turns of a flywheel, with intentionality and additionality. Their explicit personal development plan will need to be agreed openly with the employer.
From an employer’s perspective, career growth must allow for horizontal moves across functions and locations, not just progression on a vertical ladder. This will not only help with employee engagement, it will also enrich the set of experiences that the employee can bring to bear on the inherently volatile nature of the job.
Finally, employee engagement and motivation will be sustained by non-monetary factors that include not just learning opportunities or flexible work practices but also a direct link to organisational purpose and impact on society. Purpose needs to be felt through the rump of the organisation – beyond just the CSR department – with consistency between words and action. Communication must feel authentic and not be laden with jargon.
♣♣♣
Notes:

This blog post appeared first on The Business Times, Singapore. It is based on work done by CFA Future of Finance, where the author is a member of the content council. 
The post gives the views of its author, not the position of LSE Business Review or the London School of Economics.
Featured image by geralt, under a Pixabay licence
When you leave a comment, you’re agreeing to our Comment Policy.


Lutfey Siddiqi is a visiting professor-in-practice at LSE’s Centre for International Studies. He is also an adjunct professor at the National University of Singapore’s Risk Management Institute (RMI), having been a part-time member of the faculty since its inception. Previously a managing director and member of the global executive committee for foreign exchange, rates and credit at UBS investment bank, Lutfey is a member of the World Economic Forum Council on Financing and Infrastructure, the Bretton Woods Committee, the advisory board of the Systemic Risk Centre at LSE and of the Official Monetary and Financial Institutions Forum (OMFIF), the forum for central banks.
 
 
 
 
Print Friendly


About the author



Lutfey Siddiqi


Lutfey Siddiqi is a visiting professor-in-practice at LSE (IDEAS) and an adjunct professor at the National University of Singapore (Risk Management Institute). He was previously global head of emerging markets for foreign exchange, rates and credit at UBS investment bank and a member of World Economic Forum global future councils. He is a member of the advisory boards of LSE’s Systemic Risk Centre, LSE's The Inclusion Initiative, and NUS Centre for Governance and Sustainability (CGS), Singapore.



Posted In: 
              LSE Authors
                          




Leave a Reply Cancel reply 
Your email address will not be published. Required fields are marked *Comment Name * 
Email * 
Website 
 

 

This site uses Akismet to reduce spam. Learn how your comment data is processed. 






Related Posts






LSE Authors



            Taking companies to court over climate change: who is being targeted?
                      
May 3rd, 2022









Economics and Finance



            The $468 trillion climate question: how the financial system is starting to shift
                      
November 14th, 2022









LSE Authors



            How the law profession adjusts to competitive changes in the UK
                      
March 21st, 2018









Economics and Finance



            Regulating mobile money: what’s at stake
                      
March 25th, 2019








",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMimgFodHRwczovL3d3dy5zdGFuZGFyZC5jby51ay9mdXR1cmVsb25kb24vc2tpbGxzL3JveWFsLWNvbGxlZ2Utb2YtYXJ0LWxvZ2l0ZWNoLXByb2Zlc3Nvci1wYXVsLWFuZGVyc29uLWluZHVzdHJpYWwtZGVzaWduLWZhc2hpb24tdGVjaG5vbG9neS1haS1hNDEzNjk2Ni5odG1s0gEA?oc=5,"How AI is changing the way we think, work and design - Evening Standard",2019-05-09,Evening Standard,https://www.standard.co.uk,"Design courses need to be broader to develop the skills to shape the products and experiences of the future, according to figures at the Royal College of Art and Logitech.",future london skills,"Design courses need to be broader to develop the skills to shape the products and experiences of the future, according to figures at the Royal College of Art and Logitech.","Design courses need to be broader to develop the skills to shape the products and experiences of the future, according to figures at the Royal College of Art and Logitech.",Future London,N/A,"Future London | SkillsAI meaning: How artificial intelligence, machine learning and technology is changing the way we think, work and design









In association withRoyal College of Art students will be taking part in the Grand Challenge, which sees students embark on a six week project to design products to enhance human performance with help from the design team at LogitechRichard Haughton, Royal College of ArtSian Bayley9 May 2019Design courses need to be broader to develop the skills to shape the products and experiences of the future, according to figures at the Royal College of Art and Logitech.Speaking ahead of the launch of the college’s Grand Challenge, which sees students embark on a six week project to design products to enhance human performance with help from the design team at Logitech, Professor Paul Anderson, Dean of the School of Design said: “I do believe that we need to develop some new courses which are asking some really difficult questions which are moving beyond the traditional disciplines. And these new disciplines may have different names, and are not just in one design domain.”Alastair Curtis, Logitech’s chief design officer and RCA graduate, added: “I think we’re at a tipping point as far as what is a designer today, and we’re at a tipping point of how we are educated to become designers. Historically it has always been about selecting your specialist skill from industrial design or fashion, and then you would learn that skill and you may broaden out. And now I think we’re beginning to flip the way that we think about creativity and developing creativity.Read more
AI and mental health - can our happiness at work be improved?
“We’re spending more time being super broad because there’s so much as far as the technology, the art, the ethics takes us, to create more balanced individuals first and then grow their specific capability.”Speaking about the future of product design, Curtis said: “We’re here to provoke the students of what is extending human capability now, or five, ten, fifteen years from now knowing that the influence of AI is going to change the way we think, we work and we design.“As a company we think how we can evolve our products and what they do to actually enhance people’s capability. It might sound super mundane, but even if you look at video collaboration equipment that we make today, where you come into the room, try and work out how to use the machine, waste 20 minutes, even if you can eliminate all those pain points, then you’ve improved the meeting capability.“All of these things will improve our capability within the office environment. It doesn’t always have to be about running 100 meters faster.“Even though I graduated a long time ago and there’s been hundreds of thousands of students who have come after me, fundamentally most designers, one of their higher goals is to improve the way we live.”However, Professor Anderson warned of the potential ethical dangers of development: “The whole world has changed and it’s changing more and more rapidly.“I think the students are now much more aware of the consequences of their actions, so their ability to actually understand a product not just as a design to produce, create and manufacture but what happens throughout its lifecycle.“There are a lot of big issues and questions around why we should do something, because we need to do things as we go forward in the future that have a better impact than what’s been going on over the last 50 years.“Just the invention of plastic, for example, has had a massive impact and I think we’re all feeling it, as human beings around the world as we’re starting to see the impact. What we’re putting into the air, the effects of climate change and all the rest of it, that’s where design in many ways is kind of responsible.“It’s taking up the mantra, and asking to really think about the choice of materials – we’ll still end up with products, and rightly so, but I think they’ll be better designed, more responsive to our needs and maybe they’ll be more adaptable.”MORE ABOUTfuture london skillsMost Read
1



World


Bangkok: Six foreign nationals dead after 'poisoning' at luxury Grand Hyatt hotel in Thailand's capital



2



Football


Transfer news LIVE! Chelsea eye fresh Osimhen move; Arsenal suffer Merino blow; Yoro to Man Utd bid accepted



3



London


London weather: 30C heat and sunshine on its way as capital set to be hotter than Ibiza



4



Football


Arsenal braced for fresh Emile Smith Rowe bid as Crystal Palace plot move



5



TV


Jeremy Clarkson to end working partnership with Top Gear and Grand Tour co-stars James May and Richard Hammond



Sponsored Features




Meet the 12 winners of our A Lifetime of Smiles competition





An expert guide to Irish single malt whiskey





How to enjoy the perfect British summer break with your pets





Essential summer care tips to help keep your cats & dogs safe





Wolfpack Lager founders share tips for business success



Voucher Codes£300 off a 7 night stay with the deal of the week - TUI discount code10% off first orders £20+ using this ASOS promo code5% off all bookings with this Travelodge discount codeSave 20% on orders with The Body Shop discount codeSave 15% on everything with birthday orders at The Perfume Shop - Member exclusive",BreadcrumbList,https://schema.org,Evening Standard,https://www.standard.co.uk/futurelondon/skills/royal-college-of-art-logitech-professor-paul-anderson-industrial-design-fashion-technology-ai-a4136966.html,"{'@type': 'ImageObject', 'url': 'https://www.standard.co.uk/img/logo.png', 'width': 600, 'height': 121}","['https://twitter.com/standardnews', '165348596842143']","{'@type': 'WebPage', '@id': 'https://www.standard.co.uk/futurelondon/skills/royal-college-of-art-logitech-professor-paul-anderson-industrial-design-fashion-technology-ai-a4136966.html'}","AI meaning: How artificial intelligence, machine learning and technology is changing the way we think, work and design | London Evening Standard",2019-05-09T15:51:12.000Z,2019-05-09T15:51:12.000Z,"['https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200', 'https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200&height=630&fit=crop', 'https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200&height=900&fit=crop', 'https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200&height=1200&fit=crop']",Skills,"{'@type': 'Organization', '@context': 'https://schema.org', 'name': 'Evening Standard', 'url': 'https://www.standard.co.uk', 'logo': {'@type': 'ImageObject', 'url': 'https://www.standard.co.uk/img/logo.png', 'width': 600, 'height': 121}, 'sameAs': ['https://twitter.com/standardnews', '165348596842143']}","{'@type': 'Person', 'name': 'Sian Bayley', 'url': 'https://www.standard.co.uk/author/sian-bayley'}","['https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200', 'https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200&height=630&fit=crop', 'https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200&height=900&fit=crop', 'https://static.standard.co.uk/s3fs-public/thumbnails/image/2019/05/08/11/rca-5-18-402-a4-rgb.jpg?width=1200&height=1200&fit=crop']","[{'@type': 'ListItem', 'position': 1, 'name': 'Future London', 'item': 'https://www.standard.co.uk/futurelondon'}, {'@type': 'ListItem', 'position': 2, 'name': 'Skills', 'item': 'https://www.standard.co.uk/futurelondon/skills'}]",,,,,,
https://news.google.com/rss/articles/CBMihAFodHRwczovL3d3dy5mb3JiZXMuY29tL3NpdGVzL2Jlcm5hcmRtYXJyLzIwMTkvMDUvMDcvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtaXMtY3JlYXRpbmctYS1mYWtlLXdvcmxkLXdoYXQtZG9lcy10aGF0LW1lYW4tZm9yLWh1bWFucy_SAQA?oc=5,Artificial Intelligence Is Creating A Fake World -- What Does That Mean For Humans? - Forbes,2019-05-07,Forbes,https://www.forbes.com,"As artificial intelligence continues to advance, the line between real and artificial is getting more blurred. Many applications are awe-inspiring or for entertainment, but when photos, audio, text, and video is so real, you can't tell if it's artificial, it can also be used for nefarious purposes.",,"As artificial intelligence continues to advance, the line between real and artificial is getting more blurred. Many applications are awe-inspiring or for entertainment, but when photos, audio, text, and video is so real, you can't tell if it's artificial, it can also be used for nefarious purposes.","As artificial intelligence continues to advance, the line between real and artificial is getting more blurred. Many applications are awe-inspiring or for entertainment, but when photos, audio, text, and video is so real, you can't tell if it's artificial, it can also be used for nefarious purposes.",Enterprise & Cloud,N/A,"More From ForbesJul 8, 2024,09:00am EDTSee The Future Data Center At The Israeli Quantum Computing CenterJun 30, 2024,09:00am EDTWar Can’t Stop Israeli Startups Determined To Thrive In The DesertJun 6, 2024,06:00am EDTIsraeli Startup Combines Software With Medicine To Transform $1.6 Trillion Pharma MarketMay 22, 2024,12:18pm EDT$20M Fund For Connecting Early-Stage Israeli Startups To New York CityMay 20, 2024,09:00am EDTBiomed 2024 Showcases Israel’s Resilient Entrepreneurial SpiritApr 30, 2024,09:00am EDTAI Is Moving Biology From Science To Engineering, Advancing MedicineApr 10, 2024,09:00am EDTThis Startup Wants To Be OpenAI Of Stem Cell Therapy, Targets $250B MarketEdit StoryInnovationEnterprise TechArtificial Intelligence Is Creating A Fake World -- What Does That Mean For Humans?Bernard MarrContributorOpinions expressed by Forbes Contributors are their own.FollowingFollowMay 7, 2019,12:23am EDTUpdated May 14, 2019, 05:14pm EDTThis article is more than 5 years old.Share to FacebookShare to TwitterShare to LinkedinFollow me on Twitter or LinkedIn. Check out my website or some of my other work here. Bernard MarrFollowingFollowBernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is... Read MoreEditorial StandardsPrintReprints & Permissions",BreadcrumbList,http://schema.org,Artificial Intelligence Is Creating A Fake World -- What Does That Mean For Humans?,https://www.forbes.com/sites/bernardmarr/2019/05/07/artificial-intelligence-is-creating-a-fake-world-what-does-that-mean-for-humans/,,,,Artificial Intelligence Is Creating A Fake World -- What Does That Mean For Humans?,2019-05-07T00:23:00-04:00,2019-05-14T17:14:33-04:00,,Enterprise & Cloud,"{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}","{'@type': 'Person', 'name': 'Bernard Marr', 'url': 'https://www.forbes.com/sites/bernardmarr/', 'description': 'Bernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society. He has written over 20 best-selling and award-winning books and advises and coaches many of the world’s best-known organisations. He has a combined following of 4 million people across his social media channels and newsletters and was ranked by LinkedIn as one of the top 5 business influencers in the world. Follow Bernard on LinkedIn, X (Twitter) or YouTube. Join his newsletter, check out his website and books.', 'sameAs': ['https://www.linkedin.com/in/bernardmarr/', 'https://www.twitter.com/BernardMarr', 'https://bernardmarr.com/']}","{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/blogs-images/bernardmarr/files/2019/05/Artificial-Intelligence-Is-Creating-A-Fake-World-What-Does-That-Mean-For-Humans-1200x668.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Enterprise Tech', 'item': 'https://www.forbes.com/enterprise-tech/'}]",False,,,,,
https://news.google.com/rss/articles/CBMic2h0dHBzOi8vdGhlY29udmVyc2F0aW9uLmNvbS9ob3ctdG8tcHJlcGFyZS1zdHVkZW50cy1mb3ItdGhlLXJpc2Utb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtaW4tdGhlLXdvcmtmb3JjZS0xMTM3OTfSAQA?oc=5,How to prepare students for the rise of artificial intelligence in the workforce - The Conversation,2019-05-08,The Conversation,https://theconversation.com,A shift to outcomes-based education will enable students to gain critical automation-resistant competencies to succeed and thrive in the future workforce alongside AI.,N/A,A shift to outcomes-based education will enable students to gain critical automation-resistant competencies to succeed and thrive in the future workforce alongside AI.,N/A,N/A,N/A,"






        Social and cognitive skills such as drawing conclusions about emotional states and social interactions are least vulnerable to being displaced by AI.
        (Shutterstock)









            How to prepare students for the rise of artificial intelligence in the workforce
          




Published: May 8, 2019 6:34pm EDT












Greg Naterer, Memorial University of Newfoundland



Author





        Greg Naterer
      


      Dean and Professor, Faculty of Engineering and Applied Science, Memorial University of Newfoundland
    





Disclosure statement
Greg Naterer works for Memorial University of Newfoundland. He receives funding from the Natural Sciences and Engineering Research Council of Canada. 


Partners

Memorial University of Newfoundland provides funding as a founding partner of The Conversation CA.Memorial University of Newfoundland provides funding as a member of The Conversation CA-FR.
View all partners

We believe in the free flow of informationRepublish our articles for free, online or in print, under a Creative Commons license.Republish this article





 Email


 X (Twitter)162


 Facebook1.1k


 LinkedIn


 WhatsApp


 Messenger

 Print


The future impacts of artificial intelligence (AI) on society and the labour force have been studied and reported extensively. In a recent book, AI Superpowers, Kai-Fu Lee, former president of Google China, wrote that 40 to 50 per cent of current jobs will be technically and economically viable with AI and automation over the next 15 years. 
Artificial intelligence refers to computer systems that collect, interpret and learn from external data to achieve specific goals and tasks. Unlike natural intelligence displayed by humans and animals, it is an artificial form of intelligence demonstrated by machines. This has raised questions about the ethics of AI decision-making and impacts of AI in the workplace.
With computing power increasing rapidly in recent decades, the capabilities of AI have also risen dramatically. Vincent Müller, a philosopher at Eindhoven University of Technology, and Nick Bostrom, a philosopher at Oxford University, conducted a survey in 2016 about AI’s future potential.
Respondents indicated a 50 per cent likelihood that the capabilities of AI will exceed human intelligence by 2040 to 2050. Other technology leaders have predicted this will occur much earlier. Since AI continually learns and improves, a new form of AI super-intelligence may emerge well beyond human intelligence. 
How are universities responding to this challenge? Do traditional subjects and competencies taught in university need to be re-examined in view of the coming disruption of AI in the job market? 
As the Dean of Engineering and Applied Science at Memorial University, I’m concerned about this disruption from the perspective of skills that students should be learning to successfully adapt to AI in the workplace.
‘Resilient competencies’
Recently I supervised a project conducted by two undergraduate research assistants, Joud Omary, a computer engineering student, and Deep Patel, an electrical engineering student, on the susceptibility of various graduate attributes to computerisation. They analysed the probabilities of various student competencies becoming automated in the next 10 to 20 years.
The most automation-resistant skills were determined based on a Brookfield Institute report which examined the probabilities of automation of work tasks over a range of occupations associated with university degrees. 
Repetitive skills like pattern recognition, information retrieval, optimization and planning are most vulnerable to automation. On the other hand, social and cognitive skills such as creativity, problem-solving, drawing conclusions about emotional states and social interactions are least vulnerable.
The most resilient competencies (those least likely to be displaced by AI) included critical thinking, teamwork, interpersonal skills, leadership and entrepreneurship.
Yuval Harari, a historian at the Hebrew University of Jerusalem, described the rise of AI as a “cascade of ever-bigger disruptions” in higher education rather than a single event that settles into a new equilibrium. The unknown paths taken by AI will make it increasingly difficult to know what to teach students. 
Economist Carl Frey, and engineer Michael Osborne, both at the University of Oxford, reported the susceptibility of a range of professions to computerisation including those associated with traditional university degrees, e.g., accountants, auditors, geoscientists. Interestingly, even for engineers who are significant developers of AI technologies, there is a susceptibility of various disciplines of engineering to computerisation.
In such a context, “resilient competencies” are always relevant. Joseph Aoun, president of Northeastern University, and a linguist, argues in his book that what will matter most is experiential learning (co-op education), life-long learning and a curriculum focused on humanics (the study of human affairs). 
As AI technologies become more powerful and capable over a range of professions, it will become increasingly important for today’s students to be equipped with the right skills that add value beyond what AI can achieve. As AI displaces old jobs, it will also lead to new jobs.
‘Outcomes-based education’
Traditional learning outcomes in engineering programs have included a strong knowledge base, problem analysis, design and the use of engineering tools, among others. 
But engineers today have a growing diversity of demands in their professional lives. Non-technical skills are increasingly important to work effectively in a business environment. These include communication skills, project management, life-long learning and the interdisciplinary impact of engineering on society and the environment.



The Virtual Marine Simulation Laboratory at Memorial University allows marine sectors to use AI-rendered scenarios for research and training.
(Greg Naterer), Author provided


At Memorial University’s Faculty of Engineering and Applied Science, one way that we have responded to this changing environment is by mapping the learning outcomes throughout the curriculum to ensure that all graduates of our co-op programs have a desired set of graduate attributes.  
For example, a traditional fluid mechanics course measured only the technical skills of the subject matter. Today, the course tracks a range of other learning outcomes such as communication skills through written reports and presentations. 
More broadly across the curriculum, it is determined where and how each graduate attribute is taught and evaluated for each course. The evaluation of each attribute is recorded, for example through a tutorial, exam question, or assignment. Appropriate rubrics are established for each attribute. Afterwards courses are evaluated and continuously improved to see how students are gaining the desired graduate attributes.
Engineering students are also required to take complementary studies courses in other disciplines, such as humanities, to raise their awareness of the role and impact of engineering on society and culture. 
Memorial’s Faculty of Humanities and Social Sciences has strong links with the human literacy areas noted by Aoun. Across a range of disciplines – from philosophy to political science, sociology to geography – students work in teams, reflect on challenging ethical questions and engage in dialogue about important public concerns. 
Also, Memorial’s Faculty of Business Administration introduces students to AI and provides exposure to a range of key non-technical business skills. To successfully manage organizations in which work processes will be redefined by AI, graduates will more than ever need to know how to build strong relationships, work in teams and communicate effectively. 
A shift in higher education from what students are taught in the classroom to learning outcomes instead and graduate attributes will become increasingly important with the rise of AI. A shift to outcomes-based education will enable students to gain critical automation-resistant competencies to succeed and thrive in the future workforce alongside AI.





Artificial intelligence (AI)


Universities


Engineering


Future of work


Automation


Post-secondary education


Engineers


engineering studies









",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiSmh0dHBzOi8vd3d3LmNoaWNhZ29ib290aC5lZHUvbWFnYXppbmUvd29ya2luZy13aXRoLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNl0gEA?oc=5,How Can Humans Work With Artificial Intelligence? | The University of Chicago Booth School of Business - The University of Chicago Booth School of Business,2019-05-09,The University of Chicago Booth School of Business,https://www.chicagobooth.edu,"We asked three Booth experts: alumnus Justin Adams, &rsquo;10, professor Nicholas Polson, and PhD candidate Diag Davenport.",N/A,"We asked three Booth experts: alumnus Justin Adams, ’10, professor Nicholas Polson, and PhD candidate Diag Davenport.","We asked three Booth experts: alumnus Justin Adams, ’10, professor Nicholas Polson, and PhD candidate Diag Davenport.",N/A,N/A,"






Illustration by Sam Peet



We asked three Booth experts: alumnus Justin Adams, ’10, professor Nicholas Polson, and PhD candidate Diag Davenport.



By             
Alice G. Walton



May 09, 2019    
Technology








",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiR2h0dHBzOi8vd3d3LmljdHdvcmtzLm9yZy9oeXBlci1wZXJzb25hbGl6YXRpb24tYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Uv0gEA?oc=5,The Hyper-Promise of Artificial Intelligence for Hyper-Personalization - Your Weekend Long Reads - ICTworks,2019-05-11,ICTworks,https://www.ictworks.org,ICT4D community should embrace artificial intelligence as personal in character and fundamentally changing international development and humanitarian relief,N/A,ICT4D community should embrace artificial intelligence as personal in character and fundamentally changing international development and humanitarian relief,N/A,N/A,N/A,N/A,,https://schema.org,,,,,,,,,,,,,,,,"[{'@type': 'Article', '@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/#article', 'isPartOf': {'@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/'}, 'author': {'name': 'Troy Etulain', '@id': 'https://www.ictworks.org/#/schema/person/fe3fd2314c884a8db9ffad33348e94fd'}, 'headline': 'The Hyper-Promise of Artificial Intelligence for Hyper-Personalization &#8211; Your Weekend Long Reads', 'datePublished': '2019-05-11T04:36:28+00:00', 'dateModified': '2019-07-18T22:30:50+00:00', 'mainEntityOfPage': {'@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/'}, 'wordCount': 4057, 'publisher': {'@id': 'https://www.ictworks.org/#organization'}, 'image': {'@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://i0.wp.com/www.ictworks.org/wp-content/uploads/2019/05/artificial-intelligence.jpg?fit=640%2C429&ssl=1', 'keywords': ['Artificial Intelligence', 'Call Detail Records', 'Caribou Digital', 'Digital Impact Alliance', 'Hyper-Personalization', 'Troy Etulain'], 'articleSection': ['Data'], 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/', 'url': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/', 'name': 'The Hyper-Promise of Artificial Intelligence for Hyper-Personalization', 'isPartOf': {'@id': 'https://www.ictworks.org/#website'}, 'primaryImageOfPage': {'@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/#primaryimage'}, 'image': {'@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://i0.wp.com/www.ictworks.org/wp-content/uploads/2019/05/artificial-intelligence.jpg?fit=640%2C429&ssl=1', 'datePublished': '2019-05-11T04:36:28+00:00', 'dateModified': '2019-07-18T22:30:50+00:00', 'description': 'ICT4D community should embrace artificial intelligence as personal in character and fundamentally changing international development and humanitarian relief', 'breadcrumb': {'@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.ictworks.org/hyper-personalization-artificial-intelligence/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/#primaryimage', 'url': 'https://i0.wp.com/www.ictworks.org/wp-content/uploads/2019/05/artificial-intelligence.jpg?fit=640%2C429&ssl=1', 'contentUrl': 'https://i0.wp.com/www.ictworks.org/wp-content/uploads/2019/05/artificial-intelligence.jpg?fit=640%2C429&ssl=1', 'width': 640, 'height': 429, 'caption': 'artificial intelligence'}, {'@type': 'BreadcrumbList', '@id': 'https://www.ictworks.org/hyper-personalization-artificial-intelligence/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.ictworks.org/'}, {'@type': 'ListItem', 'position': 2, 'name': 'The Hyper-Promise of Artificial Intelligence for Hyper-Personalization &#8211; Your Weekend Long Reads'}]}, {'@type': 'WebSite', '@id': 'https://www.ictworks.org/#website', 'url': 'https://www.ictworks.org/', 'name': 'ICTworks', 'description': 'Expertise in sustainable ICT4D for the developing world', 'publisher': {'@id': 'https://www.ictworks.org/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.ictworks.org/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.ictworks.org/#organization', 'name': 'ICTworks', 'url': 'https://www.ictworks.org/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.ictworks.org/#/schema/logo/image/', 'url': 'https://i0.wp.com/www.ictworks.org/wp-content/uploads/2020/08/ictworks.png?fit=150%2C150&ssl=1', 'contentUrl': 'https://i0.wp.com/www.ictworks.org/wp-content/uploads/2020/08/ictworks.png?fit=150%2C150&ssl=1', 'width': 150, 'height': 150, 'caption': 'ICTworks'}, 'image': {'@id': 'https://www.ictworks.org/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/ICTworks/', 'https://x.com/ICT_works']}, {'@type': 'Person', '@id': 'https://www.ictworks.org/#/schema/person/fe3fd2314c884a8db9ffad33348e94fd', 'name': 'Troy Etulain', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.ictworks.org/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/a0c15ed50d6d84a95f0b20633bc47d47?s=96&d=monsterid&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/a0c15ed50d6d84a95f0b20633bc47d47?s=96&d=monsterid&r=g', 'caption': 'Troy Etulain'}, 'description': 'Troy Etulain is an international digital development expert. Previously, Troy worked with the ITU, FHI 360, UNHCR and USAID. He is passionate about connectivity, digital identities, and data for development.', 'url': 'https://www.ictworks.org/author/troyetulain/'}]",,,,
https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vd3d3LmJpb3NwYWNlLmNvbS9hcnRpY2xlL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWlzLXJhbXBpbmctdXAtaW4tZHJ1Zy1kZXZlbG9wbWVudC0v0gEA?oc=5,Artificial Intelligence is Ramping up in Drug Development - BioSpace,2019-05-07,BioSpace,https://www.biospace.com,"AstraZeneca announced a long-term collaboration deal with BenevolentAI, a UK-based company focused on combining computational medicine and advanced artificial intelligence.This deal only marks one of the most recent AI team-ups announced in the biopharma industry.",N/A,"AstraZeneca announced a long-term collaboration deal with BenevolentAI, a UK-based company focused on combining computational medicine and advanced artificial intelligence.This deal only marks one of the most recent AI team-ups announced in the biopharma industry.","AstraZeneca announced a long-term collaboration deal with BenevolentAI, a UK-based company focused on combining computational medicine and advanced artificial intelligence.This deal only marks one of the most recent AI team-ups announced in the biopharma industry.",Business,N/A,N/A,,http://schema.org,,,,,,,,,,,,,,,,"[{'@context': 'http://schema.org', '@type': 'Article', 'articleBody': ""AstraZeneca announced a long-term collaboration deal with BenevolentAI, a UK-based company focused on combining computational medicine and advanced artificial intelligence.This deal only marks one of the most recent AI team-ups announced in the biopharma industry. AstraZeneca announced a long-term collaboration deal with BenevolentAI, a UK-based company focused on combining computational medicine and advanced artificial intelligence. The two companies will focus on using AI and machine learning to discover and develop new drugs for chronic kidney disease (CKD) and idiopathic pulmonary fibrosis (IPF). “The vast amount of data available to research scientists is growing exponentially each year,” stated Mene Pangalo, AstraZeneca’s executive vice president and president BioPharmaceuticals R&D. “By combining AstraZeneca’s disease area expertise and large, diverse datasets with BenevolentAI’s leading AI and machine learning capabilities, we can unlock the potential of this wealth of data to improve our understanding of complex disease biology and identify new targets that could treat debilitating diseases.” No financial details were disclosed. BenevolentAI was founded in 2013. It has raised $202 million in funding since and has a research facility in Cambridge, UK. Earlier, the company inked an exclusive license deal in 2016 with Johnson & Johnson’s Janssen Pharmaceutical for several clinical-stage drug candidates. View-hotbeds.jpg This deal only marks one of the most recent AI team-ups announced in the biopharma industry. In early April, only a few weeks after Concerto HealthAI inked a deal with Bristol-Myers Squibb, it signed a similar deal with Pfizer. The commonality was Concerto focuses on oncology-specific Real-World Data (RWD) and advanced AI for Real-World Evidence (RWE) generation, an area of increasing interest for biopharma companies. Concerto HealthAI will collaborate with Pfizer on Precision Oncology using Concerto’s eurekaHealth platform, artificial intelligence (AI) models and Real World Clinical Electronic Medical Record (EMR) and healthcare claims. They will use data from clinical medical practices that participate in the American Society of Clinical Oncology’s CancerLinQ initiative and others throughout the U.S. Concerto has an exclusive license to utilize data from CancerLinQ, which might explain why big pharma companies are so eager to work with Concerto. But Concerto’s data access isn’t solely with CancerLinQ, but has access to numerous other real-world sources. In March 2019, Oxford BioMedica announced it had inked a two-year research-and-development collaboration with Microsoft Research. The goal is to improve the yield and quality of next-generation gene therapy vectors—typically viruses—using artificial intelligence (AI) and machine learning. Oxford Biomedica will focus on its expertise in vector development and large-scale manufacturing. The team in Microsoft’s Station B initiative will use AI and machine learning to increase the yield and improve the purity of Oxford Biomedica’s lentiviral vectors while cutting costs. Station B will use the Microsoft Azure intelligent cloud platform to analyze large data sets created by Oxford Biomedical and develop in silico models and novel algorithms to advance cell and gene delivery technology. The partnership will run two years and may be extended by either group. In 2018, CancerLinQ teamed with Concerto HealthAI and Tempus to speed its joint research effort with the U.S. Food and Drug Administration (FDA)’s Center for Drug Evaluation and Research (CDER) to understand real-world use, tolerability, and effectiveness of immune checkpoint inhibitors. Concerto HealthAI and Tempus started analyzing a de-identified data set that CancerLinQ gave to the FDA, which represented more than 10,000 patients treated with checkpoint inhibitors in both approved and unapproved indications. At the time, CancerLinQ’s chief executive officer, Cory Wiegert, stated, “These types of research efforts need to span across the cancer community because one organization cannot accomplish them alone. We are excited that Concerto HealthAI and Tempus are bringing their technical capacity and analytic expertise to this important effort in support of the FDA’s evaluation of these therapies outside of clinical trials.” That initiative is looking at patients who have received checkpoint inhibitors including Genentech’s Tecentriq (atezolizumab), EMD Serono and Pfizer’s Bavencio (avelumab), AstraZeneca’s Imfinzi (durvalumab), Bristol-Myers Squibb’s Yervoy (ipilimumab) and Opdivo (nivolumab), or Merck’s Keytruda (pembrolizumab). Iframe Embed And Google, as one of the biggest corporations on Earth, is very interested in the use of AI and health care. “The fundamental underlying technologies of machine learning and artificial intelligence are applicable to all manner of tasks,” Greg Corrado, a neuroscientist at Google, told NPR. And that’s true, “whether those are tasks in your daily life, like getting directions or sorting through emailing, or the kinds of tasks that doctors, nurses, clinicians and patients face every day.” It's probably not altruism that has brought Google and other tech companies to look at the life sciences and health care. John Moore, an industry analyst at Chilmark Research, told NPR, “It’s pretty hard to ignore a market that represents about 20 percent of [U.S.] GDP. So whether it’s Google or it’s Microsoft or it’s IBM or it’s Apple, everyone is taking a look at what they can do in the health care space.” Other companies are focused on AI and healthcare or drug development as well. Recursion Pharmaceuticals, headquartered in Salt Lake City, Utah, is a clinical-stage biotech that combines AI, experimental biology and automation to discover and develop drugs at scale. In January, Recursion announced progress in its collaboration with Takeda Pharmaceutical on identifying novel preclinical candidates for rare diseases. In 18 months, the partnership led to the evaluation of Takeda preclinical and clinical compounds in more than 60 unique indications. New therapeutic candidates were identified in more than six diseases. San Francisco-based Atomwise uses AI based on convolutional neural networks, the same tech used in facial recognition and self-driving cars, to search for drugs. In January Atomwise signed a strategic alliance with contract research organization (CRO) Charles River Laboratories International, to support CRL’s hit discovery, hit-to-lead, and lead optimization efforts."", 'articleSection': 'Business, News', 'author': [{'@context': 'http://schema.org', '@type': 'Person', '@id': 'https://www.biospace.com/mark-terry/#person', 'mainEntityOfPage': {'@type': 'WebPage', '@id': 'https://www.biospace.com/mark-terry/'}, 'name': 'Mark Terry', 'url': 'https://www.biospace.com/mark-terry'}], 'copyrightYear': '2024', 'datePublished': '2019-05-06T19:39:51.000Z', 'dateModified': '2019-05-07T15:25:53.000Z', 'headline': 'Artificial Intelligence is Ramping up in Drug Development', '@id': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development/#article', 'image': [{'@context': 'http://schema.org', '@type': 'ImageObject', 'width': 350, 'inLanguage': 'en-US', 'url': 'https://static.biospace.com/dims4/default/bdef53c/2147483647/strip/false/crop/625x350+0+0/resize/625x350!/quality/90/?url=https%3A%2F%2Fk1-prod-biospace.s3.us-east-2.amazonaws.com%2Fbrightspot%2Flegacy%2FBioSpace-Assets%2F02B741A4-8EA3-4A91-8BC8-CA44F91332AB.jpg', 'height': 625}, {'@context': 'http://schema.org', '@type': 'ImageObject', 'width': 675, 'inLanguage': 'en-US', 'url': 'https://static.biospace.com/dims4/default/e4002e4/2147483647/strip/false/crop/622x350+1+0/resize/1200x675!/quality/90/?url=https%3A%2F%2Fk1-prod-biospace.s3.us-east-2.amazonaws.com%2Fbrightspot%2Flegacy%2FBioSpace-Assets%2F02B741A4-8EA3-4A91-8BC8-CA44F91332AB.jpg', 'height': 1200}], 'inLanguage': 'en-US', 'mainEntityOfPage': {'@id': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development/#webpage'}, 'isPartOf': {'@id': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development/#webpage'}}, {'@context': 'http://schema.org', '@type': 'Organization'}, {'@context': 'http://schema.org', '@type': 'Website', 'inLanguage': 'en-US'}, {'@context': 'http://schema.org', '@type': 'WebPage', 'potentialAction': [{'@context': 'http://schema.org', '@type': 'ReadAction', 'target': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development'}], 'breadcrumb': {'@type': 'BreadcrumbList', 'itemListElement': [{'@context': 'http://schema.org', '@type': 'ListItem', 'item': 'https://www.biospace.com/news', 'name': 'News', 'position': 1}, {'@context': 'http://schema.org', '@type': 'ListItem', 'item': 'https://www.biospace.com/business', 'name': 'Business', 'position': 2}]}, '@id': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development/#webpage', 'inLanguage': 'en-US', 'primaryImageOfPage': {'@id': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development/#primaryimage'}, 'name': 'Artificial Intelligence is Ramping up in Drug Development', 'datePublished': '2019-05-06T19:39:51.000Z', 'dateModified': '2019-05-07T15:25:53.000Z', 'url': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development'}, {'@context': 'http://schema.org', '@type': 'ImageObject', 'width': 675, '@id': 'https://www.biospace.com/artificial-intelligence-is-ramping-up-in-drug-development/#primaryimage', 'inLanguage': 'en-US', 'url': 'https://static.biospace.com/dims4/default/e4002e4/2147483647/strip/false/crop/622x350+1+0/resize/1200x675!/quality/90/?url=https%3A%2F%2Fk1-prod-biospace.s3.us-east-2.amazonaws.com%2Fbrightspot%2Flegacy%2FBioSpace-Assets%2F02B741A4-8EA3-4A91-8BC8-CA44F91332AB.jpg', 'height': 1200}]",,,,
https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vdGhlMWEub3JnL3NlZ21lbnRzLzIwMTktMDUtMDgtcHV0dGluZy10aGUtYXJ0LWluLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlL9IBAA?oc=5,Putting The 'Art' In Artificial Intelligence - the1a.org,2019-05-08,the1a.org,https://the1a.org,"AI paintings are a new — and lucrative — phenomenon in the art world. But what do they tell us about the nature of AI, and what it means to create something?","['Arts &amp; Culture', 'Media', 'Technology']","AI paintings are a new — and lucrative — phenomenon in the art world. But what do they tell us about the nature of AI, and what it means to create something?","AI paintings are a new — and lucrative — phenomenon in the art world. But what do they tell us about the nature of AI, and what it means to create something?",N/A,N/A,"Listen34:45A woman looks at a work of art created by an algorithm by French collective named OBVIOUS which produces art using artificial intelligence, titled “Portrait of Edmond de Belamy” at Christie’s in New York on October 22, 2018. 
TIMOTHY A. CLARY/AFP/Getty ImagesLast year, a painting called “Portrait of Edmond de Belamy” sold for $432,500 in New York City.
The catch? It was produced by artificial intelligence — one of the first of its kind to come up for auction. And it’s sparked a trend in the art world.
From The Atlantic’s Ian Bogost:
The best way to get away with something is to make it feel new and surprising. Using a computer is hardly enough anymore; today’s machines offer all kinds of ways to generate images that can be output, framed, displayed, and sold—from digital photography to artificial intelligence. Recently, the fashionable choice has become generative adversarial networks, or GANs, the technology that created “Portrait of Edmond de Belamy.” Like other machine-learning methods, GANs use a sample set—in this case, art, or at least images of it—to deduce patterns, and then they use that knowledge to create new pieces. A typical Renaissance portrait, for example, might be composed as a bust or three-quarter view of a subject. The computer may have no idea what a bust is, but if it sees enough of them, it might learn the pattern and try to replicate it in an image.
AI technology does a lot for us. It sorts emails, social media posts and dating app profiles. It queues albums upon request. It answers our questions online.
But what about art? Is AI art just “found art,” or is it something more? Can AI be creative? And if so, how might that redefine what it means to create something?
Show produced by Haili Blassingame. Text by Kathryn Fink.

GuestsMarcus du Sautoyauthor, The Creativity Code: Art and Innovation in the Age of AI; Professor of Mathematics at the University of Oxford.We depend on your support...DonateFiled Under: Arts & Culture, Media, Technology",NewsArticle,http://schema.org,,https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/,,,,Putting The &#039;Art&#039; In Artificial Intelligence | 1A,2019-05-08T11:25:07+00:00,2020-01-21T05:31:20+00:00,https://the1a.org/wp-content/uploads/sites/4/2019/05/gettyimages-1052751430-594x594-150x150.jpg,,"{'@type': 'Organization', 'name': 'WAMU 88.5 - American University Radio', 'logo': {'@type': 'ImageObject', 'url': 'https://wamu.org/wp-content/themes/wamu/images/wamu_logo_big.png'}}","['Haili Blassingame', 'Kathryn Fink']",https://the1a.org/wp-content/uploads/sites/4/2019/05/gettyimages-1052751430-594x594-150x150.jpg,,,"[{'@type': 'WebPage', '@id': 'https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/', 'url': 'https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/', 'name': ""Putting The 'Art' In Artificial Intelligence | 1A"", 'isPartOf': {'@id': 'https://the1a.org/#website'}, 'primaryImageOfPage': {'@id': 'https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/#primaryimage'}, 'image': {'@id': 'https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://the1a.org/wp-content/uploads/sites/4/2019/05/gettyimages-1052751430-594x594.jpg', 'datePublished': '2019-05-08T15:25:07+00:00', 'dateModified': '2020-01-21T05:31:20+00:00', 'breadcrumb': {'@id': 'https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/#primaryimage', 'url': 'https://the1a.org/wp-content/uploads/sites/4/2019/05/gettyimages-1052751430-594x594.jpg', 'contentUrl': 'https://the1a.org/wp-content/uploads/sites/4/2019/05/gettyimages-1052751430-594x594.jpg', 'width': 594, 'height': 408, 'caption': 'A woman looks at a work of art created by an algorithm by French collective named OBVIOUS which produces art using artificial intelligence, titled ""Portrait of Edmond de Belamy"" at Christie\'s in New York on October 22, 2018.'}, {'@type': 'BreadcrumbList', '@id': 'https://the1a.org/segments/2019-05-08-putting-the-art-in-artificial-intelligence/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://the1a.org/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Segments', 'item': 'https://the1a.org/segments/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Putting The &#8216;Art&#8217; In Artificial Intelligence'}]}, {'@type': 'WebSite', '@id': 'https://the1a.org/#website', 'url': 'https://the1a.org/', 'name': '1A', 'description': 'Speak Freely', 'publisher': {'@id': 'https://the1a.org/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://the1a.org/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://the1a.org/#organization', 'name': '1A', 'url': 'https://the1a.org/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://the1a.org/#/schema/logo/image/', 'url': 'https://the1a.org/wp-content/uploads/sites/4/2020/01/cropped-1A-logo-with-tags-1-1-300x223.png', 'contentUrl': 'https://the1a.org/wp-content/uploads/sites/4/2020/01/cropped-1A-logo-with-tags-1-1-300x223.png', 'width': 300, 'height': 223, 'caption': '1A'}, 'image': {'@id': 'https://the1a.org/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/the1ashow', 'https://www.instagram.com/the1ashow/', 'https://www.youtube.com/@the1ashow']}]","['Haili Blassingame', 'Kathryn Fink']",,,
https://news.google.com/rss/articles/CBMigwFodHRwczovL3RlY2hjcnVuY2guY29tLzIwMTkvMDUvMDcvZ29vZ2xlcy1wcm9qZWN0LWV1cGhvbmlhLXdhbnRzLXRvLW1ha2Utdm9pY2UtcmVjb2duaXRpb24td29yay1mb3ItcGVvcGxlLXdpdGgtc3BlZWNoLWltcGFpcm1lbnRzL9IBAA?oc=5,Google's Project Euphonia wants to make voice recognition work for people with speech impairments - TechCrunch,2019-05-07,TechCrunch,https://techcrunch.com,"For those with speech impairments, artificial intelligence-powered voice recognition technology simply doesn't work for them. Google is trying to fix",N/A,"For those with speech impairments, artificial intelligence-powered voice recognition technology simply doesn't work for them. Google is trying to fix",N/A,N/A,N/A,N/A,,https://schema.org,,,,,,,,,,,,,,,,"[{'@type': 'NewsArticle', '@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/#article', 'isPartOf': {'@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/'}, 'author': [{'@id': 'https://techcrunch.com/#/schema/person/image/9af7ae7b2b32df9983243761e1190224'}], 'headline': 'Google&#8217;s Project Euphonia wants to make voice recognition work for people with speech impairments', 'datePublished': '2019-05-07T18:18:34+00:00', 'dateModified': '2019-05-16T17:12:53+00:00', 'mainEntityOfPage': {'@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/'}, 'wordCount': 239, 'commentCount': 0, 'publisher': {'@id': 'https://techcrunch.com/#organization'}, 'image': {'@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/#primaryimage'}, 'thumbnailUrl': 'https://techcrunch.com/wp-content/uploads/2019/05/google-project-euphonia.jpg', 'keywords': ['Google', 'Google I/O', 'diversity', 'accessibility', 'Include', 'Google I/O 2019', 'project euphonia'], 'articleSection': ['AI'], 'inLanguage': 'en-US', 'speakable': {'@type': 'SpeakableSpecification', 'cssSelector': ['#speakable-summary']}, 'copyrightYear': '2019', 'copyrightHolder': {'@id': 'https://techcrunch.com/#organization'}}, {'@type': 'WebPage', '@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/', 'url': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/', 'name': ""Google's Project Euphonia wants to make voice recognition work for people with speech impairments | TechCrunch"", 'isPartOf': {'@id': 'https://techcrunch.com/#website'}, 'primaryImageOfPage': {'@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/#primaryimage'}, 'image': {'@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/#primaryimage'}, 'thumbnailUrl': 'https://techcrunch.com/wp-content/uploads/2019/05/google-project-euphonia.jpg', 'datePublished': '2019-05-07T18:18:34+00:00', 'dateModified': '2019-05-16T17:12:53+00:00', 'description': ""For those with speech impairments, artificial intelligence-powered voice recognition technology simply doesn't work for them. Google is trying to fix"", 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://techcrunch.com/2019/05/07/googles-project-euphonia-wants-to-make-voice-recognition-work-for-people-with-speech-impairments/#primaryimage', 'url': 'https://techcrunch.com/wp-content/uploads/2019/05/google-project-euphonia.jpg', 'contentUrl': 'https://techcrunch.com/wp-content/uploads/2019/05/google-project-euphonia.jpg', 'width': 2116, 'height': 1168, 'caption': 'google project euphonia'}, {'@type': 'WebSite', '@id': 'https://techcrunch.com/#website', 'url': 'https://techcrunch.com/', 'name': 'TechCrunch', 'description': 'Startup and Technology News', 'publisher': {'@id': 'https://techcrunch.com/#organization'}, 'alternateName': 'TC', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://techcrunch.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://techcrunch.com/#organization', 'name': 'TechCrunch', 'alternateName': 'TC', 'url': 'https://techcrunch.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://techcrunch.com/#/schema/logo/image/', 'url': 'https://techcrunch.com/wp-content/uploads/2018/04/tc-logo-2018-square-reverse2x.png?resize=1200,1200', 'contentUrl': 'https://techcrunch.com/wp-content/uploads/2018/04/tc-logo-2018-square-reverse2x.png?resize=1200,1200', 'width': 1200, 'height': 1200, 'caption': 'TechCrunch'}, 'image': {'@id': 'https://techcrunch.com/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/techcrunch', 'https://x.com/TechCrunch', 'https://mstdn.social/@TechCrunch', 'https://bsky.app/profile/techcrunch.bsky.social', 'https://www.threads.net/@techcrunch']}, {'@type': 'Person', '@id': 'https://techcrunch.com/#/schema/person/image/9af7ae7b2b32df9983243761e1190224', 'name': 'Megan Rose Dickey', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://techcrunch.com/#/schema/person/image/54439f18efe1f2d2ff6f2a7e5a1ea26b', 'url': 'https://secure.gravatar.com/avatar/e64a35398ab7fb3ff0de4f0817dc1c78?s=96&d=identicon&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/e64a35398ab7fb3ff0de4f0817dc1c78?s=96&d=identicon&r=g', 'caption': 'Megan Rose Dickey'}, 'description': 'Megan Rose Dickey is a senior reporter at TechCrunch focused on labor, transportation, and diversity and inclusion in tech. She previously spent two years at Business Insider covering tech startups focused on the shared economy, IoT and music industry. She graduated from the University of Southern California in 2011 with a degree in Broadcast and Digital Journalism. - See more at: https://www.crunchbase.com/person/megan-rose-dickey#sthash.ir4VFt2z.dpuf PGP fingerprint for email is:\xa02FA7 6E54 4652 781A B365 BE2E FBD7 9C5F 3DAE 56BD', 'url': 'https://techcrunch.com/author/megan-rose-dickey/'}]",,,,
https://news.google.com/rss/articles/CBMiU2h0dHBzOi8vdGhlaW50ZXJjZXB0LmNvbS8yMDE5LzA1LzExL2Ryb25lcy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1ldXJvcGUtcm9ib3JkZXIv0gEA?oc=5,Artificially Intelligent Drones' Potential for Abuse at the Border - The Intercept,2019-05-11,The Intercept,https://theintercept.com,Critics say projects like the EU-funded Roborder could be weaponized against vulnerable people in politically-charged zones.,"['Day: Saturday', 'Time: 10.00', 'Page Type: Article', 'Article Type: Article Post', 'Long', 'WC: 2000-2999', 'Subject: Technology', 'Subject: World', 'Partner: Factiva', 'Partner: Medium', 'Partner: Smart News', 'Partner: Spoken Layer', 'Partner: Uproxx', 'Language: English']",Critics say projects like the EU-funded Roborder could be weaponized against vulnerable people in politically-charged zones.,N/A,N/A,N/A,"









Support Us









Swarms of Drones, Piloted by Artificial Intelligence, May Soon Patrol Europe’s Borders




Illustration: Soohee Cho/The Intercept


          Swarms of Drones, Piloted by Artificial Intelligence, May Soon Patrol Europe’s Borders        

            The EU is funding the development of AI-powered drones that would autonomously patrol Europe’s borders. The project's potential for military use can't be ignored.          








Zach Campbell 

      May 11 2019, 6:00 a.m.    




Support Us



Illustration: Soohee Cho/The Intercept






    Share  





          Copy link        




          Share on Facebook        




          Share on X        




          Share on LinkedIn        




          Share on WhatsApp        




Imagine you’re hiking through the woods near a border. Suddenly, you hear a mechanical buzzing, like a gigantic bee. Two quadcopters have spotted you and swoop in for a closer look. Antennae on both drones and on a nearby autonomous ground vehicle pick up the radio frequencies coming from the cell phone in your pocket. They send the signals to a central server, which triangulates your exact location and feeds it back to the drones. The robots close in.
Cameras and other sensors on the machines recognize you as human and try to ascertain your intentions. Are you a threat? Are you illegally crossing a border? Do you have a gun? Are you engaging in acts of terrorism or organized crime? The machines send video feeds to their human operator, a border guard in an office miles away, who checks the videos and decides that you are not a risk. The border guard pushes a button, and the robots disengage and continue on their patrol.
This is not science fiction. The European Union is financing a project to develop drones piloted by artificial intelligence and designed to autonomously patrol Europe’s borders. The drones will operate in swarms, coordinating and corroborating information among fleets of quadcopters, small fixed-wing airplanes, ground vehicles, submarines, and boats. Developers of the project, known as Roborder, say the robots will be able to identify humans and independently decide whether they represent a threat. If they determine that you may have committed a crime, they will notify border police.
President Donald Trump has used the specter of criminals crossing the southern border to stir nationalist political sentiment and energize his base. In Europe, two years after the height of the migration crisis that brought more than a million people to the continent, mostly from the Middle East and Africa, immigration remains a hot-button issue, even as the number of new arrivals has dropped. Political parties across the European Union are winning elections on anti-immigrant platforms and enacting increasingly restrictive border policies. Tech ethicists and privacy advocates worry that Roborder and projects like it outsource too much law enforcement work to nonhuman actors and could easily be weaponized against people in border areas.
 


 


            Read our Complete Coverage          
Drone Wars




“The development of these systems is a dark step into morally dangerous territory,” said Noel Sharkey, emeritus professor of robotics and artificial intelligence at Sheffield University in the U.K. and one of the founders of the International Committee for Robot Arms Control, a nonprofit that advocates against the military use of robotics. Sharkey lists examples of weaponized drones currently on the market: flying robots equipped with Tasers, pepper spray, rubber bullets, and other weapons. He warns of the implications of combining that technology with AI-based decision-making and using it in politically-charged border zones. “It’s only a matter of time before a drone will be able to take action to stop people,” Sharkey told The Intercept.
Roborder’s developers also may be violating the terms of their funding, according to documents about the project obtained via European Union transparency regulations. The initiative is mostly financed by an €8 million EU research and innovation grant designed for projects that are exclusively nonmilitary, but Roborder’s developers acknowledge that parts of their proposed system involve military technology or could easily be converted for military use.
Much of the development of Roborder is classified, but The Intercept obtained internal reports related to ethical considerations and concerns about the program. That documentation was improperly redacted and inadvertently released in full.
In one of the reports, Roborder’s developers sought to address ethical criteria that are tied to their EU funding. Developers considered whether their work could be modified or enhanced to harm humans and what could happen if the technology or knowledge developed in the project “ended up in the wrong hands.” These ethical issues are raised, wrote the developers, when “research makes use of classified information, materials or techniques; dangerous or restricted materials[;] and if specific results of the research could present a danger to participants or to society as a whole.”
Roborder’s developers argued that these ethical concerns did not apply to their work, stating that their only goal was to develop and test the new technology, and that it would not be sold or transferred outside of the European Union during the life cycle of the project. But in interviews with The Intercept, project developers acknowledged that their technology could be repurposed and sold, even outside of Europe, after the European project cycle has finished, which is expected to happen next year.
Beyond the Roborder project, the ethics reports filed with the European Commission suggest a larger question: When it comes to new technology with the potential to be used against vulnerable people in places with few human rights protections, who decides what we should and should not develop?
Roborder won its funding grant in 2017 and has set out to develop a marketable prototype — “a swarm of robotics to support border monitoring” — by mid-2020. Its developers hope to build and equip a collection of air, sea, and land drones that can be combined and sent out on border patrol missions, scanning for “threats” autonomously based on information provided by human operators, said Stefanos Vrochidis, Roborder’s project manager.
The drones will employ optical, infrared, and thermal cameras; radar; and radio frequency sensors to determine threats along the border. Cell phone frequencies will be used to triangulate the location of people suspected of criminal activity, and cameras will identify humans, guns, vehicles, and other objects. “The main objective is to have as many sensors in the field as possible to assist patrol personnel,” said Kostas Ioannidis, Roborder’s technical manager.
The end product will be tested by border police in several European countries, including Portugal, Hungary, and Greece, but the project has also generated considerable interest in the private sector. “Eventually, we have companies that would certainly like to exploit this commercially,” Vrochidis told The Intercept. “They might exploit the whole outcome or part of the outcome, depending. They can exploit this in Europe but also outside of Europe.”
In their grant agreement, Roborder’s developers told the European Commission that they did not foresee any exports of their technology outside of the EU. In interviews, however, developers told The Intercept that the companies involved would be open to selling their technology beyond Europe. According to a spokesperson from the grant program funding Roborder, Horizon 2020, there is nothing Roborder’s EU backers can do to control where or how the technology they bankrolled is eventually used.
The documents obtained by The Intercept show Roborder responding to some ethical concerns about the project but not about the technology itself. In their grant application, Roborder’s developers conceded that their research “may be exploited by criminal organizations and individual criminals when planning to perpetrate acts of serious crime or terrorism” but wrote that the consortium of public and private companies developing the technology would work to keep their data safe. That group includes drone manufacturing companies, several national police departments, two national guards, a defense ministry, a port authority, a cyberdefense company, a company that specializes in developing equipment for electronic warfare, and another that provides “predictive analytics” for European police forces.
As for the technology’s possible modification for future clients, the answers were less clear. The developers would not comment on the potential for military sales after the project cycle ends. Developers added that their work is delayed because one of Roborder’s key consortium partners, Portuguese drone manufacturer Tekever, has left the project. Spokespeople for Roborder, Tekever, and Horizon 2020 would not explain the rationale for Tekever’s departure.



Horizon 2020 supports many security-oriented projects but maintains that “only research that has an exclusive focus on civil applications is eligible for funding.” The grant program previously funded a project that uses artificial intelligence to detect whether travelers are lying as they pass through automated border crossings.
Yet the documents obtained by The Intercept highlight inconsistent statements about Roborder’s potential military uses. According to one report, the project has no potential for “dual use,” or both military and civil deployment. Ten pages later, asked whether their work involved items that could be considered dual-use by European standards, Roborder’s developers wrote that it did.
Roborder hired a consultant, Reinhard Hutter, as an external ethics adviser to the project, according to another document from the Horizon 2020 program that was inadvertently released in full by the European Commission. In his report, Hutter wrote that “Roborder involves technology with military potential,” and that “the results of this project have the potential to be used back in the defense sector.” The technology involved, Hutter wrote, had “some dual-use potential but no dual-use activity on the project.” In other words, it could be used for military purposes but wouldn’t be used that way within the scope of Roborder.
Hutter declined to speak to The Intercept.
This blurring of lines between military and civilian development by the EU funding program might be deliberate. In a 2014 guidebook on European funding for dual-use projects, the European Commission notes that the regulation establishing Horizon 2020 mandates that all funded projects have “an exclusive focus” on civilian development, but the document also says that “substantial parts of the research funded is of relevance for defense and can lead to technologies that will be used by defense actors.”
The authors of a 2016 study commissioned by the security and defense sub-committee of the European Parliament went further, arguing that Horizon 2020’s clause on exclusive civilian development should be reinterpreted to include defense research. In order to compete with U.S. technological development, the study’s authors advocated for the creation of a European equivalent of DARPA, the U.S. Defense Advanced Research Projects Agency, whose work contributed to the development of the internet, GPS, and other technologies. In a 2017 speech, French President Emmanuel Macron echoed that, calling for, “a European agency for disruptive innovation in the same vein as” DARPA.
The 2016 report does not represent the views of the European Parliament or its security and defense sub-committee, and was not used to develop any specific legislation, a spokesperson for the European Parliament said. A spokesperson for Horizon 2020 rejected the idea that there was any ambiguity in what kind of projects the European Union would fund.
“The European Commission does not fund research intended for military use,” she said.
The drones Roborder plans to deploy are common technology. What would be groundbreaking for the companies involved is a functional system that allows swarms of drones to operate autonomously and in unison to reliably identify targets. AI threat detection is often inaccurate, according to robotics researchers, so any system that could correctly and consistently identify people, cars, and weapons, among other things, would be a substantial and lucrative advancement.
Drone cameras will not use facial recognition technology within the scope of the project, explained Ioannidis, Roborder’s technical manager, nor will they be able to determine any human characteristics, such as height, weight, age, skin color, or perceived gender. “The system will only identify that ‘this object is human,’” he added, “nothing more.”
Still, Ioannidis admitted that adding facial recognition to the Roborder system later on would be “technologically possible.” What about weaponizing the Roborder system to act against humans? “No,” he said, firmly. “The robots don’t have any authority to take any action against anyone. It’s just monitoring and giving alerts.”
But Sharkey, the U.K. robotics professor, argues that there is a thin line between using robots to monitor a border and using them to enforce one. Weaponizing a drone is relatively easy, he said, citing the 2015 case of the Connecticut teenager who equipped a drone with a handgun and a flamethrower. Sharkey worries about the implications of developing autonomous systems to patrol borders, including how the system could be used by a country coping with a large influx of people.
“The question is, where is this going?” Sharkey asked. “The current project does not propose to weaponize them, but it would just be too tempting for countries if a tipping point were to happen with mass migration.”
Hannah Couchman, a researcher at the U.K. human rights organization Liberty, agrees. “There are deep human rights and civil liberty concerns with this technology,” she told The Intercept. “Once this tech is developed, it’s seen as a solution, as a response to austerity, and a way to do a job efficiently with a lower cost, all rolled out without proper consultation and legislative scrutiny.”
“It’s not just about mitigating the human rights risk,” Couchman said. “It’s about whether we should use the technology in the first place.”




    Share  





          Copy link        




          Share on Facebook        




          Share on X        




          Share on LinkedIn        




          Share on WhatsApp        







    Contact the author:  







            Zach Campbell          






                                      zachcampbell@pm.me                                      








                                      @notzachcampbell                                      
on X








    Related  




 


        Cameras Linked to Chinese Government Stir Alarm in U.K. Parliament      






 


        Pentagon Says All of Google’s Work on Drones Is Exempt From the Freedom of Information Act      






 


        Inside the Video Surveillance Program IBM Built for Philippine Strongman Rodrigo Duterte      






 


        Google’s Sidewalk Labs Plans to Package and Sell Location Data on Millions of Cellphones      







    Latest Stories  




 




            How I Got a Truly Anonymous Signal Account          


Nikita Mazurov
- 6:03 am


Yes, you can use Signal without sharing your personal phone number. Here’s how I did it.






 




            How Clarence Thomas Cleared Trump’s Path in Classified Docs Case          


Shawn Musgrave
- July 15


Judge Aileen Cannon followed the playbook from Thomas’s solo opinion in the Trump immunity case.






 




              Voices            

            The Only Kind of “Political Violence” All U.S. Politicians Oppose          


Natasha Lennard
- July 14


The Trump rally shooting reveals a bipartisan consensus about what constitutes political violence — and who should wield it.







 Join The Conversation












",NewsArticle,https://schema.org,,http://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/,,,"{'@type': 'WebPage', '@id': 'http://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/'}","Swarms of Drones, Piloted by Artificial Intelligence, May Soon Patrol Europe&#8217;s Borders",2019-05-11T10:00:20Z,2019-05-13T21:34:37Z,https://theintercept.com/wp-content/uploads/2019/05/drone-watching-final-07-1557511522.jpg,Articles,"{'@type': 'Organization', 'name': 'The Intercept', 'logo': ''}","[{'@type': 'Person', 'name': 'Zach Campbell'}]","{'@type': 'ImageObject', 'url': 'https://theintercept.com/wp-content/uploads/2019/05/drone-watching-final-07-1557511522.jpg?fit=1441%2C721'}",,,"[{'@type': 'WebPage', '@id': 'https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/', 'url': 'https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/', 'name': ""Artificially Intelligent Drones' Potential for Abuse at the Border"", 'isPartOf': {'@id': 'https://theintercept.com/#website'}, 'primaryImageOfPage': {'@id': 'https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/#primaryimage'}, 'image': {'@id': 'https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/#primaryimage'}, 'thumbnailUrl': 'https://theintercept.com/wp-content/uploads/2019/05/drone-watching-final-07-1557511522.jpg?fit=1441%2C721', 'datePublished': '2019-05-11T10:00:20+00:00', 'dateModified': '2019-05-13T21:34:37+00:00', 'author': {'@id': 'https://theintercept.com/#/schema/person/ed0cace95dbf96a488234a02b0a69fc8'}, 'description': 'Critics say projects like the EU-funded Roborder could be weaponized against vulnerable people in politically-charged zones.', 'breadcrumb': {'@id': 'https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/#primaryimage', 'url': 'https://theintercept.com/wp-content/uploads/2019/05/drone-watching-final-07-1557511522.jpg?fit=1441%2C721', 'contentUrl': 'https://theintercept.com/wp-content/uploads/2019/05/drone-watching-final-07-1557511522.jpg?fit=1441%2C721', 'width': 1441, 'height': 721}, {'@type': 'BreadcrumbList', '@id': 'https://theintercept.com/2019/05/11/drones-artificial-intelligence-europe-roborder/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://theintercept.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Swarms of Drones, Piloted by Artificial Intelligence, May Soon Patrol Europe&#8217;s Borders'}]}, {'@type': 'WebSite', '@id': 'https://theintercept.com/#website', 'url': 'https://theintercept.com/', 'name': 'The Intercept', 'description': '', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://theintercept.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}]",['Zach Campbell'],2019-05-11T10:00:20Z,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.emailwall'}",
https://news.google.com/rss/articles/CBMiUmh0dHBzOi8vbmV3cy5taXQuZWR1LzIwMTkvdXNpbmctYWktcHJlZGljdC1icmVhc3QtY2FuY2VyLWFuZC1wZXJzb25hbGl6ZS1jYXJlLTA1MDfSAQA?oc=5,Using AI to predict breast cancer and personalize care - MIT News,2019-05-07,MIT News,https://news.mit.edu,A team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Massachusetts General Hospital has created a deep learning model that can predict from a mammogram if a patient is likely to develop breast cancer as much as five years in the future. ,"MIT Computer Science and Artificial Intelligence Laboratory MIT CSAIL, MIT Department of Electrical Engineering and Computer Science (eecs), artificial intelligence, MIT School of Engineering, big data, medical data, AI in health care, health sciences and technology, Massachusetts General Hospital, AI in medicine, Regina Barzilay",A team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Massachusetts General Hospital has created a deep learning model that can predict from a mammogram if a patient is likely to develop breast cancer as much as five years in the future. ,N/A,N/A,N/A,"


MIT/MGH's image-based deep learning model can predict breast cancer up to five years in advance.






Adam Conner-Simons and Rachel Gordon
|
CSAIL


 Publication Date:
 May 7, 2019





Press Inquiries

  Press Contact:



      
            Rachel        

            Gordon        

  

      Email:
     rachelg@csail.mit.edu


      Phone:
              617-258-0675      
  

      
            MIT Computer Science and Artificial Intelligence Laboratory        

  








 Close














 Caption:
          The team's model was shown to be able to identify a woman at high risk of breast cancer four years (left) before it developed (right).      
          

 Credits:
          Image courtesy of the researchers      
          

















Previous image
Next image






















Despite major advances in genetics and modern imaging, the diagnosis catches most breast cancer patients by surprise. For some, it comes too late. Later diagnosis means aggressive treatments, uncertain outcomes, and more medical expenses. As a result, identifying patients has been a central pillar of breast cancer research and effective early detection.
With that in mind, a team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Massachusetts General Hospital (MGH) has created a new deep-learning model that can predict from a mammogram if a patient is likely to develop breast cancer as much as five years in the future. Trained on mammograms and known outcomes from over 60,000 MGH patients, the model learned the subtle patterns in breast tissue that are precursors to malignant tumors.
MIT Professor Regina Barzilay, herself a breast cancer survivor, says that the hope is for systems like these to enable doctors to customize screening and prevention programs at the individual level, making late diagnosis a relic of the past.
Although mammography has been shown to reduce breast cancer mortality, there is continued debate on how often to screen and when to start. While the American Cancer Society recommends annual screening starting at age 45, the U.S. Preventative Task Force recommends screening every two years starting at age 50.
“Rather than taking a one-size-fits-all approach, we can personalize screening around a woman’s risk of developing cancer,” says Barzilay, senior author of a new paper about the project out today in Radiology. “For example, a doctor might recommend that one group of women get a mammogram every other year, while another higher-risk group might get supplemental MRI screening.” Barzilay is the Delta Electronics Professor at CSAIL and the Department of Electrical Engineering and Computer Science at MIT and a member of the Koch Institute for Integrative Cancer Research at MIT.
The team’s model was significantly better at predicting risk than existing approaches: It accurately placed 31 percent of all cancer patients in its highest-risk category, compared to only 18 percent for traditional models.
Harvard Professor Constance Lehman says that there’s previously been minimal support in the medical community for screening strategies that are risk-based rather than age-based.
“This is because before we did not have accurate risk assessment tools that worked for individual women,” says Lehman, a professor of radiology at Harvard Medical School and division chief of breast imaging at MGH. “Our work is the first to show that it’s possible.”  
Barzilay and Lehman co-wrote the paper with lead author Adam Yala, a CSAIL PhD student. Other MIT co-authors include PhD student Tal Schuster and former master’s student Tally Portnoi.How it works 
Since the first breast-cancer risk model from 1989, development has largely been driven by human knowledge and intuition of what major risk factors might be, such as age, family history of breast and ovarian cancer, hormonal and reproductive factors, and breast density.
However, most of these markers are only weakly correlated with breast cancer. As a result, such models still aren’t very accurate at the individual level, and many organizations continue to feel risk-based screening programs are not possible, given those limitations.
Rather than manually identifying the patterns in a mammogram that drive future cancer, the MIT/MGH team trained a deep-learning model to deduce the patterns directly from the data. Using information from more than 90,000 mammograms, the model detected patterns too subtle for the human eye to detect.
“Since the 1960s radiologists have noticed that women have unique and widely variable patterns of breast tissue visible on the mammogram,” says Lehman. “These patterns can represent the influence of genetics, hormones, pregnancy, lactation, diet, weight loss, and weight gain. We can now leverage this detailed information to be more precise in our risk assessment at the individual level.”  Making cancer detection more equitable
The project also aims to make risk assessment more accurate for racial minorities, in particular. Many early models were developed on white populations, and were much less accurate for other races. The MIT/MGH model, meanwhile, is equally accurate for white and black women. This is especially important given that black women have been shown to be 42 percent more likely to die from breast cancer due to a wide range of factors that may include differences in detection and access to health care.
“It’s particularly striking that the model performs equally as well for white and black people, which has not been the case with prior tools,” says Allison Kurian, an associate professor of medicine and health research/policy at Stanford University School of Medicine. “If validated and made available for widespread use, this could really improve on our current strategies to estimate risk.”
Barzilay says their system could also one day enable doctors to use mammograms to see if patients are at a greater risk for other health problems, like cardiovascular disease or other cancers. The researchers are eager to apply the models to other diseases and ailments, and especially those with less effective risk models, like pancreatic cancer.
“Our goal is to make these advancements a part of the standard of care,” says Yala. “By predicting who will develop cancer in the future, we can hopefully save lives and catch cancer before symptoms ever arise.”








Share this news article on:










X











Facebook















LinkedIn




































Reddit


















Print









Press Mentions


PBSA new FRONTLINE documentary spotlights Prof. Regina Barzilay’s work developing an AI system that can aid the detection of breast cancer. “A technology that had not yet penetrated the hospital setting now has the potential to save many thousands of lives each year,” writes Patrice Taddonio for PBS.











Full story via PBS →
New York TimesWriting for The New York Times, Susan Gubar spotlights Prof. Regina Barzilay’s quest to transform cancer care by using AI technologies to improve the detection and diagnosis of cancer. “Dr. Barzilay and her collaborators want to usher in the day when no woman is surprised by a late-stage diagnosis and when all breast cancers are curable,” writes Gubar.











Full story via New York Times →
CNNCNN reporter Nell Lewis spotlights how MIT researchers have developed an algorithm that can help predict from a mammogram a patient’s risk of developing breast cancer. “In the early stages cancer is a treatable disease,” says Barzilay. “If we can identify many more women early enough, and either prevent their disease or treat them at the earliest stages, this will make a huge difference.”











Full story via CNN →
Fast CompanyFast Company reporter Michael Grothaus writes that CSAIL researchers have developed a deep learning model that could predict whether a woman might develop breast cancer. The system “could accurately predict about 31% of all cancer patients in a high-risk category,” Grothaus explains, which is “significantly better than traditional ways of predicting breast cancer risks.”











Full story via Fast Company →
WCVBWCVB-TV’s Jennifer Eagan reports that researchers from MIT and MGH have developed a deep learning model that can predict a patient’s risk of developing breast cancer in the future from a mammogram image. Prof. Regina Barzilay explains that the model “can look at lots of pixels and variations of the pixels and capture very subtle patterns.”











Full story via WCVB →
HealthDay NewsHealthDay News reporter Amy Norton writes that MIT researchers have developed an AI system that can help predict a woman’s risk of developing breast cancer and provide more personalized care. “If you know a woman is at high risk, maybe she can be screened more frequently, or be screened using MRI,” explains graduate student Adam Yala.











Full story via HealthDay News →















Previous item
Next item



















Related Links

Paper: ""A Deep Learning Mammography-based Model for Improved Breast Cancer Risk Prediction""Regina BarzilayComputer Science and Artificial Intelligence Laboratory (CSAIL)Department of Electrical Engineering and Computer ScienceSchool of Engineering






Related Topics

Computer Science and Artificial Intelligence Laboratory (CSAIL)
Electrical Engineering & Computer Science (eecs)
School of Engineering
Artificial intelligence
Health care
Health sciences and technology
Medicine
Cancer
Koch Institute
Data



Related Articles











Automated system identifies dense tissue, a risk factor for breast cancer, in mammograms













Putting data in the hands of doctors













How a computer can help your doctor better diagnose cancer

















Previous item
Next item
















",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMivAFodHRwczovL25ld3MubWljcm9zb2Z0LmNvbS9lbi1oay8yMDE5LzA1LzA4L21pY3Jvc29mdC1pZGMtc3R1ZHktYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtYWRvcHRpb24tdG8taW5jcmVhc2UtcmF0ZS1vZi1pbm5vdmF0aW9uLWFuZC1lbXBsb3llZS1wcm9kdWN0aXZpdHktZ2FpbnMtYnktbW9yZS10aGFuLWRvdWJsZS1ieS0yMDIxL9IBAA?oc=5,Microsoft – IDC Study: Artificial Intelligence adoption to increase rate of innovation and employee productivity gains by ... - Microsoft,2019-05-08,Microsoft,https://news.microsoft.com,N/A,N/A,N/A,N/A,N/A,N/A,"


			June 19, 2024		

		Available Today! Top things to know about Copilot+ PCs from Microsoft Surface	

",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiemh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvamVhbm5lbWVpc3Rlci8yMDE5LzA1LzA3L3RoZS1mdXR1cmUtb2Ytd29yay10aGUtcmlzZS1vZi13b3JrZXJzLXdoby1zZWxmLWF1dG9tYXRlLXRoZWlyLWpvYnMv0gEA?oc=5,The Future of Work: The Rise of Workers Who Self Automate Their Jobs - Forbes,2019-05-08,Forbes,https://www.forbes.com,"We know AI and automation will transform the nature of the activities within jobs, but, as leaders, we need to ask a new question: what happens when either full time workers or gig workers self automate their jobs?",,"We know AI and automation will transform the nature of the activities within jobs, but, as leaders, we need to ask a new question: what happens when either full time workers or gig workers self automate their jobs?","We know AI and automation will transform the nature of the activities within jobs, but, as leaders, we need to ask a new question: what happens when either full time workers or gig workers self automate their jobs?",Leadership Strategy,N/A,"More From ForbesAug 27, 2019,04:42pm EDTRobots Are Actually Teaching Humans To Be More CompassionateAug 26, 2019,09:52am EDTAI Tests A 200-Year-Old Evolutionary TheoryAug 23, 2019,10:48am EDTAI Making Waves In News And JournalismAug 23, 2019,07:30am EDTHow Artificial Intelligence Is Preventing Cognitive Overload, Compassion Fatigue And Job BurnoutAug 21, 2019,10:34am EDTExplainable AI Could Help Us Audit AI Startup ClaimsAug 21, 2019,02:20am EDTArtificial Intelligence Beyond The Buzzword From Two Fintech CEOsAug 20, 2019,05:46pm EDTWhat Is Artificial Intelligence?Edit StoryLeadershipLeadership StrategyThe Future of Work: The Rise of Workers Who Self Automate Their JobsJeanne MeisterContributorOpinions expressed by Forbes Contributors are their own.I write about Trends Shaping The Future of WorkFollowingFollowMay 7, 2019,08:10am EDTUpdated May 8, 2019, 06:41pm EDTThis article is more than 5 years old.Tweet This Self automating jobs are gaining steam among programmers, but workers across other business functions are automating their jobs as well. Who should benefit when workers develop a ""hack"" into how their job gets done?Share to FacebookShare to TwitterShare to Linkedin









The headlines tell a confusing story about the impact of automation and artificial intelligence on jobs. Pew Research Center asked 1,896 experts to comment on the impact of automation and AI on the future of jobs. Roughly half of these experts, 48%, envisioned a future where robots and digital agents will displace significant numbers of both blue and white-collar workers. The other 52% of experts expect technology to create more jobs than it displaces by 2025.  This is consistent with the estimate from Gartner that says by 2020 AI will create 2.3 million jobs in 2020, while eliminating 1.8 million jobs.

One area on which these experts agree is that automation will transform all jobs. In fact, McKinsey predicts roughly 30 percent of the activities in 60 percent of all occupations could be automated. This means that most workers, from those working in a factory or warehouse, to customer call center operators, recruiting schedulers, and even CEOs, will be working alongside digital assistants. The nature of all jobs is changing as organizations leverage the power of artificial intelligence and machine learning in the workplace.
But I think the conversation has the wrong focus. We know AI and automation will transform the nature of the activities within jobs, but, as leaders, we need to ask a new question: what happens when either full time workers or gig workers self automate their jobs? Actually, this question was asked by a programmer on The Workplace Stack Exchange, one of the web’s most important forums for programmers with the post: ""Is it unethical for me to not tell my employer I’ve automated my job?”
PROMOTED
This question was viewed over 480,000 times and asks us to address an emerging issue in the workplace: if workers' self automate their jobs, who benefits and what is the impact on other jobs? While the threat of automation is playing itself out in the media, some workers have taken matters into their own hands and started to automate various aspects of their jobs from data entry to inventory management and database administration.
 Self automating jobs are gaining steam among programmers, but workers across other business functions are automating their jobs as well. They are experimenting with using Google Assistant to manage busywork and accessing a range of productivity bots to schedule meetings (X.ai or Clara) or to follow-up after meetings (Gridspace Sift or Geekbot). All of this is having broad implications for both the employee and the employer. HR, Business, and IT leaders need to ask themselves five questions as the wave of self-automation spreads beyond programmers to a myriad of job functions and industries:
1) What will be the impact of AI and automation at work?
Automation and the impact of AI on the workplace is happening now and it is not going away. In fact, McKinsey estimates that 15% of the global workforce or 400 million workers could be displaced by automation in the period 2016–2030.  There are many factors that will impact the pace and scope of workforce automation, such as labor market dynamics and associated wages for certain jobs, as well as the technical feasibility of automation and the acceptance of automation by a company's culture. But one thing is certain: as machines complement human labor in the workplace, we will all be impacted and need to adjust the nature of our jobs. There are numerous blog posts and how-to articles with titles like “How I Automated My Job With Node JS,” or even books, a recent one titled, Automate the Boring Stuff with Python, showing this to be a growing cottage industry.
2) What happens when key job families begin to self automate their jobs? 
Programmers are a job family ripe for self automation since they routinely use tools to automate various aspects of their job. These workers offer a glimpse of what it looks like when automation is started not by a top-down executive strategy but by the workers themselves. For HR, business, and IT leaders, the opportunity is to inculcate a learning mindset among workers so when (not if) self automation occurs, workers will be encouraged to learn new skills and tackle new challenges.
3)  Who should benefit when workers develop a ""hack"" into how their job gets done?
It seems to me both the employer and the employee should benefit from workers who find better ways to do their jobs. While many knowledge workers routinely sign employment contracts stipulating that any intellectual property developed on company time belongs to the employer, should this continue to be the norm as jobs are being automated by technology?  If a worker creates an efficiency ""hack"", why not reward this, communicate it company-wide, and identify ways the worker can use their new found time at work to develop additional skills?
4) How can job automation become less a top down mandate and more a bottom up movement?
Currently, the focus has been on when employers will pull the switch and automate jobs, but this does not consider the spread of self automation across various job functions, starting with programmers, but expanding to customer service center operators, recruiting coordinators and more. In fact, we must ask ourselves this: will self automation become a new skill set for workers?  How can we encourage workers to look for ways to automate parts of their jobs, and in the process, reclaim parts of their work day. We might even envision a time when self automating your job allows you to have a four-day work week. Workers already believe they can get their work done in less time. According to research conducted by our firm, Future Workplace, and Kronos, three in four full-time employees (78%) say they could do their job in under seven hours each day if they could work uninterrupted, while almost half (45%) think they could wrap things up in just five hours each day.
5) How do business leaders start a dialogue about self automating jobs?
""Here's what we don't want to happen,"" says Ryan Duguid, Chief Evangelist, Nintex. ""We don't want workers who self automate to keep this to themselves. We want to reward their agility and curiosity."" Often companies focus only on finding organizational productivity gains, but the time is now to recognize workers who discover personal productivity gains. HR, business and IT leaders need to start discussing how some workers have self automated parts of their jobs, increased their productivity and satisfaction at work, as well as developed new skills adding more value to their jobs. It's time we turn the conversation from employers using automation to displace jobs to workers using technology to find better ways to do their jobs!Follow me on Twitter or LinkedIn. Check out my website. Jeanne MeisterFollowingFollowI'm a global HR consultant and Jeanne is a workplace visionary, advising clients to think differently about work, the worker, the workplace, and the... Read MoreEditorial StandardsPrintReprints & Permissions",BreadcrumbList,http://schema.org,The Future of Work: The Rise of Workers Who Self Automate Their Jobs,https://www.forbes.com/sites/jeannemeister/2019/05/07/the-future-of-work-the-rise-of-workers-who-self-automate-their-jobs/,,,,The Future of Work: The Rise of Workers Who Self Automate Their Jobs,2019-05-07T08:10:00-04:00,2019-05-08T18:41:50-04:00,,Leadership Strategy,"{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}","{'@type': 'Person', 'name': 'Jeanne Meister', 'url': 'https://www.forbes.com/sites/jeannemeister/', 'description': ""I'm a global HR consultant and Jeanne is a workplace visionary, advising clients to think differently about work, the worker, the workplace, and the workforce. I am widely regarded as an Innovator in human resources and learning, with decades of experience forming and managing peer networks composed of HR and education leaders in global 2,000 companies."", 'sameAs': ['https://www.linkedin.com/in/jeannemeister/', 'https://www.twitter.com/jcmeister', 'https://www.jeannemeister.com']}","{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/blogs-images/jeannemeister/files/2019/05/shutterstock_674389504_SQ.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Leadership', 'item': 'https://www.forbes.com/leadership/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Leadership Strategy', 'item': 'https://www.forbes.com/leadership-strategy/'}]",True,,,,,
https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vd3d3Lm1hcmt0ZWNocG9zdC5jb20vMjAxOS8wNS8wNy9ob3ctZ2VuZXJhdGl2ZS1hZHZlcnNhcmlhbC1uZXR3b3Jrcy1nYW5zLXdvcmsv0gFaaHR0cHM6Ly93d3cubWFya3RlY2hwb3N0LmNvbS8yMDE5LzA1LzA3L2hvdy1nZW5lcmF0aXZlLWFkdmVyc2FyaWFsLW5ldHdvcmtzLWdhbnMtd29yay8_YW1w?oc=5,How Generative Adversarial Networks (GANs) work? - MarkTechPost,2019-05-07,MarkTechPost,https://www.marktechpost.com,How Generative Adversarial Networks (GANs) work?. Generative Adversarial Networks were first introduced in 2014 in a research paper.,N/A,"Generative Adversarial Networks were first introduced in 2014 in a research paper. They have also been called  “the most interesting idea in the last ten years in Machine Learning” by Yann LeCun, Facebook's AI research director. To understand the concept of adversarial networks, we need to understand discriminative models, based on deep learning. The discriminative models take sample input data and process them to generate groupings to identify the data. On the contrary, generative networks can produce new features based on defined conditions. Image source: O’Reilly Need for GANs The discriminative networks can be error-prone due to small quantities of","Generative Adversarial Networks were first introduced in 2014 in a research paper. They have also been called  “the most interesting idea in the last ten years in Machine Learning” by Yann LeCun, Facebook's AI research director. To understand the concept of adversarial networks, we need to understand discriminative models, based on deep learning. The discriminative models take sample input data and process them to generate groupings to identify the data. On the contrary, generative networks can produce new features based on defined conditions. Image source: O’Reilly Need for GANs The discriminative networks can be error-prone due to small quantities of",N/A,N/A,"


How Generative Adversarial Networks (GANs) work?

By Vaibhavi Joshi -   May 7, 2019 



RedditVoteFlipShareTweet0 Shares
Generative Adversarial Networks were first introduced in 2014 in a research paper. They have also been called  “the most interesting idea in the last ten years in Machine Learning” by Yann LeCun, Facebook’s AI research director.
To understand the concept of adversarial networks, we need to understand discriminative models, based on deep learning. The discriminative models take sample input data and process them to generate groupings to identify the data. On the contrary, generative networks can produce new features based on defined conditions.
  [FREE AI WEBINAR] 'Optimise Your Custom Embedding Space: How to find the right embedding model for YOUR data.' (July 18, 2024) [Promoted] 

Image source:   O’Reilly 
Need for GANs
The discriminative networks can be error-prone due to small quantities of noise in the data. The results can be miscalculated and misleading. The primary reason behind this limitation is the way machine learning models are designed to learn. Most of them learn from the limited data, which may contain inaccurate, non-linear relationship between the input and output. 
Structure of GANs
A Generative Adversarial Network contains a “generator”(G) neural network and a “discriminator”(D) neural network. The generator produces dummy data samples to mislead the discriminator. The discriminator tries to determine the difference between the dummy and real data. 
  [Synthetic Data Webinar] Learn how Gretel’s synthetic data platform, powered by generative AI, make’s data generation easier than ever before: July 30, 2024 | 9:00 am PT, 12:00 pm ET 

The above process takes place with the following steps:
Random variable generationTransformation of the random variable into a complex variable.Generation of output random variable containing probabilitiesthe final output of the generative network
Both the neural networks compete with each other during the formative period. The training steps are performed repeatedly by both the models and they gradually get improved after several cycles. 
The goal of the generative model is the maximization of the probability of the distributor being incorrect. The discriminator aims for minimization of the estimation of accuracy of the sample data.
To effectively train the GANs, consider the following points: 
While training the discriminator, the adversary should be constant, and while training the adversary, the discriminator should be constant.To avoid one network from dominating the another, both the models should have an equivalent skill threshold.
Benefits and drawbacks
The advantages of GANs involve:
Diminished need for direct data example inputs which can be replaced with gradients to the discriminator. Ability to generate sharp distributions which are superior to the Markov chains which need the blurry distribution to achieve the correct mix. 
The main drawback of the GANs is their tendency to take a lot of time for the training. It may take hours for a GAN to train, which in turn make them complex to form and use. 

Note: This is a guest post, and opinion in this article is of the guest writer. If you have any issues with any of the articles posted at www.marktechpost.com please contact at asif@marktechpost.com  


 Vaibhavi Joshi | WebsiteVaibhavi is an AI-ML enthusiast, keen observer, and a data-story teller. She writes about cutting-edge research areas and technologies in the field of data science. She also covers ethical and social sides of AI as well as behavioral insights in technology. Her other areas of interests are digital marketing, business aspects of AI, design thinking as well as content strategy.Combining Deep and Reinforcement learningHow reinforcement learning can help in solving real-world problems?Harnessing curiosity in Reinforcement LearningRedditVoteFlipShareTweet0 Shares

  Join the Fastest Growing AI Research Newsletter Read by Researchers from Google + NVIDIA + Meta + Stanford + MIT + Microsoft and many others... 



Previous articleUnderstanding the Four Types of Data AnalyticsNext articleEU Unveils a New Code of Ethics Guidelines Built For Artificial Intelligence Vaibhavi Joshi  
",BreadcrumbList,https://schema.org,,,,,,,,,,,,,,"[{'@type': 'ListItem', 'position': 1, 'item': {'@type': 'WebSite', '@id': 'https://www.marktechpost.com/', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@type': 'WebPage', '@id': 'https://www.marktechpost.com/category/technology/artificial-intelligence/', 'name': 'Artificial Intelligence'}}, {'@type': 'ListItem', 'position': 3, 'item': {'@type': 'WebPage', '@id': 'https://www.marktechpost.com/category/technology/artificial-intelligence/applications/', 'name': 'Applications'}}, {'@type': 'ListItem', 'position': 4, 'item': {'@type': 'WebPage', '@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/', 'name': 'How Generative Adversarial Networks (GANs) work?'}}]",,"[{'@type': 'Article', '@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#article', 'isPartOf': {'@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/'}, 'author': {'name': 'Vaibhavi Joshi', '@id': 'https://www.marktechpost.com/#/schema/person/1d664ce666a142d781b43eb5e7ae25a8'}, 'headline': 'How Generative Adversarial Networks (GANs) work?', 'datePublished': '2019-05-07T21:04:19+00:00', 'dateModified': '2019-05-07T21:15:51+00:00', 'mainEntityOfPage': {'@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/'}, 'wordCount': 456, 'commentCount': 0, 'publisher': {'@id': 'https://www.marktechpost.com/#organization'}, 'image': {'@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#primaryimage'}, 'thumbnailUrl': 'https://www.marktechpost.com/wp-content/uploads/2019/05/evolution-4118695_1920.jpg', 'keywords': ['GAN'], 'articleSection': ['Applications', 'Artificial Intelligence', 'Artificial Intelligence', 'Data Science', 'Data Science', 'Deep Learning', 'Editors Pick', 'Education', 'Guest Post', 'Machine Learning', 'Natural Language Processing', 'Resources', 'Technology'], 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'CommentAction', 'name': 'Comment', 'target': ['https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#respond']}], 'copyrightYear': '2019', 'copyrightHolder': {'@id': 'https://www.marktechpost.com/#organization'}}, {'@type': 'WebPage', '@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/', 'url': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/', 'name': 'How Generative Adversarial Networks (GANs) work? - MarkTechPost', 'isPartOf': {'@id': 'https://www.marktechpost.com/#website'}, 'primaryImageOfPage': {'@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#primaryimage'}, 'image': {'@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#primaryimage'}, 'thumbnailUrl': 'https://www.marktechpost.com/wp-content/uploads/2019/05/evolution-4118695_1920.jpg', 'datePublished': '2019-05-07T21:04:19+00:00', 'dateModified': '2019-05-07T21:15:51+00:00', 'description': 'How Generative Adversarial Networks (GANs) work?. Generative Adversarial Networks were first introduced in 2014 in a research paper.', 'breadcrumb': {'@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#primaryimage', 'url': 'https://www.marktechpost.com/wp-content/uploads/2019/05/evolution-4118695_1920.jpg', 'contentUrl': 'https://www.marktechpost.com/wp-content/uploads/2019/05/evolution-4118695_1920.jpg', 'width': 1920, 'height': 1280, 'caption': 'Image by Gerd Altmann from Pixabay'}, {'@type': 'BreadcrumbList', '@id': 'https://www.marktechpost.com/2019/05/07/how-generative-adversarial-networks-gans-work/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.marktechpost.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'How Generative Adversarial Networks (GANs) work?'}]}, {'@type': 'WebSite', '@id': 'https://www.marktechpost.com/#website', 'url': 'https://www.marktechpost.com/', 'name': 'MarkTechPost', 'description': 'An Artificial Intelligence News Platform', 'publisher': {'@id': 'https://www.marktechpost.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.marktechpost.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.marktechpost.com/#organization', 'name': 'MarkTechPost Media Inc.', 'url': 'https://www.marktechpost.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.marktechpost.com/#/schema/logo/image/', 'url': 'https://www.marktechpost.com/wp-content/uploads/2022/04/cropped-Favicon-512-x-512-1-1.png', 'contentUrl': 'https://www.marktechpost.com/wp-content/uploads/2022/04/cropped-Favicon-512-x-512-1-1.png', 'width': 512, 'height': 512, 'caption': 'MarkTechPost Media Inc.'}, 'image': {'@id': 'https://www.marktechpost.com/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/MarkTechPost/', 'https://x.com/Marktechpost', 'https://www.linkedin.com/in/asifrazzaq/']}, {'@type': 'Person', '@id': 'https://www.marktechpost.com/#/schema/person/1d664ce666a142d781b43eb5e7ae25a8', 'name': 'Vaibhavi Joshi', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.marktechpost.com/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/88ecb21c9aa456c4bba6b4fb0b1777d2?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/88ecb21c9aa456c4bba6b4fb0b1777d2?s=96&d=mm&r=g', 'caption': 'Vaibhavi Joshi'}, 'description': 'Vaibhavi is an AI-ML enthusiast, keen observer, and a data-story teller. She writes about cutting-edge research areas and technologies in the field of data science. She also covers ethical and social sides of AI as well as behavioral insights in technology. Her other areas of interests are digital marketing, business aspects of AI, design thinking as well as content strategy.', 'sameAs': ['http://www.marktechpost.com', 'https://www.facebook.com/vaibhavi.joshi201', 'https://www.linkedin.com/in/vaibhavij/', 'https://x.com/https://twitter.com/vaibhavi9901'], 'url': 'https://www.marktechpost.com/author/vaibhavi-joshi/'}]",,,,
https://news.google.com/rss/articles/CBMijgFodHRwczovL2ZlZGVyYWxuZXdzbmV0d29yay5jb20vYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UvMjAxOS8wNS9hcm15LWV4cGxvcmluZy1haS1jb21iYXQtcG9zc2liaWxpdGllcy13aGlsZS1wcmlvcml0aXppbmctcXVpY2stY290cy1hZG9wdGlvbnMv0gEA?oc=5,Army exploring AI combat possibilities while prioritizing quick COTS adoptions - Federal News Network,2019-05-09,Federal News Network,https://federalnewsnetwork.com,"The Army isn’t looking to build most of its artificial intelligence technologies from scratch. Instead, it’s looking to tweak existing, commercial off-the-shelf technologies to suit its purposes.","['army combat capabilities development command', 'army futures command', 'cedric wins', 'machine learning']","The Army isn’t looking to build most of its artificial intelligence technologies from scratch. Instead, it’s looking to tweak existing, commercial off-the-shelf technologies to suit its purposes.","The Army isn’t looking to build most of its artificial intelligence technologies from scratch. Instead, it’s looking to tweak existing, commercial off-the-shelf technologies to suit its purposes.",N/A,N/A,"

DoD laying groundwork for ‘multi-generational’ effort on AI


Artificial Intelligence
Read more

",NewsArticle,http://schema.org,,https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/,,,"{'@type': 'WebPage', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/'}",Army exploring AI combat possibilities while prioritizing quick COTS adoptions,2019-05-09T19:47:42Z,2020-05-13T19:56:42Z,https://federalnewsnetwork.com/wp-content/uploads/2016/11/army-cyber-21-150x150.jpg,Artificial Intelligence,"{'@type': 'Organization', 'name': 'Federal News Network', 'logo': {'@type': 'ImageObject', 'url': 'https://federalnewsnetwork.com/wp-content/themes/wfed/assets/img/icons/logo.png'}}","[{'@type': 'Person', 'name': 'Daisy Thornton'}]","{'@type': 'ImageObject', 'url': 'https://federalnewsnetwork.com/wp-content/uploads/2016/11/army-cyber-21.jpg', 'width': 1626, 'height': 1080}",,,"[{'@type': 'BreadcrumbList', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#breadcrumblist', 'itemListElement': [{'@type': 'ListItem', '@id': 'https://federalnewsnetwork.com/#listItem', 'position': 1, 'name': 'Home', 'item': 'https://federalnewsnetwork.com/', 'nextItem': 'https://federalnewsnetwork.com/artificial-intelligence/2019/#listItem'}, {'@type': 'ListItem', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/#listItem', 'position': 2, 'name': '2019', 'item': 'https://federalnewsnetwork.com/artificial-intelligence/2019/', 'nextItem': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/#listItem', 'previousItem': 'https://federalnewsnetwork.com/#listItem'}, {'@type': 'ListItem', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/#listItem', 'position': 3, 'name': 'May', 'item': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/', 'nextItem': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#listItem', 'previousItem': 'https://federalnewsnetwork.com/artificial-intelligence/2019/#listItem'}, {'@type': 'ListItem', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#listItem', 'position': 4, 'name': 'Army exploring AI combat possibilities while prioritizing quick COTS adoptions', 'previousItem': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/#listItem'}]}, {'@type': 'NewsArticle', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#newsarticle', 'name': 'Army exploring AI combat possibilities while prioritizing quick COTS adoptions', 'headline': 'Army exploring AI combat possibilities while prioritizing quick COTS adoptions', 'author': {'@id': 'https://federalnewsnetwork.com/author/dthornton/#author'}, 'publisher': {'@id': 'https://federalnewsnetwork.com/#organization'}, 'image': {'@type': 'ImageObject', 'url': 'https://federalnewsnetwork.com/wp-content/uploads/2016/11/army-cyber-21.jpg', 'width': 1626, 'height': 1080, 'caption': 'Uniformed and civilian cyber and military intelligence specialists monitor Army networks in the Cyber Mission Unit’s Cyber Operations Center at Fort Gordon, Georgia'}, 'datePublished': '2019-05-09T19:47:42-04:00', 'dateModified': '2020-05-13T19:56:42-04:00', 'inLanguage': 'en-US', 'mainEntityOfPage': {'@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#webpage'}, 'isPartOf': {'@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#webpage'}, 'articleSection': 'All News, Army, Artificial Intelligence, Defense, Defense News, Insight of the Month, Technology, Army Combat Capabilities Development Command, Army Futures Command, Cedric Wins, machine learning, daisy-thornton', 'dateline': 'Published on May 9, 2019.'}, {'@type': 'Organization', '@id': 'https://federalnewsnetwork.com/#organization', 'name': 'Federal News Network', 'url': 'https://federalnewsnetwork.com/', 'logo': {'@type': 'ImageObject', 'url': 'https://federalnewsnetwork.com/wp-content/themes/wfed/assets/img/media_kit/JPG/fnn-logo-hor-small.jpg', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#organizationLogo'}, 'image': {'@id': 'https://federalnewsnetwork.com/#organizationLogo'}, 'sameAs': ['https://twitter.com/FederalNewsNet', 'https://www.instagram.com/federalnewsnet/', 'https://www.youtube.com/@federalnewsnetwork', 'https://www.linkedin.com/company/federal-news-network', 'https://en.wikipedia.org/wiki/WFED'], 'contactPoint': {'@type': 'ContactPoint', 'telephone': '+12028955086', 'contactType': 'Newsroom'}}, {'@type': 'Person', '@id': 'https://federalnewsnetwork.com/author/dthornton/#author', 'url': 'https://federalnewsnetwork.com/author/dthornton/', 'name': 'Daisy Thornton'}, {'@type': 'WebPage', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#webpage', 'url': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/', 'name': 'Army exploring AI combat possibilities while prioritizing quick COTS adoptions', 'description': 'The Army isn’t looking to build most of its artificial intelligence technologies from scratch. Instead, it’s looking to tweak existing, commercial off-the-shelf technologies to suit its purposes.', 'inLanguage': 'en-US', 'isPartOf': {'@id': 'https://federalnewsnetwork.com/#website'}, 'breadcrumb': {'@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#breadcrumblist'}, 'author': {'@id': 'https://federalnewsnetwork.com/author/dthornton/#author'}, 'creator': {'@id': 'https://federalnewsnetwork.com/author/dthornton/#author'}, 'image': {'@type': 'ImageObject', 'url': 'https://federalnewsnetwork.com/wp-content/uploads/2016/11/army-cyber-21.jpg', '@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#mainImage', 'width': 1626, 'height': 1080, 'caption': 'Uniformed and civilian cyber and military intelligence specialists monitor Army networks in the Cyber Mission Unit’s Cyber Operations Center at Fort Gordon, Georgia'}, 'primaryImageOfPage': {'@id': 'https://federalnewsnetwork.com/artificial-intelligence/2019/05/army-exploring-ai-combat-possibilities-while-prioritizing-quick-cots-adoptions/#mainImage'}, 'datePublished': '2019-05-09T19:47:42-04:00', 'dateModified': '2020-05-13T19:56:42-04:00'}, {'@type': 'WebSite', '@id': 'https://federalnewsnetwork.com/#website', 'url': 'https://federalnewsnetwork.com/', 'name': 'Federal News Network', 'alternateName': 'WFED', 'description': 'Helping feds meet their mission.', 'inLanguage': 'en-US', 'publisher': {'@id': 'https://federalnewsnetwork.com/#organization'}}]",['Daisy Thornton'],2019-05-09T19:47:42Z,,"{'@type': 'SpeakableSpecification', 'cssSelector': ['.schema-title', '.schema-summary']}"
