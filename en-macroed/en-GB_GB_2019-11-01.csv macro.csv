URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,mainEntityOfPage,headline,image,author,datePublished,dateModified,publisher,itemListElement,article:section,article:summary,article text,name,thumbnailUrl,articleSection,@graph,url,potentialAction,logo,sameAs,@id,isBasedOn,video,creator,dateCreated
https://news.google.com/rss/articles/CBMiU2h0dHBzOi8vd3d3LmZpbmV4dHJhLmNvbS9uZXdzYXJ0aWNsZS8zNDY4MC9haS13aWxsLW5vdC1iZS1qb2Ita2lsbGVyLS0taWJtLXJlc2VhcmNo0gEA?oc=5,AI will not be job killer - IBM research - Finextra,2019-11-01,Finextra,https://www.finextra.com,"Contrary to the conventional wisdom, new technologies such as AI will actually kill off few jobs, according to research from the MIT-IBM Watson AI Lab.","Finextra,news,online,bank,banking,technology,finance,financial,fin,tech,fintech,IT,breaking,latest,retail,transaction,trade,execution,headlines,blockchain,digital,investment,mobile,business,challenger,payments,regtech,insurtech,services","Contrary to the conventional wisdom, new technologies such as AI will actually kill off few jobs, according to research from the MIT-IBM Watson AI Lab.","Contrary to the conventional wisdom, new technologies such as AI will actually kill off few jobs, according to research from the MIT-IBM Watson AI Lab.",http://schema.org,BreadcrumbList,"{'@type': 'WebPage', '@id': 'https://www.finextra.com/newsarticle/34680/ai-will-not-be-job-killer---ibm-research'}",AI will not be job killer - IBM research,"{'@type': 'ImageObject', 'url': 'https://www.finextra.com/finextra-images/top_pics/xl/ai brain.jpg', 'height': 270, 'width': 480}","{'@type': 'Person', 'name': 'Editorial Team'}",2019-11-01T00:01:00,2019-11-01T09:07:57,"{'@type': 'Organization', 'name': 'Finextra', 'logo': {'@type': 'ImageObject', 'url': 'https://www.finextra.com/about/finextra-logo.png', 'width': 512, 'height': 512}}","[{'@type': 'ListItem', 'position': 1, 'name': 'home page', 'item': 'https://www.finextra.com'}, {'@type': 'ListItem', 'position': 2, 'name': 'latest news', 'item': 'https://www.finextra.com/latest-news'}, {'@type': 'ListItem', 'position': 3, 'name': 'ai', 'item': 'https://www.finextra.com/channel/ai'}, {'@type': 'ListItem', 'position': 4, 'name': 'news-article', 'item': 'https://www.finextra.com/newsarticle/34680/ai-will-not-be-job-killer---ibm-research'}]",N/A,N/A,N/A,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiYGh0dHBzOi8vd3d3LnNpbGljb24uY28udWsvZS1pbm5vdmF0aW9uL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2dvb2dsZS1kaXNiYW5kcy1haS1jb3VuY2lsLTI0MzY0N9IBZGh0dHBzOi8vd3d3LnNpbGljb24uY28udWsvZS1pbm5vdmF0aW9uL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2dvb2dsZS1kaXNiYW5kcy1haS1jb3VuY2lsLTI0MzY0Ny9hbXA?oc=5,Google Disbands AI Advisory Council - Silicon UK,2019-11-05,Silicon UK,https://www.silicon.co.uk,That lasted long. Google pulls the plug on AI council over concern of a couple of its female members,N/A,That lasted long. Google pulls the plug on AI council over concern of a couple of its female members,N/A,https://schema.org,BreadcrumbList,"{'@type': 'WebPage', 'id': 'https://www.silicon.co.uk/e-innovation/artificial-intelligence/google-disbands-ai-council-243647'}",Google Disbands AI Advisory Council,"{'@type': 'ImageObject', 'url': 'https://www.silicon.co.uk/wp-content/uploads/2015/01/Google3.jpg', 'width': '1000', 'height': '667'}","{'@type': 'Person', 'name': 'Tom Jowitt'}",2019-04-05T08:55:37+01:00,2019-11-05T21:15:23+00:00,"{'@type': 'Organization', 'name': 'Silicon UK', 'logo': {'@type': 'ImageObject', 'url': 'https://www.silicon.co.uk/wp-content/themes/kamino/assets/images/favicons_silicon/mstile-70x70.png', 'width': '', 'height': ''}}","[{'@type': 'ListItem', 'position': 1, 'item': 'https://www.silicon.co.uk/', 'name': 'All Tech News'}, {'@type': 'ListItem', 'position': 2, 'item': 'https://www.silicon.co.uk/news/e-innovation', 'name': 'Innovation'}, {'@type': 'ListItem', 'position': 3, 'item': 'https://www.silicon.co.uk/news/e-innovation/artificial-intelligence', 'name': 'Artificial Intelligence'}, {'@type': 'ListItem', 'position': 4, 'item': 'https://www.silicon.co.uk/e-innovation/artificial-intelligence/google-disbands-ai-council-243647', 'name': 'Google Disbands AI Advisory Council'}]",N/A,N/A,"


Google Disbands AI Advisory Council

Tom Jowitt, April 5, 2019, 8:55 am | Updated on 5 November 2019, 21:15 















That lasted long. Google pulls the plug on AI council over concern of a couple of its female members

 Google has said it is “going back to the drawing board” as it considers the best guidance on ethical issues relating to artificial intelligence (AI) and machine learning.
Late last month Google created the ‘Advanced Technology External Advisory Council (ATEAC)’, to offer guidance on the ethical use of AI.
Yet just a week later Google has announced it is “ending the council”, apparently due to staff concerns at the inclusion of two female members in the council.

Troubled week
“It’s become clear that in the current environment, ATEAC can’t function as we wanted,” Google said in a statement. “So we’re ending the council and going back to the drawing board. We’ll continue to be responsible in our work on the important issues that AI raises, and will find different ways of getting outside opinions on these topics.”
The eight-member council includes Joanna Bryson, an associate professor in computing at the University of Bath; William J. Burns, a former US deputy secretary of state, and leading mathematician Bubacarr Bah.
The creation of the council had come after Alphabet CEO Sundar Pichai in June 2018 created new principles for AI use at Google, and pledged not to use AI for technology that causes injury to people.
But according to Vox.com, Google took the decision because of tensions with its staff and cancelled the council after less than one week.
Specifically the staff were angered about the inclusion of one person and the comments she had made about transsexual people.
And the inclusion of a drone company executive also prompted anger considering Google’s withdraw from a Pentagon drone project that utilised Google’s AI technology.
“Thousands of Google employees signed a petition calling for the removal of one board member, Heritage Foundation president Kay Coles James, over her comments about trans people and her organisation’s scepticism of climate change,” Vox reported. “Meanwhile, the inclusion of drone company CEO Dyan Gibbens reopened old divisions in the company over the use of the company’s AI for military applications.”
Board member Alessandro Acquisti had also announced his resignation from the board on Twitter, saying it wasn’t the right forum for him to continue dealing with “key ethical issues of fairness, rights & inclusion in AI.”
Project Maven
Google has had a chequered time with its AI development.
Last year it would not renew a contract to do artificial intelligence work for the US Pentagon.
The project was codenamed Project Maven, and Google’s decision to withdraw came after internal pressure from Google staff, some of whom had resigned over the matter.
Google’s involvement in Project Maven aimed to speed up the analysis of drone footage.
Essentially, the search engine giant was said to be using machine-learning algorithms and AI to help the US military assess drone footage quickly, in order to distinguish people and objects in drone videos.
Put your knowledge of artificial intelligence (AI) to the test. Try our quiz!
Advertising
 Read also : 
Japan’s SoftBank Acquires AI Chip Start-up Graphcore
Silicon UK In Focus PodcastsponsoriséSilicon In Focus Podcast: The Value of Data00:0000:0000:0000:00SubscribeEdisoundRSS FeedSpotifyDeezerAmazon MusicApple PodcastsShare EpisodeFacebookXLinkedInEpisode linkCopied !





 
Recommend this article:




                    0                









                    0                










 NEWSLETTER 
 Subscribe to our best articles 









 


 Facebook
                            




  Twitter




  Linkedin









Advertising



",Google Disbands AI Advisory Council,https://www.silicon.co.uk/wp-content/uploads/2015/01/Google3-600x400.jpg,Artificial Intelligence,,,,,,,,,,
https://news.google.com/rss/articles/CBMiaWh0dHBzOi8vd3d3LnBicy5vcmcvd2diaC9mcm9udGxpbmUvYXJ0aWNsZS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1hbGdvcml0aG1pYy1iaWFzLXdoYXQteW91LXNob3VsZC1rbm93L9IBAA?oc=5,Artificial Intelligence Can Be Biased. Here's What You Should Know. - PBS,2019-11-05,PBS,https://www.pbs.org,Artificial intelligence has already started to shape our lives in ubiquitous and occasionally invisible ways. A researcher in algorithmic bias talks to FRONTLINE about what she thinks the public needs to know about these systems.,N/A,Artificial intelligence has already started to shape our lives in ubiquitous and occasionally invisible ways. A researcher in algorithmic bias talks to FRONTLINE about what she thinks the public needs to know about these systems.,Artificial intelligence has already started to shape our lives in ubiquitous and occasionally invisible ways. A researcher in algorithmic bias talks to FRONTLINE about what she thinks the public needs,https://schema.org,,,,,,,,,,N/A,N/A,"



Artificial Intelligence Can Be Biased. Here’s What You Should Know.

Share:



Twitter


Facebook


E-mail


 








A display shows a facial recognition system for law enforcement during the NVIDIA GPU Technology Conference in Washington, DC, Nov. 1, 2017. (SAUL LOEB/AFP via Getty Images)





November 5, 2019

by


Priyanka Boghani





Artificial intelligence has already started to shape our lives in ubiquitous and occasionally invisible ways. In its new documentary, In The Age of AI, FRONTLINE examines the promise and peril this technology. AI systems are being deployed by hiring managers, courts, law enforcement, and hospitals — sometimes without the knowledge of the people being screened. And while these systems were initially lauded for being more objective than humans, it’s fast becoming clear that the algorithms harbor bias, too.
It’s an issue Joy Buolamwini, a graduate researcher at the Massachusetts Institute of Technology, knows about firsthand. She founded the Algorithmic Justice League to draw attention to the issue, and earlier this year she testified at a congressional hearing on the impact of facial recognition technology on civil rights.
“One of the major issues with algorithmic bias is you may not know it’s happening,” Buolamwini told FRONTLINE. We spoke to her about how she encountered algorithmic bias, about her research, and what she thinks the public needs to know.
This interview has been edited for length and clarity.
On her first encounter with algorithmic bias.
The first time I had issues with facial detection technology was actually when I was an undergraduate at Georgia Tech, and I was working on a robot. The idea with this robot was to see if I could get it to play peek-a-boo with me. And peek-a-boo doesn’t really work if your robot can’t see you, and my robot couldn’t see me. To get my project done, I borrowed my roommate’s face. She was lighter skinned than I was. …That was my first time really using facial analysis technology and seeing that it didn’t work for me the same way it worked for other people. …
I went on to do many things and became a graduate student at MIT and I started working on projects that used facial analysis technology, face detection. So one project I did was something called the Aspire Mirror. You look into a mirror, a camera detects your face and then a lion can appear on you, or you can be somebody you’re inspired by…
[I]t wasn’t detecting my face consistently, so I got frustrated. So what do you do when you get frustrated with your program? You debug. I started trying to figure out ways to make it work. I actually drew a face on my hand, and the system detected the face on my palm. And I was like, “Wait, wait, wait, if it’s detecting the face I just drew on my palm, then anything’s a possibility now.” So I looked around my office and the white mask was there. So I was like, “There’s no way! But why not?”
I pick up the white mask, and I put it on and it’s instantaneous when I put on that white mask, and I mean just — the symbolism of it was not lost to me. This is ridiculous that the system can detect this white mask that is not a real person, but cannot necessarily detect my face. So this is really when I started thinking, “Okay, let’s a dig a bit deeper with what’s going on with these systems.” …
On digging a bit deeper into facial analysis technology.
Here was a question: Do these systems perform differently on various faces? There was already a 2012 report that actually came out from an FBI facial analysis expert showing that facial recognition systems in particular worked better on white faces than black faces. They didn’t work as well on youthful faces. And they didn’t work as well on women as compared to men. This was 2012, and why I keep bringing that up is this was before the deep learning revolution…
Now we had a different approach that was supposed to be working much better. My question was, given these new approaches to facial analysis and facial recognition, are there still biases? Because what I’m experiencing, what my friends are experiencing — and what I’m reading about with reports that say, “Oh, we’ve solved face recognition,” or “We’re 97% accurate from benchmarks” — those reports were not lining up to my reality. …
What I focused on specifically was gender classification. …I wanted to choose something that I thought would be straightforward to explain, not that gender is straightforward — it’s highly complex. But insomuch as we were seeing binary gender classification, I thought that would be a place to start. By this time my weekend hobby was literally running my face through facial analysis and seeing what would happen. So some wouldn’t detect my face and others would label me male. And I do not identify as male. This is what led down that corridor.
On finding the “gold standard benchmarks” were not representative.
When I ran this test, the first issue that I ran into which gave me some more insight with the issue we’re talking about — algorithmic bias — was that our measures for how well these systems perform were not representative of the world. …We’ve supposedly done well on gold standard benchmarks. So I started looking at the benchmarks. These are essentially the data sets we use to analyze how well we’re doing as a research community or as an industry on specific AI tasks. So facial recognition is one of these tasks that people are benchmarked on all the time.
“What I started to see was something I call ‘power shadows’ — when either the inequalities or imbalances that we have in the world become embedded in our data.”
The thing is, we often times don’t question the status quo or the benchmark. This is the benchmark, why would I question it? But sometimes the gold standard turns out to be pyrite. And that is what was happening in this case. When I went to look at the research on the breakdown of various facial analysis systems, what I found was one of the leading gold standards, labeled “Faces in the Wild,” was over 70% male and 80% white. This is when I started looking into more and more data sets and seeing that you had massive skews. Sometimes you had massive skews because you were using celebrities. I mean, celebrities don’t necessarily look like the rest of the world. What I started to see was something I call “power shadows” — when either the inequalities or imbalances that we have in the world become embedded in our data. …
All this to say, the measures that we had for determining progress with facial analysis technology were misleading because they weren’t representative of people — at least the U.S. in that case. …We didn’t have data sets that were actually reflective of the world, so for my thesis at MIT, I created what I call the Pilot Parliaments Benchmark. I went to UN women’s websites, I got a list of the top 10 nations in the world by their representation of women in parliament. … So I chose European countries and African nations to try to get a spread on opposite ends of skin types, lighter skin and darker skin. After I ran into this issue that the benchmarks were misleading, I needed to make the benchmark.
On what her research found.
Then finally, I could get to the research question. …So I wanted to know how accurate are they at this reduced task of binary gender classification — which is not at all inclusive — when it comes to guessing the gender of the face? And it turned out that there were major gaps. This was surprising because these were commercially sold products. … You know how the story goes. It turns out, the systems work better on male-labeled faces than female-labeled faces, they work better on lighter faces than darker-skinned faces.
But one thing we did for this study, which I would stress for anybody who’s thinking about doing research in algorithmic bias or concerned with algorithmic bias and AI harms, is we did an intersectional analysis. We didn’t just look at skin type. We didn’t just look at gender. We looked at the intersection. And the inspiration for this was from Kimberlé Crenshaw, a legal scholar who in 1989 introduced the term of intersectionality. …What would happen with the analysis is if you did it in aggregate just based on race, or if you did it in aggregate based on just gender, you might find based on those axes that there isn’t substantial evidence of discrimination. But if you did it at the intersection, you would find there was a difference. And so I started looking at the research studies around facial analysis technologies and facial recognition technologies and I saw that usually we just have aggregate results — just one number for accuracy. People are just optimizing for that overall accuracy, which means we don’t get a sense of the various ways in which the system performs for different types of people. It’s the differences in the performance, the accuracy disparities that I was fascinated by, but not just on a single axis but also on the intersection. So when we did the intersectional breakdown — oooh, it was crazy. …
We weren’t doing anything to try to trick the system. It was an optimistic test. This is why I was very surprised, because even with this optimistic test, in the worst-case scenario for the darkest-skinned women, you actually had error rates as high as 47% on a binary classification task. …
I shared the results with the companies and I got a variety of responses. But I think the overall response, at least with the first study, was there was an acknowledgement of an issue with algorithmic bias.
On how AI is already affecting people’s lives.
There’s a paper that just came out from Science which is devastating, showing risk assessment algorithms used in health care… actually have racial bias against black patients. We’re talking about health care where the whole point is to try to optimize the benefit and what they were seeing was because they used how much money is spent on an individual as a proxy for how sick they were, it turned out it was not a good proxy because black patients who were actually sick were being said to be not as sick as they were. …
“When these systems fail, they fail most the people who are already marginalized, the people who are already vulnerable.”
You also have AIs that are determining the kind of ads people see. And so there have been studies that show you can have discriminatory ad targeting. Or you can have a situation where you have an ad for CEO and the system over time learns to present that CEO ad to mainly men. You were saying, how do you know if you’ve encountered bias — the thing is you might never know if you’ve encountered the bias. …Something that might happen to other people — you see phenotypic fails with passport renewals. So you have a report from a New Zealand man of Asian descent being told that his eyes are closed and he needs to upload another photo. Meanwhile, his eyes are not closed. You have, in the UK, a black man being told his mouth is open. His mouth was not open. You have these systems that are seeping into every day.
You have AI systems that are meant to verify if you’re who you say you are. And so one way that can happen is with ride share apps. Uber, for example, will ping drivers to have them verify their ID. There’s actually a report from trans drivers who were saying that they were being repeatedly [asked] to submit their IDs because they were not matching. They were being either kicked out of the system or having to stop the car, test it again, which means you’re not getting the same level of economic opportunity. …
When these systems fail, they fail most the people who are already marginalized, the people who are already vulnerable. And so when we think about algorithmic bias, we really have to be thinking about algorithmic harm. That’s not to say we don’t also have the risk of mass surveillance, which impacts everybody. We also have to think about who’s going to be encountering the criminal justice system more often because of racial policing practices and injustices.
On what the public needs to know about algorithmic bias.
There’s no requirement for meaningful transparency, so these systems can easily be rolled out without our ever knowing. So one thing I wish people would do more of and something that companies also ought to do more of is having transparency so that you even know that an AI system was used in the first place. You just might never get the callback. You just might pay the higher price. You would never actually know. What I want the public to know is AI systems are being used in hidden ways that we should demand are made public.
The other thing I want the public to have is actually choice — affirmative consent. Not only should I know if an AI system is being used, but let’s say it makes the wrong decision or something that I contest. There’s no path to due process that’s mandated right now. So if something goes wrong, what do you do?
Sometimes I’ll hear, at least in the research community, efforts to “de-bias” AI or eradicate algorithmic bias. And it’s a tempting notion, let’s just get rid of the bias and make the systems more fair, more inclusive, some ideal. And I always ask, but have we gotten rid of the humans? Because even if you create some system you believe is somehow more objective, it’s being used by humans at the end of the day. …I don’t think we can ever reach a true state of something being unbiased, because there are always priorities. This is something I call the “coded gaze.” The “coded gaze” is a reflection of the priorities, the preferences and also the prejudices of those who are shaping technology. This is not to say we can’t do our best to try to create systems that don’t produce harmful outcomes. I’m not saying that at all. What I am saying is we also have to accept the fact that being human we’re going to miss something. We’re not going to get it all right. …
“What I want the public to know is AI systems are being used in hidden ways that we should demand are made public.”
Instead of thinking about “Oh, we’re going to get rid of bias,” what we can think about is bias mitigation — knowing that we have flaws, knowing that our data has flaws, understanding that even systems we try to perfect to the best of our abilities are going to be used in the real world with all of its problems. …
Before we get to the point where it’s having major harms with real world consequences, there need to be processes in place to check through different types of bias that could happen. So, for example, AI [systems] now have algorithmic risk assessments that they have as a process of really thinking through what the societal impact of the system are in its design and development stages before you get to the deployment. Those kinds of approaches, I believe, are extremely helpful, because then we can be proactive instead of reacting to the latest headline and playing bias whack-a-mole. …
On proposals for oversight and regulation.
You have a proposal for an Algorithmic Accountability Act, this is a nationwide push that would actually require assessing systems for their social impact. And I think that’s really important. We have something with the Algorithmic Justice League that’s called the Safe Face Pledge, which outlines actionable steps companies can take to mitigate harms of AI systems. …
I absolutely think regulation needs to be the first and foremost tool, but alongside regulation providing not just the critique of what’s wrong with the system, but also steps that people can take to do better. Sometimes the step to take to do better is to commit to not developing a particular kind of technology or particular use case for technology. So with facial analysis systems, one of our banned uses is any situation where lethal force can be used. So it would mean we’re not supporting facial recognition on police body cameras. Or facial recognition on lethal autonomous weapons. …
And I think the most important thing about the Safe Face Pledge that I’ve seen is one, the conversations that I’ve had with different vendors, where whether or not they adopt it actually going through those steps and thinking about their process and changes they can make in the process I believe has made internal shifts that likely would not hit the headlines. Because people would rather quietly make certain kinds of changes. The other thing is making it where the commitments have to be part of your business processes. Not a scouts’ honor pledge, just trust us. If you are committed to actually making this agreement, it means you have to change your terms of service and your business contracts to reflect what these commitments are. …
On what should be done to fix the problem.
One, I think, demand transparency and ask questions. Ask questions if you’re using a platform, if you’re going to a job interview. Is AI being used? The other thing I do think is supporting legislative moves. …
When I started talking about this, I think in 2016, it was such a foreign concept in the conversations that I would have. And now, today, I can’t go online without seeing some kind of news article or story about a biased AI system of some shape or form. I absolutely think there has been an increase in public awareness, whether through books like Cathy O’Neil’s Weapons of Math Destruction. There’s a great new book out by Dr. Ruha Benjamin — Race After Technology.
People know it’s an issue and so I’m excited about that. Has there been enough done? Absolutely not. Because people are just now waking up to the fact that there’s a problem. Awareness is good, and then that awareness needs to lead to action. That is the phase we’re in. Companies have a role to play, governments have a role to play and individuals have a role to play.
When you see the bans in San Francisco [of facial recognition technology by the city’s agencies]… what you saw was a very powerful counter-narrative. What we were hearing was that this technology is inevitable, there’s nothing you can do. …When you hear there’s nothing you can do, you stop trying. But what was extremely encouraging to me with the San Francisco ban — and then you have Somerville that came from the folks who are in Boston — people have a voice and people have a choice. This technology is not inherently inevitable. We have to look at it and say: What are the benefits and what are the harms? If the harms are too great, we can put restrictions and we can put limitations. And this is necessary. I do look to those examples and they give me hope.








Priyanka Boghani, Digital Editor, FRONTLINE


Email:
priyanka_boghani@wgbh.org


Twitter:
@priyankaboghani







Journalistic Standards











Watch the Documentary
In the Age of AI






    Support Provided By
    Learn more







See What FRONTLINE Is Working On Now






Get Our Newsletter





Related Articles




How China’s Government Is Using AI on Its Uighur Muslim Population
November 21, 2019





The Jobs Robots Can't Do (At Least Not Yet)
November 5, 2019





Could the Rise of Artificial Intelligence Put Truckers’ Jobs in Peril?
November 5, 2019





Topics


Social Issues






",,,,"[{'@type': 'NewsArticle', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/#article', 'isPartOf': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/'}, 'author': [{'@type': 'Person', 'name': 'Priyanka Boghani', 'url': 'https://www.pbs.org/wgbh/frontline/person/priyanka-boghani/', 'affiliation': {'@type': 'NewsMediaOrganization', 'name': 'FRONTLINE', 'url': 'https://www.pbs.org/wgbh/frontline/about-us/contact-us/'}, 'knowsAbout': [{'@type': 'Thing', 'name': 'foreign policy'}]}], 'headline': 'Artificial Intelligence Can Be Biased. Here&#8217;s What You Should Know.', 'datePublished': '2019-11-06T00:23:31+00:00', 'dateModified': '2019-11-06T01:32:48+00:00', 'mainEntityOfPage': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/'}, 'wordCount': 3285, 'publisher': {'@type': 'Organization', 'name': 'Frontline PBS', 'logo': {'@type': 'ImageObject', 'url': 'https://www.pbs.org/wgbh/frontline/wp-content/themes/fl-responsive-theme/library/images/frontline_logo_og.png'}}, 'image': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/#primaryimage'}, 'thumbnailUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/GettyImages-869082432.jpg', 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/', 'url': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/', 'name': ""Artificial Intelligence Can Be Biased. Here's What You Should Know. | FRONTLINE"", 'isPartOf': {'@id': 'https://www.pbs.org/wgbh/frontline/#website'}, 'primaryImageOfPage': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/#primaryimage'}, 'image': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/#primaryimage'}, 'thumbnailUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/GettyImages-869082432.jpg', 'datePublished': '2019-11-06T00:23:31+00:00', 'dateModified': '2019-11-06T01:32:48+00:00', 'description': 'Artificial intelligence has already started to shape our lives in ubiquitous and occasionally invisible ways. A researcher in algorithmic bias talks to FRONTLINE about what she thinks the public needs to know about these systems.', 'breadcrumb': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/#primaryimage', 'url': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/GettyImages-869082432.jpg', 'contentUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/GettyImages-869082432.jpg', 'width': 1920, 'height': 1221, 'caption': 'A display shows a facial recognition system for law enforcement during the NVIDIA GPU Technology Conference in Washington, DC, Nov. 1, 2017.'}, {'@type': 'BreadcrumbList', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-algorithmic-bias-what-you-should-know/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.pbs.org/wgbh/frontline/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Articles', 'item': 'https://www.pbs.org/wgbh/frontline/articles/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Social Issues', 'item': 'https://www.pbs.org/wgbh/frontline/topic/social-issues/'}, {'@type': 'ListItem', 'position': 4, 'name': 'Artificial Intelligence Can Be Biased. Here&#8217;s What You Should Know.'}]}, {'@type': 'WebSite', '@id': 'https://www.pbs.org/wgbh/frontline/#website', 'url': 'https://www.pbs.org/wgbh/frontline/', 'name': 'FRONTLINE', 'description': 'FRONTLINE', 'publisher': {'@id': 'https://www.pbs.org/wgbh/frontline/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.pbs.org/wgbh/frontline/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.pbs.org/wgbh/frontline/#organization', 'name': 'FRONTLINE | PBS', 'url': 'https://www.pbs.org/wgbh/frontline/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.pbs.org/wgbh/frontline/#/schema/logo/image/', 'url': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2020/10/frontline_logo_opengraph.png', 'contentUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2020/10/frontline_logo_opengraph.png', 'width': 1200, 'height': 630, 'caption': 'FRONTLINE | PBS'}, 'image': {'@id': 'https://www.pbs.org/wgbh/frontline/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/frontline', 'https://twitter.com/frontlinepbs', 'https://www.instagram.com/frontlinepbs/', 'https://www.youtube.com/user/pbsfrontline'], 'publishingPrinciples': 'https://www.pbs.org/wgbh/frontline/about-us/editorial-standards-and-ethics', 'noBylinePolicy': 'https://www.pbs.org/wgbh/frontline/about-us/editorial-standards-and-ethics/#verification-and-fact-checking-standards'}]",,,,,,,,,
https://news.google.com/rss/articles/CBMiVGh0dHBzOi8vaGJyLm9yZy8yMDE5LzExL2FzLWpvYnMtYXJlLWF1dG9tYXRlZC13aWxsLW1lbi1hbmQtd29tZW4tYmUtYWZmZWN0ZWQtZXF1YWxsedIBAA?oc=5,"As Jobs Are Automated, Will Men and Women Be Affected Equally? - HBR.org Daily",2019-11-01,HBR.org Daily,https://hbr.org,"What will work look like for the next generation of women, especially as more of their roles are being automated — or even replaced — by artificial intelligence (AI)? And how can leaders ensure that AI does not lead to gender bias in their organizations? Recent research is beginning to answer these questions, and the outlook is mixed: on the one hand, women may be spared from the job disruptions men will face in the longer-term. On the other, the lack of gender diversity in AI-related jobs could be reflected in the tools that are created, affecting whether women are hired or promoted. Employers should be thinking about this job re-distribution in advance, to help ensure that a wave of redundancies following technological change does not lead to a sudden worsening in organizational gender balance.",N/A,"What will work look like for the next generation of women, especially as more of their roles are being automated — or even replaced — by artificial intelligence (AI)? And how can leaders ensure that AI does not lead to gender bias in their organizations? Recent research is beginning to answer these questions, and the outlook is mixed: on the one hand, women may be spared from the job disruptions men will face in the longer-term. On the other, the lack of gender diversity in AI-related jobs could be reflected in the tools that are created, affecting whether women are hired or promoted. Employers should be thinking about this job re-distribution in advance, to help ensure that a wave of redundancies following technological change does not lead to a sudden worsening in organizational gender balance.",N/A,https://schema.org,WebSite,,,,,,,,,Gender,N/A,N/A,,,,,https://hbr.org/,"{'@type': 'SearchAction', 'target': 'https://hbr.org/search?term={search_term_string}', 'query-input': 'required name=search_term_string'}",,,,,,,
https://news.google.com/rss/articles/CBMiaWh0dHBzOi8vY2FyZGlvdmFzY3VsYXJidXNpbmVzcy5jb20vdG9waWNzL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2hvdy1haS13aWxsLWltcGFjdC1oZWFsdGhjYXJlLXdvcmtmb3JjZdIBAA?oc=5,How AI Will Impact the Healthcare Workforce - Cardiovascular Business,2019-11-04,Cardiovascular Business,https://cardiovascularbusiness.com,The American Hospital Association predicts how hospitals will operate when AI and machine learning are on board.,"Artificial Intelligence, Patient Care",The American Hospital Association predicts how hospitals will operate when AI and machine learning are on board.,The American Hospital Association predicts how hospitals will operate when AI and machine learning are on board.,https://schema.org,,,,,,,,,,N/A,N/A,"  How AI Will Impact the Healthcare WorkforceAmy Baxter | November 04, 2019 | Artificial Intelligencetweetprintsharesharemail   In the not-too-distant future, hospitals will operate very differently than they do today thanks to the artificial intelligence (AI) boom, according to a new Market Insights report by the American Hospital Association’s (AHA) Center for Health Innovation. For many members of the healthcare workforce, the most tangible changes will manifest in how their work is completed, with AI, machine learning and robotic process automation (RPA) all having an impact. The AHA cites research concluding that 40 percent of the tasks currently performed by non-clinical staff and 33 percent of clinicians’ jobs could be done by AI. In the future, some tasks will be outsourced to technology, and healthcare workers will see their roles change. Make way for automation … The AHA expects that RPA will have the biggest impact on most healthcare jobs because of its potential to add capacity, cut staffing costs and reduce human error by automating manual, repetitive and rules-based tasks. Such tasks will include billing, claims submission, patient enrollment, insurance verification, patient scheduling, inventory management and contract management.For many, RPA will be a welcome entrant into their workflow, freeing them to spend more time assisting and caring for patients or tackling other duties that technology can’t perform. For workers who specialize in tasks like these, however, RPA also could result in job loss and the need to shift to other work. … And a data influx Clinicians may find the benefits of AI extending beyond automating tasks and toward incorporating more data into their decision-making. Machine-learning models with clinical decision support capabilities already are being woven into healthcare workflows, with predictive tasks improving diagnoses and disease classification. “In the future, AI will make sense of the overwhelming amount of data created from genomics, biosensors, smartphone apps, the electronic health record, unstructured notes and data on social determinants of health, and create a broader context for clinicians to deliver high-quality, patient-centered care,” the AHA report reads.However, for machine learning to be useful to clinicians, the data will need to be accurate—a requirement that could result in healthcare workers taking on new tasks related to ensuring data integrity. They may need to learn new digital skills, such as digital and AI acumen, data appreciation and agility.“With AI as their new co-worker, staff will need to acquire new skill sets and competencies to take advantage of AI capabilities, and the educational pipeline needs to equip those entering the health care workforce with new skills,” the AHA says.As new jobs emerge … One result of having these new technologies in the healthcare workplace will be the premium placed on positions that facilitate the technology. Among them: data scientists, AI engineers, data governance experts, data entry experts, data engineers and chief AI officers.… Patient-centric priorities will endureTo its hospital and health system constituents who are considering how to prepare for such changes, the AHA urges a continued prioritization of people skills and the patient relationship. Those aspects of healthcare aren’t likely to change, the organization predicts.“The workforce of the future not only will need people with technical skills, but also soft skills like communication and empathy to take full advantage of what AI gives them to do their jobs,” the AHA says. More...  November/December 2019   Cardiology’s Challenge for the 2020s: Turning the Trend on Rising Mortality   The Art of Storytelling: Innovation, Advocacy & the War on ‘Fake News’ in Medicine    Destination Question: Should All STEMI Patients Recover in the ICU?    Spread Too Thin? Strategies for Deploying Cardiology Teams Across Sites of Service    How AI Will Impact the Healthcare Workforce   Look Before You Leap & Other Advice for Cardiologists Considering Telemedicine   Employers Clamp Down on Rising Healthcare Benefit Costs   Making a Statement: ACC Takes a Stand on Workplace Equity   Performance Report: Critical Characteristics of High-performing Cardiology Programs—Start at the Top   Storytelling Is AdvocacyAmy BaxterAmy joined TriMed Media as a Senior Writer for HealthExec after covering home care for three years. When not writing about all things healthcare, she fulfills her lifelong dream of becoming a pirate by sailing in regattas and enjoying rum. Fun fact: she sailed 333 miles across Lake Michigan in the Chicago Yacht Club ""Race to Mackinac.""Related ContentAI creates accurate 4D heart scans in secondsFemale cardiologists much more likely to receive negative reviewsRemote monitoring, AI to play key roles in the future of cardiologyFDA clears new AI algorithm for 1-year AFib riskDual approvals: AliveCor gains FDA clearance for advanced AI model, handheld ECG systemAI-powered platform for arrhythmia detection gains FDA approvalAround the webHealth Imaging New imaging protocols proposed to curb rise of cardiovascular infectionsEleven medical societies have signed on to a consensus statement aimed at standardizing imaging for suspected cardiovascular infections.Radiology Business What does radiology have to do with climate change?Kate Hanneman, MD, explains why many vendors and hospitals want to lower radiology's impact on the environment. ""Taking steps to reduce the carbon footprint in healthcare isn’t just an opportunity,"" she said. ""It’s also a responsibility.""Radiology Business Philips launches new AI-enabled CT scanner aimed at cardiology at ECR 2024Philips introduced a new CT system at ECR aimed at the rapidly growing cardiac CT market, incorporating numerous AI features to optimize workflow and image quality.",,,,"[{'@type': 'NewsArticle', 'headline': 'How AI Will Impact the Healthcare Workforce', 'name': 'How AI Will Impact the Healthcare Workforce', 'image': {'@type': 'ImageObject', 'url': 'https://cardiovascularbusiness.com/sites/default/files/styles/facebook/public/2019-11/ai_ci.jpg?h=75e7b748&itok=Axkz0w62', 'width': '1200', 'height': '630'}, 'datePublished': '2019-11-04T17:46:34+0000', 'dateModified': '2022-02-07T19:24:20+0000', 'publisher': {'@type': 'Organization', 'name': 'Cardiovascular Business', 'url': 'https://cardiovascularbusiness.com/'}}]",,,,,,,,,
https://news.google.com/rss/articles/CBMiSWh0dHBzOi8vYnVpbHRpbi5jb20vYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UvZGVzaWduaW5nLWFpLXdpdGgtaHVtYW4tdG91Y2jSAQA?oc=5,How Humility Helps AI Work Better With Human Users - Built In,2019-11-05,Built In,https://builtin.com,Making AI smarter is just the first step. Getting users to like it can be just as important.,N/A,Making AI smarter is just the first step. Getting users to like it can be just as important.,Making AI smarter is just the first step. Getting users to like it can be just as important.,https://schema.org,,,,,,,,,,N/A,N/A,"
























In the 1950s, Alan Turing proposed a contest that would become the gold standard for measuring AI sophistication: the Turing Test, wherein a machine attempts to trick people into thinking it’s human. 
But as AI’s ability to interact with humans has progressed, so, too, have companies’ understanding of what it should feel like to talk to a robot. 

“Even if you can make AI feel human, you don’t necessarily want to.” 

“Even if you can make AI feel human, you don’t necessarily want to,” said David Vandegrift, who is a former investor turned co-founder and CTO of an AI startup. “Because people will feel betrayed if they find that they’ve been talking to something that felt human, but wasn’t actually.”
Vandegrift’s Chicago startup, 4Degrees, leverages artificial intelligence to help individuals identify their most important professional relationships. Their challenge lies in designing an AI system that nudges professionals to network with the right people, without being annoying or creepy about it. It’s a practice that has long proved resistant to disruption by tech, because it requires a human touch. 
Networking brings up feelings of anxiety for many, Vandegrift said, which can stop someone from reaching out to their old boss to catch up over lunch, for example. It’s hard to use technology to convince someone to do something they feel fundamentally uncomfortable about. It’s also tough when a machine reveals you know fewer people than you thought you did. And who wants AI telling them their connections are shallow? 
Even building a system for predicting who would be a good professional match presents a complication. Because the outcome of an AI system is dependent on the information it’s trained on, ensuring a system presents a nuanced perspective — and not just, say, a venture capitalist’s thoughts on who’s important to know — can be tricky. 
“For the most part, people today are still building relationships the same way they were 10, or 100, or even 1,000 years ago,” Vandegrift said. “It’s a problem that people have been trying to solve for quite awhile. But it’s a problem that’s proven very resilient, in terms of like, nobody's solved it.” 
In taking on the challenge, Vandegrift and his team learned some hard-earned lessons about what humans really want from AI. 
Even if you could build an AI that seems human, you may not want to. Many people respond poorly to being tricked by computers.
	 
Think carefully about the “voice” and tone of your application. Don’t make your users feel like they're being bossed around. 
	 
Your AI will inevitably get things wrong. Interactions that emphasize humility will make the user more likely to forgive you. 
	 
Building nuanced AI requires nuanced training data. Your models won’t incorporate perspectives they’ve never been exposed to.
Image via 4degreesNuanced AI needs nuanced training data
4Degrees derives most of its insights from email and calendar data. It currently focuses on metadata, like whom you email, how often they reply and how long it takes them to do so. It also leverages a set of approximately 250 tags to comb data from users’ Twitter, LinkedIn and other sources to find the most accurate way to categorize them by profession, industry, interests and skills. 
“Based on essentially the words that they’re using, we can infer [whether they are VCs or healthtech entrepreneurs]’ — things like that,” Vandegrift said.
Vandegrift — who recently published the book The Future of Business, about AI and its practical implications — said the main challenge in building an AI system is being thoughtful about the underlying data the system is trained on. Using training data that is reflective of a variety of perspectives is essential in ensuring that an AI model makes the best suggestions for everyone. 

“What data are you actually feeding into the model? Who are the people you asked, ‘Is this someone worth knowing or not?’” 

“What data are you actually feeding into the model? Who are the people you asked, ‘Is this someone worth knowing or not?’” Vandegrift asked. “Because that’s all going to show up in the outcomes of the model.”
4Degrees has steered clear of ranking an individual’s connections because that can introduce bias into the AI system. For example, CEOs of Fortune 500 companies are important people for any venture capitalist or investor to know, Vandegrift said. But the vast majority of company heads are also white men. Training the AI system to prioritize those relationships would inadvertently cause the system to discriminate against women and people of color.   
“We’re being proactive in what we’re not developing,” Vandegrift said.  
 
4Degrees cofounders David vandegrift (left) and Ablorde Ashigbi (right) Good UX isn’t always about making things easier
4Degrees aims to change what it means to network. In the process, it has altered users’ understanding of their own connections. 
Most people think they have “hundreds or thousands” of connections in their industry, Vandegrift said. But after two years on the market, he’s convinced their reality really lies in Dunbar’s number, a theory that states there is a cognitive limit to the number of stable social relationships an individual can hold. Most people have about 100 strong connections, he said. After that, the quality of an individual’s relationships starts to falter. 
Vandegrift said most of 4Degrees users were surprised — or insulted — when the platform first revealed how few connections they actually had.
When people inflate the size of their networks, they tend to spend less time than they need to on growing their professional base. And because networking isn’t a task that requires immediate attention, it’s easy to prioritize other items instead. But, Vandegrift cautions people against cutting corners. For example: 4Degrees receives a lot of requests from users to build features that simply write and send emails for them — say, congratulating a CEO friend for receiving an award, or an old coworker for starting a new job. 
It sounds good in theory, but Vandegrift is reluctant to try it.

“We believe that the relationship component should come from you and it should be thoughtful.” 

“We believe that the relationship component should come from you and it should be thoughtful,” he said. 
 
People don’t like being bossed around by AI
Google CEO Sundar Pichai presents Duplex at Google's annual I/O conference in 2018. (Image via Google)  In 2018, Google unveiled its Duplex assistant at its annual developer conference. In front of a crowd numbered in the thousands, the machine called a hair salon and showcased conversational skills so strong that the receptionist on the other line had no idea they were talking with a robot. Attendees of Google’s I/O conference were shocked by Duplex’s technological advances — and a little creeped out. 
The machine passed the Turing Test, but was that a good thing? Google eventually committed to notifying all end users of Duplex that they were engaging with a robot.
In Vandegrift’s mind, an AI system should accomplish its task and leave people feeling comfortable after their interaction. For 4Degrees, comfort comes in the form of how it structures its suggestions and illustrates its emails. 
When the company first launched, Vandegrift said 4Degrees’ system was designed so that, if a user failed to contact their connection during the suggested amount of time, 4Degrees would issue them a warning message, calling them out for missing a deadline the robot imposed. 
Vandegrift said 4Degrees eventually removed the feature because it made people feel bad for missing a deadline they hadn’t really agreed to. Instead of admonishing users for missing an opportunity to connect, now when the time comes for an individual to reach out, 4Degrees suggests additional opportunities for users to call up their professional connections, subtly nudging them to invite their old boss to coffee. 
“We’re prioritizing updates about those people now, rather than saying, ‘It’s so bad that you haven’t reached out,’” Vandegrift said.  
 
Humility will make your AI easier to forgive
In its suggestions about when to connect, 4Degrees also structures its findings as suggestions, rather than declarative statements. If the system notices a flight booked to San Francisco, for example, it will ask about the upcoming trip, rather than go straight to making suggestions. By structuring its ask with humility, Vandegrift said it helps users forgive the system when it inevitably gets things wrong — AI operates on educated guesses, after all.
To Vandegrift, the key to building an AI systems’ user experience is to highlight the fact that it’s not human. In the emails 4Degrees sends to users with opportunities to reach out, for example, the firm features an illustrated cartoon robot.
Vandegrift named the Seattle-based Textio as an example of an AI company that is thoughtful about its user experience. Textio analyzes company job postings and helps firms make them better by focusing on its own research, which, for example, found that postings with the word “rockstar” in them attract half as many women candidates as career advertisements that do not. 
After running its AI through a company’s job posting, Textio then presented its suggestions to the client with an explanation of why they chose to eliminate certain words, as well as add others. The customer determines for themselves how they want to use the AI’s recommendations. By explaining the research that backs the AI system, Vandegrift said Textio drives human trust in the machine. 

""You actually need to approach the design of AI systems fundamentally differently than traditional systems."" 

“You actually need to approach the design of AI systems fundamentally differently than traditional systems,” Vandegrift said. “You have to bring humility to that suggestion versus the confidence of like, ‘No, I know that.’ It’s actually a skill set that you should look for in your product manager, your designer and your engineers.” 
Want to read more about AI?Check out our guide to artificial intelligence.



",,,,"[{'@context': 'https://schema.org', '@type': 'Article', 'headline': 'How Humility Helps AI Work Better With Human Users', 'name': 'How Humility Helps AI Work Better With Human Users', 'description': 'In the 1950s, Alan Turing proposed a contest that would\xa0become the gold standard for measuring AI sophistication: the Turing Test, wherein a machine attempts to trick people into thinking it’s human.\xa0&#13;', 'image': {'@type': 'ImageObject', 'url': 'https://builtin.com/sites/www.builtin.com/files/artificial%2520intelligence%2520networking.jpg', 'representativeOfPage': True}, 'mainEntityOfPage': {'@type': 'WebPage', '@id': 'https://builtin.com/artificial-intelligence/designing-ai-with-human-touch', 'name': 'How Humility Helps AI Work Better With Human Users'}, 'url': 'https://builtin.com/artificial-intelligence/designing-ai-with-human-touch', 'about': {'@type': 'Thing', 'name': 'Artificial Intelligence'}, 'author': {'@type': 'Person', '@id': 'https://builtin.com/authors/nona-tepper', 'name': 'Nona Tepper', 'description': 'Nona Tepper is a former Built In staff reporter covering technology industry trends. She holds a masters of journalism from Northwestern University and a bachelor of arts in journalism and english from Indiana University Bloomington. Prior to joining Built In, Tepper was a reporter and editor for Forest Park Review and Wednesday Journal, and her work has appeared in The Washington Post, Crain’s Chicago Business, Slate, VICE and MarketWatch. She is currently a staff reporter at Modern Healthcare.\r\n', 'jobTitle': 'Staff Reporter', 'sameAs': 'https://www.linkedin.com/in/nona-tepper-71534346/', 'url': 'https://builtin.com/authors/nona-tepper'}, 'datePublished': '2019-11-05T16:49:54+00:00', 'publisher': {'@type': 'Organization', '@id': 'https://builtin.com', 'name': 'Built In', 'url': 'https://builtin.com', 'sameAs': ['https://www.facebook.com/BuiltInHQ/', 'https://twitter.com/builtin', 'https://www.instagram.com/builtin/', 'https://www.linkedin.com/company/built-in'], 'brand': {'@type': 'Brand', 'name': 'Built In'}, 'logo': {'@type': 'ImageObject', 'url': 'https://static.builtin.com/dist/images/built-logo.png', 'representativeOfPage': True}}}]",,,,,,,,,
https://news.google.com/rss/articles/CBMiO2h0dHBzOi8vd3d3LmNhdGFseXN0Lm9yZy9yZXNlYXJjaC93b21lbi1mdXR1cmUtd29yay1yZXBvcnQv0gEA?oc=5,Women and the Future of Work (Report) - Catalyst,2019-11-05,Catalyst,https://www.catalyst.org,"Catalyst explores the future of work with a gender, diversity, and inclusion lens.",N/A,Catalyst's new report on the future of work and women examines how to reimagine diversity and inclusion for the 21st century.,"Catalyst explores the future of work with a gender, diversity, and inclusion lens.",https://schema.org,,,,,,,,,,N/A,N/A,N/A,,,,"[{'@type': 'WebPage', '@id': 'https://www.catalyst.org/research/women-future-work-report/', 'url': 'https://www.catalyst.org/research/women-future-work-report/', 'name': 'Women and the Future of Work (Report) | Catalyst', 'isPartOf': {'@id': 'https://www.catalyst.org/#website'}, 'primaryImageOfPage': {'@id': 'https://www.catalyst.org/research/women-future-work-report/#primaryimage'}, 'image': {'@id': 'https://www.catalyst.org/research/women-future-work-report/#primaryimage'}, 'thumbnailUrl': 'https://www.catalyst.org/wp-content/uploads/2019/11/woman-futuristic-office.png', 'datePublished': '2019-11-05T12:34:36+00:00', 'dateModified': '2023-08-21T22:05:22+00:00', 'description': 'Catalyst explores the future of work with a gender, diversity, and inclusion lens.', 'breadcrumb': {'@id': 'https://www.catalyst.org/research/women-future-work-report/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.catalyst.org/research/women-future-work-report/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.catalyst.org/research/women-future-work-report/#primaryimage', 'url': 'https://www.catalyst.org/wp-content/uploads/2019/11/woman-futuristic-office.png', 'contentUrl': 'https://www.catalyst.org/wp-content/uploads/2019/11/woman-futuristic-office.png', 'width': 1200, 'height': 800, 'caption': 'women in futuristic office space'}, {'@type': 'BreadcrumbList', '@id': 'https://www.catalyst.org/research/women-future-work-report/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.catalyst.org/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Research Elements', 'item': 'https://www.catalyst.org/research/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Women and the Future of Work (Report)'}]}, {'@type': 'WebSite', '@id': 'https://www.catalyst.org/#website', 'url': 'https://www.catalyst.org/', 'name': 'Catalyst', 'description': 'Catalyst, a global nonprofit organization, helps build workplaces that work for women with preeminent thought leadership and actionable solutions.', 'publisher': {'@id': 'https://www.catalyst.org/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.catalyst.org/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.catalyst.org/#organization', 'name': 'Catalyst', 'url': 'https://www.catalyst.org/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.catalyst.org/#/schema/logo/image/', 'url': 'https://www.catalyst.org/wp-content/uploads/2019/03/Catalyst-Logo-1200x630.png', 'contentUrl': 'https://www.catalyst.org/wp-content/uploads/2019/03/Catalyst-Logo-1200x630.png', 'width': 1200, 'height': 630, 'caption': 'Catalyst'}, 'image': {'@id': 'https://www.catalyst.org/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/catalystinc/', 'https://x.com/CatalystInc', 'https://www.instagram.com/catalystinc/', 'https://www.linkedin.com/company/catalystinc/', 'https://www.youtube.com/user/CatalystClips', 'https://en.wikipedia.org/wiki/Catalyst_(nonprofit_organization)']}]",,,,,,,,,
https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LmM0aXNybmV0LmNvbS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS8yMDE5LzExLzA0LzUtY29uY2VybnMtdGhlLXVzLW11c3QtdGFja2xlLXRvLWNvbXBldGUtaW4tYWkv0gEA?oc=5,5 concerns the US must tackle to compete in AI - C4ISRNET,2019-11-04,C4ISRNET,https://www.c4isrnet.com,The National Security Commission on Artificial Intelligence hopes to roll its recommendations into congressional budgets as time goes on.,"['artificial-intelligence', 'eric-schmidt', 'National-Security-Commission-on-Artificial-Intelligence', 'eric-schmidt', 'bob-work', 'ai-commission', 'artificial-intelligence', 'stop-killer-robots', 'ai-warfare', 'artificial-intelligence-war', 'killer-robots', 'trump-robots', 'pentagon-ai', 'defense-AI', 'five-eyes', 'circulated-c4isrnet', 'circulated-undefined', 'circulated-defense-news', 'circulated-federal-times']",The National Security Commission on Artificial Intelligence hopes to roll its recommendations into congressional budgets as time goes on.,N/A,http://schema.org,NewsArticle,"{'type': 'WebPage', '@id': 'https://www.c4isrnet.com/artificial-intelligence/2019/11/04/5-concerns-the-us-must-tackle-to-compete-in-ai/'}",5 concerns the US must tackle to compete in AI,"{'url': 'https://www.c4isrnet.com/resizer/iJlRTQZy6DeUA6SMpQFuZABbbVY=/1024x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/archetype/C5ZAKRQUJFBRVMC745AIBMCF3A.jpg', '@type': 'ImageObject'}","[{'@type': 'Person', 'name': 'Aaron Mehta'}]",2019-11-04T20:08:15.483Z,2022-08-19T15:10:17.951Z,"{'@type': 'Organization', 'name': 'C4ISRNet', 'url': 'https://www.c4isrnet.com/', 'logo': {'@type': 'ImageObject', 'url': '/resources/img/c4isrnet-logo-white.png?d=124'}}",,Artificial Intelligence,N/A,"A group of technology experts chartered by Congress to guide American efforts in artificial intelligence have released their initial report on how to ensure AI development stays on track inside the United States. And, overall, there’s a lot of work to do.Created by the National Defense Authorization Act in 2018, the National Security Commission on Artificial Intelligence is explicitly tasked with reviewing “advances in artificial intelligence, related machine learning developments, and associated technologies,” for the express purpose of addressing “the national and economic security needs of the United States, including economic risk, and any other associated issues.”The Commission — led by Eric Schmidt, the former head of Google parent Alphabet, and Bob Work, the former deputy secretary of defense — released its interim findings Nov. 4, focusing on five key areas of concern.While warning of the need to go fast, the Commission will not deliver its full conclusions until March 2021, which could mean that the recommendations from the group will not appear until the fiscal year 2023 budget. However, Work told reporters Monday morning that there is a plan to slowly roll in recommendations as they are finalized.“As recommendations become locked in as a consensus of the group, and we’re ready to do it, we will be giving those directly to Congress, since they’re our primary customer, and they can use them as we see fit,” Work said, particularly calling out Rep. Elise Stefanek, R-NY., whose legislative push helped created the Commission, as someone who has asked for rolling updates.In the meantime, the interim report shows the Commission has narrowed its focus onto the following five key areas:Invest in AI research and development: “Despite the transformative potential of AI, the U.S. government has not yet responded with the resources necessary to meet current research needs and set conditions for future innovation,” the Commission writes, warning that “over the past five years, federal R&D funding for computer science (which houses AI) increased by 12.7 percent, barely sustaining a field in which tenure track positions grew by 118 percent over the same period.”Put simply, the U.S. government needs to increase its funding for AI research and development if it wants to be able to compete with China. “The country benefits from broad scale investments in science, in particular in the competition we expect from other global leaders,” said Schmidt. “That investment is key.”It’s not just money, either, but the ever-present threat of government bureaucracy.“We have found that red tape in the DoD-owned lab network slows its ability to innovate,” the report reads. ""Layers of management and long approval processes lead researchers to choose older hardware and software for their work, because these can be obtained more quickly than the best products available. Such issues are creating risks that DoD labs will fall behind the curve of current AI research and development.”Apply AI to national security missions. Getting technology from paper to the battlefield is always a challenge for the Pentagon, and once again the Commission flags the DoD’s bureaucratic nature as a potential hurdle. To make AI systems valuable in the battlefield, it will take serious “top-down leadership,” the report reads.One question that the Commission has yet to tackle is the role of lethal autonomous systems, a hot-button topic. As part of their study, the Commission plans to invite groups like the Ban Killer Robots campaign to have a discussion about those concerns.Train and recruit AI talent. One theme the Commission expects to tackle going forward is the question of how to recruit top technological talent to the Pentagon — certainly not a new problem, but one that takes on extra urgency given the expected importance of AI in the near-future.That’s not just STEM talent, however. Both men said that part of that talent base needs to include ethicists looking at questions of AI in order to ensure that American artificial intelligence systems come from a base of American values.Protect and build upon U.S. technology advantages. China, while extremely capable in the AI realm, is still a follower, according to Schmidt. But they are a “fast follower,” one who is able to quickly pick up on American innovations and incorporate them for its own needs.Hence, export control mechanisms will remain important in the future, the Commission found; however, changes will be needed, as “item-based export controls and narrowly scoped foreign investment reviews are by themselves insufficient to sustain U.S. competitiveness in AI.”Both men acknowledged there is a camp in Washington that wants to “decouple” from China entirely and try to set strict firewalls that would keep Chinese researches away from American AI R&D efforts. Work said the Commission will attempt to “thread the needle” on that issue, acknowledging those concerns while not pushing to lock Chinese researchers out.Schmidt went further, stating flatly that “We are dependent on Chinese researchers and Chinese graduate students"" and adding that “a decoupling at the human level would hurt the United States … If you take those people out of our research chain, it will hurt the United States. Because they are strong, helpful, many of them stay within the country if they can get visas.”Work added that the United States has “been the magnet for global innovation talent for the last 70 years,” and that AI competition means the United States has to maintain that edge.Marshal global AI cooperation. The report highlights the importance of working with allies and partner nations on developing AI technologies. The United States must “establish a network of like-minded nations dedicated to collectively building AI expertise and capabilities,” the authors write. “The government should organize itself as soon as possible to conduct a sustained, long-term diplomatic campaign to support America’s AI agenda.”The panel suggests starting to work through the Five Eyes intelligence-sharing nations, as that network is likely the easiest to share key classified technologies through. While NATO may represent another path forward in the future, the Commission feels that is more difficult given the wide range of budgets, budget planning procedures and domestic requirements.The report also calls for the United States and its partners to “lower the barriers to the movement of people and data among nations” in order to benefit from the best brains around the world — an opinion that appears unaligned with many of the Trump administration’s immigration stances, but one Schmidt has relayed in the past.Overall, both men acknowledged that the interim report doesn’t contain any shocking findings that have not been already flagged as concerns elsewhere in the government — something they said is a good sign, as it shows broad consensus around how to attack the AI problem.Still, Work stressed that the Commission’s decision to come at the problem with a whole-of-America focus, as opposed to just national or economic security, means the final report will prove valuable recommendations across the board.“One thing this report may do is it makes very, very clear what the stakes of the competition are,” Work said. “This is a competition between authoritarian regimes and democratic governments, and technologies reflect the values of the governments that implement them.”About  Aaron MehtaAaron Mehta was deputy editor and senior Pentagon correspondent for Defense News, covering policy, strategy and acquisition at the highest levels of the Defense Department and its international partners.Share:More In Artificial IntelligenceSouth Korea to deploy laser weapons to intercept North Korean dronesThe Defense Acquisition Program Administration said it will deploy at least one anti-air laser weapons system — called “Block-I” — this year.US to send Tomahawks, hypersonics, other long-range fires to GermanyThe move is in-line with the Army's plan to provide such capability to its Multidomain Task Force there.Beavers takes reins from Sherman as acting DOD information officerAs the most senior IT advisor to the secretary of defense, Beavers is taking over from former CIO John Sherman.Quieting Discord: A new frontier in military leaks and extremismFrom secret Pentagon leaks to radicalization in the military community, Discord is continuing to grapple with keeping bad actors off the popular platform.Astronauts say Boeing space capsule can safely return them to EarthNASA test pilots Butch Wilmore and Suni Williams launched aboard Starliner capsule early last month.",C4ISRNet,,Artificial Intelligence,,https://www.c4isrnet.com/artificial-intelligence/2019/11/04/5-concerns-the-us-must-tackle-to-compete-in-ai/,,/resources/img/c4isrnet-logo-white.png,"['https://www.facebook.com/https://www.facebook.com/C4ISRNet', 'https://twitter.com/c4isrnet']",https://www.c4isrnet.com/#publisher,https://www.c4isrnet.com/artificial-intelligence/2019/11/04/5-concerns-the-us-must-tackle-to-compete-in-ai/,,,
https://news.google.com/rss/articles/CBMiYGh0dHBzOi8vcmVzdGF1cmFudGJ1c2luZXNzb25saW5lLmNvbS9maW5hbmNpbmcvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtdGFrZXMtaG9sZC11cy1yZXN0YXVyYW50c9IBAA?oc=5,Artificial intelligence takes hold at U.S. restaurants - Restaurant Business Online,2019-11-01,Restaurant Business Online,https://restaurantbusinessonline.com,Working Lunch: The podcast from Align Public Strategies also delves into an idea to eliminate taxes on tips as well as developments in Ohio and Massachusetts. ,"News, Quick_Service, technology","Machines are doing a lot more work at chains such as McDonald’s, Starbucks and Taco Bell, and there’s evidence it’s working.","Machines are doing a lot more work at chains such as McDonald’s, Starbucks and Taco Bell, and there’s evidence it’s working.",http://schema.org,Article,"{'@type': 'WebPage', '@id': 'https://restaurantbusinessonline.com/operations/how-will-supreme-courts-chevron-ruling-affect-restaurants'}",How will the Supreme Court’s Chevron ruling affect restaurants?,"{'@type': 'ImageObject', 'url': 'https://cdn.winsightmedia.com/platform/files/public/2024-07/background/Align%20Small%20Square%20%281%29.jpg?VersionId=WTlmywoEdcgpz0hidKsyhGXnKUeO3pTT'}","[{'@type': 'Person', 'name': 'Restaurant Business Staff', 'url': '/profile/restaurant-business-staff'}]",2024-07-15T18:06:20+00:00,2024-07-15T18:56:26+00:00,"{'@type': 'Organization', 'name': 'Restaurant Business', 'logo': {'@type': 'ImageObject', 'url': 'https://cdn.winsightmedia.com/platform/files/rb/images/logo-rb-json-ld.png', 'width': 600, 'height': 60}}",,Quick Service,N/A,"FacebookTwitterLinkedInPhotograph courtesy of McDonald's Corp.Machines are doing a lot more work inside U.S. fast-food restaurants.At Starbucks, artificial intelligence helps with labor scheduling. Taco Bell is using AI in its in-store kiosks to suggest certain items, the same thing McDonald’s is doing at its drive-thru menu boards. The Chicago-based giant then doubled down on the idea by acquiring an artificial intelligence company called Apprente. Machines are taking voice orders at 40 Domino’s Pizza locations.To be sure, artificial intelligence has a growing role inside many industries, particularly as internet-enabled businesses have taken hold. But it takes on a different meaning in a restaurant industry that employs a lot of people who frequently interact directly with consumers.The chains’ investments promise to change how customers interact with restaurants not only over the phone but also inside the locations themselves. And there’s evidence it’s already having an impact on what customers order. Starbucks has been working with an artificial intelligence effort over the past year that it calls “Deep Brew.” That system powers the company’s personalization engine and helps manage inventory. But it also optimizes labor.That system has helped reduce the number of administrative tasks inside restaurants, which has freed employees to interact with customers. That’s been driving sales: Same-store sales at the Seattle-based coffee giant rose 6% last quarter thanks to a mix of traffic and higher average check.“We continue to see a strong correlation between Starbucks partner engagement and customer connection, which leads to increased consumer frequency,” CEO Kevin Johnson said on the company’s earnings call this week. “We are making targeted investments to elevate the partner experience with clear evidence that this in turn elevates the customer experience and drives growth.”The industry is making these investments at a time of intense competition. Traffic for many chains is difficult to come by, and companies are investing in more technology to give themselves an advantage.At the same time, the efforts are coming as restaurants face more labor pressure than at any time in recent history. Chains believe the technology can not only help sales but also can ease some of the labor challenges inside their restaurants.Many of these efforts are customer-facing. Earlier this year, McDonald’s acquired Dynamic Yield, a company that gives Amazon-like abilities to its digital drive-thru menu boards. The boards display items based on the time of day, the weather and how busy the store is. Operators say the technology is already paying dividends in the form of higher average check.As if to double down on this idea, the company turned around and bought an artificial intelligence company called Apprente that will ultimately use voice technology to take orders in the chain’s drive-thrus.“We see voice technology playing an increasing role in all of our lives,” McDonald’s CEO Steve Easterbrook said on the company’s earnings call. “At McDonald’s, this is particularly significant because of the importance of drive-thrus to our portfolio.”McDonald’s isn’t the only one. Taco Bell has kiosks in 6,100 locations. Those kiosks feature AI-driven product recommendations, David Gibbs, chief operating officer for Taco Bell parent company Yum Brands, said this week.Sister company KFC, meanwhile, is planning to test artificial intelligence to take orders in its drive-thrus. “What I’m focused on from a tech perspective is to continue to drive sales by making our brand easier,” said Christopher Caldwell, chief information officer for KFC U.S., in an interview with Restaurant Business. It’s not just big chains, either. Breakfast and lunch chain Snooze, an A.M. Eatery and burger chain Good Times are both testing AI-powered software to take orders. The program has already demonstrated an ability to get consumers to add more items to their orders.AI-enabled ordering is also coming to an unexpected area: phone calls.Wingstop and Domino’s are both working to add voice ordering capabilities to their phone orders. For those companies, the strategy is practical: Despite the rapid growth in online ordering for items such as pizza and chicken wings, a percentage of the consumer still prefers to simply call the restaurant.Digitizing those orders frees up workers to make pizzas and chicken wings. Domino’s voice-ordering test is in 40 restaurants, and the Ann Arbor, Mich.-based pizza chain is eager to get it into more locations.Those efforts take time, however, because voice ordering technology needs to learn how to interact with humans. “It’s complex to do,” Domino’s Chief Technology Officer Kelly Garcia said. “I’d argue that our bot is way ahead of other technology.”Members help make our journalism possible. Become a Restaurant Business member today and unlock exclusive benefits, including unlimited access to all of our content. Sign up here.News 
															Quick_Service 
															technology 
													
						Restaurant Business Editor-in-Chief Jonathan Maze is a longtime industry journalist who writes about restaurant finance, mergers and acquisitions and the economy, with a particular focus on quick-service restaurants.
					View All Articles by This Author",,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibGh0dHBzOi8vd3d3Lm1pY3Jvc29mdC5jb20vZW4tdXMvbWljcm9zb2Z0LTM2NS9ibG9nLzIwMTkvMTEvMDQvYWktY29ydGFuYS1taWNyb3NvZnQtMzY1LXBlb3BsZS1hdC10aGUtY2VudGVyL9IBAA?oc=5,AI and Cortana in Microsoft 365 put people at the center - Microsoft,2019-11-04,Microsoft,https://www.microsoft.com,"At Ignite 2019, Microsoft announced AI and Cortana features in Microsoft 365 that empower people to be more productive on the go.",N/A,"At Ignite 2019, Microsoft announced AI and Cortana features in Microsoft 365 that empower people to be more productive on the go.","The new AI and Cortana features in Microsoft 365 put people at the center with Play my Emails, Scheduler and more.",http://schema.org,WebPage,,,,,,,,,N/A,N/A,"


 




					
						January 27, 2022					
				

						5 min read read					



From empowering frontline workers to accessibility improvements—here’s what’s new in Microsoft 365 




",,,,"[{'@type': 'Article', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/#article', 'isPartOf': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/'}, 'author': [{'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#/schema/person/image/c6068ba13fb8f8212a60f0e054e752ce'}], 'headline': 'AI and Cortana in Microsoft 365 put people at the center', 'datePublished': '2019-11-04T14:00:32+00:00', 'dateModified': '2022-06-28T19:14:18+00:00', 'mainEntityOfPage': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/'}, 'wordCount': 1432, 'publisher': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#organization'}, 'image': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/#primaryimage'}, 'thumbnailUrl': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2019/11/AI-and-Cortana-in-Microsoft-365-card.jpg', 'keywords': ['Mac'], 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/', 'url': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/', 'name': 'AI and Cortana in Microsoft 365 put people at the center | Microsoft 365 Blog', 'isPartOf': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#website'}, 'primaryImageOfPage': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/#primaryimage'}, 'image': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/#primaryimage'}, 'thumbnailUrl': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2019/11/AI-and-Cortana-in-Microsoft-365-card.jpg', 'datePublished': '2019-11-04T14:00:32+00:00', 'dateModified': '2022-06-28T19:14:18+00:00', 'description': 'At Ignite 2019, Microsoft announced AI and Cortana features in Microsoft 365 that empower people to be more productive on the go.', 'breadcrumb': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/#primaryimage', 'url': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2019/11/AI-and-Cortana-in-Microsoft-365-card.jpg', 'contentUrl': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2019/11/AI-and-Cortana-in-Microsoft-365-card.jpg', 'width': 440, 'height': 268}, {'@type': 'BreadcrumbList', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/2019/11/04/ai-cortana-microsoft-365-people-at-the-center/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.microsoft.com/en-us/microsoft-365/blog/'}, {'@type': 'ListItem', 'position': 2, 'name': 'AI and Cortana in Microsoft 365 put people at the center'}]}, {'@type': 'WebSite', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#website', 'url': 'https://www.microsoft.com/en-us/microsoft-365/blog/', 'name': 'Microsoft 365 Blog', 'description': 'Microsoft 365 brings together Office 365, Windows 10, and Enterprise Mobility + Security. It delivers a complete, intelligent, and secure solution to empower people.', 'publisher': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.microsoft.com/en-us/microsoft-365/blog/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#organization', 'name': 'Microsoft 365 Blog', 'url': 'https://www.microsoft.com/en-us/microsoft-365/blog/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#/schema/logo/image/', 'url': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2019/08/ms-logo-amp.png', 'contentUrl': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2019/08/ms-logo-amp.png', 'width': 279, 'height': 60, 'caption': 'Microsoft 365 Blog'}, 'image': {'@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#/schema/logo/image/'}}, {'@type': 'Person', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#/schema/person/image/c6068ba13fb8f8212a60f0e054e752ce', 'name': 'Andrew Shuman', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.microsoft.com/en-us/microsoft-365/blog/#/schema/person/image/b1c83d0c765c4833531958fcbb993bd7', 'url': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2022/06/logo_whitespace-62bcba38c32e9-150x150.webp', 'contentUrl': 'https://www.microsoft.com/en-us/microsoft-365/blog/wp-content/uploads/sites/2/2022/06/logo_whitespace-62bcba38c32e9-150x150.webp', 'width': 150, 'height': 150, 'caption': 'Andrew Shuman'}, 'description': 'Corporate Vice President, Experience and Devices', 'url': 'https://www.microsoft.com/en-us/microsoft-365/blog/author/andrew-shuman/'}]",,,,,,,"[{'@type': 'VideoObject', 'contentUrl': 'https://www.microsoft.com/en-us/videoplayer/embed/RE46Eiu\r', 'description': 'AI in Microsoft 365 is driving a significant shift in how people interact with Microsoft 365 applications.', 'name': 'Play My Emails', 'uploadDate': '2019-11-04T23:04:39', 'thumbnailUrl': ['http://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE46Eiv?ver=8632', 'http://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE46Eiv?ver=8632', 'http://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE46Eiv?ver=8632', 'http://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE46Eiv?ver=8632']}]",,
https://news.google.com/rss/articles/CBMiYWh0dHBzOi8vd3d3LnNpbGljb24uY28udWsvZS1pbm5vdmF0aW9uL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL3JvYm90cy1oaXQtbG93LXBhaWQtaGFyZGVzdC0yMjY1MTPSAWVodHRwczovL3d3dy5zaWxpY29uLmNvLnVrL2UtaW5ub3ZhdGlvbi9hcnRpZmljaWFsLWludGVsbGlnZW5jZS9yb2JvdHMtaGl0LWxvdy1wYWlkLWhhcmRlc3QtMjI2NTEzL2FtcA?oc=5,Workplace Robots To Hit Low Paid The Hardest - Silicon UK,2019-11-05,Silicon UK,https://www.silicon.co.uk,"Bad robot? Increasing automation must not drive workplace wage inequality, think tank warns",N/A,"Bad robot? Increasing automation must not drive workplace wage inequality, think tank warns",N/A,https://schema.org,BreadcrumbList,"{'@type': 'WebPage', 'id': 'https://www.silicon.co.uk/e-innovation/artificial-intelligence/robots-hit-low-paid-hardest-226513'}",Workplace Robots To Hit Low Paid The Hardest,"{'@type': 'ImageObject', 'url': 'https://www.silicon.co.uk/wp-content/uploads/2017/04/Robots-making-a-car.jpg', 'width': '1024', 'height': '576'}","{'@type': 'Person', 'name': 'Tom Jowitt'}",2017-12-28T10:42:54+00:00,2019-11-05T20:00:12+00:00,"{'@type': 'Organization', 'name': 'Silicon UK', 'logo': {'@type': 'ImageObject', 'url': 'https://www.silicon.co.uk/wp-content/themes/kamino/assets/images/favicons_silicon/mstile-70x70.png', 'width': '', 'height': ''}}","[{'@type': 'ListItem', 'position': 1, 'item': 'https://www.silicon.co.uk/', 'name': 'All Tech News'}, {'@type': 'ListItem', 'position': 2, 'item': 'https://www.silicon.co.uk/news/e-innovation', 'name': 'Innovation'}, {'@type': 'ListItem', 'position': 3, 'item': 'https://www.silicon.co.uk/news/e-innovation/artificial-intelligence', 'name': 'Artificial Intelligence'}, {'@type': 'ListItem', 'position': 4, 'item': 'https://www.silicon.co.uk/e-innovation/artificial-intelligence/robots-hit-low-paid-hardest-226513', 'name': 'Workplace Robots To Hit Low Paid The Hardest'}]",N/A,N/A,"


Workplace Robots To Hit Low Paid The Hardest

Tom Jowitt, December 28, 2017, 10:42 am | Updated on 5 November 2019, 20:00 















Bad robot? Increasing automation must not drive workplace wage inequality, think tank warns

 The government is being warned by a leading think tank that robots and workplace automation must be carefully managed, if predictions of mass unemployment and economic disruption are to be avoided.
According to the Institute for Public Policy Research, the government specifically needs to intervene to stop automation driving up wage inequality, although it did state that automation will ‘significantly increase productivity’ and that jobs would be reallocated, rather than lost.
The report comes amid a debate over the future where artificial intelligence (AI), robots, and increasing levels of automation are common-place. Some experts such as  renowned physicist Dr Stephen Hawking has previously warned that that artificial intelligence could spell the end of life as we know it on Planet Earth.

Careful Rollout
The report starts off by stating that despite what some would have us believe, we are not on the “cusp of a ‘post-human’ economy,” where robots do all the jobs that humans currently do.
“Automation will produce significant productivity gains that will reshape specific sectors and occupations,” the report stated. “In aggregate, however, these gains are likely to be recirculated, with jobs reallocated rather than eliminated, economic output increased, and new sources of wealth created.”
However it warned that the challenge for governments and regulators is to ensure that the benefits of automation are fairly shared, although there is no guarantee that this will occur, and it could impact the poorest in society the hardest.
“Managed poorly, automation could create a ‘paradox of plenty’: society would be far richer in aggregate, but, for many individuals and communities, technological change could reinforce inequalities of power and reward.”
And the report did warn that lower-skilled jobs were much more likely to be phased out in the coming decades, with only higher-skilled workers would be able to command better wages.
The sectors most at risk from increasing levels of automation are transportation (where 63 percent of jobs could be automated); manufacturing (58 percent of jobs could be automated); and wholesale and retail trade (where 65 percent of jobs could be automated).
“Automation is likely to lead to the steady redeployment of labour over a period of decades, rather than a sudden and rapid elimination of employment,” said the report. “The task contents of most jobs will evolve, changing the nature of work.”
“In the absence of policy intervention, the most likely outcome of automation is an increase in inequalities of wealth, income and power,” said the report. “The economic dividends of automation are likely to flow to the owners of technologies and businesses, and the highly skilled, as income shifts from labour to capital and the labour market polarises between high- and low-skilled jobs.”
And it said jobs with the highest potential for automation typically have lower wages.
However, automation can potentially deliver a powerful boost to UK productivity, the report stated. “An accelerated trajectory of automation could raise productivity growth by between 0.8 to 1.4 percent annually, boosting GDP by 10 percent by 2030.”
The report also said that “an authority for the ethical use of Robotics and Artificial Intelligence should be established to regulate the use of automating technologies.”
A spokesman for the Department for Business, Energy and Industrial Strategy was quoted by the BBC as saying that the UK’s labour market was “resilient and diverse” and that advances in technology were helping to “bring new jobs”.
“The government is committed to ensuring that the UK is to able to seize the opportunities and overcome the obstacles that exist in this area,” he said.
“Government is working closely with industry to ensure the benefits of new technologies are felt across different sectors of the economy up and down the country, while creating new high-skill, well-paid jobs,” the spokesman said.
AI Concern
The report comes amid a debate as to the possible uses of artificial intelligence and robots in the years ahead.
Professor Stephen Hawking has previously said that a thinking machine could “redesign itself at an ever-increasing rate” and “supersede” humans.
Elon Musk, the South Africa-born inventor and entrepreneur best known as the co-founder of PayPal and chief executive of both SpaceX and Tesla Motors, has called AI “our biggest existential threat”.
And Microsoft co-founder Bill Gates has said he was “concerned” about AI and that he agreed with Musk’s view.
In October Google’s DeepMind division announced a significant breakthrough after its AI system became even smarter without any human input at all, after it learnt how to defeat the leading system playing the ancient Chinese game of Go.
Put your knowledge of artificial intelligence to the test. Try our quiz!
Advertising
 Read also : 
Samsung AI-Upgraded Bixby Voice Assistant Coming This Year
Silicon UK In Focus PodcastsponsoriséSilicon In Focus Podcast: The Value of Data00:0000:0000:0000:00SubscribeEdisoundRSS FeedSpotifyDeezerAmazon MusicApple PodcastsShare EpisodeFacebookXLinkedInEpisode linkCopied !





 
Recommend this article:




                    0                









                    4                










 NEWSLETTER 
 Subscribe to our best articles 









 


 Facebook
                            




  Twitter




  Linkedin









Advertising



",Workplace Robots To Hit Low Paid The Hardest,https://www.silicon.co.uk/wp-content/uploads/2017/04/Robots-making-a-car-600x338.jpg,Artificial Intelligence,,,,,,,,,,
https://news.google.com/rss/articles/CBMib2h0dHBzOi8vd3d3Lmluc2lkZWhpZ2hlcmVkLmNvbS9uZXdzLzIwMTkvMTEvMDQvYWktYXNzZXNzZWQtam9iLWludGVydmlld2luZy1ncm93cy1jb2xsZWdlcy10cnktcHJlcGFyZS1zdHVkZW50c9IBAA?oc=5,"As AI-assessed job interviewing grows, colleges try to prepare students - Inside Higher Ed",2019-11-03,Inside Higher Ed,https://www.insidehighered.com,"Getting a job increasingly requires going through an interview on an AI platform. Some colleges are trying to prepare their students for these nonhuman interactions, but many institutions are just getting acclimated to this new technology.","Higher, Education, News, Jobs, Events, Career",N/A,N/A,https://schema.org,,,,,,,,,,N/A,N/A,"



You have 4/5 articles left.Sign up for a free account or log in.

Sign Up, It’s FREE
Login




 

Istockphoto.com/karelnoppe


Miguel Santiago, a senior at Baruch College in Manhattan, is graduating soon and already considering his next move -- maybe to a job at Goldman Sachs or somewhere else in banking.
In at least six of his interviews, he's been questioned by a computer and not a live person.
“They’ve basically replaced the first round with the HireVue,” he said, referring to the video and artificial intelligence platform increasingly being used by employers for job interviews.
When a candidate applies to a job at a company that uses HireVue, they are asked to go on to the platform, allow use of their webcam and respond to interview questions on video. The candidate’s answers are recorded and then saved to the platform.
Santiago helped bring Goldman and Bank of America employees to his campus to discuss the HireVue process when he was events director for Baruch’s chapter of the Association of Latino Professionals for America.
“What they really want the students to know is that the camera that they’re speaking to when they’re answering the questions from HireVue, it’s just another person,” he said. The banking employees told him that a human would be reviewing the footage, he said.
But according to HireVue’s own advertising materials and recent reporting by The Washington Post, that’s not the only way employers use the platform. Employers can choose to have the recorded answers evaluated by artificial intelligence. If the proprietary technology that HireVue uses to evaluate the recordings concludes that a candidate does well in matching the demeanor, enthusiasm, facial expressions or word choice of current employees of the company, it recommends the candidate for the next round. If the candidate is judged by the software to be out of step, that candidate is not likely to move on.
Although colleges are ostensibly preparing students to enter the workforce, many institutions appear to be unfamiliar with or unprepared for this latest trend in the job market. Unilever, Atlanta Public Schools, Hilton Hotels and Resorts, and nearly 100 other employers now use HireVue, but little advice concerning HireVue interviews can be found on university websites.
To many people who study workforce and AI issues, the software is something of a black box. What the artificial intelligence is looking for -- a smile, a loud speaking voice or a particular keyword -- can’t really be ascertained. Helping prepare students for AI-assessed interviews is challenging, as a result.
“No one is really sure what they look for when it comes to who moves on and who doesn’t,” Santiago said. “It’s definitely kind of a weird experience to have.”
Alex Engler, a former data scientist and a current fellow at the Brookings Institution, where he focuses on AI, is doubtful about the AI's qualification assessment capabilities.
Engler contrasted HireVue's evaluation with other AI-based hiring software such as résumé scanners, which he said are concerning but are based on a plausible connection between résumé qualifications and job readiness.
“HireVue is doing something that goes past that, which is looking at how candidates act in interviews. Their gestures and pose, if they’re leaning with their arms on the table, their tone and cadence,” he said. “Inferring personality traits from a 25-minute video interview, I think, is probably incredibly difficult or impossible. And to tie it to outcomes like, can you be an investment banker or an accountant, is to me very far-fetched.”
HireVue says its assessments are based on ""100 percent validated science.""
""We follow leading psychological research showing the behaviors, skills, traits, thinking styles, and competencies that predict success at work,"" a HireVue spokesperson said via email.
Supporters of such AI-based hiring methods say traditional in-person interviews are even more unfair, and that human evaluators might judge job candidates on arbitrary or illegal criteria such as their physical attributes, dress style or ethnicity.
“People are rejected all the time based on how they look, their shoes, how they tucked in their shirts and how ‘hot’ they are,” Loren Larsen, HireVue’s chief technology officer, told The Washington Post. “Algorithms eliminate most of that in a way that hasn’t been possible before.”
Ifeoma Ajunwa, a professor of labor and employment law at Cornell University who has previously written about AI-based hiring, says that line of thinking is misguided.
“We have to remember that automated hiring platforms are still created by humans,” she said. “The same biases that humans have would also be transferred to any platforms they create.”
HireVue works by having current employees answer the questions on video and then evaluating the candidates on how well they match those employees.
Annelies Goger, a fellow at the Brookings Institution who focuses on workforce development, said the use of the platform could create anxiety for job seekers trying to access industries that are historically segregated and exclusive.
For example, she said, in an occupation primarily filled by young, white men, the system is likely trained to recognize success as young, white men exhibit it.
“If I’m a woman of color and trying to get in,” she said, “I would be pretty anxious about, is this system really going to assess me fairly?”
The HireVue spokesperson said the company's AI assessments are actually increasing diversity at companies, because the algorithms don't notice appearance.
""Each algorithm or assessment model is trained not to 'notice' age, gender, ethnicity, and other personal characteristics that are irrelevant to job success, so it helps to level the playing field,"" the spokesperson wrote in the email.
But to critics, the things that the AI does notice -- faces and gestures -- are the root of the problem, not appearance. Motions like these can vary widely based on culture and ability status, they say.
""You have to exhibit similar qualities to the people already in those jobs and performing well,"" said Engler. ""That doesn’t actually have to mean qualities that are actually useful for the job -- you just have to share the same qualities.""
The effects for people with disabilities could be drastic, he said.
“The way that disabilities can affect people is very broad, and as a result some of the characteristics that people with disabilities exhibit are unlikely to exist in the AI’s training data.”
""HireVue offers various accommodations for people with disabilities and is actively working with international disability groups, as well as with Integrate Autism Employment Advisors to ensure that the tools and processes are fair and accessible,"" the company spokesperson wrote.
Goger said no established ""legal infrastructure"" exists to protect a person facing discrimination from AI.
“Who’s going to check for ADA compliance in these systems?” she said.
Trying to Prepare
Some colleges are trying to prepare their students with the little knowledge that career counselors do have about how HireVue's AI evaluation works. Oftentimes that advice doesn’t look very different from the guidance they provide for in-person interviews.
Michael Kalish, associate director of on-campus recruiting at Baruch’s career center, says career counselors commonly suggest that students dress in a full suit and use industry-specific lingo, since it has been suggested that HireVue scans a candidate's answers for keywords.
“We instruct students in general that when they’re interviewing they should always be using that kind of terminology regardless,” Kalish said. “Not even just to be prepared for [HireVue] interviews, but just, in general, to show the interviewer that they are prepared, that they’ve done their homework, that they’re generally interested in that field.”
Students can also practice for interviews using a mock interview platform called Symplicity that asks industry-specific interview questions and records their answers via webcam. This software predates the widespread use of virtual interviews, Kalish said. Unlike with HireVue, students can rewatch their answers on the Symplicity software.
“They can actually watch it and learn from their mistakes,” Kalish said. “They can meet with a counselor. We can watch it with them and give them constructive criticism and feedback on areas that they should improve upon.”
At Duke University, a document from the economics department lists typical HireVue questions (“Tell me about a time you worked on a team?” and “What does integrity mean to you?”) as well as a few tips for students. The suggestions range from the general interview advice (“Try to give structured concise responses”) to the more technical (ridding the screen of your own image makes it easier to look into the camera), but they don’t really touch on how to nail the mannerisms of past employees other than to “act natural.”
Brigham Young University Idaho is one of the few colleges that advertise mock interviews specifically for HireVue on the university's website. Students can schedule a HireVue mock interview and meet with a career mentor to hear feedback, according to the website. BYU Idaho career center staff declined to discuss these resources, which they said are still in their early stages.
The University of Colorado at Boulder also uses HireVue, but not to help prepare students for interviews. The college uses the software for nearly all of its own hiring.
Andrew Horovitz, an assistant director of talent acquisition at Boulder, said that HireVue makes the process more efficient and lends flexibility to both hiring managers and candidates who don't have to travel or take off work for the video interview. The college does not use HireVue's AI assessment tool, only the video interviewing, he said. The AI feature is an add-on that the university would have to pay for.
""I don’t foresee us using that any time in the near future,"" he said of the AI tool. ""We do a lot of work on our campus around mitigating bias in the interview and hiring process as it is already, and so we want to make sure any tool we introduce or functionality we introduce is in line with that.”
Yajin Wang, a professor of marketing at the University of Maryland, says that most of the things students can do to prepare for HireVue job interviews will make them better public speakers.
The University of Maryland’s business school posted an article in June of this year featuring Wang giving advice to students on how to do their best on HireVue and other platforms.
“Ace an AI interview by incorporating keywords and phrases that explain what you can contribute, echoing the exact language of the job posting,” the article said. “Use gestures, smile, and nod frequently.”
Wang suggested candidates record themselves and watch their answers to prepare.
HireVue advised that students visit the company candidate center and focus on giving examples from relevant work.
""We encourage candidates to 'show what they know,' give specific examples of successful projects when asked, and 'be themselves,'"" the company spokesperson said in the email.
The HireVue Effect
Frederick Hess, director of education policy studies at the American Enterprise Institute, said while he is “not impressed” by HireVue's platform, which he called “dystopian” and “pseudoscience,” the overall trend toward assessment-based hiring may undermine the economic value of a college degree. He said if companies begin to hire based on skills tests that are, unlike HireVue, non-prejudicial and legally sound, they will stop using college degrees as a proxy for knowledge and employability.
“I think these efforts to build new tools, hiring platforms, hiring systems, which will stand up to legal scrutiny because they are specific and clearly attached to the job you’re going to do, and can be defended that they are non-prejudicial, that stuff should worry the heck out of colleges,” he said. “If you can apply without having to go through all the stuff for the degree, then employers can pay less, and you still feel like you’re getting enough.”
“In terms of value as a proxy for skill and talent, I think college degrees themselves are limited, and they’re one instrument that is probably overused in the labor market,” said Goger, the Brookings fellow. But HireVue “is replacing one flawed instrument with another.”
She said colleges could focus on providing students more opportunities for face-to-face interaction and getting a foot in the door.
“They can help candidates understand how to communicate their value to an employer in a specific role,” she said.
Ajunwa, the Cornell professor, said job candidates being assessed by AI should use keywords from the job description, avoid hinting at gender or ethnicity on their résumé, and explain employment gaps by saying what they were specifically doing, such as having a child.
“I worry that these systems might be sending a wrong message to students,” she said. “They’re so anxious and focused on cracking the résumé or cracking the CV to beat the system and then feeling perhaps that their work experience or their skills that they acquired in school aren’t that important. College should be a time to explore and should be a time to learn various skills … I want students to really focus on that, more than worrying about beating these automated hiring systems.”





Want articles like this sent straight to your inbox?

Subscribe to a Newsletter



",,,,"[{'@type': 'Article', 'headline': 'As AI-assessed job interviewing grows, colleges try to prepare students', 'image': {'@type': 'ImageObject', 'url': '/sites/default/files/media/iStock-1022348426.jpg'}, 'datePublished': '2019-11-03EST14:00:00EST', 'dateModified': '2023-02-28EST11:44:03EST', 'author': {'@type': 'NewsMediaOrganization', 'name': 'Lilah Burke'}}, {'@type': 'ImageObject', 'url': ''}]",,,,,,,,,
https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vd3d3LnBicy5vcmcvd2diaC9mcm9udGxpbmUvYXJ0aWNsZS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS13b3JrLWpvYnMtcm9ib3RzLXYtaHVtYW5zL9IBAA?oc=5,Experts Weigh in on the Work Robots Can't Do (Yet) - PBS,2019-11-05,PBS,https://www.pbs.org,"In the age of artificial intelligence, predicting which jobs will fall to automation is as much about what machines can do as it is about what they can’t.",N/A,"In the age of artificial intelligence, predicting which jobs will fall to automation is as much about what machines can do as it is about what they can’t.","In the age of artificial intelligence, predicting which jobs will fall to automation is as much about what machines can do as it is about what they can’t.
More than half of all jobs in America —",https://schema.org,,,,,,,,,,N/A,N/A,"



The Jobs Robots Can’t Do (At Least Not Yet)

Share:



Twitter


Facebook


E-mail


 








(FRONTLINE) 





November 5, 2019

by


Zoe Todd





In the age of artificial intelligence, predicting which jobs will fall to automation is as much about what machines can do as it is about what they can’t.
More than half of all jobs in America — both blue and white-collar — are resistant to automation, according to an acclaimed study published in 2013 by two Oxford University researchers.
Co-author Carl Benedikt Frey, who directs Oxford’s Technology and Employment program, broke down three areas where human intelligence still beats artificial intelligence: perception and manipulation, social intelligence; and creativity. Each type has what Frey calls a “bottleneck,” which slows the pace at which certain workforces can be automated.
The premise is simple: Technology won’t replace human workers if it can’t do the job. Six years later, Frey said the argument holds. Here’s a look at the sectors proving to be robot resistant:
Perception and Manipulation
Humans outpace robots when it comes to perception and motor skills, especially in so-called “unstructured work environments” – spaces cluttered with many different objects. Until technology catches up, work such as surgery is best left to humans, Frey said. Similarly, jobs in unstructured environments, such as homes, are more difficult to automate than jobs in predictable, structured environments such as factories, warehouses, airports and hospitals.
Social Intelligence
Social intelligence is another bottleneck, as machines can’t yet compete with humans at work involving negotiation, persuasion or care. In particular, machines struggle to recognize and respond to human emotions. Work requiring high social intelligence is therefore less susceptible to automation. Frey lists public relations and event planning as examples.
Creativity
Robots also can’t keep up with human creativity: the ability to form new and valuable ideas such as poetry, music, recipes, jokes, fashion design or scientific theories. Though technology is capable of randomly combining old ideas to create new ones, the result doesn’t necessarily make sense — or have value. In this case, the novelty of a random, machine-generated idea should not be confused with creativity, Frey said.
What About the Rest? 
Of course, that leaves roughly half the jobs in the U.S. with tasks machines can now do better than humans. But Frey emphasized that doesn’t mean those jobs will vanish outright, either.
“There are a lot of people that took our estimates to suggest that all of these jobs are going to disappear within 10 to 20 years, but the paper did not really say that,” he said.
If anything, technology will likely create new kinds of work for humans, says James Bessen, who heads the Technology and Policy Research Initiative at Boston University.
“People are focusing on the aspects of the technology where the machines can replace humans at certain tasks,” Bessen said. “Most of what technology does, is actually enhance humans at doing certain tasks.”
He uses bank tellers as an example. The profession was largely mechanized decades ago by the ATM – literally, “automated teller machine.” Yet, despite becoming a fixture at banks across the country, ATMs did not push out human tellers. On the contrary, research shows the number of bank tellers in America rose nearly in tandem with the number of machines.
Though ATMs did replace some workers, the cost savings allowed banks to open new branches for which they then hired additional people, Bessen explained. As such, the human jobs were not replaced but displaced. Moreover, the ATMs freed their flesh-and-blood colleagues from routine tasks such as depositing money. Instead, they were able to focus on inherently human work like interacting with clients.
Still, researchers acknowledge the transition to a new American workforce as these industries adapt to and integrate automation will be painful.
People will continue to lose jobs to new technology, and some workers may be pushed into long periods of unemployment, Bessen said. Already, many are forced to relocate or retrain, and those who can’t afford to do so will be left behind.
Women disproportionately hold the jobs at highest risk of automation, according to Molly Kinder, who studies the future of work at a D.C. think tank called Work, Workers and Technology.
“That’s not really being talked about,” Kinder said. “And that’s in part because women are overrepresented in some of these marginalized occupations like a cashier or a fast-food worker, and also in a large numbers in clerical jobs in offices.”
“There’s real pain involved,” Bessen said. “There’s a real question about how these gains get distributed, about who’s suffering, who’s bearing the brunt in these transitions.”
For American workers, finding ways to navigate the transition to new technology is the real challenge, Bessen said, adding that their success will in part be up to government policies — and pushback by workers themselves.








Zoe Todd, Former Abrams Journalism Fellow, FRONTLINE/Columbia Journalism School Fellowships


Twitter:
@ZoeHTodd







Journalistic Standards











Watch the Documentary
In the Age of AI






    Support Provided By
    Learn more







See What FRONTLINE Is Working On Now






Get Our Newsletter





Related Articles




How China’s Government Is Using AI on Its Uighur Muslim Population
November 21, 2019





Artificial Intelligence Can Be Biased. Here's What You Should Know.
November 5, 2019





Could the Rise of Artificial Intelligence Put Truckers’ Jobs in Peril?
November 5, 2019





Topics


Business and Economy






",,,,"[{'@type': 'NewsArticle', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/#article', 'isPartOf': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/'}, 'author': [{'@type': 'Person', 'name': 'Zoe Todd', 'url': 'https://www.pbs.org/wgbh/frontline/person/zoe-todd/'}], 'headline': 'The Jobs Robots Can&#8217;t Do (At Least Not Yet)', 'datePublished': '2019-11-05T22:52:15+00:00', 'dateModified': '2019-11-05T22:52:15+00:00', 'mainEntityOfPage': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/'}, 'wordCount': 797, 'publisher': {'@type': 'Organization', 'name': 'Frontline PBS', 'logo': {'@type': 'ImageObject', 'url': 'https://www.pbs.org/wgbh/frontline/wp-content/themes/fl-responsive-theme/library/images/frontline_logo_og.png'}}, 'image': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/#primaryimage'}, 'thumbnailUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/3805_Screengrabs200.jpg', 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/', 'url': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/', 'name': ""Experts Weigh in on the Work Robots Can't Do (Yet)"", 'isPartOf': {'@id': 'https://www.pbs.org/wgbh/frontline/#website'}, 'primaryImageOfPage': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/#primaryimage'}, 'image': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/#primaryimage'}, 'thumbnailUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/3805_Screengrabs200.jpg', 'datePublished': '2019-11-05T22:52:15+00:00', 'dateModified': '2019-11-05T22:52:15+00:00', 'description': 'In the age of artificial intelligence, predicting which jobs will fall to automation is as much about what machines can do as it is about what they can’t.', 'breadcrumb': {'@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/#primaryimage', 'url': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/3805_Screengrabs200.jpg', 'contentUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2019/11/3805_Screengrabs200.jpg', 'width': 1920, 'height': 1080, 'caption': '(FRONTLINE)'}, {'@type': 'BreadcrumbList', '@id': 'https://www.pbs.org/wgbh/frontline/article/artificial-intelligence-work-jobs-robots-v-humans/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.pbs.org/wgbh/frontline/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Articles', 'item': 'https://www.pbs.org/wgbh/frontline/articles/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Business and Economy', 'item': 'https://www.pbs.org/wgbh/frontline/topic/business-and-economy/'}, {'@type': 'ListItem', 'position': 4, 'name': 'The Jobs Robots Can&#8217;t Do (At Least Not Yet)'}]}, {'@type': 'WebSite', '@id': 'https://www.pbs.org/wgbh/frontline/#website', 'url': 'https://www.pbs.org/wgbh/frontline/', 'name': 'FRONTLINE', 'description': 'FRONTLINE', 'publisher': {'@id': 'https://www.pbs.org/wgbh/frontline/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.pbs.org/wgbh/frontline/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.pbs.org/wgbh/frontline/#organization', 'name': 'FRONTLINE | PBS', 'url': 'https://www.pbs.org/wgbh/frontline/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.pbs.org/wgbh/frontline/#/schema/logo/image/', 'url': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2020/10/frontline_logo_opengraph.png', 'contentUrl': 'https://www.pbs.org/wgbh/frontline/wp-content/uploads/2020/10/frontline_logo_opengraph.png', 'width': 1200, 'height': 630, 'caption': 'FRONTLINE | PBS'}, 'image': {'@id': 'https://www.pbs.org/wgbh/frontline/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/frontline', 'https://twitter.com/frontlinepbs', 'https://www.instagram.com/frontlinepbs/', 'https://www.youtube.com/user/pbsfrontline'], 'publishingPrinciples': 'https://www.pbs.org/wgbh/frontline/about-us/editorial-standards-and-ethics', 'noBylinePolicy': 'https://www.pbs.org/wgbh/frontline/about-us/editorial-standards-and-ethics/#verification-and-fact-checking-standards'}]",,,,,,,,,
https://news.google.com/rss/articles/CBMiO2h0dHBzOi8vZmVkc2Nvb3AuY29tL2dvb2dsZS1wcm9qZWN0LW1hdmVuLWNhbmFyeS1jb2FsLW1pbmUv0gEA?oc=5,Google's departure from Project Maven was a 'little bit of a canary in a coal mine' - FedScoop,2019-11-05,FedScoop,https://fedscoop.com,"Leaders of the artificial intelligence project failed to be transparent upfront and did not control the narrative, says Lt. Gen. Jack Shanahan.","['artificial intelligence (ai)', 'department of defense (dod)', 'google', 'jack shanahan', 'joint artificial intelligence center (jaic)', 'kent walker', 'national security commission on artificial intelligence', 'project maven']","Leaders of the artificial intelligence project failed to be transparent upfront and did not control the narrative, says Lt. Gen. Jack Shanahan.",N/A,https://schema.org,NewsArticle,"{'@type': 'WebPage', '@id': 'http://fedscoop.com/google-project-maven-canary-coal-mine/'}",Google&#8217;s departure from Project Maven was a &#8216;little bit of a canary in a coal mine&#8217;,"{'@type': 'ImageObject', 'url': 'https://fedscoop.com/wp-content/uploads/sites/5/2019/03/5099102.jpg'}","[{'@type': 'Person', 'name': 'Billy Mitchell'}]",2019-11-05T20:10:02Z,2019-11-05T20:18:47Z,"{'@type': 'Organization', 'name': 'FedScoop', 'logo': 'https://fedscoop.com/wp-content/uploads/sites/5/2023/01/cropped-fs_favicon-3.png'}",,N/A,N/A,"






Defense




								Google’s departure from Project Maven was a ‘little bit of a canary in a coal mine’							

								Leaders of the artificial intelligence project failed to be transparent upfront and did not control the narrative, says Lt. Gen. Jack Shanahan.							


By
Billy Mitchell



November 5, 2019






 
											The director of the Joint Artificial Intelligence Center, U.S. Air Force Lt. Gen. Jack Shanahan, listens during a round table meeting at the Pentagon on Feb. 12, 2019. (DOD / U.S. Army Sgt. Amber I. Smith)										





There’s nothing controversial about the Department of Defense’s Project Maven, if you ask Lt. Gen. Jack Shanahan.
Shanahan, who has led the military’s program to use artificial intelligence for full-motion video analysis since its inception, said Tuesday that Google‘s decision to back away from involvement with Maven came from a lack of transparency around the project. As a result, the DOD lost the narrative around its work, he said.
“This idea of transparency and a willingness to talk about what each side is trying to achieve may be the biggest lessons of all that I took from it,” Shanahan, who now heads DOD’s Joint AI Center, said of Project Maven at an event in conjunction with the National Security Commission on AI’s release of its interim report.
Because Google and DOD weren’t minding the public perception what Project Maven was meant to do, “we started hearing these wild stories and assumptions about what Project Maven was and was not to the point where if you Googled it today … the adjective ‘controversial’ has now been permanently inserted in front of Project Maven, Shanahan said. “It was not controversial to me. It was not controversial to the team. I say it’s not controversial to anybody right now beyond some people who just don’t like what we’re doing.”


Advertisement



The project was, quite simply, created to provide computer vision on drones to detect objects from above, Shanahan said. It certainly wasn’t “a weapons project” — although the project’s official name, the Algorithmic Warfare Cross-Functional Team, does sound very weapon-y.
Those impressions were enough to set off protests by Google employees. Google eventually announced it would not renew its contract. It also stepped out of the running for DOD’s Joint Enterprise Defense Infrastructure (JEDI) cloud contract around the same time, reportedly because of ethical concerns centering on the department’s use of artificial intelligence.
Lesson learned, Shanahan said. On Tuesday he seemed to almost take a tone of acceptance about the situation, saying that “it happened,” and if it didn’t happen to his team, “it would have happened to somebody else at some point.”
“I view what happened with Google and Maven as a little bit of a canary in a coal mine,” Shanahan said. “The fact that it happened when it did as opposed to on the verge of a conflict or a crisis where we’re asking for help, we’ve gotten some of that out of the way,” and can now move on.
Shanahan said reports that Google seemingly didn’t want to work with the Pentagon in the aftermath were unfortunate, because “some of the software engineers on the project, they got to the point where they almost felt a little bit ostracized because others criticized them for working with the Department of Defense” on Project Maven. “But day-to-day, from the senior-most leaders down to the people working on the Project Maven team, we had tremendous support in Maven from Google” and “got products we were very pleased with.”


Advertisement




Google’s take on Project Maven
It just so happened that Shanahan was joined on stage at the event by Kent Walker, senior vice president of global affairs for Google, who also wanted to take the moment to clear the air and “set the record straight.”
“It’s been frustrating to hear concerns around our commitment to national security and defense,” he said. Not only has Google been criticized as soft in its support of U.S. defense and national security since its withdrawal from Project Maven, but it’s also been condemned for its work providing AI in China.
Walker explained Google stepping away from Maven as “an area where it’s right that we decided to press the reset button until we had the opportunity to develop our own set of AI principles, our own work with regard to internal standards and review processes. But that was a decision focused on a discrete contract — not a broader statement about our willingness or our history of working with the Department of Defense.”
He said Google continues to work with the U.S. military on countless other efforts and pointed such partnerships as part of “a long tradition of work throughout [Silicon Valley] on national security generally.”


Advertisement




“It’s important to remember the history of the Valley in large measure builds on government technologies from radar to the internet to GPS to some of the work on autonomous vehicles and personal assistants that you’re seeing now,” Walker said. “This is a shared responsibility to get this right.”
Shanahan agreed, alluding to a necessary equilateral triangle of partnership among the government, industry and academia for the U.S. to succeed in developing and using AI competitively.
Even for those who are “suspicious” of the federal government or have trouble with the fact that the U.S. is in strategic competition with China, Shanahan said, “I would hope they would still agree with us that AI is a critical component of our nation’s prosperity, vitality and self-sufficiency. In other words, no matter where you stand with respect to the government’s future use of AI-enabling technologies, I submit that we can never attain [ the nation’s vision for it] without industry and academia with us together in an equal partnership. There’s too much at stake to do otherwise.”








Written by Billy Mitchell
			Billy Mitchell is Senior Vice President and Executive Editor of Scoop News Group's editorial brands. He oversees operations, strategy and growth of SNG's award-winning tech publications, FedScoop, StateScoop, CyberScoop, EdScoop and DefenseScoop. 

After earning his journalism degree at Virginia Tech and winning the school's Excellence in Print Journalism award, Billy received his master's degree from New York University in magazine writing while interning at publications like Rolling Stone.		


In This Story



														Google													



														Artificial Intelligence (AI)													



														Department of Defense (DOD)													



														Project Maven													



														Joint Artificial Intelligence Center (JAIC)													



														Jack Shanahan													



														National Security Commission on Artificial Intelligence													



														Kent Walker													








Share




Facebook





LinkedIn





Twitter





Copy Link











Advertisement






Advertisement





More Like This






								MPEs gain momentum for sharing information with allied partners			



By 

						Scoop News Group					










								State Department officials say they’re trying to set the tone globally on AI usage, as lawmakers question if it’s enough			



By 

						Caroline Nihill					










								VA plans to award AI tech sprint winners contracts for ambient medical transcription services			



By 

						Caroline Nihill					









Advertisement





Top Stories






								FedRAMP ‘undeniably’ in state of limbo without final OMB modernization guidance, Rep. Connolly says			



By 

						Caroline Nihill					










								After 2023 outage that paused flights nationwide, FAA now has backup system			



By 

						Rebecca Heilweil					










								How the IRS’s ‘cautious’ approach with Direct File prevented its ‘failure’			



By 

						Matt Bracken					










								Social Security Administration transitioning long-time users to Login.gov			



By 

						Rebecca Heilweil					










								Energy Department’s national labs get AI boost in bipartisan Senate bill			



By 

						Matt Bracken					










								GSA begins FedRAMP pilot to change request process			



By 

						Caroline Nihill					










								White House to require increased cybersecurity protocols for R&D institutions			



By 

						Caroline Nihill					










Advertisement






",,https://fedscoop.com/wp-content/uploads/sites/5/2019/03/5099102.jpg?w=150&h=150&crop=1,Defense,"[{'@type': 'WebPage', '@id': 'https://fedscoop.com/google-project-maven-canary-coal-mine/', 'url': 'https://fedscoop.com/google-project-maven-canary-coal-mine/', 'name': ""Google's departure from Project Maven was a 'little bit of a canary in a coal mine' | FedScoop"", 'isPartOf': {'@id': 'https://fedscoop.com/#website'}, 'primaryImageOfPage': {'@id': 'https://fedscoop.com/google-project-maven-canary-coal-mine/#primaryimage'}, 'image': {'@id': 'https://fedscoop.com/google-project-maven-canary-coal-mine/#primaryimage'}, 'thumbnailUrl': 'https://fedscoop.com/wp-content/uploads/sites/5/2019/03/5099102.jpg', 'datePublished': '2019-11-05T20:10:02+00:00', 'dateModified': '2019-11-05T20:18:47+00:00', 'description': 'Leaders of the artificial intelligence project failed to be transparent upfront and did not control the narrative, says Lt. Gen. Jack Shanahan.', 'breadcrumb': {'@id': 'https://fedscoop.com/google-project-maven-canary-coal-mine/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://fedscoop.com/google-project-maven-canary-coal-mine/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://fedscoop.com/google-project-maven-canary-coal-mine/#primaryimage', 'url': 'https://fedscoop.com/wp-content/uploads/sites/5/2019/03/5099102.jpg', 'contentUrl': 'https://fedscoop.com/wp-content/uploads/sites/5/2019/03/5099102.jpg', 'width': 2805, 'height': 1543, 'caption': 'The director of the Joint Artificial Intelligence Center, U.S. Air Force Lt. Gen. Jack Shanahan, listens during a round table meeting at the Pentagon on Feb. 12, 2019. (DOD / U.S. Army Sgt. Amber I. Smith)'}, {'@type': 'BreadcrumbList', '@id': 'https://fedscoop.com/google-project-maven-canary-coal-mine/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://fedscoop.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Google&#8217;s departure from Project Maven was a &#8216;little bit of a canary in a coal mine&#8217;'}]}, {'@type': 'WebSite', '@id': 'https://fedscoop.com/#website', 'url': 'https://fedscoop.com/', 'name': 'FedScoop', 'description': 'FedScoop delivers up-to-the-minute breaking government tech news and is the government IT community&#039;s platform for education and collaboration through news, events, radio and TV. FedScoop engages top leaders from the White House, federal agencies, academia and the tech industry both online and in person to discuss ways technology can improve government, and to exchange best practices and identify how to achieve common goals.', 'publisher': {'@id': 'https://fedscoop.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://fedscoop.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://fedscoop.com/#organization', 'name': 'FedScoop', 'url': 'https://fedscoop.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://fedscoop.com/#/schema/logo/image/', 'url': 'https://fedscoop.com/wp-content/uploads/sites/5/2023/01/FedScoop-Black.png', 'contentUrl': 'https://fedscoop.com/wp-content/uploads/sites/5/2023/01/FedScoop-Black.png', 'width': 1231, 'height': 182, 'caption': 'FedScoop'}, 'image': {'@id': 'https://fedscoop.com/#/schema/logo/image/'}}]",http://fedscoop.com/google-project-maven-canary-coal-mine/,,,,,,,['Billy Mitchell'],2019-11-05T20:10:02Z
https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vd3d3LnNoZWVwY2VudHJhbC5jb20vYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtbWFrZXMtYWNjdXJhdGUtc2hlZXAtY291bnRpbmctYS1yZWFsaXR5L9IBAA?oc=5,Artificial intelligence makes accurate sheep counting a reality - Sheep Central,2019-11-01,Sheep Central,https://www.sheepcentral.com,N/A,N/A,A LIVESTOCK counting system using artificial intelligence will be ready for the live export industry this year and for use in sheep saleyards by April 2020...Read More,N/A,https://schema.org,,,,,,,,,,N/A,N/A,N/A,,,,"[{'@type': 'WebPage', '@id': 'https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/', 'url': 'https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/', 'name': 'Artificial intelligence makes accurate sheep counting a reality - Sheep Central', 'isPartOf': {'@id': 'https://www.sheepcentral.com/#website'}, 'primaryImageOfPage': {'@id': 'https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/#primaryimage'}, 'image': {'@id': 'https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/#primaryimage'}, 'thumbnailUrl': 'https://www.sheepcentral.com/wp-content/uploads/2019/11/Sheep-counting-above-the-loading-ramps-of-a-live-export-vessel.-167x300.jpg', 'datePublished': '2019-11-01T03:39:17+00:00', 'dateModified': '2019-11-10T00:57:53+00:00', 'author': {'@id': 'https://www.sheepcentral.com/#/schema/person/b1057e1f39f3486b508db95e0235faf9'}, 'breadcrumb': {'@id': 'https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/#primaryimage', 'url': 'https://www.sheepcentral.com/wp-content/uploads/2019/11/Sheep-counting-above-the-loading-ramps-of-a-live-export-vessel..jpg', 'contentUrl': 'https://www.sheepcentral.com/wp-content/uploads/2019/11/Sheep-counting-above-the-loading-ramps-of-a-live-export-vessel..jpg', 'width': 436, 'height': 784}, {'@type': 'BreadcrumbList', '@id': 'https://www.sheepcentral.com/artificial-intelligence-makes-accurate-sheep-counting-a-reality/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.sheepcentral.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Artificial intelligence makes accurate sheep counting a reality'}]}, {'@type': 'WebSite', '@id': 'https://www.sheepcentral.com/#website', 'url': 'https://www.sheepcentral.com/', 'name': 'Sheep Central', 'description': 'Sheep News Australia', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.sheepcentral.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Person', '@id': 'https://www.sheepcentral.com/#/schema/person/b1057e1f39f3486b508db95e0235faf9', 'name': 'Terry Sim', 'url': 'https://www.sheepcentral.com/author/terry/'}]",,,,,,,,,
