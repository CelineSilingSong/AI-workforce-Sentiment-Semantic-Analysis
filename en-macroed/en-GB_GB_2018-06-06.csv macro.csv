URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,url,image,author,publisher,headline,datePublished,dateModified,articleSection,name,isAccessibleForFree,itemListElement,article:section,article:summary,article text,isPartOf,dateline,mainEntityOfPage,dateCreated,creator,copyrightHolder,copyrightYear,thumbnailUrl,inLanguage,printSection,printEdition,foundingDate,logo,sameAs,alternativeHeadline,hasPart,sourceOrganization,@id,diversityPolicy,ethicsPolicy,masthead,@graph,articleBody,isBasedOn,identifier
https://news.google.com/rss/articles/CBMikQFodHRwczovL3d3dy5mb3JiZXMuY29tL3NpdGVzL2Jlcm5hcmRtYXJyLzIwMTgvMDYvMDYvcm9ib3RpYy1wcm9jZXNzLWF1dG9tYXRpb24tYW5kLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWluLWhyLWFuZC1idXNpbmVzcy1zdXBwb3J0LWl0cy1jb21pbmcv0gEA?oc=5,Robotic Process Automation And Artificial Intelligence In HR And Business Support - It's Coming - Forbes,2018-06-06,Forbes,https://www.forbes.com,"Business support functions such as HR have been slower at applying smart robots and artificial intelligence tools, than other parts of business and industry. Here we look at how this is going to change and how robotic process automation and machine learning are starting to make an impact.",,"Business support functions such as HR have been slower at applying smart robots and artificial intelligence tools, than other parts of business and industry. Here we look at how this is going to change and how robotic process automation and machine learning are starting to make an impact.","Business support functions such as HR have been slower at applying smart robots and artificial intelligence tools, than other parts of business and industry. Here we look at how this is going to change and how robotic process automation and machine learning are starting to make an impact.",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/bernardmarr/2018/06/06/robotic-process-automation-and-artificial-intelligence-in-hr-and-business-support-its-coming/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/blogs-images/bernardmarr/files/2018/06/AdobeStock_202519848-1200x775.jpeg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Bernard Marr', 'url': 'https://www.forbes.com/sites/bernardmarr/', 'description': 'Bernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society. He has written over 20 best-selling and award-winning books and advises and coaches many of the world’s best-known organisations. He has a combined following of 4 million people across his social media channels and newsletters and was ranked by LinkedIn as one of the top 5 business influencers in the world. Follow Bernard on LinkedIn, X (Twitter) or YouTube. Join his newsletter, check out his website and books.', 'sameAs': ['https://www.linkedin.com/in/bernardmarr/', 'https://www.twitter.com/BernardMarr', 'https://bernardmarr.com/']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",Robotic Process Automation And Artificial Intelligence In HR And Business Support - It's Coming,2018-06-06T00:28:00-04:00,2018-06-06T00:28:02-04:00,Tech,Robotic Process Automation And Artificial Intelligence In HR And Business Support - It's Coming,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Enterprise Tech', 'item': 'https://www.forbes.com/enterprise-tech/'}]",Tech,N/A,"More From ForbesMar 15, 2024,07:08am EDTUK And Germany Double Down On Joint AI, Clean Energy R&D EffortsNov 28, 2023,01:05am ESTDecarbonizing Heavy Transportation: Quantron's Michael Perschke On Pioneering Hydrogen SolutionsNov 20, 2023,01:59am ESTBridging The Digital Divide In The AI Era - The UNDP WayOct 15, 2023,01:36pm EDTBig Tech Ramps Up Content Moderation Amid EU PressureOct 10, 2023,01:34pm EDTSpain's Successful Miura 1 Launch Reignites Europe's Hopes For Space ExplorationOct 7, 2023,02:39am EDTUNESCO And The Netherlands Launch Initiative To Ensure Ethical Oversight Of AISep 23, 2023,04:47am EDTEurope's Bid To Become A Semiconductor SuperpowerEdit StoryForbesInnovationEnterprise TechRobotic Process Automation And Artificial Intelligence In HR And Business Support - It's ComingBernard MarrContributorOpinions expressed by Forbes Contributors are their own.FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got itJun 6, 2018,12:28am EDTUpdated Jun 6, 2018, 12:28am EDTThis article is more than 6 years old.Share to FacebookShare to TwitterShare to LinkedinThe use of robotics and advanced analytics are apparent and mature in industrial settings, while in support functions such as HR, it has the potential to be just as revolutionary but the uptake has been a little slower.








Adobe Stock
Adobe Stock






For example, in HR, onboarding – the process of hiring new members of staff and putting them to work following recruitment– generally takes around one month, according to recent research by CareerBuilder. The various processes – taking up references, verifying identities, carrying out health and safety assessments and ensuring hires have an understanding of company practices, policies and culture – requires a complex set of actions and toolsets which are not easy to automate.
However, there’s a lot to gain – as well as driving efficiency by cutting down time spent on mundane but vital processes and compliance, automating tasks like this will free up skilled workers to apply themselves more creatively.
PROMOTED
In fact, the potential up-sides mean that automation of support functions is an inevitability, as market forces drive businesses to strive for ever greater efficiency efficiency. CareerBuilder’s survey bears this out – with 72% of employers predicting that certain recruitment and HR roles will be completely automated by 2028.

An important point I am making in many of my articles and my book Data-Driven HR is that this doesn’t mean we will replace HR departments with robots but that automation will severely augment the jobs people will be doing in support functions like HR. Parts of the workload – dealing with interpersonal or disciplinary issues – still require a human touch.
These skills are likely to become increasingly important to the HR support professional – as well as the ability to work with the automated tools taking care of the mundane aspects of the job.









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



This month PeopleDoc launched their solution to the problem automating the disparate toolsets used across business support functions. Their Robotic Process Automation platform uses “PeopleBots” which run alongside existing systems to “listen” for events and processes which can be automated.
The idea is that they will use analytic technologies including machine learning algorithms to learn to automatically execute repetitive tasks and follow-on actions, leading to more accurate results and reducing the need for humans to carry out tedious repetitive actions.
COO and co-founder Clément Buyse told me that they focus on HR infrastructure because core business functions, as well as sales and marketing, are already well-served with automation.
“HR is not as sexy but its very people-centric, and it all starts with the user experience,” he said.
“Now our customers are reaching a point where they would like to go further in terms of automation … they want to take a hybrid approach, mixing processes with people and machines, injecting some machine learning.”
The nature of HR and administrative tasks is that often they vary little from one business or use case to another. This means what a machine learning algorithm could learn about driving efficiency within one organization’s HR activities will carry over very well to the next operation where it is deployed.
Within an admin or HR environment, automated bots can be used to offer assistance when a user appears to be having difficulty at a particular point during a process, and therefore go on to “learn” where obstacles are likely to be encountered.
They can also be used to keep disparate records synchronized between different departments or users, and the different systems that might be in use.
For example, in the case of onboarding, an automated process could be triggered by a manager changing the status of an applicant’s file to “hired”. This would trigger documents such as application forms and resumes to be routed to the correct departments for archiving, a job offer letter being dispatched by post, user accounts created on internal IT systems and an ID card generated.
For support functions, moving towards automation is likely to present certain challenges, which will be different for each of them. IT departments, for example, may more readily take to the changing nature of the technology they are asked to work with. Legal departments may be less enthusiastic about “handing over” processes of compliance to an automated colleague. And from the top, there may be pressure from the c-suite to focus tech-driven change on sexier, customer-facing activities.
But from studying businesses which have already experienced success at implementing their own slice of the “fourth industrial revolution” it’s clear that digital transformation is most effective when it is implemented throughout an organization at all levels and functions.
This is going to involve innovative thinking as well as reassurances that technology is implemented to assist, rather than replace. But for organizations which are able to pull it all together, the rewards are there for the taking.Follow me on Twitter or LinkedIn. Check out my website or some of my other work here. Bernard MarrFollowingFollowBernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is... Read MoreEditorial StandardsPrintReprints & Permissions
1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGE",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMic2h0dHBzOi8vd3d3LmVjb25vbWlzdC5jb20vc2NpZW5jZS1hbmQtdGVjaG5vbG9neS8yMDE4LzA2LzA5L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXdpbGwtaW1wcm92ZS1tZWRpY2FsLXRyZWF0bWVudHPSAQA?oc=5,Artificial intelligence will improve medical treatments - The Economist,2018-06-09,The Economist,https://www.economist.com,It will not imminently put medical experts out of work,[],It will not imminently put medical experts out of work,It will not imminently put medical experts out of work,https://schema.org,BreadcrumbList,https://www.economist.com,https://www.economist.com/sites/default/files/images/print-edition/20180609_STD001_0.jpg,"{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}, 'url': 'https://www.economist.com/'}","{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}",Artificial intelligence will improve medical treatments,2018-06-09T00:00:00.000Z,,Science and technology,The Economist,False,"[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://www.economist.com/science-and-technology/', 'name': 'Science and technology'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://www.economist.com/weeklyedition/2018-06-09', 'name': 'June 9th 2018 edition'}}]",N/A,N/A,"Science and technology | From A&E to AIArtificial intelligence will improve medical treatmentsIt will not imminently put medical experts out of work Jun 9th 2018ShareFOUR years ago a woman in her early 30s was hit by a car in London. She needed emergency surgery to reduce the pressure on her brain. Her surgeon, Chris Mansi, remembers the operation going well. But she died, and Mr Mansi wanted to know why. He discovered that the problem had been a four-hour delay in getting her from the accident and emergency unit of the hospital where she was first brought, to the operating theatre in his own hospital. That, in turn, was the result of a delay in identifying, from medical scans of her head, that she had a large blood clot in her brain and was in need of immediate treatment. It is to try to avoid repetitions of this sort of delay that Mr Mansi has helped set up a firm called Viz.ai. The firm’s purpose is to use machine learning, a form of artificial intelligence (AI), to tell those patients who need urgent attention from those who may safely wait, by analysing scans of their brains made on admission.That idea is one among myriad projects now under way with the aim of using machine learning to transform how doctors deal with patients. Though diverse in detail, these projects have a common aim. This is to get the right patient to the right doctor at the right time.In Viz.ai’s case that is now happening. In February the firm received approval from regulators in the United States to sell its software for the detection, from brain scans, of strokes caused by a blockage in a large blood vessel. The technology is being introduced into hospitals in America’s “stroke belt”—the south-eastern part, in which strokes are unusually common. Erlanger Health System, in Tennessee, will turn on its Viz.ai system next week.Already have an account?Log inContinue with a free trialExplore all our independent journalism for free for one month. Cancel any timeFree trialOr continue reading this articleRegister nowScience & technology June 9th 2018Artificial intelligence will improve medical treatmentsExtracting carbon dioxide from the air is possible. But at what cost?The Cambrian explosion was caused by a lack of oxygen, not an abundanceShareReuse this contentThe Economist todayHandpicked stories, in your inboxA daily newsletter with the best of our journalismSign upYes, I agree to receive exclusive content, offers and updates to products and services from The Economist Group. I can change these preferences at any time.More from Science and technologyFreeze-dried chromosomes can survive for thousands of yearsThey contain unprecedented detail about their long-dead parent organismsResearchers are figuring out how large language models workSuch insights could help make them safer, more truthful and easier to useA scientific discovery could lead to leak-free period productsPolymers from algae can turn menstrual blood into a gelMore from Science and technologyFreeze-dried chromosomes can survive for thousands of yearsThey contain unprecedented detail about their long-dead parent organismsResearchers are figuring out how large language models workSuch insights could help make them safer, more truthful and easier to useA scientific discovery could lead to leak-free period productsPolymers from algae can turn menstrual blood into a gelVaccines could keep salmon safe from sea liceA successful jab would be a boon to fish farmersNew yeast strains can produce untapped flavours of lagerOne Chilean hybrid has a spicy taste, with hints of cloveA new technique could analyse tumours mid-surgeryIt would be fast enough to guide the hands of neurosurgeons","{'@type': ['NewsArticle', 'Product'], 'name': 'The Economist', 'productID': 'economist.com:showcase'}",,https://www.economist.com/science-and-technology/2018/06/09/artificial-intelligence-will-improve-medical-treatments,2018-06-09T00:00:00.000Z,"{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}","{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}",2018.0,https://www.economist.com/sites/default/files/images/print-edition/20180609_STD001_0.jpg,en,Science and technology,2018-06-09T00:00:00Z,1843,"{'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}","['https://www.facebook.com/theeconomist', 'https://www.instagram.com/theeconomist', 'https://www.twitter.com/theeconomist', 'https://www.linkedin.com/company/the-economist', 'https://www.youtube.com/user/economistmagazine', 'https://en.wikipedia.org/wiki/The_Economist']",,,,,,,,,,,
https://news.google.com/rss/articles/CBMiU2h0dHBzOi8vd3d3LmVmZi5vcmcvZGVlcGxpbmtzLzIwMTgvMDYvaG93LWdvb2QtYXJlLWdvb2dsZXMtbmV3LWFpLWV0aGljcy1wcmluY2lwbGVz0gEA?oc=5,How Good Are Google's New AI Ethics Principles? - EFF,2018-06-07,EFF,https://www.eff.org,"Today Google released a new set of AI ethics principles, which were prompted, at least in part, by the controversy over the company's work on the US military's Project Maven. This post contains some quick preliminary analysis on the strengths and weaknesses of those principles.On many fronts, the...",N/A,"Today Google released a new set of AI ethics principles, which were prompted, at least in part, by the controversy over the company's work on the US military's Project Maven. This post contains some quick preliminary analysis on the strengths and weaknesses of those principles.On many fronts, the...","Today Google released a new set of AI ethics principles, which were prompted, at least in part, by the controversy over the company's work on the US military's Project Maven. This post contains some",,,,,,,,,,,,,,N/A,N/A,"

Today Google released a new set of AI ethics principles, which were prompted, at least in part, by the controversy over the company's work on the US military's Project Maven. This post contains some quick preliminary analysis on the strengths and weaknesses of those principles.
On many fronts, the principles are well thought-out and promising. With some caveats, and recognizing that the proof will be in their application by Google, we recommend that other tech companies consider adopting similar guidelines for their AI work. But we do also have some concerns that we recommend Google and other tech companies address:

One concern is that Google hasn't committed to the type of independent, informed and transparent review which would be ideal for ensuring the principles are always applied and applied well. Without that, the public will have to rely on the company's internal, secret processes to ensure that these guidelines are followed. That's a common (and generally unfortunate) pattern in corporate governance and social accountability, but there's an argument that AI ethics is so important and the stakes can be so high, that there should be independent review as well, with at least some public accountability.
Another concern is that by relying on “widely accepted principles of international law and human rights” for the purposes that Google will not pursue, the company is potentially sidestepping some harder questions.  It is not at all settled — at least in terms of international agreements and similar law —  how many key international law and human rights principles should be applied to various AI technologies and applications.  This lack of clarity is one of the key reasons that we and others have called on companies like Google to think so hard about their role in developing and deploying AI technologies, especially in military contexts.  Google and other companies developing and deploying AI need not only to follow “widely accepted principles” but to take the lead in articulating where, how and why their work is consistent with principles of international law and human rights.  

On surveillance, however, we do have some specifics for Google and other companies to follow. Google has so far constrained itself to only assisting AI surveillance projects that don't violate internationally accepted norms. We want to hear clearly that those include the Necessary and Proportionate Principles, and not merely the prevailing practice of many countries spying on the citizens of almost every other country. In fact, in the light of this practice, it would be better if Google tried to avoid building AI-assisted surveillance systems altogether.


We hope Google will consider addressing these issues with their principles. There may be other issues that come to light with further analysis. But beyond that, we think this is a good first step by the company, and with some improvements on these fronts, could become an excellent model for AI ethics guidelines across the tech industry.  And we're ready to hear from the rest of that industry that they too are stepping up. 
 
",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWWh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMTgvMDYvMDcvdGVjaG5vbG9neS9nb29nbGUtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utd2VhcG9ucy5odG1s0gEA?oc=5,Google Promises Its A.I. Will Not Be Used for Weapons (Published 2018) - The New York Times,2018-06-07,The New York Times,https://www.nytimes.com,"Facing an employee rebellion over work with the Pentagon, the internet giant said its “principles” for A.I. technology would also limit some surveillance work.",N/A,"Facing an employee rebellion over work with the Pentagon, the internet giant said its “principles” for A.I. technology would also limit some surveillance work.","Facing an employee rebellion over work with the Pentagon, the internet giant said its “principles” for A.I. technology would also limit some surveillance work.",https://schema.org,NewsMediaOrganization,https://www.nytimes.com/,"[{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2018/06/08/business/08GOOGLE/08GOOGLE-videoSixteenByNineJumbo1600.jpg', 'height': 900, 'width': 1600, 'contentUrl': 'https://static01.nyt.com/images/2018/06/08/business/08GOOGLE/08GOOGLE-videoSixteenByNineJumbo1600.jpg', 'caption': 'Sundar Pichai, Google’s chief executive, at its annual Google I/O developer conference last month in Mountain View, Calif. On Thursday he laid out objectives for the company’s use of A.I. technology.', 'creditText': 'Jim Wilson/The New York Times'}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2018/06/08/business/08GOOGLE/merlin_137903793_ea958c97-8b0f-4b4f-bd95-c3205efb4e40-superJumbo.jpg', 'height': 1365, 'width': 2048, 'contentUrl': 'https://static01.nyt.com/images/2018/06/08/business/08GOOGLE/merlin_137903793_ea958c97-8b0f-4b4f-bd95-c3205efb4e40-superJumbo.jpg', 'caption': 'Sundar Pichai, Google’s chief executive, at its annual Google I/O developer conference last month in Mountain View, Calif. On Thursday he laid out objectives for the company’s use of A.I. technology.', 'creditText': 'Jim Wilson/The New York Times'}]","[{'@context': 'https://schema.org', '@type': 'Person', 'url': 'https://www.nytimes.com/by/daisuke-wakabayashi', 'name': 'Daisuke Wakabayashi'}, {'@context': 'https://schema.org', '@type': 'Person', 'url': 'https://www.nytimes.com/by/cade-metz', 'name': 'Cade Metz'}]","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",Google Promises Its A.I. Will Not Be Used for Weapons,2018-06-07T19:01:02.000Z,2018-06-07T19:36:33.000Z,,The New York Times,False,,Technology,N/A,"Artificial IntelligenceFine Print ChangesQuiz: Fake or Real Images?Apple Enters A.I. FrayMeta’s A.I. ScrapingHumane’s A.I. Device FlopAdvertisementSKIP ADVERTISEMENTSupported bySKIP ADVERTISEMENTGoogle Promises Its A.I. Will Not Be Used for WeaponsShare full articleRead in appSundar Pichai, Google’s chief executive, at its annual Google I/O developer conference last month in Mountain View, Calif. On Thursday he laid out objectives for the company’s use of A.I. technology.Credit...Jim Wilson/The New York TimesBy Daisuke Wakabayashi and Cade MetzJune 7, 2018SAN FRANCISCO — Google, reeling from an employee protest over the use of artificial intelligence for military purposes, said Thursday that it would not use A.I. for weapons or for surveillance that violates human rights. But it will continue to work with governments and the military.The new rules were part of a set of principles Google unveiled relating to the use of artificial intelligence. In a company blog post, Sundar Pichai, the chief executive, laid out seven objectives for its A.I. technology, including “avoid creating or reinforcing unfair bias” and “be socially beneficial.”Google also detailed applications of the technology that the company will not pursue, including A.I. for “weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people” and “technologies that gather or use information for surveillance violating internationally accepted norms of human rights.”But Google said it would continue to work with governments and military using A.I. in areas including cybersecurity, training and military recruitment.AdvertisementSKIP ADVERTISEMENT“We recognize that such powerful technology raises equally powerful questions about its use. How A.I. is developed and used will have a significant impact on society for many years to come,” Mr. Pichai wrote.Concern over the potential uses of artificial intelligence bubbled over at Google when the company secured a contract to work on the Pentagon’s Project Maven program, which uses A.I. to interpret video images and could be used to improve the targeting of drone strikes.More than 4,000 Google employees signed a petition protesting the contract, and a handful of employees resigned. In response, Google said it would not seek to renew the Maven contract when it expired next year and pledged to draft a set of guidelines for appropriates uses of A.I.Mr. Pichai did not address the Maven program or the pressure from employees. It’s not clear whether these guidelines would have precluded Google from pursuing the Maven contract, since the company has insisted repeatedly that its work for the Pentagon was not for “offensive purposes.”AdvertisementSKIP ADVERTISEMENTGoogle has bet its future on artificial intelligence, and company executives believe the technology could have an impact comparable to the development of the internet.Google promotes the benefits of artificial intelligence for tasks like early diagnosis of diseases and the reduction of spam in email. But it has also experienced some of the perils associated with A.I., including YouTube recommendations pushing users to extremist videos or Google Photos image-recognition software categorizing black people as gorillas.While most of Google’s A.I. guidelines are unsurprising for a company that prides itself on altruistic goals, it also included a noteworthy rule about how its technology could be shared outside the company.“We will reserve the right to prevent or stop uses of our technology if we become aware of uses that are inconsistent with these principles,” the company said.Like most of the top corporate A.I. labs, which are laden with former and current academics, Google openly publishes much of its A.I. research. That means others can recreate and reuse many of its methods and ideas. But Google is joining other labs in saying it may hold back certain research if it believes others will misuse it.DeepMind, a top A.I. lab owned by Google’s parent company, Alphabet, is considering whether it should refrain from publishing certain research because it may be dangerous. OpenAI, a lab founded by the Tesla chief executive Elon Musk and others, recently released a new charter indicating it could do much the same — even though it was founded on the principle that it would openly share all its research.Follow Daisuke Wakabayashi on Twitter: @daiwaka. A version of this article appears in print on June 8, 2018, Section B, Page 2 of the New York edition with the headline: Google Vows Its A.I. Will Not Be Used for Weapons. Order Reprints | Today’s Paper | SubscribeSee more on: Alphabet Inc.Share full articleRead in appAdvertisementSKIP ADVERTISEMENTEnjoy unlimited access to all of The Times.6-month Welcome Offeroriginal price:   $6.25sale price:   $1/weekLearn more","{'@type': ['CreativeWork', 'Product'], 'name': 'The New York Times', 'productID': 'nytimes.com:basic'}",,https://www.nytimes.com/2018/06/07/technology/google-artificial-intelligence-weapons.html,,,"{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",2024.0,,,,,1851-09-18,"{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'height': 291, 'width': 291, 'contentUrl': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'creditText': 'The New York Times'}",https://en.wikipedia.org/wiki/The_New_York_Times,Google Vows Its A.I. Will Not Be Used for Weapons,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.meteredContent'}","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",https://www.nytimes.com/#publisher,https://www.nytco.com/company/diversity-and-inclusion/,https://www.nytco.com/company/standards-ethics/,https://www.nytimes.com/interactive/2023/01/28/admin/the-new-york-times-masthead.html,,,,
https://news.google.com/rss/articles/CBMidGh0dHBzOi8vbmF0aW9uYWxpbnRlcmVzdC5vcmcvZmVhdHVyZS93aHktYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utd29udC1iZS1iYWQlRTIlODAlOTRvci1nb29kJUUyJTgwJTk0LWV2ZXJ5b25lLTI2MjA50gEA?oc=5,Why Artificial Intelligence Won't Be as Bad—or as Good—as Everyone Thinks - The National Interest Online,2018-06-10,The National Interest Online,https://nationalinterest.org,Past technological waves have created the same hopes and the same worries over job loss.,"Security, Artificial Intelligence, AI, Robots, Jobs, War",Past technological waves have created the same hopes and the same worries over job loss.,Past technological waves have created the same hopes and the same worries over job loss.,https://schema.org,,,,,,,,,,,,,N/A,N/A,"



Share on Facebook
F




Share on Twitter
L




Share on LinkedIn
I




Subscribe to RSS
R




Print








June 10, 2018

Topic: Security

Region: Americas

Tags: Artificial IntelligenceAIRobotsJobsWar

Why Artificial Intelligence Won't Be as Bad—or as Good—as Everyone Thinks
Past technological waves have created the same hopes and the same worries over job loss.

by Milton Ezrati



 


Robotics and artificial intelligence (AI) have fed two kinds of dreams, the first of the hopeful pleasant sort, and the other a nightmare. The first tells of great abundance, convenience and wealth. The other warns of job loss and widespread unemployment, among both workers and the managerial class. Both have some validity, but only up to a point. AI, like most technological advances before it, will offer society great advances in prosperity and productivity, though they will emerge at a slower pace than the enthusiasts predict. It will also take some jobs, but contrary to much commentary on the subject, it will not lead to mass unemployment. It will instead likely create more new jobs than it destroys and will create occupations that didn't exist before.



SPONSORED CONTENT

Recommended by

 





Popular commentary in the United States on this matter has given both dreams lavish attention. Describing AI and modern robotics as different from any previous technological waves, much analysis gushes about the prospects for immense convenience and great wealth. Some pieces describe a future of aristocratic-like lifestyles, except with robot instead of human servants. At the same time, commentary worries about the tendency for the new technology to eliminate jobs. A January 2018 Gallup poll concluded: ""Economists agree"" AI is the ""single biggest threat to future job growth."" Such thinking, especially within the tech community, goes on to envision the rise of a large class of permanently unemployed. It almost always concludes that the nation must care for these people and protect social cohesion by providing a universal basic income (UBI) to all, financing it with a tax on the vast wealth created by AI. Depending on who is speaking, the tax would fall either on robots themselves, their users, or their producers (but seldom the person calling for the tax.) 

Before framing policy around such thinking, all should note that America has heard this song before. Past technological waves have created the same hopes and the same worries over job loss. They have all also included calls for society to provide for a universal basic income or something much like it. In the 1930s, a group called the Industrial Workers of the World (IWW) published a report blaming the widespread unemployment of the time on the use of machinery in production. Its analysis noted how in the prior twenty years the introduction of machinery into factories had cut the labor hours needed to produce an automobile by three quarters and the labor required to create a ton of steel by more than eighty-five percent. It further noted how in the prior seventy-five years, the labor needed to produce the wheat crop of the United States had fallen to a mere hundredth of what it had been. IWW thinkers concluded that those who had benefited from these surges in productivity should provide incomes to the workers displaced in the process.
If the IWW looks ridiculous today, it is only because the Great Depression gave way to a great prosperity that employed millions more. However, new calls of this kind arose again, this time amid great prosperity. In the opening years of the 1960s a large group of U.S. academics, including several economists and scientific Nobel laureates issued a report that identified the ""new kinds of automation"" as having ""broken"" the once-secure ""link between jobs and incomes."" Concerns along with these lines were in fact widespread. At about the same time as this public-spirited report appeared, American President John F. Kennedy warned how automation, among other things, pointed to a future haunted by the ""dark menace of industrial dislocation, increasing unemployment, and deepening poverty."" With this in mind, he created an Office of Automation and Manpower in the Labor Department. It would address ""the major domestic challenge of the Sixties: to maintain full employment at a time when automation, of course, is replacing man."" He followed up by recommending that Congress fund ""readjustment allowances"" for ""workers displaced by technological change."" Later in that decade, U.S. President Lyndon B. Johnson, also worried over the employment effects of technology, brought together a panel of experts. They concluded that the government should create ""a guaranteed minimum income for each family.""





Apart from the changes political correctness has imposed on language, such concerns and conclusions sound precisely like those uttered today. And no doubt they seemed just as compelling at the time. Yet, the widespread unemployment forecast at each technological wave failed to become a lasting part of America's social landscape. To be sure, each period of innovation destroyed jobs. The railroads cost canal builders and workers their occupations. The rise of the automobile not only killed the work of buggy makers but also millions of jobs and acres of land dedicated to the breeding, training, shipping, selling, and stabling the millions of horses then needed in the economy. The invention of shipping containers in the 1950s put millions of longshoremen out of work. But at the same time, new jobs emerged from these new technologies themselves and the tremendous wealth fostered by them.



SPONSORED CONTENT

Recommended by

 




Despite the presence of concerns at each stage, jobs and wealth creation has typified each wave of technological innovation, probably going back to the invention of the wheel and the breeding of animals for draft and transportation. The statistics, sadly, do not go back as far as the wheel, but the ones we do have by economic historians verify the benefits of each advancement. For all the technological advancement over the centuries, economies, except for brief interludes, have always reliably provided work for some ninety-five percent of the population that wants to work. If technology indeed destroyed jobs, that number would have fallen over time.
Instead, often the new technology itself creates additional employment. The application of the spinning jenny and machine loom in eighteenth and early nineteenth century Britain put hand weavers and others out of work. But by creating a much more profitable trade on textiles, the inventions brought about such an expansion in the industry that it ultimately employed more people than previously, including a more significant part of the nation's workforce as well. More recently, the advancement of automatic teller machines (ATM) made everyday banking so much more efficient, the industry employed higher numbers of men and women, including the tellers that people feared would wind up on the unemployment line.



SPONSORED CONTENT

Recommended by

 





Of greater significance than the employment question is how much wealth created by each technological advance. To be sure, each wave creates a class of super-wealthy from those lucky or smart enough to have made themselves a part of it. The great American railroad barons of the nineteenth century stand as examples, as do today's computer-based tech barons. No doubt AI will create its own class of this sort. But the technology also drives the general economy, creating new demands for products and services of all kinds that in turn create new jobs. This job-producing leverage is evident in a simple thought experiment. Imagine if AI could bring the growth rate of labor productivity in America from the present pace of about one percent a year up to the two to three percent pace averaged in the 1960s, 70s, and 80s. If that were to happen, the overall U.S. economy would expand at something close to 3.5 percent a year instead of the recent two percent pace. Over the next ten years, that difference would produce $3.6 trillion additional annual national product than otherwise, a huge cumulative addition.
Of course, each U.S. generation, though it has access to this history, worries that its technology is somehow different. The response is understandable. People can see the jobs the new technology will destroy a lot clearer than the jobs it will create, especially since some of the new jobs have yet to be even imagined. For instance, when containers put American longshoremen out of work in the late 1950s and early 1960s, it would have taken a bold forecaster indeed to predict that containers would be a good thing. In fact, those containers would foster a 2,000 percent increase in world trade in just five years and create more jobs for higher-paid, better-trained workers in the industry and beyond it. Furthermore, in the 1980s when widespread use of word processing drove out U.S. typists, who were mostly women, the number of women in the workforce still increased, as did the proportion of women in paid employment. The natural inability to see the sources of growth shows clearly, if rather comically, in how the 1975 report of the American Council of Economic Advisors (incidentally authored by Alan Greenspan) did not once use the word ""computer.""
There is no doubt that robotics and AI will cause dislocation and hardship for some. In this case, the nature of the new technology will extend the pain to occupations once thought immune (which may be why the fears have become unusually intense among U.S. journalists and bureaucrats.) There is every reason to seek programs and funds that can retrain and retool such people for other work. Though, as always, it is difficult to identify what that future work will involve (and a fool's game to try to identify it). The good thing is that the evidence of history, a small part of which this article reviews, and the logic of economic growth suggests that a jobless future is the least likely of outcomes and that the argument for a UBI misses that point.
 
1
2
Next





Recommended for you

Recommended by

 









",,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'NewsArticle', 'headline': ""Why Artificial Intelligence Won't Be as Bad—or as Good—as Everyone Thinks"", 'description': 'Past technological waves have created the same hopes and the same worries over job loss.', 'author': {'@type': 'Person', 'name': 'Milton Ezrati', 'url': ''}, 'datePublished': 'June 10, 2018 - 19:04', 'dateModified': 'June 10, 2018 - 19:04', 'image': {'@type': 'ImageObject', 'url': 'https://nationalinterest.org/sites/default/files/styles/desktop__1260_/public/main_images/image-2018-06-08_5.jpg?itok=7taxiVra', 'width': '858', 'height': '578'}, 'mainEntityOfPage': 'https://nationalinterest.org/feature/why-artificial-intelligence-wont-be-bad%E2%80%94or-good%E2%80%94-everyone-26209', 'about': ['Security', 'Artificial Intelligence', 'AI', 'Robots', 'Jobs', 'War'], 'publisher': {'@type': 'Organization', '@id': 'https://nationalinterest.org', 'name': 'The National Interest', 'url': 'https://nationalinterest.org', 'logo': {'@type': 'ImageObject', 'url': '/sites/default/files/national-interest.png'}}}, {'@type': 'ImageObject', 'url': 'https://nationalinterest.org/sites/default/files/styles/desktop__1260_/public/main_images/image-2018-06-08_5.jpg?itok=7taxiVra', 'width': '858', 'height': '578'}, {'@type': 'Organization', '@id': 'https://nationalinterest.org', 'name': 'The National Interest'}]",,,
https://news.google.com/rss/articles/CBMiWGh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9nb29nbGUtc2V0cy1saW1pdHMtb24taXRzLXVzZS1vZi1haS1idXQtYWxsb3dzLWRlZmVuc2Utd29yay_SAQA?oc=5,"Google Sets Limits on Its Use of AI, but Allows Defense Work - WIRED",2018-06-07,WIRED,https://www.wired.com,"In response to employee unrest over a Pentagon contract, Google CEO Sundar Pichai offered rules for the company's use of artificial intelligence.","['business', 'ai hub', 'ethics', 'alphabet', 'google', 'government', 'big company', 'defense', 'artificial intelligence', 'project maven', 'pentagon', 'web']","In response to employee unrest over a Pentagon contract, Google CEO Sundar Pichai offered rules for the company's use of artificial intelligence.","In response to employee unrest over a Pentagon contract, Google CEO Sundar Pichai offered rules for the company's use of artificial intelligence.",https://schema.org/,BreadcrumbList,https://www.wired.com/story/google-sets-limits-on-its-use-of-ai-but-allows-defense-work/,"['https://media.wired.com/photos/5b1962ea66dc162128d46462/16:9/w_2400,h_1350,c_limit/reaperdrone.jpg', 'https://media.wired.com/photos/5b1962ea66dc162128d46462/4:3/w_2400,h_1800,c_limit/reaperdrone.jpg', 'https://media.wired.com/photos/5b1962ea66dc162128d46462/1:1/w_1800,h_1800,c_limit/reaperdrone.jpg']","[{'@type': 'Person', 'name': 'Tom Simonite', 'sameAs': 'https://www.wired.com/author/tom-simonite/'}]","{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}","Google Sets Limits on Its Use of AI, but Allows Defense Work",2018-06-07T15:00:00.000-04:00,2018-06-07T16:17:00.000-04:00,business,,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'Business', 'item': 'https://www.wired.com/business/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Google', 'item': 'https://www.wired.com/tag/google/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Google Sets Limits on Its Use of AI but Allows Defense Work'}]",tags,N/A,"Tom SimoniteBusinessJun 7, 2018 4:17 PMGoogle Sets Limits on Its Use of AI but Allows Defense WorkIn response to employee unrest over a Pentagon contract, Google CEO Sundar Pichai offered rules for the company's use of artificial intelligence.Project Maven uses artificial intelligence to interpret images from drones similar to this one at a base in Afghanistan.Josh Smith/ReutersSave this storySaveSave this storySaveThe AI Database →ApplicationEthicsCompanyAlphabetGoogleEnd UserGovernmentBig companySectorDefenseEarlier this year, Google CEO Sundar Pichai described artificial intelligence as more profound to humanity than fire. Thursday, after protests from thousands of Google employees over a Pentagon project, Pichai offered guidelines for how Google will—and won’t—use the technology. One thing Pichai says Google won’t do: work on AI for weapons. But the guidelines leave much to the discretion of company executives and allow Google to continue to work for the military.The ground rules are a response to more than 4,500 Googlers signing a letter protesting the company’s involvement in a Pentagon project called Maven that uses machine learning to interpret drone surveillance video.The dissenting employees asked Google to swear off all military work. Pichai’s response? We hear you, but you can trust us to do this responsibly. “We will continue our work with governments and the military in many other areas,” Pichai’s post says. “These collaborations are important and we’ll actively look for more ways to augment the critical work of these organizations and keep service members and civilians safe.”Trending NowFear Not the Robot SingularityThat could be read as allowing continued work on Project Maven. But Google told employees last week that it would not renew the Maven contract when it expires next year. And a spokesperson said Thursday that if Maven came up again today the company likely wouldn’t participate, because the project doesn’t follow the spirit of the new guidelines.Although prompted by the protest over Maven, the guidelines posted Thursday address a much broader range of concerns. Google pledges to avoid creating systems that reinforce societal biases on gender, race, or sexual orientation, for example. They say privacy safeguards should be incorporated into AI technologies, which often gain their power from training on vast data sets like those Google holds from its billions of customers.“How AI is developed and used will have a significant impact on society for many years to come,” Pichai writes in an introduction to the guidelines. “As a leader in AI, we feel a special responsibility to get this right.” Google also released a set of “recommended practices” on topics such as fairness and testing AI systems to help other companies use and develop AI responsibly.AdvertisementThe logo for Project Maven.
Department of DefenseMost PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDProject Maven is a broad effort at the Department of Defense to inject more artificial intelligence into operations. The first project involved using AI to track objects such as buildings, vehicles, and people in drone video. Google’s precise role in the project is not clear, but the company has maintained that it was limited to “non-offensive purposes.”Google’s new guidelines build on that line, saying it won’t apply AI to weapons or “other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.” The document also states that Google won’t work on surveillance technology “violating internationally accepted norms of human rights.”One Google employee reached Thursday before the guidelines were released said that any such rules would be hard to trust if only interpreted and enforced internally. External oversight would be needed to reduce the risk of business concerns skewing decision making, the person argued. Google is bidding for a multibillion-dollar Pentagon cloud computing contract called JEDI.Peter Eckersley, chief computer scientist at the Electronic Frontier Foundation, agrees that Google should get outside help. “If any tech company is going to wade into a morally complex area like AI defense contracting, we'd recommend they form an independent ethics board to help guide their work,” he says. “Google has a real opportunity to be a leader on AI ethics here, and they shouldn't waste it.”In a letter to Google cofounder Sergey Brin released Thursday, Faisal ali bin Jaber, who says his brother-in-law Salem was killed by a US drone strike in Yemen, offered himself as an external ethics adviser to Google on future defense work. “Google can protect people like Salem rather than making it easier to kill them,” he wrote. “Let us discuss how Google can set ethical standards in this area for other companies to follow, rather than be co-opted by government.”Enter your email to get the Wired newsletterclose dialogRecommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyPlease enter abovesign upUsed consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.Recommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyYou're signed up!Used consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.close dialogMore Great WIRED StoriesThe laser battle against blood-sucking parasites of the seaPHOTO ESSAY: The trailblazing women who fight California's firesIsrael's self-flying 'Cormorant' whisks wounded soldiers to safetyClimate change has made zombie ants even more cunningThe state of federal government cybersecurity is bleaker than you thinkHungry for even more deep-dives on your next favorite topic? Sign up for the Backchannel newsletter","{'@type': 'CreativeWork', 'name': 'WIRED'}",,"{'@type': 'WebPage', '@id': 'https://www.wired.com/story/google-sets-limits-on-its-use-of-ai-but-allows-defense-work/'}",,,,,"https://media.wired.com/photos/5b1962ea66dc162128d46462/1:1/w_1800,h_1800,c_limit/reaperdrone.jpg",,,,,,,"In response to employee unrest over a Pentagon contract, Google CEO Sundar Pichai offered rules for the company's use of artificial intelligence.",,,,,,,,"The ground rules are a response to more than 4,500 Googlers signing a letter protesting the company’s involvement in a Pentagon project called Maven that uses machine learning to interpret drone surveillance video.
The dissenting employees asked Google to swear off all military work. Pichai’s response? We hear you, but you can trust us to do this responsibly. “We will continue our work with governments and the military in many other areas,” Pichai’s post says. “These collaborations are important and we’ll actively look for more ways to augment the critical work of these organizations and keep service members and civilians safe.”
That could be read as allowing continued work on Project Maven. But Google told employees last week that it would not renew the Maven contract when it expires next year. And a spokesperson said Thursday that if Maven came up again today the company likely wouldn’t participate, because the project doesn’t follow the spirit of the new guidelines.
Although prompted by the protest over Maven, the guidelines posted Thursday address a much broader range of concerns. Google pledges to avoid creating systems that reinforce societal biases on gender, race, or sexual orientation, for example. They say privacy safeguards should be incorporated into AI technologies, which often gain their power from training on vast data sets like those Google holds from its billions of customers.
“How AI is developed and used will have a significant impact on society for many years to come,” Pichai writes in an introduction to the guidelines. “As a leader in AI, we feel a special responsibility to get this right.” Google also released a set of “recommended practices” on topics such as fairness and testing AI systems to help other companies use and develop AI responsibly.
Project Maven is a broad effort at the Department of Defense to inject more artificial intelligence into operations. The first project involved using AI to track objects such as buildings, vehicles, and people in drone video. Google’s precise role in the project is not clear, but the company has maintained that it was limited to “non-offensive purposes.”
Google’s new guidelines build on that line, saying it won’t apply AI to weapons or “other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.” The document also states that Google won’t work on surveillance technology “violating internationally accepted norms of human rights.”
One Google employee reached Thursday before the guidelines were released said that any such rules would be hard to trust if only interpreted and enforced internally. External oversight would be needed to reduce the risk of business concerns skewing decision making, the person argued. Google is bidding for a multibillion-dollar Pentagon cloud computing contract called JEDI.
Peter Eckersley, chief computer scientist at the Electronic Frontier Foundation, agrees that Google should get outside help. “If any tech company is going to wade into a morally complex area like AI defense contracting, we'd recommend they form an independent ethics board to help guide their work,” he says. “Google has a real opportunity to be a leader on AI ethics here, and they shouldn't waste it.”
In a letter to Google cofounder Sergey Brin released Thursday, Faisal ali bin Jaber, who says his brother-in-law Salem was killed by a US drone strike in Yemen, offered himself as an external ethics adviser to Google on future defense work. “Google can protect people like Salem rather than making it easier to kill them,” he wrote. “Let us discuss how Google can set ethical standards in this area for other companies to follow, rather than be co-opted by government.”
More Great WIRED Stories

The laser battle against blood-sucking parasites of the sea
PHOTO ESSAY: The trailblazing women who fight California's fires
Israel's self-flying 'Cormorant' whisks wounded soldiers to safety
Climate change has made zombie ants even more cunning
The state of federal government cybersecurity is bleaker than you think
Hungry for even more deep-dives on your next favorite topic? Sign up for the Backchannel newsletter",,
https://news.google.com/rss/articles/CBMiS2h0dHBzOi8vd3d3LmZhc3Rjb21wYW55LmNvbS80MDU4MDMzMC9maXZlLXdheXMtYWktd2lsbC1tYWtlLXlvdXItam9iLWVhc2llctIBAA?oc=5,Five ways AI will make your job easier - Fast Company,2018-06-07,Fast Company,https://www.fastcompany.com,N/A,N/A,N/A,N/A,,,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibWh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDE4LzYvNy8xNzQzOTMxMC9nb29nbGUtYWktZXRoaWNzLXByaW5jaXBsZXMtd2FyZmFyZS13ZWFwb25zLW1pbGl0YXJ5LXByb2plY3QtbWF2ZW7SAQA?oc=5,"Google pledges not to develop AI weapons, but says it will still work with the military - The Verge",2018-06-07,The Verge,https://www.theverge.com,"Google has released a set of principles to guide its work in artificial intelligence, making good on a promise to do so last month following a months-long controversy over its involvement in a Department of Defense drone project. Notably, Google CEO Sundar Pichai says his company will never develop AI weapons, but he does not rule out working with the military in the future.",N/A,Google is laying out an ethical roadmap for AI,N/A,http://schema.org/,NewsArticle,https://www.theverge.com/2018/6/7/17439310/google-ai-ethics-principles-warfare-weapons-military-project-maven,"[{'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/3K_ac0OUCbLS7tmg3etQSxS2w8c=/0x0:2040x1360/1400x788/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/10125269/acastro_180130_1777_0003.jpg', 'width': 1400, 'height': 788}, {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/6R_UfyfGihkJlFtx95GVMU1Gn5Y=/0x0:2040x1360/1400x1050/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/10125269/acastro_180130_1777_0003.jpg', 'width': 1400, 'height': 1050}, {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/XKd-SzDOYDIoHv6oGOha_SG2Wlo=/0x0:2040x1360/1400x1400/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/10125269/acastro_180130_1777_0003.jpg', 'width': 1400, 'height': 1400}]","[{'@type': 'Person', 'name': 'Nick Statt', 'url': 'https://www.theverge.com/authors/nick-statt'}, {'@type': 'Person', 'name': 'James Vincent', 'url': 'https://www.theverge.com/authors/james-vincent'}]","{'@type': 'Organization', 'name': 'The Verge', 'logo': {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/uploads/chorus_asset/file/24015294/verge_duet_google_news.png', 'width': 250, 'height': 50}}","Google pledges not to develop AI weapons, but says it will still work with the military",2018-06-07T19:31:44.000Z,2018-06-07T19:31:44.000Z,,,,,N/A,N/A,"Tech/Google/Artificial IntelligenceGoogle pledges not to develop AI weapons, but says it will still work with the militaryGoogle pledges not to develop AI weapons, but says it will still work with the military / The company has released much-needed guidelines on its approach on AI researchBy  Nick Statt and  James Vincent Jun 7, 2018, 3:31 PM EDTShare this story0 Comments / 0 New Illustration by Alex Castro / The VergeGoogle has released a set of principles to guide its work in artificial intelligence, making good on a promise to do so last month following controversy over its involvement in a Department of Defense drone project. The document, titled “Artificial Intelligence at Google: our principles,” does not directly reference this work, but makes clear that the company will not develop AI for use in weaponry. It also outlines a number of broad guidelines for AI, touching issues like bias, privacy, and human oversight. While the new principles forbid the development of AI weaponry, they state that Google will continue to work with the military “in many other areas.” Speaking to The Verge, a Google representative said that had these principles been published earlier, the company would likely not have become involved in the Pentagon’s drone project, which used AI to analyze surveillance footage. Although this application was for “non-offensive purposes,” and therefore hypothetically permitted under these guidelines, the representative said it was too close for comfort — suggesting Google will play it safe with future military contracts. As well as forbidding the development of AI for weapons, the principles say Google will not work on AI surveillance projects that violate “internationally accepted norms,” or projects which contravene “widely accepted principles of international law and human rights.” The company says that its main focuses for AI research are to be “socially beneficial.” This means avoiding unfair bias; remaining accountable to humans and subject to human control; upholding “high standards of scientific excellence,” and incorporating privacy safeguards. “At Google, we use AI to make products more useful—from email that’s spam-free and easier to compose, to a digital assistant you can speak to naturally, to photos that pop the fun stuff out for you to enjoy,” Google CEO Sundar Pichai wrote in an accompanying blog post. “We recognize that such powerful technology raises equally powerful questions about its use. How AI is developed and used will have a significant impact on society for many years to come. As a leader in AI, we feel a deep responsibility to get this right.”Google has faced significant scrutiny over its use of AI after its work for the Department of Defense was revealed in a report by Gizmodo earlier this year. Thousands of employees signed an open letter urging Google to cut ties with the program, named Project Maven, and at least a dozen or so employees even resigned over the company’s continued involvement. Google says it plans to honor its contract with the Pentagon, but will end its involvement with Project Maven when that expires in 2019. A blog post by Google Cloud CEO Diane Greene described the work as simply “low-res object identification using AI.” However, it was reported that the work was, in part, a try out for Google to win a lucrative contract with the Pentagon estimated to be worth $10 billion. IBM, Microsoft, and Amazon are all thought to be competing, and a Google representative confirmed to The Verge that it would continue to pursue parts of the contract — if the work in question fit these new principles. Google’s decision to outline its ethical stance on AI development comes after years of worry over the impending threat posed by automated systems, as well as more sensational warnings about the development of artificial general intelligence — or AI with human-level intelligence. Just last month, a coalition of human rights and technology groups came together to put out a document titled The Toronto Declaration that calls for governments and tech companies to ensure AI respects basic principles of equality and nondiscrimination.Over the years, criticism and commentary regarding AI development has come from a wide-ranging group, from pessimists on the subject like Tesla and SpaceX founder Elon Musk to more reasonable voices in the industry like Facebook scientist Yann LeCun. Now, Silicon Valley companies are beginning to put more significant resources toward AI safety research, with help from ethics-focused organizations like the nonprofit Open AI and other research groups around the world.However, as Google’s new ethical principles demonstrate, it’s difficult to make rules that are broad enough to encompass a wide range of scenarios, but flexible enough to not exclude potentially useful work. As ever, public scrutiny and debate are necessary to ensure that AI is deployed fairly and in a socially beneficial manner. Google will have to get used to talking about it.  	Update June 7th, 5:00PM ET: Updated with additional comment from Google. Comments0 Comments / 0 NewFeatured Videos From The VergeThe next next thing in AI and AR | The Vergecast
1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGEThe Verge's Nilay Patel, Alex Cranz, and Alex Heath discuss Apple's Vision Pro team reportedly refocusing on a cheaper headset, Meta launching a new ""Wearables"" organization, a new AI company startup from former OpenAI chief scientist, and a whole lot more tech news.Most PopularMost PopularGoogle is reportedly planning its biggest startup acquisition everAT&T reportedly gave $370,000 to a hacker to delete its stolen customer dataHere’s how much Valve pays its staff — and how few people it employsAfter initially rejecting it, Apple has approved the first PC emulator for iOSA supercheap Android phone with looks to spareVerge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly.Email (required)Sign upBy submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.From our sponsorAdvertiser Content From",,,,,,,,https://cdn.vox-cdn.com/thumbor/3K_ac0OUCbLS7tmg3etQSxS2w8c=/0x0:2040x1360/1400x788/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/10125269/acastro_180130_1777_0003.jpg,,,,,,,,,,,,,,,"Google has released a set of principles to guide its work in artificial intelligence, making good on a promise to do so last month following controversy over its involvement in a Department of Defense drone project. The document, titled “Artificial Intelligence at Google: our principles,” does not directly reference this work, but makes clear that the company will not develop AI for use in weaponry. It also outlines a number of broad guidelines for AI, touching issues like bias, privacy, and human oversight. 

While the new principles forbid the development of AI weaponry, they state that Google will continue to work with the military “in many other areas.” Speaking to The Verge, a Google representative said that had these principles been published earlier, the company would likely not have become involved in the Pentagon’s drone project, which used AI to analyze surveillance footage. Although this application was for “non-offensive purposes,” and therefore hypothetically permitted under these guidelines, the representative said it was too close for comfort — suggesting Google will play it safe with future military contracts. 

As well as forbidding the development of AI for weapons, the principles say Google will not work on AI surveillance projects that violate “internationally accepted norms,” or projects which contravene “widely accepted principles of international law and human rights.” The company says that its main focuses for AI research are to be “socially beneficial.” This means avoiding unfair bias; remaining accountable to humans and subject to human control; upholding “high standards of scientific excellence,” and incorporating privacy safeguards. 

“At Google, we use AI to make products more useful—from email that’s spam-free and easier to compose, to a digital assistant you can speak to naturally, to photos that pop the fun stuff out for you to enjoy,” Google CEO Sundar Pichai wrote in an accompanying blog post. “We recognize that such powerful technology raises equally powerful questions about its use. How AI is developed and used will have a significant impact on society for many years to come. As a leader in AI, we feel a deep responsibility to get this right.”

[Media: https://twitter.com/sundarpichai/status/1004800469405876226?s=21]

Google has faced significant scrutiny over its use of AI after its work for the Department of Defense was revealed in a report by Gizmodo earlier this year. Thousands of employees signed an open letter urging Google to cut ties with the program, named Project Maven, and at least a dozen or so employees even resigned over the company’s continued involvement. 

Google says it plans to honor its contract with the Pentagon, but will end its involvement with Project Maven when that expires in 2019. A blog post by Google Cloud CEO Diane Greene described the work as simply “low-res object identification using AI.” However, it was reported that the work was, in part, a try out for Google to win a lucrative contract with the Pentagon estimated to be worth $10 billion. IBM, Microsoft, and Amazon are all thought to be competing, and a Google representative confirmed to The Verge that it would continue to pursue parts of the contract — if the work in question fit these new principles. 

Google’s decision to outline its ethical stance on AI development comes after years of worry over the impending threat posed by automated systems, as well as more sensational warnings about the development of artificial general intelligence — or AI with human-level intelligence. Just last month, a coalition of human rights and technology groups came together to put out a document titled The Toronto Declaration that calls for governments and tech companies to ensure AI respects basic principles of equality and nondiscrimination.

Over the years, criticism and commentary regarding AI development has come from a wide-ranging group, from pessimists on the subject like Tesla and SpaceX founder Elon Musk to more reasonable voices in the industry like Facebook scientist Yann LeCun. Now, Silicon Valley companies are beginning to put more significant resources toward AI safety research, with help from ethics-focused organizations like the nonprofit Open AI and other research groups around the world.

However, as Google’s new ethical principles demonstrate, it’s difficult to make rules that are broad enough to encompass a wide range of scenarios, but flexible enough to not exclude potentially useful work. As ever, public scrutiny and debate are necessary to ensure that AI is deployed fairly and in a socially beneficial manner. Google will have to get used to talking about it.  	

Update June 7th, 5:00PM ET: Updated with additional comment from Google. 
",,
https://news.google.com/rss/articles/CBMigQFodHRwczovL3d3dy53YXNoaW5ndG9ucG9zdC5jb20vbmV3cy90aGUtc3dpdGNoL3dwLzIwMTgvMDYvMDcvZ29vZ2xlLWJhbnMtZGV2ZWxvcG1lbnQtb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtdXNlZC1pbi13ZWFwb25yeS_SAQA?oc=5,Google bans development of artificial intelligence used in weaponry - The Washington Post,2018-06-07,The Washington Post,https://www.washingtonpost.com,"The new rules could set the tone for the deployment of AI far beyond Google, as rivals in Silicon Valley and around the world compete for supremacy in self-driving cars, automated assistants, robotics, military AI and other industries.","Google, Sundar Pichai, AI, artificial intelligence, Department of Defense, Pentagon, Project Maven, Alphabet, ethics","The new rules could set the tone for the deployment of AI far beyond Google, as rivals in Silicon Valley and around the world compete for supremacy in self-driving cars, automated assistants, robotics, military AI and other industries.","The new rules could set the tone for the deployment of AI far beyond Google, as rivals in Silicon Valley and around the world compete for supremacy in self-driving cars, automated assistants, robotics, military AI and other industries.",https://schema.org,BreadcrumbList,,"[{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://www.washingtonpost.com/wp-apps/imrs.php?src=http://www.washingtonpost.com/blogs/the-switch/files/2018/06/ZQLHOCOH.jpg&w=1600&h=900', 'height': 900, 'width': 1600}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://www.washingtonpost.com/wp-apps/imrs.php?src=http://www.washingtonpost.com/blogs/the-switch/files/2018/06/ZQLHOCOH.jpg&w=1800&h=1800', 'height': 1800, 'width': 1800}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://www.washingtonpost.com/wp-apps/imrs.php?src=http://www.washingtonpost.com/blogs/the-switch/files/2018/06/ZQLHOCOH.jpg&w=800&h=600', 'height': 800, 'width': 600}]","{'@type': 'Person', 'name': 'Drew Harwell', 'url': 'https://www.washingtonpost.com/people/drew-harwell/'}","{'@id': 'washingtonpost.com', '@type': 'NewsMediaOrganization', 'logo': {'@type': 'ImageObject', 'url': 'https://www.washingtonpost.com/wp-stat/img/wplogo_344x60_blk.png', 'width': {'@type': 'Distance', 'name': '344 px'}, 'height': {'@type': 'Distance', 'name': '60 px'}}, 'name': 'The Washington Post'}",Google bans development of artificial intelligence used in weaponry,2018-06-07T19:00:11.000Z,2021-12-05T20:21:36.459Z,,,False,"[{'@context': 'https://schema.org', '@type': 'ListItem', 'name': 'Technology', 'position': 1, 'item': 'https://www.washingtonpost.com/technology/'}, {'@context': 'https://schema.org', '@type': 'ListItem', 'name': 'Tech Policy', 'position': 2, 'item': 'https://www.washingtonpost.com/technology/tech-policy/'}]",Tech Policy,N/A,"Google chief Sundar Pichai speaks during the Google I/O developers conference in Mountain View, Calif., last month. (David Paul Morris/Bloomberg News)ShareAdd to your saved storiesSaveGoogle is banning the development of artificial-intelligence software that can be used in weapons, chief executive Sundar Pichai said Thursday, setting strict new ethical guidelines for how the tech giant should conduct business in an age of increasingly powerful AI.Subscribe for unlimited access to The PostYou can cancel anytime.SubscribeThe new rules could set the tone for the deployment of AI far beyond Google, as rivals in Silicon Valley and around the world compete for supremacy in self-driving cars, automated assistants, robotics, military AI and other industries.“We recognize that such powerful technology raises equally powerful questions about its use,” Pichai wrote in a blog post. “As a leader in AI, we feel a special responsibility to get this right.”Story continues below advertisementThe ethical principles are a response to a firestorm of employee resignations and public criticism over a Google contract with the Defense Department for software that could help analyze drone video, which critics argued had nudged the company one step closer to the “business of war.” Google executives said last week that they would not renew the deal for the military’s AI endeavor, known as Project Maven, when it expires next year.Google to drop Pentagon AI contract after employee objections to the ‘business of war’Google, Pichai said, will not pursue the development of AI when it could be used to break international law, cause overall harm or surveil people in violation of “internationally accepted norms of human rights.”AdvertisementThe company will, however, continue to work with governments and the military in cybersecurity, training, veterans health care, search and rescue, and military recruitment, Pichai said. The Web giant — famous for its past “Don’t be evil” mantra — is in the running for two multibillion-dollar Defense Department contracts for office and cloud services.Story continues below advertisementGoogle’s $800 billion parent company, Alphabet, is considered one of the world’s leading authorities on AI and employs some of the field’s top talent, including at its London-based subsidiary DeepMind.💻Follow TechnologyFollowBut the company is steeped in a fierce competition for researchers, engineers and technologies with Chinese AI firms and domestic competitors, such as Facebook and Amazon, who could contend for the kinds of lucrative contracts Google says it will give up.Google’s ties to Chinese telecom firm Huawei raise alarm in CongressThe principles offer limited detail into how the company would seek to follow its rules. But Pichai outlined seven core tenets for its AI applications, including that they be socially beneficial, be built and tested for safety, and avoid creating or reinforcing unfair bias. The company, Pichai said, would also evaluate its work in AI by examining how closely its technology could be “adaptable to a harmful use.”AdvertisementStory continues below advertisementA Google representative who requested anonymity to speak candidly about the process said the company has been developing the ethical principles for months and will bring in outside advisors to conduct internal reviews to ensure the AI guidelines are enforced.GET CAUGHT UPStories to keep you informedPreviousNextDissenting Republican delegates sign protest of Trump platform At issue is the treatment of abortion in the new document.The “minority report” criticizes the 2024 platform’s lack of a “human life amendment.”
The platform focused less on abortion and dropped the call for a 20-week abortion ban.
SparkleSummary is AI-generated, newsroom-reviewed.See moreExpand contentU.S., Germany foiled Russian plot to assassinate CEO of arms manufacturer, officials sayRheinmetall is a major German arms manufacturer and a key supplier for Ukraine.Russia has ramped up efforts to undermine Western support for Ukraine.NATO and the U.S. have intensified their focus on thwarting Russian subversion efforts.
SparkleSummary is AI-generated, newsroom-reviewed.See moreExpand contentFamily of teen who died after ‘One Chip Challenge’ sues snack companyThe lawsuit accuses Paqui of aggressively marketing the extremely spicy chip to children.An autopsy said the cause of death was cardiac arrest and also cited heart conditions.
The company discontinued the product and expressed condolences to the family.SparkleSummary is AI-generated, newsroom-reviewed.See moreExpand contentLa Niña is coming. Here’s how it could change the weather.The pattern could have a cooling effect on global heat.It’s also likely to increase Atlantic hurricane activity this fall.
But there’s uncertainty over its impact amid a period of record temperatures.SparkleSummary is AI-generated, newsroom-reviewed.See moreExpand contentDo landlords have to provide AC? Here’s what renters should know.Air conditioning rights for renters vary by lease and location. 
Some states require AC, but most don’t include it as an essential service.Repair timelines are often vague, depending on the state. Filing a written complaints is best.SparkleSummary is AI-generated, newsroom-reviewed.See moreExpand contentAI is a critical piece of Google’s namesake Web tools, including in image search and recognition, and automatic language translation. But it is also key to its future ambitions, many of which involve ethical minefields of their own, including its self-driving Waymo division and Google Duplex, a system that can be used to make dinner reservations by mimicking a human’s voice over the phone.But Google’s new limits appear to have done little to slow the Pentagon’s technological researchers and engineers, who say other contractors will still compete to help develop technologies for the military and national defense. Peter Highnam, the deputy director of the Defense Advanced Research Projects Agency, the Pentagon agency that did not handle Project Maven but is credited with helping invent the Internet, said there are “hundreds if not thousands of schools and companies that bid aggressively” on DARPA’s research programs in technologies such as AI.“Our goal, our objective, is to create and prevent technological surprise. So we’re looking at what’s possible,”  John Everett, a deputy director of DARPA’s Information Innovation Office, said in an interview Wednesday. “Any organization is free to participate in this ongoing exploration or not.”Tony Romm contributed to this report.ShareNewsletterAs news breaksTech News AlertsBreaking news email alerts on technology and the tech industry.Sign up","{'@type': ['CreativeWork', 'Product'], 'name': 'The Washington Post', 'productID': 'washingtonpost.com:basic', 'description': 'Breaking news and analysis on politics, business, world, national news, entertainment and more. In-depth DC, Virginia, Maryland news coverage including traffic, weather, crime, education, restaurant reviews and more.', 'sku': 'https://subscribe.washingtonpost.com', 'image': 'https://www.washingtonpost.com/resizer/2CjPNwqvXHPS_2RpuRTKY-p3eVo=/1484x0/www.washingtonpost.com/pb/resources/img/twp-social-share.png', 'brand': {'@type': 'brand', 'name': 'The Washington Post'}, 'offers': {'@type': 'offer', 'url': 'https://subscribe.washingtonpost.com/acquisition?promo=o26'}}",,https://www.washingtonpost.com/news/the-switch/wp/2018/06/07/google-bans-development-of-artificial-intelligence-used-in-weaponry/,,,,,,,,,,,,Google bans development of artificial intelligence used in weaponry,"{'@type': 'WebPageElement', 'cssSelector': '.meteredContent', 'isAccessibleForFree': False}",,,,,,,,,
https://news.google.com/rss/articles/CBMiR2h0dHBzOi8vdGhlbmV4dHdlYi5jb20vbmV3cy9nb29nbGVzLWFpLXByaW5jaXBsZXMtNy1zaGFkZXMtb2YtZ3JheS1hcmVh0gEA?oc=5,Google's principles for developing AI aren't good enough - TNW,2018-06-08,TNW,https://thenextweb.com,Google recently revealed seven principles for developing AI that don't address the ethical concerns that led to the company's recent scathing by pundits.,N/A,Google recently revealed seven principles for developing AI that don't address the ethical concerns that led to the company's recent scathing by pundits.,Google recently revealed seven principles for developing AI that don't address the ethical concerns that led to the company's recent scathing by pundits.,http://schema.org,BreadcrumbList,,,,,,,,,,,"[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://thenextweb.com', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://thenextweb.com/artificial-intelligence', 'name': 'Artificial-intelligence'}}, {'@type': 'ListItem', 'position': 3, 'item': {'@id': 'https://thenextweb.com/news/googles-ai-principles-7-shades-of-gray-area', 'name': 'Google&#8217;s principles for developing AI aren&#8217;t good enough'}}]",Artificial Intelligence,N/A,"











Google CEO Sundar Pichai yesterday published his company’s new rules governing the development of AI. Over the course of seven principles he lays out a broad (and useless) policy leaving more wiggle room than a pair of clown pants.
If you tell the story of Google’s involvement in building AI for the US military backwards it makes perfect sense. In such a case, the tale would begin with the Mountain View company creating a policy for developing AI, and then it would use those principles to guide its actions.
Unfortunately the reality is the company has been developing AI for as long as it’s been around. It’s hard to gloss over the fact that only now, after the company’s ethics are being called into question over a military contract, is the CEO concerned about having these guidelines.
Of course, this isn’t to suggest that it’s a company that’s been developing AI technology with no oversight. In fact it’s clear that Google engineers, researchers, and scientists are among the world’s finest and many of those employees are of the highest ethical character. But at the company level, it feels like the lawyers are running the show.
No, my point is to suggest that Pichai’s blog post is nothing more than thinly-veiled trifle aimed at technology journalists and other pundits in hopes we’ll fawn over the declarative statements like “Google won’t make weapons.” Unfortunately there’s no substance to any of it.The The latest rumblings from the EU tech scene, a story from our wise ol' founder Boris, and some questionable AI art. It's free, every week, in your inbox. Sign up now!Your email address*I agree to TNW storing and processing my personal data to receive the requested newsletter(s). For more information check out TNW's Privacy Policy.*
It starts with the first principle of Google’s new AI policy: be socially beneficial. This part lays out lip service saying it will strive to develop AI that benefits society, but doesn’t discuss what that means or how it’ll accomplish such an abstract principle.
Oddly, the final sentence under principle one is “And we will continue to thoughtfully evaluate when to make our technologies available on a non-commercial basis.” That’s just a word salad with no more depth than saying “Google is a business that will keep doing business stuff.”
Instead of “be socially beneficial,” I would have much preferred to see something more like “refuse to develop AI for any entity that doesn’t have a clear set of ethical guidelines for its use.”
Unfortunately, as leaked emails show, Google’s higher-ups were more concerned with government certifications than ethical considerations when they entered into a contract with the US government – an entity with no formal ethical guidelines on the use of AI.
In appearance, each of the seven principles laid out by Pichai are general bullet points that read like cover-your-own-ass statements. And, each corresponds with a very legitimate concern that the company seems to be avoiding discussing. After the aforementioned first principle, it just gets more vapid:

“Avoid creating or reinforcing unfair bias.” This, instead of a commitment to developing methods to fight bias.
“Be built and tested for safety.” Pichai says “We will continue to develop and apply strong safety and security practices to avoid unintended results that create risks of harm.” It’s interesting that Pichai’s people don’t seem to think there’s any risk of unintended consequences for teaching the military how to develop image processing AI for drones.
“Be accountable to people.” Rather than “develop AI with transparency,” which would be great, this just says Google will ultimately hold a human responsible for creating its AI.
“Incorporate privacy design principles.” Apple just unveiled technology designed to keep big data companies from gathering your data. Google just said it cares about privacy. Actions speak louder than words.
“Uphold high standards of scientific excellence.” Google’s research happens inside of an internal scientific echo chamber. Numbers 4, 5, and 6 should be replaced with “be transparent.”
“Be made available for uses that accord with these principles.” In this same document Pichai points out that Google makes a large amount of its work in AI available as open-source code. It’s easy to say you’ll only develop AI with the best of intentions and use it for only good, as long as you take no responsibility for how it’s used once your company’s geniuses finish inventing it.

Pichai’s post on Google’s AI principles serve little more purpose than to, perhaps, eventually end up as a hyperlinked reference in a future apology.
If Google wants to fix its recently-tarnished reputation, it should take the issue of developing AI serious enough to come up with a realistic set of principles to guide future development– one that addresses the ethical concerns head on. It’s current attempt is nothing more than seven shades of gray area, and that doesn’t help anyone.











                                        Story by
                                    


                                            Tristan Greene
                                        



                                        Editor, Neural by TNW




                                            Tristan is a futurist covering human-centric artificial intelligence advances, quantum computing, STEM, physics, and space stuff. Pronouns: 

                                                                                            
                                                    (show all)
                                                


                                            Tristan is a futurist covering human-centric artificial intelligence advances, quantum computing, STEM, physics, and space stuff. Pronouns: He/him
                                        




Get the TNW newsletter
Get the most important tech news in your inbox each week.
Your email address*I agree to TNW storing and processing my personal data to receive the requested newsletter(s). For more information check out TNW's Privacy Policy.*

Also tagged with



Google




                                Published June 8, 2018 - 8:49 pm UTC

Back to top





























Story by
Tristan Greene




























Jobs For You BY Amply




 
Vermögensberater Bankwesen (w/m/d) Augsburg Teilzeit/Vollzeit
Targobank AG
Augsburg
 



 
Software Engineer Kotlin (all genders)
adesso SE
Berlin
 



 
IT-Administrator Netzwerk- und Security-Services (w/m/d)
Dataport AöR
Hamburg
 



 
Chauffeur koerier rijbewijs B
BOFROST
Aarschot
€2.400 per maand
 



 
SAP Solution Architect (m/w/d)
DPD Deutschland GmbH
Germany
 


Search More Roles






",,,,,,,,,,,,,,,,,,#Breadcrumb,,,,,,,
https://news.google.com/rss/articles/CBMiaGh0dHBzOi8vbWVkaXVtLmRhdGFkcml2ZW5pbnZlc3Rvci5jb20vYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtdGhlLXRyYW5zbGF0b3JzLW5ldy1jby13b3JrZXItNGRhODY3MzljZjdm0gEA?oc=5,Artificial Intelligence: The Translator’s New Co-worker | by Emma Wynne - DataDrivenInvestor,2018-06-08,DataDrivenInvestor,https://medium.datadriveninvestor.com,"Over the past decade, developments in artificial intelligence (AI), machine learning and deep learning have revolutionised the translation industry, with tech giants, Facebook, Google and Microsoft…",N/A,"Although many working professionals are worried about the development of superintelligence, AI is set to benefit translators greatly.","Although many working professionals are worried about the development of superintelligence, AI is set to benefit translators greatly.",http://schema.org,NewsArticle,https://medium.datadriveninvestor.com/artificial-intelligence-the-translators-new-co-worker-4da86739cf7f,['https://miro.medium.com/v2/resize:fit:1200/1*ifwq4skDU7bXkhkp76HdOg.jpeg'],"{'@type': 'Person', 'name': 'Emma Wynne', 'url': 'https://medium.datadriveninvestor.com/@EmmaWynne123'}","{'@type': 'Organization', 'name': 'DataDrivenInvestor', 'url': 'medium.datadriveninvestor.com', 'logo': {'@type': 'ImageObject', 'width': 308, 'height': 60, 'url': 'https://miro.medium.com/v2/resize:fit:616/1*OMF3fSqH8t4xBJ9-6oZDZw.png'}}",Artificial Intelligence: The Translator’s New Co-worker,2018-06-08T12:33:02.266Z,2018-06-15T19:03:31.121Z,,Artificial Intelligence: The Translator’s New Co-worker,,,N/A,N/A,"Artificial Intelligence: The Translator’s New Co-workerAlthough many working professionals are worried about the development of superintelligence, AI is set to benefit translators greatly.Emma Wynne·FollowPublished inDataDrivenInvestor·6 min read·Jun 8, 20189ListenShareOver the past decade, developments in artificial intelligence (AI), machine learning and deep learning have revolutionised the translation industry, with tech giants, Facebook, Google and Microsoft, leading the way.Nowadays, translators have access to a multitude of tools — many of which are powered by AI. Facebook’s translations are now generated exclusively by AI, with approximately 4.5 billion automatic translations being made on the social network every day, Microsoft has created a universal translator, which uses AI to translate face-to-face conversations in real time, and Google Translate has generated its own neural network, allowing it to take into account entire sentences and phrases, rather than individual words — thus providing more accurate translations.Although some translators are worried about advancements in AI, others are delighted. Improvements in AI will allow translators to translate documents faster and more efficiently than before, giving them the opportunity to make more money from their craft by increasing productivity levels.Machine translation in an AI eraSince IBM exhibited the first demonstration of machine translation (MT) in 1954, MT technology has improved immensely, but it isn’t quite there yet. You’ve probably noticed that, although MT is a wonderful tool, it still has some flaws. Luckily, the exponential rate in which AI is developing is set to vastly improve the performance of MT systems.The growing popularity in MT use is evident, but it does not exclude the need for highly-skilled human translators. Improvements in MT have exacerbated the need for accurate post-editing, as the volume of texts to be translated is increasing as we witness continuing progression in MT. Accurate post-editing is especially important where publishable quality is sought, and the changing nature in the translation profession has meant that translators are becoming “revisers” as opposed to text producers.The usage of correct terminology in MT is especially important. In fact, the majority of expert discussions on using MT systems for better performance discuss the understanding that greater output begins with translation work which concentrates on ensuring terminological accuracy and reliability. For accurate results, MT systems have to be able to use resources which contain up-to-date terminology.MT users can do a lot to optimize MT quality, using term extraction methods, which can help to develop mass from scratch without prior databases. After a terminology database has been created, it should be used in conjunction with authoring and Computer-Assisted Translation (CAT) tools to generate high-quality texts with consistent terminology. Another way of improving MT translations is by continually feeding back post-edits of MT translations into the MT system.Source: Boston Consulting Group, Time to Double Down on AI and Robotics (2017)Google’s remarkable breakthrough in machine translationGoogle spent the best part of 2016, developing its translation app to be solely powered by AI, with remarkable results. Google Translate, which was once known for producing wooden but passable translations, is now able to generate highly accurate prose, which, to the untrained eye, could be mistaken for the work of a human translator. The New York Times has hailed this, “The Great A.I Awakening”.Source: The New York Times, The Great A.I Awakening (2016)Google’s old system was powered using a narrow form of AI, which adhered to this set of rules:· The app breaks up the sentence into sections· It translates the sections using a dictionary of statistically-derived terms· It then uses a set of rules to rearrange the translates terms into a sentence in the target languageWhile these rules make it possible to provide simple translations, they don’t take into account the complexity of language. For example, the same word being used in different contexts. The tech geniuses at Google realised this, and created a new system which has rapidly changed MT.Google now uses a Neural Machine Translation System (NMTS). This system is able to work with a multitude of languages, while learning from the language examples that it experiences. This is done using deep learning, an element of AI which involves building artificial neural networks, which are designed to mimic the way living brains sort and process information.Deep learning has transformed the machine translation landscape, by allowing Google Translate to learn from the languages it uses. For example, if the machine can translate between English and Italian, and English and French, it would be able to translate between French and Italian without being programmed to do so.Furthering its ambitious AI agenda, at its Pixel 2 launch, Google claimed that it was creating wireless headphones that will be able to translate 40 languages in real-time.Say hello to your new co-workerAlthough many professionals view AI with a large amount of scepticism, the threat of superintelligence completely replacing humans in the translation industry is virtually non-existent. No matter how complex the code is, machines simply cannot understand cultural differences or the feeling behind language.Professor of Artificial Intelligence at the University of Bristol, Nello Christiani, says, “Both neural translation and statistical translation are based on the same idea: by training a learning algorithm on a vast bilingual corpus, we can teach computers to translate new text.”Adding, “The performance is increasing, and for simple factual texts it is possible that machines will make some translation jobs redundant, such as instruction manuals for machinery. But in the areas where tone and mood need to be conveyed, when cultural references need to be kept in consideration, even cultural sensitivities, as well as artistic aspects such as the sound of a text and voice of an author, there is currently no replacement for a good human.”In contrast to the scepticism surrounding AI, technological developments have already given interpreters and translators unprecedented liberties, and translation remains listed as one of the top career paths.Piero Toto, a Senior Lecturer in Languages (Translation) at London Metropolitan University, said: “As both an academic and a practitioner, I find that technological advancements in the translation industry are occurring at such a rate that — while indeed fascinating — it has become more and more challenging to keep up.“I personally embrace the advent of AI in the translation market and in my own workflow. I can see how it’s partially shifting the role of translators and creating the need for new skills which will need to be acquired by or taught to (old and new) translators, but it’s also an opportunity for professional growth.“That’s why I advocate a closer relationship between academia and the industry to foster talents and address the needs of our ever-changing and fast-paced environment.”As AI continues to improve translation systems, translators can be optimistic about the opportunities that these developments will bring to the translation environment and to the profession in general.SourcesOng, T. (2017). The Verge. Facebook’s translations are now powered completely by AI. [online] Available at: https://www.theverge.com/2017/8/4/16093872/facebook-ai-translations-artificial-intelligence[Accessed 13 Mar. 2018].Nieva, R. (2018). CNET. Google Translate just got a lot smarter. [online] Available at: https://www.cnet.com/news/google-translate-machine-learning-neural-networks/ [Accessed 13 Mar. 2018].Forrest, C. (2018). Tech Republic. Microsoft’s universal translator uses AI to translate face-to-face conversations in real time. [online]Available at: Conner Forrest https://www.techrepublic.com/article/microsofts-universal-translator-uses-ai-to-translate-face-to-face-conversations-in-real-time/%20[Accessed 13 Mar. 2018].Pestov, I. (2018). Free Code Camp. A history of machine translation from the Cold War to deep learning. [online] Available at: https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5 [Accessed 13 Mar. 2018].Dredge, S. (2018). The Guardian. 10 things you need to know about translation technology. [online]Available at: https://www.theguardian.com/technology/2014/jul/16/10-things-know-about-translation-technolology-apps-smartphone [Accessed 13 Mar. 2018].CSB Tech Emporium. (2018). How Machine Translation is Transforming With AI Developments. [online] Available at: http://www.csbtechemporium.com/research/ai-transforming-machine-translation/ [Accessed 15 Mar. 2018].Eisold, C. (2018). Terminology Usage in Machine Translation. [online] Available at: http://kv-emptypages.blogspot.lu/2017/05/terminology-usage-in-machine-translation.html [Accessed 19 Mar. 2018].",,,https://medium.datadriveninvestor.com/artificial-intelligence-the-translators-new-co-worker-4da86739cf7f,2018-06-08T12:33:02.266Z,['Emma Wynne'],,,,,,,,,,,,,,,,,,,,4da86739cf7f
https://news.google.com/rss/articles/CBMiS2h0dHBzOi8vZnV0dXJpc20uY29tL2FpLWNhbi1ub3ctbWFuaXB1bGF0ZS1wZW9wbGVzLW1vdmVtZW50cy1pbi1mYWtlLXZpZGVvc9IBAA?oc=5,AI Can Now Manipulate People's Movements In Fake Videos - Futurism,2018-06-06,Futurism,https://futurism.com,"A new AI algorithm can create the most advanced deepfakes yet, complete with emotions and gestures, after just a few minutes of training.",N/A,The AI system can create the best manipulated video yet.,"A new AI algorithm can create the most advanced deepfakes yet, complete with emotions and gestures, after just a few minutes of training.",https://schema.org,NewsArticle,https://futurism.com/ai-can-now-manipulate-peoples-movements-in-fake-videos,https://wordpress-assets.futurism.com/2018/06/deep-fakes.png,"{'@type': 'Person', 'name': 'Dan Robitzski', 'url': 'https://futurism.com/authors/danrobitzski'}","{'@type': 'NewsMediaOrganization', '@id': 'https://futurism.com#organization', 'url': 'https://futurism.com', 'description': 'Science and Technology News and Videos', 'foundingDate': '2015-11-25', 'masthead': 'https://futurism.com/masthead', 'name': 'Futurism', 'logo': {'@type': 'ImageObject', 'height': '60', 'width': '191', 'url': 'https://futurism.com/schema/OrgFuturism.png'}}",AI Can Now Manipulate People’s Movements In Fake Videos,2018-06-06T16:52:37-04:00,2018-06-06T17:24:33-04:00,Artificial Intelligence,,,,N/A,N/A,"FuturismUpdated 6.6.18, 5:24 PM EDT by Dan Robitzski/Artificial IntelligenceAI Can Now Manipulate People’s Movements In Fake VideosWhy do we keep making tools to do this?/ Artificial Intelligence/ Ai/ Artificial Intelligence/ DeepfakesUpdated 6.6.18, 5:24 PM EDT by Dan RobitzskiImage by Christian Theobalt / Youtube / Emily ChoThere are already fake videos on the internet, manipulated to make it look like people said things (or appeared in porn) that they never did. And now they're about to get way better, thanks to some new tools powered by artificial intelligence.Instead of just moving a source video’s lips and face, an artificial intelligence-powered system can create photorealistic videos in which people can sway, turn their heads, blink their eyes, and emote. Basically, everything that an actor does and says in an input video will be translated into the video being altered. According to the research, which will be presented at the VR filmmaking conference SIGGRAPH in August, the team ran a number of tests comparing its new algorithm to existing means of manipulating lifelike videos and images, many of which have been at least partially developed by Facebook and Google. Their system outperformed all the others, and participants in an experiment struggled to determine whether or not the resulting videos were real.The researchers, who received some funding from Google, hope that their work will be used to improve virtual reality technology. And because the AI system only needs to train on a few minutes of source video to work, the team feels that its new tools will help make high-end video editing software more accessible.The researchers also know their work might, uh, worry some folks.“I’m aware of the ethical implications of those reenactment projects,"" researcher Justus Thies told The Register. ""That is also a reason why we published our results. I think it is important that the people get to know the possibilities of manipulation techniques.”But at what point do we get tired of people “raising awareness” by further developing the problem? In the paper itself, there is just one sentence dedicated to ethical concerns — the researchers suggest that someone ought to look into better watermarking technologies or other ways to spot fake videos.Not them, though. They’re too busy making it easier than ever to create flawless manipulated videos.Share This ArticleRead This NextPleading the FifthWashington Post Launches AI to Answer Climate Questions, But It Won't Say Whether AI Is Bad for the ClimatePress GenerateLeak Reveals the New York Times Experimented With Using AI to Write HeadlinesWhat in the BioshockIn Fresh Hell, American Vending Machines Are Selling Bullets Using Facial RecognitionFake LoveResearcher Studying Married Men With AI GirlfriendsVirus PanicChatGPT-4o Is Sending Users to a Scammy Website That Floods Your Screen With Fake Virus Warnings",,,"{'@type': 'WebPage', '@id': 'https://futurism.com/ai-can-now-manipulate-peoples-movements-in-fake-videos'}",,,"{'@type': 'NewsMediaOrganization', '@id': 'https://futurism.com#organization', 'url': 'https://futurism.com', 'description': 'Science and Technology News and Videos', 'foundingDate': '2015-11-25', 'masthead': 'https://futurism.com/masthead', 'name': 'Futurism', 'logo': {'@type': 'ImageObject', 'height': '60', 'width': '191', 'url': 'https://futurism.com/schema/OrgFuturism.png'}}",2018.0,https://wordpress-assets.futurism.com/2018/06/deep-fakes.png,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiUGh0dHBzOi8vd3d3LmVjb25vbWlzdC5jb20vbGVhZGVycy8yMDE4LzA2LzA3L2FpLXJhZGlvbG9neS1hbmQtdGhlLWZ1dHVyZS1vZi13b3Jr0gEA?oc=5,"AI, radiology and the future of work - The Economist",2018-06-07,The Economist,https://www.economist.com,Clever machines will make workers more productive more often than they will replace them,[],Clever machines will make workers more productive more often than they will replace them,Clever machines will make workers more productive more often than they will replace them,https://schema.org,BreadcrumbList,https://www.economist.com,https://www.economist.com/sites/default/files/images/print-edition/20180609_LDD002_0.jpg,"{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}, 'url': 'https://www.economist.com/'}","{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}","AI, radiology and the future of work",2018-06-07T14:45:22.000Z,,Leaders,The Economist,False,"[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://www.economist.com/leaders/', 'name': 'Leaders'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://www.economist.com/weeklyedition/2018-06-09', 'name': 'June 9th 2018 edition'}}]",N/A,N/A,"Leaders | Images aren’t everythingAI, radiology and the future of workClever machines will make workers more productive more often than they will replace them Jun 7th 2018ShareRADIOLOGISTS, say the pessimists, will be first against the wall when the machines take over. Analysing medical images is a natural fit for “deep learning”, an artificial-intelligence (AI) technique which first attracted attention for its ability to teach computers to recognise objects in pictures. A variety of companies hope that bringing AI into the clinic will make diagnosis faster and cheaper. The machines may even be able to see nuances that humans cannot, assessing how risky a patient’s cancer is simply by looking at a scan.Some AI researchers think that human beings can be dispensed with entirely. “It’s quite obvious that we should stop training radiologists,” said Geoffrey Hinton, an AI luminary, in 2016. In November Andrew Ng, another superstar researcher, when discussing AI’s ability to diagnose pneumonia from chest X-rays, wondered whether “radiologists should be worried about their jobs”. Given how widely applicable machine learning seems to be, such pronouncements are bound to alarm white-collar workers, from engineers to lawyers.Already have an account?Log inContinue with a free trialExplore all our independent journalism for free for one month. Cancel any timeFree trialOr continue reading this articleRegister nowLeaders June 9th 2018Donald Trump’s demolition theory of foreign policy won’t workAmerica’s allies should stand up to its reckless trade policyWhat Spain owes the ejected Mariano RajoyAI, radiology and the future of workFor all its faults, the World Cup in Russia is worth celebratingShareReuse this contentThe Economist todayHandpicked stories, in your inboxA daily newsletter with the best of our journalismSign upYes, I agree to receive exclusive content, offers and updates to products and services from The Economist Group. I can change these preferences at any time.More from LeadersFortunately, Donald Trump’s would-be killer failed. What next?Politicians should try to lower the political temperatureNow wake the braveFaddish thinking is hobbling education in the rich worldTest scores have been stagnant or worse for more than a decadeMore from LeadersFortunately, Donald Trump’s would-be killer failed. What next?Politicians should try to lower the political temperatureNow wake the braveFaddish thinking is hobbling education in the rich worldTest scores have been stagnant or worse for more than a decadeBritain’s skewed election reinforces the case for voting reform. After 2029The new government has more important things to deal with firstHow to prevent strongmen from hijacking the fight against dodgy moneyEgypt, India and Turkey are regular abusersHow to raise the world’s IQSimple ways to make the next generation more intelligent","{'@type': ['NewsArticle', 'Product'], 'name': 'The Economist', 'productID': 'economist.com:showcase'}",,https://www.economist.com/leaders/2018/06/07/ai-radiology-and-the-future-of-work,2018-06-07T14:45:22.000Z,"{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}","{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}",2018.0,https://www.economist.com/sites/default/files/images/print-edition/20180609_LDD002_0.jpg,en,Leaders,2018-06-09T00:00:00Z,1843,"{'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}","['https://www.facebook.com/theeconomist', 'https://www.instagram.com/theeconomist', 'https://www.twitter.com/theeconomist', 'https://www.linkedin.com/company/the-economist', 'https://www.youtube.com/user/economistmagazine', 'https://en.wikipedia.org/wiki/The_Economist']",,,,,,,,,,,
https://news.google.com/rss/articles/CBMidWh0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2RlbXlzdGlmeWluZy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1zdHJhdGVnaWVzLW9mLWNvdW50cmllc9IBf2h0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L2FtcC9zdG9yeS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS9kZW15c3RpZnlpbmctYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utc3RyYXRlZ2llcy1vZi1jb3VudHJpZXM?oc=5,Demystifying Artificial Intelligence Strategies of Countries - Analytics Insight,2018-06-09,Analytics Insight,https://www.analyticsinsight.net,,"Artificial Intelligence,AI,Strategy,Artificial Intelligence Strategy,Analytics Insight",Artificial intelligence (AI) has the potential to radically transform economies and societies for the better. While AI's impact is clearly top-of-mind for most ,Artificial intelligence (AI) has the potential to radically transform economies and societies for the better. While AI's impact is clearly top-of-mind for most ,http://schema.org,NewsArticle,https://www.analyticsinsight.net/artificial-intelligence/demystifying-artificial-intelligence-strategies-of-countries,"{'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2018/06/AI-World.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}","[{'@type': 'Person', 'givenName': 'Shweta Mayekar', 'name': 'Shweta Mayekar', 'url': 'https://www.analyticsinsight.net/author/shweta-mayekar'}]","{'@type': 'Organization', '@context': 'http://schema.org', 'name': 'Analytics Insight', 'url': 'https://www.analyticsinsight.net', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'author': 'analyticsinsight', 'contentUrl': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'url': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'name': 'logo', 'width': '', 'height': ''}, 'sameAs': ['https://www.linkedin.com/company/analytics-insight/', 'https://www.facebook.com/analyticsinsight.net', 'https://twitter.com/analyticsinme', 'https://www.youtube.com/channel/UCgF2J0b46YP0vvVEbgL_GuQ', 'https://www.instagram.com/analyticsinsightmagazine/', 'https://in.pinterest.com/analyticsinsightsubmissions/_created/'], 'id': 'https://www.analyticsinsight.net'}",Demystifying Artificial Intelligence Strategies of Countries,2018-06-09T01:50:38Z,2018-06-09T01:50:38Z,Artificial Intelligence,Demystifying Artificial Intelligence Strategies of Countries,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.analyticsinsight.net'}, {'@type': 'ListItem', 'position': 2, 'name': 'Artificial Intelligence', 'item': 'https://www.analyticsinsight.net/artificial-intelligence'}, {'@type': 'ListItem', 'position': 3, 'name': 'Demystifying Artificial Intelligence Strategies of Countries', 'item': 'https://www.analyticsinsight.net/artificial-intelligence/demystifying-artificial-intelligence-strategies-of-countries'}]",N/A,N/A,Best Real-Time Projects for Computer Science Students,"{'@type': 'WebPage', 'url': 'https://www.analyticsinsight.net/artificial-intelligence/demystifying-artificial-intelligence-strategies-of-countries', 'primaryImageOfPage': {'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2018/06/AI-World.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}}",,"{'@type': 'WebPage', '@id': 'https://www.analyticsinsight.net/artificial-intelligence/demystifying-artificial-intelligence-strategies-of-countries'}",2018-06-09T01:50:38Z,,,,https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2018/06/AI-World.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true,,,,,,,,,,,,,,,"Artificial intelligence (AI) has the potential to radically transform economies and societies for the better. While AI's impact is clearly top-of-mind for most organizations, governments and policymakers of different countries have also embarked on strategies to integrate AI across industries in order to stay ahead in the competitive race. Below is a global analysis of strategies adopted in different parts of the world with planned investments ahead..France.Earlier this year, Cedric Villani, a renowned and slightly unconventional French mathematician, presented a long paper on &quot;For a meaningful Artificial Intelligence&quot;. The paper touched upon various ideas on AI for humanity. French President Emmanuel Macron has shown a very acute interest towards France's AI strategy and how the disrupting technology should be embraced in the light of day..Villani, who is also a member of the French Parliament, believes that Europe has everything it needs to become a leading player in AI. Macron and Villani believe that it is not purely about economic policies but also about ethics &amp; how AI must not become a new way of excluding parts of the population. Ethics could be a core part of the European AI strategy and we can all see the measures set up by Macron unravel in the forthcoming months..President Macron also announced a $1.6 billion investment in AI including new research centers and data-sharing initiatives. When most people are concerned with the job layoffs due to the emergence of AI tech, Macron echoes a view held by many experts –  AI technology has the ability to boost overall economic productivity, wage growth and employment opportunities..China.When the world is finding ways to eliminate bias in AI tech, China is more focused to move as fast as possible. The Chinese government laid out a plan for global AI dominance in July 2017 and its goal is to lead the world in AI by 2030. With such statements in sight, no wonder other economies are bringing in their best players to this Global AI tech game..Baidu, China's dominant search engine, is betting its future on Artificial Intelligence in Asia and beyond. Robin Li, co-founder of Baidu, missed being on the forefront of the last tech revolution – a shift from browser to mobile. So Li does not want to repeat the same mistake. Baidu hopes to deliver AI tech that infuses everything &amp; every system – be it from medicine, entertainment or cars – with artificial intelligence..Tencent, another multinational internet-services company in China, aspires to add AI in all. YouTu Lab by Tencent is focused on machine learning and is credited with work in multiple technologies under audio, video and image analysis..United States.Speaking of Global AI strategy, how can we not talk about the US' multi-billion dollar hand in it. As per reports, US leads over every other country in dealing with AI hardware whereas China is ahead in mobile, data and building commercial AI companies..The Obama government was clear that AI should be a key focus of government strategy. But the Trump administration has renounced this vision – They believe that there is no need for government intervention in AI tech or even to create an AI plan. Still, whenever there is a technological disruption, leadership does matter..Core AI breakthroughs have their origins in academia, and experts suggesting that the US government must focus on funding opportunities. The US has been at the top of their AI game since the beginning of time. But the government needs to help imply existing laws &amp; regulations when it comes to certain AI tech. There have been several cases of accidents involving self-driving vehicles. Federal investigations are being taken place to check if any rules were broken..United Arab Emirates.In 2017, the UAE appointed a Minister for artificial intelligence to spearhead the country's AI strategy game. The UAE aims to build AI to speed up the government's productivity and improve overall departmental performance..The UAE has also imposed a government law on the safe use of AI. And the AI strategy issued by them in 2017 includes various sectors – transport (to reduce accidents, cut operational costs), health (alleviate chronic diseases), renewable energy (to manage facilities), education (cut costs &amp; enhance a desire for education) – are amongst the many sectors that the strategy involves..As per a report by Accenture, AI will boost UAE's economic growth by 1.6% adding a potential $182 billion to the national economy by 2035..Germany.France, Britain and Germany are fighting to attain the top position in AI in Europe. Earlier the German Chancellor, Angela Merkel announced her plans for establishing a national AI strategy. The investment is likely to be over 1 billion pounds. Merkel along with French President Macron has repeatedly called for investments in AI for Europe..McKinsey has predicted that with the right investments and the way AI is moving in Germany, by 2030 AI could dramatically boost Germany's economy by 10 billion euros..Germany also has a tech hub for stimulating an AI ecosystem – Cyber Valley. This hub is attempting to create new ways of collaboration between academics and businesses. The agenda of Cyber Valley is to attract the best brains in the area of AI to advance the R&amp;D of intelligent systems..No Country For Old Tech .Michael Chui, a McKinsey partner stated, &quot;If you look globally, it's a two-horse race in AI.&quot; China and USA are the clear leaders when it comes to AI – With their billions of dollars of investments and relentless innovative tech creations on-the-go..However, it is clear how other economies don't want to be left behind. AI is flourishing and most countries have their money and best minds working on it..Disclaimer: Analytics Insight does not provide financial advice or guidance. Also note that the cryptocurrencies mentioned/listed on the website could potentially be scams, i.e. designed to induce you to invest financial resources that may be lost forever and not be recoverable once investments are made. You are responsible for conducting your own research (DYOR) before making any investments. Read more here.",,
https://news.google.com/rss/articles/CBMibWh0dHBzOi8vd3d3LnNjY29ubGluZS5jb20vYmxvZy9wb3N0LzIwMTgvMDYvMDcvbW91bnRpbmctYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utd2hlcmUtYXJlLXdlLW9uLXRoZS10aW1lbGluZS_SAQA?oc=5,Mounting Artificial Intelligence: Where are we on the timeline? | SCC Times - SCC Online,2018-06-07,SCC Online,https://www.scconline.com,N/A,N/A,by Vaishali Singh*,by Vaishali Singh*,https://schema.org,,,,,,,,,,,,,N/A,N/A,"


Mounting Artificial Intelligence: Where are we on the timeline?

by Vaishali Singh*
Click to share on Facebook (Opens in new window)Click to share on Twitter (Opens in new window)Click to share on LinkedIn (Opens in new window)Click to share on WhatsApp (Opens in new window)MoreClick to print (Opens in new window)Click to email a link to a friend (Opens in new window)Click to share on Telegram (Opens in new window)Click to share on Reddit (Opens in new window)Click to share on Tumblr (Opens in new window)Click to share on Pinterest (Opens in new window)Click to share on Pocket (Opens in new window)Click to share on Skype (Opens in new window) 

Published on June 7, 2018February 19, 2021By Saba 



 





Advertisement 




Artificial Intelligence (AI) is not a new concept, especially to the readers of science fiction. In recent times however, it is becoming more science and less fiction. The world of technology is changing rapidly, and Artificial Intelligence systems have been gaining widespread momentum. With sophisticated technologies being incorporated in the same, it is only a matter of time these systems start to produce marvellous inventions without human intervention of any kind. AI, simply put, is the capability of a machine to imitate intelligent behaviour. It is an umbrella term that refers to information systems inspired by biological systems, and encompasses multiple technologies including machine learning, deep learning, computer vision, natural language processing (NLP) machine reasoning, and strong AI.[1]
Most definitions of Artificial Intelligence in the standard texts are overly complex for a general survey of the field, so here is a simple one that will suffice instead: Artificial Intelligence is the science of mimicking human mental faculties in a computer.
Modern definitions:
Artificial Intelligence refers to the ability of a computer or a computer-enabled robotic system to process information and produce outcomes in a manner similar to the thought process of humans in learning, decision-making and solving problems.
 

                                                                           Figure 1[2]
Artificial Intelligence and IP law
The intellectual property (IP) industry is the most noteworthy market where AI could have a profound effect. With the clear visibility of remarkable extent of creativity and knowledge exhibited by AI, concerns pertaining to IP protection ought to be there in the minds of those enforcing the rights associated with the intellectual property. On this it is required to go on to the more deliberative end of the copyright debate in connection with AI solutions and the relation of patent laws with AI systems.
A draft report of the European Parliament to the Commission on Civil Law Rules on Robotics[3] mentions that in the future, AI will leave no stratum of the society untouched and also calls on the Commission to elaborate criteria for an “own intellectual creation” for copyrightable works produced through AI. Now, there are machines which automatically create works which would qualify for a copyright protection, if it were produced by a human. There have been several high degree computational creative innovations until now and this has sparked debates all over the world for the re-examination of copyright standards for AIs. Recently, a San Francisco Court denied a copyright to a macaque monkey who clicked selfies which went viral. With copyrights for animals out of the picture now, a similar situation has arisen for AIs. Recently, many copyright offices across the world have already mentioned that they would not register machine produced works. Similarly, under patent law, if novel inventions are made by AI machines, issues may arise regarding the ownership of such inventions. Without any human intervention, who will own the patents on novel inventions filed by AI machines? Will the machine/robot be the owner of future inventions? When ownership rights are distributed amongst different entities, who will be able to enforce such rights. And if an AI plagiarises a creation or reproduces an invention, how will damages be determined? These are a few basic but puzzling questions which patent laws now face.[4] With the monumental growth of AIs in every industry of the market, it is incumbent upon IP theorists and lawmakers to design laws which can be adopted in a world with AIs.[5]
Position in India
There is a wide variety of intellectual property legislations which would impact/affect the functioning of AI in India. Such legislations are discussed in detail below.
A. Copyright
Indian Copyright law requires that in order for a “work” to qualify for copyright protection, it would firstly have to meet the “modicum of creativity” standard laid down in Eastern Book Co. v. D.B. Modak[6]. From a reading of the test laid down in the aforementioned judgment however, there is no definitive conclusion that may have arrived at wherein it may be stated that an AI cannot meet the “modicum of creativity” as required.
In addition to the above, the second requirement to be satisfied by an AI when it comes to the ownership of copyrighted works is the requirement to fall under the aegis of an “author” as is defined under the Copyright Act, 1957. This would be problematic as an AI has generally been regarded to not have a legal personality. Under Section 2(d) of the Copyright Act, 1957:
2.(d) “author” means,—
(vi) in relation to any literary, dramatic, musical or artistic work which is computer-generated, the person who causes the work to be created;”
The first issue under the above mentioned definition is its usage of the terms “the person who causes the work to be created”. Determining who “causes” a work to be created is a question of the proximity of a natural or legal person to the creation of the “expression” in the content in question — the more closely or directly a person is involved in creating the “expression”, the more he or she contributes to it, and the more likely he or she is to qualify as a person “who causes the work to be created”. As a result of the above, the current legal framework under the Copyright Act, 1957 may not effectively deal with/prescribe for creation of works where the actual creator or a contributor of the “expression” is not a human or a legal person. Thus, when it comes to works that are created by AI, their authorship would be contentious under Indian copyright laws.[7]
B. Patents
Section 6 of the Patents Act, 1970 states that an application for a patent for any invention can be made only by the true and first inventor of the invention or the persons provided upon request only assigned by such person. Whereas, Section 2(y) of the Act confines the definition of “true and first inventor” to the extent of excluding the first importer of an invention into India, or a person to whom an invention is first communicated outside India, and nothing further. These provisions do not expressly impose the requirement of an inventor to be a natural person. Therefore, from a bare reading of these provisions, it may be interpreted that an AI may fall under the definition of an inventor as provided in Section 2(y) of the Patents Act, 1970. However, in practice the “true and first inventor” is always assumed to be a natural person. Thus, it will be interesting to track the jurisprudence on this front especially the stand taken by the patent office when the “true and first inventor” on the patent application form is not a natural person. However, AI will certainly play an important role in the evolution of patent law itself. Sophisticated use of natural language processing has been adopted in generating variants of existing patent claims so as to enlarge the invention’s scope. The publication of these patent claims using such technology would help preclude obvious and easily derived ideas from being patented as they will form the corpus of the prior art that is available in public domain. If the trend of using such services gains a foothold in the industry, it will substantially increase the uncertainty associated with the enforceability of a patent as the risk of not discovering prior art that invalidates the patent would increase. As a result, it could be anticipated that AI would be developed to assist in discovery of prior art and correspondingly this would certainly increase the demand of AI (from a patent law perspective) in this sector.[8]
AI and IP law: A way frontward
With decades of R&D, IBM’s AI engine Watson, is now capable of detecting a type of cancer in just 10 minutes and this once helped save a person’s life. It was said that the same would have gone undetected under conventional diagnosis methods. From autocorrecting our text messages to saving people’s lives, AI has only begun influencing our lives in a great way. But IP laws are far from matching the progress being made in AIs. Currently IP law focusses only on human actors as IP creators/infringers. It is time now, for policymakers to come up with standards and liability criteria when it comes to IP surrounding AIs. It will also be interesting to see how IP sharing works in the AI realm. It is also believed that the future of many industries depend on AIs and therefore, IP sharing will be a crucial aspect of the overall development agenda and sustainability. The AIs will slowly become a part of every imaginable industry and even though the European Commission has taken baby steps in raising questions over IP laws for AIs, the future is full of intriguing prospects for innovators and businessmen, alike.
Conclusion
The penetration of self-driven cars, robots and fully-automated machines, which are currently being used in various economies around the world, is only expected to increase with the passage of time. As a result, the dependency of entities and individuals on AI systems is also expected to increase proportionately. Whilst addressing the position of Artificial Intelligence, it would be imperative that the regulators undertake a reasonable and balanced approach. A positive step towards the recognition of AIs could be that, all member countries of multilateral trading forums begin to recognise the same, for instance, in the form of an amendment to Trips.  The AIs today perform human like functions in every sphere. It would not be amusing if, tomorrow they can perform functions better than humans and take their decisions themselves. To keep a track of the same, a legislation governing AIs should be drafted, to provide for guidance/clarity as to the rights and obligations of programmers or creators of AI systems, in order to crystallise the broad ethical standards to which they are required to abide to whilst programming/creating AI and robotics systems. Due to the lack of legal jurisprudence on this subject, it is hoped that in the near future legal and tax principles are established which will not only foster the development of AI but also ensure that the necessary safeguards are in place.
 
* Vaishali Singh is Research Associate, GNLU-Microsoft IPR Chair, Gujarat National Law University.
[1]    Raquel Acosta, Artificial Intelligence and Authorship Rights, Harvard Journal of Law and Technology (17-2-2012), available at: <http://jolt.law.harvard.edu/digest/copyright/artificial-intelligence-and-authorship-rights>.
[2]    PWC analysis, available at <https://www.pwc.in/publications/2017/artificial-intelligence-and-robotics-2017.html>.
[3]    See <http://www.europarl.europa.eu/RegData/etudes/ATAG/2017/599250/EPRS_ATA(2017)599250_EN.pdf>.
[4]    Mailer, Artificial Intelligence & Intellectual Property Rights, October 2016  available at <www.clairvolex.com>.
[5]    R. Kurzweil, The Age of Intelligent Machines, 272-275 (MIT Press: 1990).
[6]   (2008) 1 SCC 1.
[7]   Final Report of the National Commission on New Technological Uses of Copyrighted Works 4 (1978), available at <http://eric.ed.govPDFS/ED160122.pdf>.
[8] Office Order No. 36(2017), Intellectual Property Office (India), <http://www.ipindia.nic.in/writereaddata/Portal/Images/pdf/Office_Order_No_36_of_2017_for_Revised__Guidelines_for_Examination_of_CRIs.pdf>; Balaji Subramanian, Patent Office Reboots CRI Guidelines Yet Again: Removes “novel Hardware” Requirement, SpicyIP, <https://spicyip.com/2017/07/patent-office-reboots-cri-guidelines-yet-again-removes-novel-hardware-requirement.html>.

Click to share on Facebook (Opens in new window)Click to share on Twitter (Opens in new window)Click to share on LinkedIn (Opens in new window)Click to share on WhatsApp (Opens in new window)MoreClick to print (Opens in new window)Click to email a link to a friend (Opens in new window)Click to share on Telegram (Opens in new window)Click to share on Reddit (Opens in new window)Click to share on Tumblr (Opens in new window)Click to share on Pinterest (Opens in new window)Click to share on Pocket (Opens in new window)Click to share on Skype (Opens in new window)

Related
 



Tags :

Artificial Intelligence 





1 Comment 




",,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'WebPage', '@id': 'https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/', 'url': 'https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/', 'name': 'Mounting Artificial Intelligence: Where are we on the timeline? | SCC Times', 'isPartOf': {'@id': 'https://www.scconline.com/blog/#website'}, 'primaryImageOfPage': {'@id': 'https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/#primaryimage'}, 'image': {'@id': 'https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/#primaryimage'}, 'thumbnailUrl': 'https://www.scconline.com/blog/wp-content/uploads/2018/06/artificial-intelligence.jpg', 'datePublished': '2018-06-07T07:44:21+00:00', 'dateModified': '2021-02-19T04:31:24+00:00', 'author': {'@id': 'https://www.scconline.com/blog/#/schema/person/e8e76b10dfc9c0d576324bfdbb2c2785'}, 'breadcrumb': {'@id': 'https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/#primaryimage', 'url': 'https://www.scconline.com/blog/wp-content/uploads/2018/06/artificial-intelligence.jpg', 'contentUrl': 'https://www.scconline.com/blog/wp-content/uploads/2018/06/artificial-intelligence.jpg', 'width': 1330, 'height': 887}, {'@type': 'BreadcrumbList', '@id': 'https://www.scconline.com/blog/post/2018/06/07/mounting-artificial-intelligence-where-are-we-on-the-timeline/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.scconline.com/blog/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Mounting Artificial Intelligence: Where are we on the timeline?'}]}, {'@type': 'WebSite', '@id': 'https://www.scconline.com/blog/#website', 'url': 'https://www.scconline.com/blog/', 'name': 'SCC Times', 'description': 'Bringing you the Best Analytical Legal News', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.scconline.com/blog/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Person', '@id': 'https://www.scconline.com/blog/#/schema/person/e8e76b10dfc9c0d576324bfdbb2c2785', 'name': 'Saba', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.scconline.com/blog/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/0b0013f4acb4fe2902d0ea49c3219402?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/0b0013f4acb4fe2902d0ea49c3219402?s=96&d=mm&r=g', 'caption': 'Saba'}, 'url': 'https://www.scconline.com/blog/post/author/editor_2/'}]",,,
