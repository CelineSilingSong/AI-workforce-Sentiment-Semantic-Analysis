URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,url,publisher,mainEntityOfPage,datePublished,dateCreated,dateModified,heading,image,author,article:section,article:summary,article text,@graph,potentialAction,@id,name,logo,sameAs,masthead,ethicsPolicy,publishingPrinciples,headline,articleSection,inLanguage,copyrightHolder,sourceOrganization,creator,thumbnailUrl,alternativeHeadline,hasPart,comment,commentCount,copyrightYear,isAccessibleForFree,isPartOf,diversityPolicy,foundingDate,articleBody,isBasedOn,itemListElement
https://news.google.com/rss/articles/CBMifGh0dHBzOi8vd3d3Lm1ja2luc2V5LmNvbS9jYXBhYmlsaXRpZXMvb3BlcmF0aW9ucy9vdXItaW5zaWdodHMvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtY29uc3RydWN0aW9uLXRlY2hub2xvZ3lzLW5leHQtZnJvbnRpZXLSAQA?oc=5,Artificial intelligence: Construction technology's next frontier - McKinsey,2018-04-04,McKinsey,https://www.mckinsey.com,"Engineering and construction is behind the curve in implementing artificial intelligence solutions. Based on extensive research, we survey applications and algorithms to help bridge the technology gap.",N/A,"Engineering and construction is behind the curve in implementing artificial intelligence solutions. Based on extensive research, we survey applications and algorithms to help bridge the technology gap.","Engineering and construction is behind the curve in implementing artificial intelligence solutions. Based on extensive research, we survey applications and algorithms to help bridge the technology gap.",https://schema.org,Article,https://www.mckinsey.com,"{'@type': 'Organization', 'name': 'McKinsey & Company', 'logo': {'@type': 'ImageObject', 'url': 'https://www.mckinsey.com/~/media/Thumbnails/Mck_Logo'}}","{'@type': 'WebPage', '@id': 'https://www.mckinsey.com/capabilities/operations/our-insights/artificial-intelligence-construction-technologys-next-frontier'}",2018-04-04T01:00:00Z,2018-04-03T06:44:07Z,2018-04-04T00:00:00Z,Artificial intelligence: Construction technology&rsquo;s next frontier,https://www.mckinsey.com/~/media/mckinsey/business%20functions/operations/our%20insights/artificial%20intelligence%20construction%20technologys%20next%20frontier/artificial-intelligence-construction_1_1536x1536.jpg,"[{'@type': 'Person', 'name': 'Jose Luis Blanco', 'url': 'https://www.mckinsey.com/our-people/jose-luis-blanco'}, {'@type': 'Person', 'name': 'Steffen Fuchs', 'url': 'https://www.mckinsey.com/our-people/steffen-fuchs'}, {'@type': 'Person', 'name': 'Matthew Parsons'}, {'@type': 'Person', 'name': 'Maria João Ribeirinho', 'url': 'https://www.mckinsey.com/our-people/maria-joao-ribeirinho'}]",N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiTWh0dHBzOi8vbmV3cy5hcnRuZXQuY29tL2FydC13b3JsZC9hcnQtbWFkZS1ieS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS0xMjU4NzQ10gFWaHR0cHM6Ly9uZXdzLmFydG5ldC5jb20vYXJ0LXdvcmxkL2FydC1tYWRlLWJ5LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLTEyNTg3NDUvYW1wLXBhZ2U?oc=5,Why One Collector Bought a Work of Art Made by Artificial Intelligence—and Is Open to Acquiring More - artnet News,2018-04-03,artnet News,https://news.artnet.com,"The Parisian art collector Nicolas Laugero-Lasserre bought a new artwork created by an artificial intelligence for around $12,000.","artificial intelligence artwork, AI art, urban contemporary art, Nicolas Laugero-Lasserre, obvious art, obvious, comte de belamy, john goodfellow, artnet-news, Contemporary","The Parisian art collector Nicolas Laugero-Lasserre bought a new artwork created by an artificial intelligence for around $12,000.",N/A,https://schema.org,,,,,,,,,,,Art World,N/A,"





Art World

Why One Collector Bought a Work of Art Made by Artificial Intelligence—and Is Open to Acquiring More
See the novel work, which joins Nicolas Laugero-Lasserre's pieces by Banksy, Shepard Fairey, and other human artists.





Collector Nicolas Laugero–Lasserre. Courtesy of Laugero–Lasserre. 










Naomi Rea 
April 3, 2018


 ShareShare This Article




The Paris-based collector Nicolas Laugero-Lasserre is known for his extensive collection of urban art by the likes of Shepard Fairey, Invader, Banksy, and Swoon. But recently, he made a novel acquisition by a very different kind of artist. His latest purchase, Le Comte de Belamy, was created by artificial intelligence.
Laugero-Lasserre bought the work directly from Obvious, the collective that created the AI behind it, for around €10,000 ($12,000) in February. The AI portrait joins 150 works in his collection made by 50 human artists. It is now on view alongside other highlights from his holdings at Paris’s Art 42, a gallery inside technology entrepreneur Xavier Niel’s foundation école 42.
“I just find it amazing that some young people built a program allowing the creation of an original artwork, based on a selection of the ‘bests’ from past art history,” Laugero-Lasserre, who is also the director of ICART, a private art school, tells artnet News. He calls the innovation “grotesque and amazing at the same time.”
Le Comte de Belamy. Image courtesy Obvious.
“Have we ever imagined that creativity could come from a machine?” Laugero-Lasserre asks. “It’s not just inspiration from the past, but actual new pieces.” 
Might he consider splurging on another work made by AI? “Why not?” he says. “If I fall for another one.”
Signature on Le Comte de Belamy. Image courtesy Obvious.
The work was made without human intervention by a machine using an artificial intelligence system called Generative Adversarial Networks (GAN). It works by mimicking characteristics of images in a training data set (in this instance, paintings from the 14th to the 18th centuries) until it can fool an observer into thinking the image is human made.
The French collective responsible for the AI behind Le Comte de Belamy, Obvious, says this is only the beginning. They will soon offer a companion AI-generated piece, La Comtesse de Belamy, at auction at a starting price of 10,000 euros ($12,000). (They have not yet determined which auction house will offer it and remain open to private offers.)
La Comtesse de Belamy. Image courtesy Obvious.
A spokesperson for Obvious told us that although the works were created independently, the collective chose the titles of both artworks, which are the first in a limited series. “We chose ‘Belamy’ as a tribute to Ian Goodfellow, the AI researcher that, together with his team, came up with the mathematical formula which is at the basis of the models we use to create these artworks.” (Goodfellow—who, just to make you feel unaccomplished, was born in 1985—roughly translates into “Belamy” in French.)
 The collective says that although they are not the first to engineer AI-produced artworks, nothing has previously been good enough to be sold due to computational limitations. The legitimization of AI-made work on the market, they believe, is a sign of the dawn of a new artistic movement.
What Obvious calls “GANism,” a movement that takes its name from its AI system of choice, “brings, through the generation of art with machines, a new complexity, an increased attention for detail, and the multiplication of possibilities of interpretation to the very notion of art,” the collective tells artnet News.
The proceeds from the recent and forthcoming sales will be used to further the collective’s research into training its algorithm and delving into 3D modeling.
Watch the generation process for La Comtesse de Belamy below.



Follow Artnet News on Facebook: 

Want to stay ahead of the art world? Subscribe to our newsletter to get the breaking news, eye-opening interviews, and incisive critical takes that drive the conversation forward.








 Share This ArticleShare This Article







                                                        Article topics
                                                    

Contemporary









 
 
 Naomi Rea
Acting Editor-in-Chief
 




 








            The best of Artnet News in your inbox.
        

            Sign up to our daily newsletter.
        








Please enter a valid email address
Signup failed. Please try again later.



Thank you!
You have successfully subscribed to Artnet News.











 

                Related Articles
            










Auctions




The First AI-Generated Portrait Ever Sold at Auction Shatters Expectations, Fetching $432,500—43 Times Its Estimate


			By
							Eileen Kinsella,
							Oct 25, 2018
						


 









Market




Is the Art Market Ready to Embrace Work Made by Artificial Intelligence? Christie’s Will Test the Waters This Fall


			By
							Naomi Rea,
							Aug 20, 2018
						


 










Art World




Please Enjoy This Video of Jerry Saltz Reviewing Art Made by Artificial Intelligence


			By
							Sarah Cascone,
							Feb 20, 2018
						


 





            The best of Artnet News in your inbox.
        

            Sign up to our daily newsletter.
        








Please enter a valid email address
Signup failed. Please try again later.



Thank you!
You have successfully subscribed to Artnet News.




 




                                More Trending Stories
                            



Books
                                                    







                                                            Art Lovers, Here Are 7 New Non-Fiction Books to Add to Your Summer Reading List
                                                        





Art & Exhibitions
                                                    







                                                            There’s More to Rome Than Ancient History. Check Out These Must-See Contemporary Gems
                                                        





Art & Exhibitions
                                                    







                                                            The Story Behind a Rare 19th-Century Portrait of a Person of Color Emerges
                                                        





Artists
                                                    







                                                            ‘The More Repellent You Are, the More Attractive You Become’: George Condo on Contemporary Mythmaking
                                                        









                                                            Books
                                                        


                                                                Art Lovers, Here Are 7 New Non-Fiction Books to Add to Your Summer Reading List
                                                            













                                                            Art & Exhibitions
                                                        


                                                                There’s More to Rome Than Ancient History. Check Out These Must-See Contemporary Gems
                                                            













                                                            Art & Exhibitions
                                                        


                                                                The Story Behind a Rare 19th-Century Portrait of a Person of Color Emerges
                                                            













                                                            Artists
                                                        


                                                                ‘The More Repellent You Are, the More Attractive You Become’: George Condo on Contemporary Mythmaking
                                                            











","[{'@type': 'Organization', '@id': 'https://news.artnet.com/#organization', 'name': 'Artnet News', 'url': 'https://news.artnet.com/', 'sameAs': [], 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://news.artnet.com/#/schema/logo/image/', 'url': 'https://news.artnet.com/app/news-upload/2023/02/news-logo-64x64-1.png', 'contentUrl': 'https://news.artnet.com/app/news-upload/2023/02/news-logo-64x64-1.png', 'width': 64, 'height': 64, 'caption': 'Artnet News'}, 'image': {'@id': 'https://news.artnet.com/#/schema/logo/image/'}}, {'@type': 'WebSite', '@id': 'https://news.artnet.com/#website', 'url': 'https://news.artnet.com/', 'name': 'Artnet News', 'description': 'The world’s most-read and best trusted art publication', 'publisher': {'@id': 'https://news.artnet.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://news.artnet.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#primaryimage', 'url': 'https://news.artnet.com/app/news-upload/2018/04/nl-e1522767970935.jpeg', 'contentUrl': 'https://news.artnet.com/app/news-upload/2018/04/nl-e1522767970935.jpeg', 'width': 1000, 'height': 1500, 'caption': 'Collector Nicolas Laugero–Lasserre. Courtesy of Laugero–Lasserre.'}, {'@type': 'WebPage', '@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#webpage', 'url': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745', 'name': 'Why One Collector Bought a Work of Art Made by Artificial Intelligence—and Is Open to Acquiring More | Artnet News', 'isPartOf': {'@id': 'https://news.artnet.com/#website'}, 'primaryImageOfPage': {'@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#primaryimage'}, 'datePublished': '2018-04-03T15:23:10+00:00', 'dateModified': '2018-08-17T13:07:06+00:00', 'description': 'The Parisian art collector Nicolas Laugero-Lasserre bought\xa0a new artwork created by an artificial intelligence for around $12,000.', 'breadcrumb': {'@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745']}]}, {'@type': 'BreadcrumbList', '@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://news.artnet.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Why One Collector Bought a Work of Art Made by Artificial Intelligence—and Is Open to Acquiring More'}]}, {'@type': 'NewsArticle', '@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#article', 'isPartOf': {'@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#webpage'}, 'author': {'@id': 'https://news.artnet.com/#/schema/person/ba743d578bca1480ede0230f5a0e91be'}, 'headline': 'Why One Collector Bought a Work of Art Made by Artificial Intelligence—and Is Open to Acquiring More', 'datePublished': '2018-04-03T15:23:10+00:00', 'dateModified': '2018-08-17T13:07:06+00:00', 'mainEntityOfPage': {'@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#webpage'}, 'wordCount': 606, 'publisher': {'@id': 'https://news.artnet.com/#organization'}, 'image': {'@id': 'https://news.artnet.com/art-world/art-made-by-artificial-intelligence-1258745#primaryimage'}, 'thumbnailUrl': 'https://news.artnet.com/app/news-upload/2018/04/nl-e1522767970935.jpeg', 'articleSection': 'Art World', 'inLanguage': 'en-US', 'isAccessibleForFree': 'True', 'hasPart': {'@type': 'WebPageElement', 'isAccessibleForFree': 'True', 'cssSelector': '.article-body'}, 'alternativeHeadline': 'Why One Collector Bought a Work of Art Made by Artificial Intelligence—and Is Open to Acquiring More | Artnet News', 'description': {}, 'keywords': ['Contemporary']}, {'@type': 'Person', '@id': 'https://news.artnet.com/#/schema/person/ba743d578bca1480ede0230f5a0e91be', 'name': 'Naomi Rea', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://news.artnet.com/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/a36033d037b111ee6cd86f7936667840?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/a36033d037b111ee6cd86f7936667840?s=96&d=mm&r=g', 'caption': 'Naomi Rea'}, 'description': ""Naomi Rea is Artnet News's Acting Editor-in-Chief, where she has worked since 2017. She is based in London, U.K."", 'sameAs': ['https://www.instagram.com/naomikrea/', 'https://twitter.com/naomikrea'], 'url': 'https://news.artnet.com/about/naomi-rea-419'}]",,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiU2h0dHBzOi8vaGJyLm9yZy8yMDE4LzA0L2lmLXlvdXItZGF0YS1pcy1iYWQteW91ci1tYWNoaW5lLWxlYXJuaW5nLXRvb2xzLWFyZS11c2VsZXNz0gEA?oc=5,"If Your Data Is Bad, Your Machine Learning Tools Are Useless - Harvard Business Review",2018-04-02,Harvard Business Review,https://hbr.org,"Poor data quality is enemy number one to the widespread, profitable use of machine learning. The quality demands of machine learning are steep, and bad data can rear its ugly head twice both in the historical data used to train the predictive model and in the new data used by that model to make future decisions. To ensure you have the right data for machine learning, you must have an aggressive, well-executed quality program. It requires the leaders of the overall effort to take the following five steps: First, clarify your objectives and assess whether you have the right data to support these objectives. Second, build plenty of time to execute data quality fundamentals into your overall project plan. Third, maintain an audit trail as you prepare the training data. Fourth, charge a specific individual or team with responsibility for data quality as you turn your model loose. Finally, obtain independent, rigorous quality assurance.",N/A,"Poor data quality is enemy number one to the widespread, profitable use of machine learning. The quality demands of machine learning are steep, and bad data can rear its ugly head twice both in the historical data used to train the predictive model and in the new data used by that model to make future decisions. To ensure you have the right data for machine learning, you must have an aggressive, well-executed quality program. It requires the leaders of the overall effort to take the following five steps: First, clarify your objectives and assess whether you have the right data to support these objectives. Second, build plenty of time to execute data quality fundamentals into your overall project plan. Third, maintain an audit trail as you prepare the training data. Fourth, charge a specific individual or team with responsibility for data quality as you turn your model loose. Finally, obtain independent, rigorous quality assurance.",N/A,https://schema.org,WebSite,https://hbr.org/,,,,,,,,,Analytics and data science,N/A,N/A,,"{'@type': 'SearchAction', 'target': 'https://hbr.org/search?term={search_term_string}', 'query-input': 'required name=search_term_string'}",,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiUmh0dHBzOi8vb2JzZXJ2ZXIuY29tLzIwMTgvMDQvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtZmFzaGlvbi1mdXR1cmUtZGVzaWduZXItam9icy_SAVZodHRwczovL29ic2VydmVyLmNvbS8yMDE4LzA0L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWZhc2hpb24tZnV0dXJlLWRlc2lnbmVyLWpvYnMvYW1wLw?oc=5,"When Artificial Intelligence Clashes With Fashion, How Will Our Future Dresses Look? - Observer",2018-04-02,Observer,https://observer.com,"Having algorithms perform creative tasks may seem like a failed experiment, but tech-native companies firmly believe in its potential. ","['FIT', 'Prime Wardrobe', 'Stitch Fix', 'Business', 'Lifestyle', 'Fashion', 'Technology', 'Mary Meeker', 'Natalie Nudell', 'Eric Colson', 'Kavita Bala', 'Amazon', 'Google']","Having algorithms perform creative tasks may seem like a failed experiment, but tech-native companies firmly believe in its potential.",N/A,https://schema.org,NewsArticle,https://observer.com/2018/04/artificial-intelligence-fashion-future-designer-jobs/,{'@id': 'https://observer.com#publisher'},https://observer.com/2018/04/artificial-intelligence-fashion-future-designer-jobs/,2018-04-02 15:07:14,,2018-04-03 21:42:14,,"{'@type': 'ImageObject', 'url': 'https://observer.com/wp-content/uploads/sites/2/2018/03/clem-onojeghuo-160190-unsplash.jpg?quality=80', 'representativeOfPage': True, 'height': 3600, 'width': 5400}","[{'@type': 'Person', 'name': 'Sissi Cao', 'url': 'https://observer.com/author/sissi-cao/'}]",N/A,N/A,"


Business  •  Technology 
When Artificial Intelligence Clashes With Fashion, How Will Our Future Dresses Look?
Having algorithms perform creative tasks may seem like a failed experiment, but tech-native companies firmly believe in its potential. 

			By Sissi Cao • 04/02/18 11:07am 













































 


To Stitch Fix’s “hybrid design” tool, fashion design is a puzzle work of 30 to 80 pieces. Clem Onojeghuo/Unsplash
On the website of online personal shopping service Stitch Fix, the company features a customer review that reads, “I love that my stylist listens to my feedback. The personal note included in my Fix shows how much pride she takes in serving each client.”








						Sign Up For Our Daily Newsletter					







						Sign Up					


					Thank you for signing up!				


					By clicking submit, you agree to our <a href=""http://observermedia.com/terms"">terms of service</a> and acknowledge we may use your information to send you emails, product samples, and promotions on this website and other properties. You can opt out anytime.				


					See all of our newsletters				



Stitch Fix’s personal stylist is the best of its kind. Indeed, few stylists in the industry has achieved the same level of success at outfit pairing and shopping recommendation. Measured by business terms, Stitch Fix’s stylist has transformed a startup in an apartment into a public company in just seven years.
Except that the “stylist” is not a “she”—nor is it a “he.” 
Stitch Fix picks clothes for each customer from a sea of inventory with the help of a set of algorithms coded by the company’s 85-person data team. Although a human stylist would make final decisions on clothes selection, algorithms help recommend items within each customer’s price, size and style preferences, a company spokesperson told Observer. 
For each customer, the algorithm-assisted “stylist” analyzes fashion preferences based on a customer’s profile with the site (including information on size, general preference on color, price range and occasion, etc.), purchase and return history with Stitch Fix, and activities on social media such as fashion pictures saved on Pinterest. The more data the algorithm collects about the customer, the better it knows about that person’s fashion tastes and the better it gets at recommending outfits.
The technology behind Stitch Fix’s robot-assisted styling tool, which venture capitalist Mary Meeker called e-commerce’s “aha” moment, is now being used in the field of original design. 
The initiative, named “hybrid design,” aims to ultimately create clothes from scratch just like a human designer does.  
“We noticed gaps in the market and an opportunity to produce something that doesn’t exist, but should,” Eric Colson, Stitch Fix’s chief algorithm officer, told Co.Design when the initiative took off last year.  
The market gaps, Colson said, are a result of an unprecedented trove of user data thanks to the proliferation of social media and subscription sites like Stitch Fix.
“We’re uniquely suited to do this. This didn’t exist before because the necessary data didn’t exist. A Nordstrom doesn’t have this type of data because people try things on in the fitting room, and you don’t know what they didn’t buy or why. We have this access to great data, and we can do a lot with it,” he said in a separate interview with Glossy.
To the “hybrid design” algorithm, fashion design is a puzzle work of 30 to 80 pieces—color, fabric type, collar shape, the number of buttons, just to name a few. As its name implies, however, “hybrid design” still relies on human designers to come up with the actual garments based on guidance provided by the algorithm’s analysis. 
“It’s machine learning with expert human judgment,” Colson said.
Currently, “hybrid design” produces about one percent of Stitch Fix’s total inventory. The company said these items were well received among customers. And yet, perhaps unsurprisingly, the approach has drawn skepticism from industry insiders.
Stitch Fix analyzes a shopper’s fashion preference based on online activities on its website, as well as on social media sites. Stitch Fix
“I find it hard to imagine that AI algorithms can produce anything avant-garde or creative on purpose,” Natalie Nudell, a fashion historian who teaches at the Fashion Institute of Technology in New York, told Observer.
“It’s possible that they can make something truly creative by accident, though,” she joked.
The possibility of A.I. robots taking over human jobs has been a hot topic among tech communities and social scientists for some time, but such discussion has so far largely focused on repetitive, low-skill jobs such as supermarket cashiers and, at best, tasks dealing with a finite pool of data, like basic stock analysis and language translation. 
Having an algorithm perform creative tasks sounds overly ambitious, if not totally unconceivable.
The most obvious shortcoming is a lack of a cognizant consistency that only humans share.
The first company to experiment with this idea was Google (GOOGL). In 2015, a Google engineer released a project called DeepDream, where a neural network learned to fill in gaps in an incomplete images with graphic elements that it guessed would naturally fit.
The results, though, were hardly anything “natural.”
Nevertheless, tech-native companies believe firmly in the potential of artificial intelligence, at least in fashion. 
Last year, Amazon (AMZN) launched a “virtual fitting room” service called Prime Wardrobe that would allow buyers one week to try on three or more clothing items (shipping included in Prime membership) and return items they don’t like. With these keep-and-return records, Amazon analyzes a customer’s fashion preference to fine-tune its outfit recommendations. 
The fundamental technology behind automatic outfit selection or design isn’t new. 
Similar algorithms have been used in language translation (Google Translate) and speech recognition (Amazon’s Alexa). A key challenge of making algorithms work for fashion is visual search, a still nascent area in machine learning research. As a leading A.I. scientist recently told Observer, discerning subtle differences in images takes a more complex “intelligence” process than recognizing text or sound.  
“Visual search is still a new and challenging field, yet it’s one of the most exciting technologies for both retailers and consumers,” Kavita Bala, a computer science professor at Cornell University, told Observer.
The most promising area, Bala said, is improving trend forecasting in the fashion industry. 
“Designers will continue to be the driving force for creativity, but they represent a top-down cycle of fashion, where new designs debut on the runway and then spread to a wider base. These aspirational designs are usually not accessible to the average person,” Bala explained. “What machine learning can influence is ‘bottom-up’ fashion. For example, street style, where fashion trends emerge from average people on Instagram, Pinterest and other social media.”
At Cornell, Bala co-leads a research project called “StreetStyle,” where a deep learning algorithm aims to derive fashion trends in cities across the world through analyzing millions of photos posted on Instagram and other social media platforms. 
“As machine learning gets more sophisticated, it will be able to detect micro trends before they become widely popular. This will be the holy grail of retail. With this, retailers can respond a lot faster to changing preferences,” she added. “If we can forecast trends fast, it will fundamentally change how designs spread and might open the doors for ‘smaller’ designers to reach larger audiences. A more democratic spread of design ideas can potentially be enabled.”
To consumers, that means fast fashion will likely get faster. And whether it is a robot or not that’s sketching out the dress for the next season, fashion still seems to need a human touch—at least for now.
“It’s just interesting to me that Stitch Fix doesn’t mention ‘A.I.’ or other tech terms on their website. They even refer to their stylist as ‘she,’” Nudell said. 





",,,https://observer.com#publisher,Observer,"{'@context': 'https://schema.org/', '@type': 'ImageObject', 'url': 'https://observer.com/wp-content/themes/newyorkobserver-2014/images/observer-logo-2015.png', 'height': 60, 'width': 428}","['https://en.wikipedia.org/wiki/The_New_York_Observer', 'https://www.facebook.com/observer', 'https://twitter.com/observer', 'https://www.linkedin.com/company/observer.com', 'https://www.instagram.com/observer/']",https://observer.com/about/,https://observer.com/ethics-policy/,https://observer.com/ethics-policy/,"When Artificial Intelligence Clashes With Fashion, How Will Our Future Dresses Look?",Business,en-US,{'@id': 'https://observer.com#publisher'},{'@id': 'https://observer.com#publisher'},"[{'@type': 'Person', 'name': 'Sissi Cao', 'url': 'https://observer.com/author/sissi-cao/'}]",https://observer.com/wp-content/uploads/sites/2/2018/03/clem-onojeghuo-160190-unsplash.jpg?quality=80,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVWh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMTgvMDQvMDQvdGVjaG5vbG9neS9nb29nbGUtbGV0dGVyLWNlby1wZW50YWdvbi1wcm9qZWN0Lmh0bWzSAQA?oc=5,'The Business of War': Google Employees Protest Work for the Pentagon (Published 2018) - The New York Times,2018-04-04,The New York Times,https://www.nytimes.com,Thousands of employees have signed a letter calling on their C.E.O. to pull out of a project that could be used to improve drone strike targeting.,N/A,Thousands of employees have signed a letter calling on their C.E.O. to pull out of a project that could be used to improve drone strike targeting.,Thousands of employees have signed a letter calling on their C.E.O. to pull out of a project that could be used to improve drone strike targeting.,https://schema.org,NewsMediaOrganization,https://www.nytimes.com/,"{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",https://www.nytimes.com/2018/04/04/technology/google-letter-ceo-pentagon-project.html,2018-04-04T14:55:32.000Z,,2021-11-04T02:00:22.655Z,,"[{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2018/04/05/business/05GOOGLE/04GOOGLE-videoSixteenByNineJumbo1600-v4.jpg', 'height': 899, 'width': 1600, 'contentUrl': 'https://static01.nyt.com/images/2018/04/05/business/05GOOGLE/04GOOGLE-videoSixteenByNineJumbo1600-v4.jpg', 'caption': 'Thousands of Google employees have signed a letter to Sundar Pichai, the company&rsquo;s chief executive, protesting Google&rsquo;s role in a program that could be used to improve drone strike targeting.', 'creditText': 'Michael Short/Bloomberg'}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2018/04/05/business/05GOOGLE/04GOOGLE-superJumbo-v4.jpg', 'height': 1364, 'width': 2048, 'contentUrl': 'https://static01.nyt.com/images/2018/04/05/business/05GOOGLE/04GOOGLE-superJumbo-v4.jpg', 'caption': 'Thousands of Google employees have signed a letter to Sundar Pichai, the company&rsquo;s chief executive, protesting Google&rsquo;s role in a program that could be used to improve drone strike targeting.', 'creditText': 'Michael Short/Bloomberg'}]","[{'@context': 'https://schema.org', '@type': 'Person', 'url': 'https://www.nytimes.com/by/scott-shane', 'name': 'Scott Shane'}, {'@context': 'https://schema.org', '@type': 'Person', 'url': 'https://www.nytimes.com/by/daisuke-wakabayashi', 'name': 'Daisuke Wakabayashi'}]",Technology,N/A,"Artificial IntelligenceFine Print ChangesQuiz: Fake or Real Images?Apple Enters A.I. FrayMeta’s A.I. ScrapingHumane’s A.I. Device FlopAdvertisementSKIP ADVERTISEMENTSupported bySKIP ADVERTISEMENT‘The Business of War’: Google Employees Protest Work for the PentagonShare full article316Read in appThousands of Google employees have signed a letter to Sundar Pichai, the company’s chief executive, protesting Google’s role in a program that could be used to improve drone strike targeting.Credit...Michael Short/BloombergBy Scott Shane and Daisuke WakabayashiApril 4, 2018WASHINGTON — Thousands of Google employees, including dozens of senior engineers, have signed a letter protesting the company’s involvement in a Pentagon program that uses artificial intelligence to interpret video imagery and could be used to improve the targeting of drone strikes.The letter, which is circulating inside Google and has garnered more than 3,100 signatures, reflects a culture clash between Silicon Valley and the federal government that is likely to intensify as cutting-edge artificial intelligence is increasingly employed for military purposes.(Read the text of the letter.)“We believe that Google should not be in the business of war,” says the letter, addressed to Sundar Pichai, the company’s chief executive. It asks that Google pull out of Project Maven, a Pentagon pilot program, and announce a policy that it will not “ever build warfare technology.”ImageThat kind of idealistic stance, while certainly not shared by all Google employees, comes naturally to a company whose motto is “Don’t be evil,” a phrase invoked in the protest letter. But it is distinctly foreign to Washington’s massive defense industry and certainly to the Pentagon, where the defense secretary, Jim Mattis, has often said a central goal is to increase the “lethality” of the United States military.AdvertisementSKIP ADVERTISEMENTFrom its early days, Google has encouraged employees to speak out on issues involving the company. It provides internal message boards and social networks where workers challenge management and one another about the company’s products and policies. Recently, the heated debate around Google’s efforts to create a more diverse work force spilled out into the open.Google employees have circulated protest petitions on a range of issues, including Google Plus, the company’s lagging competitor to Facebook, and Google’s sponsorship of the Conservative Political Action Conference.Employees raised questions about Google’s involvement in Project Maven at a recent companywide meeting. At the time, Diane Greene, who leads Google’s cloud infrastructure business, defended the deal and sought to reassure concerned employees. A company spokesman said most of the signatures on the protest letter had been collected before the company had an opportunity to explain the situation.The company subsequently described its work on Project Maven as “non-offensive” in nature, though the Pentagon’s video analysis is routinely used in counterinsurgency and counterterrorism operations, and Defense Department publications make clear that the project supports those operations. Both Google and the Pentagon said the company’s products would not create an autonomous weapons system that could fire without a human operator, a much-debated possibility using artificial intelligence.AdvertisementSKIP ADVERTISEMENTBut improved analysis of drone video could be used to pick out human targets for strikes, while also better identifying civilians to reduce the accidental killing of innocent people.Without referring directly to the letter to Mr. Pichai, Google said in a statement on Tuesday that “any military use of machine learning naturally raises valid concerns.” It added, “We’re actively engaged across the company in a comprehensive discussion of this important topic.” The company called such exchanges “hugely important and beneficial,” though several Google employees familiar with the letter would speak of it only on the condition of anonymity, saying they were concerned about retaliation.The statement said the company’s part of Project Maven was “specifically scoped to be for non-offensive purposes,” though officials declined to make available the relevant contract language. The Defense Department said that because Google is a subcontractor on Project Maven to the prime contractor, ECS Federal, it could not provide either the amount or the language of Google’s contract. ECS Federal did not respond to inquiries.Google said the Pentagon was using “open-source object recognition software available to any Google Cloud customer” and based on unclassified data. “The technology is used to flag images for human review and is intended to save lives and save people from having to do highly tedious work,” the company said.Some of Google’s top executives have significant Pentagon connections. Eric Schmidt, former executive chairman of Google and still a member of the executive board of Alphabet, Google’s parent company, serves on a Pentagon advisory body, the Defense Innovation Board, as does a Google vice president, Milo Medin.AdvertisementSKIP ADVERTISEMENTIn an interview in November, Mr. Schmidt acknowledged “a general concern in the tech community of somehow the military-industrial complex using their stuff to kill people incorrectly, if you will.” He said he served on the board in part “to at least allow for communications to occur” and suggested that the military would “use this technology to help keep the country safe.”An uneasiness about military contracts among a small fraction of Google’s more than 70,000 employees may not pose a major obstacle to the company’s growth. But in the rarefied area of artificial intelligence research, Google is engaged in intense competition with other tech companies for the most talented people, so recruiters could be hampered if some candidates are put off by Google’s defense connections.As Google defends its contracts from internal dissent, its competitors have not been shy about publicizing their own work on defense projects. Amazon touts its image recognition work with the Department of Defense, and Microsoft has promoted the fact that its cloud technology won a contract to handle classified information for every branch of the military and defense agencies.The current dispute, first reported by Gizmodo, is focused on Project Maven, which began last year as a pilot program to find ways to speed up the military application of the latest A.I. technology. It is expected to cost less than $70 million in its first year, according to a Pentagon spokeswoman. But the signers of the letter at Google clearly hope to discourage the company from entering into far larger Pentagon contracts as the defense applications of artificial intelligence grow.Google is widely expected to compete with other tech giants, including Amazon and Microsoft, for a multiyear, multibillion-dollar contract to provide cloud services to the Defense Department. John Gibson, the department’s chief management officer, said last month that the Joint Enterprise Defense Infrastructure Cloud procurement program was in part designed to “increase lethality and readiness,” underscoring the difficulty of separating software, cloud and related services from the actual business of war.AdvertisementSKIP ADVERTISEMENTThe employees’ protest letter to Mr. Pichai, which has been circulated on an internal communications system for several weeks, argues that embracing military work could backfire by alienating customers and potential recruits.“This plan will irreparably damage Google’s brand and its ability to compete for talent,” the letter says. “Amid growing fears of biased and weaponized AI, Google is already struggling to keep the public’s trust.” It suggests that Google risks being viewed as joining the ranks of big defense contractors like Raytheon, General Dynamics and the big-data firm Palantir.“The argument that other firms, like Microsoft and Amazon, are also participating doesn’t make this any less risky for Google,” the letter says. “Google’s unique history, its motto Don’t Be Evil, and its direct reach into the lives of billions of users set it apart.”Like other onetime upstarts turned powerful Silicon Valley behemoths, Google is being forced to confront the idealism that guided the company in its early years. Facebook started with the lofty mission of connecting people all over the world, but it has recently come under fire for becoming a conduit for fake news and being used by Russia to influence the 2016 election and sow dissent among American voters.Paul Scharre, a former Pentagon official and author of “Army of None,” a forthcoming book on the use of artificial intelligence to build autonomous weapons, said the clash inside Google was inevitable, given the company’s history and the booming demand for A.I. in the military.“There’s a strong libertarian ethos among tech folks, and a wariness about the government’s use of technology,” said Mr. Scharre, a senior fellow at the Center for a New American Security in Washington. “Now A.I. is suddenly and quite quickly moving out of the research lab and into real life.”Scott Shane reported from Washington, and Daisuke Wakabayashi from San Francisco. Cecilia Kang contributed reporting from Washington. A version of this article appears in print on April 5, 2018, Section A, Page 1 of the New York edition with the headline: A Google Military Project Fuels Internal Dissent. Order Reprints | Today’s Paper | SubscribeSee more on: Alphabet Inc.Read 316 CommentsShare full article316Read in appAdvertisementSKIP ADVERTISEMENTComments 316‘The Business of War’: Google Employees Protest Work for the PentagonSkip to CommentsThe comments section is closed.
      To submit a letter to the editor for publication, write to
      letters@nytimes.com.Tell us about yourself. Take the survey.",,,https://www.nytimes.com/#publisher,The New York Times,"{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'height': 291, 'width': 291, 'contentUrl': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'creditText': 'The New York Times'}",https://en.wikipedia.org/wiki/The_New_York_Times,https://www.nytimes.com/interactive/2023/01/28/admin/the-new-york-times-masthead.html,https://www.nytco.com/company/standards-ethics/,,‘The Business of War’: Google Employees Protest Work for the Pentagon,,,"{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",,,A Google Military Project Fuels Internal Dissent,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.meteredContent'}",{'@id': '#commentsContainer'},316.0,2024.0,False,"{'@type': ['CreativeWork', 'Product'], 'name': 'The New York Times', 'productID': 'nytimes.com:basic'}",https://www.nytco.com/company/diversity-and-inclusion/,1851-09-18,,,
https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vd3d3LnRlZW52b2d1ZS5jb20vc3RvcnkvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtaXNudC1nb29kLWZvci13b21lbi1idXQtd2UtY2FuLWZpeC1pdNIBAA?oc=5,AI Isn't Good for Women — But We Can Fix It - Teen Vogue,2018-04-04,Teen Vogue,https://www.teenvogue.com,"The way artificial intelligence is developed can reinforce biases, sexism, and racism — but there are ways to fix it.","['tech', 'artificial intelligence', 'web']","""It's actually showing the same biases that society already has, it's extremely problematic.”","""It's actually showing the same biases that society already has, it's extremely problematic.”",https://schema.org/,BreadcrumbList,https://www.teenvogue.com/story/artificial-intelligence-isnt-good-for-women-but-we-can-fix-it,"{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'Teen Vogue', 'logo': {'@type': 'ImageObject', 'url': 'https://www.teenvogue.com/verso/static/teen-vogue/assets/logo-seo.png', 'width': '667px', 'height': '100px'}, 'url': 'https://www.teenvogue.com'}","{'@type': 'WebPage', '@id': 'https://www.teenvogue.com/story/artificial-intelligence-isnt-good-for-women-but-we-can-fix-it'}",2018-04-04T18:03:28.822-04:00,,2018-04-04T18:03:28.822-04:00,,"['https://assets.teenvogue.com/photos/5ac52cc2c917d6234bc2876e/16:9/w_2656,h_1494,c_limit/Tout.jpg', 'https://assets.teenvogue.com/photos/5ac52cc2c917d6234bc2876e/4:3/w_2664,h_1998,c_limit/Tout.jpg', 'https://assets.teenvogue.com/photos/5ac52cc2c917d6234bc2876e/1:1/w_2667,h_2667,c_limit/Tout.jpg']","[{'@type': 'Person', 'name': 'Nicole Kobie', 'sameAs': 'https://www.teenvogue.com/contributor/nicole-kobie'}]",tags,N/A,"TechArtificial Intelligence Isn't Good for Women, But We Can Fix It""It's actually showing the same biases that society already has, it's extremely problematic.”By Nicole KobieApril 4, 2018BSIPArtificial intelligence isn't necessarily good for women, but we can make it better.Because we build and train AI, it reflects our biases and assumptions — and our racism and sexism. That's problematic as AI can be used everywhere: it controls driverless cars and powers voice assistants such as Siri and Alexa, but also helps HR departments sift through resumes, decides who gets parole, and examines medical images. As its uses get more widespread and important, AI missteps and abuses become more dangerous.If we don't get it right, sexism, racism, and other biases will be literally encoded into our lives, trained on incorrect data that continues to leave women and people of color out of decision making. ""We're ending up coding into our society even more bias, and more misogyny and less opportunity for women,"" says Tabitha Goldstaub, cofounder of AI startup CognitionX. ""We could get transported back to the dark ages, pre-women's lib, if we don't get this right.""Trending NowThe History of YellowfaceWhat is AI?AI is made of up of a myriad of different, related technologies that let computers ""think"" and make decisions, helping us automate tasks. That includes ideas such as neural networks, a machine-learning technique that can be trained on datasets before being set loose to use that knowledge. Show it a bunch of pictures of dogs, and it learns what dogs look like — well, sometimes the machines manage it, other times they can't tell chihuahuas from muffins.AdvertisementAI is meant to make our lives easier. It's good at filtering information and making quick decisions, but if we build it poorly and train it on biased or false data, it could hurt people.""A lot of people assume that artificial intelligence… is just correct and it has no errors,"" says Tess Posner, co-founder of AI4All. ""But we know that that’s not true, because there's been a lot of research lately on these examples of being incorrect and biased in ways that amplify or reflect our existing societal biases.""How AI can hurtThe impacts can be obvious, such as a resume bot favoring male, ""white""-sounding names, but they can also be subtle, says Professor Kathleen Richardson, of the school of computer science and informatics at De Montfort University. ""It's not like we go out into the world and the bank machine doesn't work for us because we're female,"" she says. ""Some things do work for us. It's more just about the priorities that we start to have as a society. Those priorities, for example, often become the priorities of a small elite.""For example, researchers from the University of Washington have shown how one image-recognition system had gender bias, associating kitchens and shopping with women and sports with men — a man standing at a stove was labeled as a woman. ""The biases that are inherited in our own language and our own society are getting... reflected in these algorithms,"" Posner says.And those biased labels and data are used to make decisions that impact lives. Goldstaub points to research at Carnegie Mellon University that found Google's recommendation algorithm was more likely to recommend ""high-prestige and high-paying jobs to men rather than to women,"" while separate research from Boston University showed CV-sifting AI put men at the top of the pile for jobs such as programming.Enter your email to get Teen Vogue's newsletterclose dialogGet all your news in one place, daily.email addressPlease enter aboveGoBy signing up, you agree to our user agreement (including the  class action waiver and arbitration provisions), our privacy policy and cookie statement, and to receive marketing and account-related emails from Teen Vogue. You can unsubscribe at any time.close dialogAnother example is COMPAS, an AI-based risk assessment tool used across the U.S. to make decisions about how likely a criminal is to reoffend. ""This was shown to be biased against African Americans,"" Posner said. A report by ProPublica showed that COMPAS rated black people as more likely to reoffend than their white counterparts, and as such was ""remarkably unreliable"" at its job of forecasting who would break the law again.""It's actually showing the same biases that society already has, it's extremely problematic,"" says Posner. ""It's affecting people's lives, whether they're getting parole and what decisions the court is making… it's going to further marginalize certain populations.""Consider health care, says Goldstaub. ""Men and women have different symptoms when having a heart attack — imagine if you trained an AI to only recognize male symptoms,"" she says. ""You'd have half the population dying from heart attacks unnecessarily."" It's happened before: crash test dummies for cars were designed after men; female drivers were 47% more likely to be seriously hurt in accidents. Regulators only started to require car makers to use dummies based on female bodies in 2011.""It's a good example of what happens if we don't have diversity in our training sets,"" says Goldstaub. ""When it comes to health care, it's life or death — not getting a job is awful, but health care is life or death.""Educating womenThere's one obvious way to encourage better systems, says Richardson: ""we need more women in robots and AI.""Right now that's not happening. According to the AAUW, only 26% of computing professionals in the U.S. are women — and there used to be more. Back in the 1990s, more than a third of those working in tech were female. According to Google's own diversity figures in 2017, 31 percent of its workforce is women, but only 20 percent of its technical roles are filled by women. And, only 1 percent of all their tech employees (of any gender) are black; 3 percent are hispanic. For AI in particular, Goldstaub suggests about 13 percent of those working in AI are women.""I believe as a feminist the more women we can get into roles, the more diverse the output will be — and fewer shockers will get through,"" Goldstaub says.Thankfully, groups such as AI4ALL have sprung up to help women step into careers in AI by encouraging high school students to take science, technology, engineering, and math (STEM) subjects.  ""When we look at the research about underrepresented populations and why they don’t go into the field, a lot of the research shows this actually stems back in high school at around age 15, which is when folks get discouraged or lose interest in STEM fields,"" says Posner.Why is that? Posner points to a lack of role models, no exposure to technical subjects or innovation, and a general lack of encouragement. To fight back, AI4All shows high school students — in particular girls, those from low-income families, and from different ethnic backgrounds — the path to an AI career, offering educational camps and mentorships with industry leaders. ""And then we’re supporting you throughout your career path and into your career, if this is the path that you choose,"" she says.To help push selected students toward creating ethical AI, the camps work on projects under the AI for Good banner, which designs systems specifically for humanitarian causes such as computer vision for hospitals or natural-language processing for disaster relief efforts. ""We've seen that it's actually really effective to teach rigorous AI concepts in the context of societal impact,"" she says.And some of the projects AI4All students have made have been incredible, Posner says — by not including a diverse range of people in AI development, we not only risk biases but also miss out on better ideas. ""When we give access to more people incredible things happen and things that we could have never imagined before,"" she says. ""That’s why it’s especially critical to let’s not miss out on the potential inventions and talents of all these amazing underutilized groups.""Being heardCompanies need to remember that there's more to diversity than hiring a token lady for the team, and women shouldn't be made to feel they need to represent their entire gender or race. ""A lot of women go into science and the last thing they talk about is sexism or gender or differences like that,"" Richardson says. ""When they enter these fields, the last thing they want to do is make an issue out of being a woman, if you know what I mean.""That means it's not just women's responsibility to encourage their female colleagues to feel comfortable speaking up. ""What tends to happen is when the most powerful groups let in other people with less power, is the people with less power go along with the people with the most power,"" Richardson says. ""I've done it myself.""Simply having women in the room isn't enough; they need to be heard — and often enough, that means we need to stand up and make people listen. ""You have to be brave and courageous to come in and challenge people with authority and power,"" Richardson says.Degendering AIOne way to make AI less problematic for women is to take gender out of the equation. Alexa and Siri have something in common: they're both clearly female characters and female voices. That's taken further with virtual girlfriends such as Gatebox in Japan — and that's before we start talking about sex robots. But Alexa and Siri are a good place to start.""What they tend to do is keep reproducing this idea of women as sexual objects to be used, to be appropriated,"" Richardson says, explaining that giving objects female personas cements existing power dynamics. ""Women are expected to give away power, to acknowledge and look after men, to laugh at their jokes, flatter their ego — these kinds of things. So when you've got men then creating models of relationships [with AI assistants], they're creating a model of relationship that is very egocentric, not very neutral… I think that's what's underlying a lot of robots and AI.""Because of that, such tools should be gender neutral, Goldstaub argues. ""We should degender our AI, so it's like a washing machine rather than a Tamagotchi. Things that are meant to stay as tools should stay as tools.""She adds: ""If I was in the room [when the decision that Alexa would be a woman was made], I would have suggested we try some other voices,"" says Goldstaub. ""Clearly that didn't happen.""Algorithmic accountabilityEven if we flood the labs and offices developing AI with women, and in particular with women of color — which we should do — there will still be abuses of this technology as well as unintended consequences. And we need to be able to spot both.That's why some researchers are arguing for algorithmic accountability. As it stands, many machine-learning and AI-based systems are essentially black boxes to end users: put data in, magic happens, and we get an answer. That's problematic when the data being pulled in is demographic, and the output is whether or not to keep an individual in jail pending trial.We need to see how algorithms work to make sure that they do. That could be through companies that make AI systems opening them up to researchers and regulators, or by forcing developers to publish their methods. Others suggest ethics boards that oversee such projects.It also means the rest of us need to understand how AI works — and not see it as dark magic. ""It's not just developers that need to understand — it's also healthcare workers, law enforcement, criminal justice, policy makers. You wouldn't think that they would have to deal with the impacts of AI, but they absolutely will,"" Posner says. ""So demystifying it so the average person knows this is just a math tool, a technology tool, is important.""No single answerSuch a complicated problem requires multiple solutions: we need to encourage more women into tech and AI development, and support them when they get there; companies need to stop conflating women and objects, and remove gender from AI; and we need transparency around the algorithms we use and not be intimidated or confused by them.If we don't get this right, there's a risk beyond the immediate damage: we may refuse to use it all, missing out on the potential benefits. ""The technology itself also has tremendous potential for good and for creating benefits to human society,"" Posner says. ""But we have to make sure that the ability to create with it and shape it is in the hands of as many people as possible that represent the diverse general population.""Related: It’s Not Just Facebook — Google Has Your Info, Too",,,,,,,,,,"Artificial Intelligence Isn't Good for Women, But We Can Fix It",my life,,,,,"https://assets.teenvogue.com/photos/5ac52cc2c917d6234bc2876e/1:1/w_2667,h_2667,c_limit/Tout.jpg","The way artificial intelligence is developed can reinforce biases, sexism, and racism — but there are ways to fix it.",,,,,True,"{'@type': 'CreativeWork', 'name': 'Teen Vogue'}",,,"Artificial intelligence isn't necessarily good for women, but we can make it better.
Because we build and train AI, it reflects our biases and assumptions — and our racism and sexism. That's problematic as AI can be used everywhere: it controls driverless cars and powers voice assistants such as Siri and Alexa, but also helps HR departments sift through resumes, decides who gets parole, and examines medical images. As its uses get more widespread and important, AI missteps and abuses become more dangerous.
If we don't get it right, sexism, racism, and other biases will be literally encoded into our lives, trained on incorrect data that continues to leave women and people of color out of decision making. ""We're ending up coding into our society even more bias, and more misogyny and less opportunity for women,"" says Tabitha Goldstaub, cofounder of AI startup CognitionX. ""We could get transported back to the dark ages, pre-women's lib, if we don't get this right.""
What is AI?
AI is made of up of a myriad of different, related technologies that let computers ""think"" and make decisions, helping us automate tasks. That includes ideas such as neural networks, a machine-learning technique that can be trained on datasets before being set loose to use that knowledge. Show it a bunch of pictures of dogs, and it learns what dogs look like — well, sometimes the machines manage it, other times they can't tell chihuahuas from muffins.
AI is meant to make our lives easier. It's good at filtering information and making quick decisions, but if we build it poorly and train it on biased or false data, it could hurt people.
""A lot of people assume that artificial intelligence… is just correct and it has no errors,"" says Tess Posner, co-founder of AI4All. ""But we know that that’s not true, because there's been a lot of research lately on these examples of being incorrect and biased in ways that amplify or reflect our existing societal biases.""
How AI can hurt
The impacts can be obvious, such as a resume bot favoring male, ""white""-sounding names, but they can also be subtle, says Professor Kathleen Richardson, of the school of computer science and informatics at De Montfort University. ""It's not like we go out into the world and the bank machine doesn't work for us because we're female,"" she says. ""Some things do work for us. It's more just about the priorities that we start to have as a society. Those priorities, for example, often become the priorities of a small elite.""
For example, researchers from the University of Washington have shown how one image-recognition system had gender bias, associating kitchens and shopping with women and sports with men — a man standing at a stove was labeled as a woman. ""The biases that are inherited in our own language and our own society are getting... reflected in these algorithms,"" Posner says.
And those biased labels and data are used to make decisions that impact lives. Goldstaub points to research at Carnegie Mellon University that found Google's recommendation algorithm was more likely to recommend ""high-prestige and high-paying jobs to men rather than to women,"" while separate research from Boston University showed CV-sifting AI put men at the top of the pile for jobs such as programming.
Another example is COMPAS, an AI-based risk assessment tool used across the U.S. to make decisions about how likely a criminal is to reoffend. ""This was shown to be biased against African Americans,"" Posner said. A report by ProPublica showed that COMPAS rated black people as more likely to reoffend than their white counterparts, and as such was ""remarkably unreliable"" at its job of forecasting who would break the law again.
""It's actually showing the same biases that society already has, it's extremely problematic,"" says Posner. ""It's affecting people's lives, whether they're getting parole and what decisions the court is making… it's going to further marginalize certain populations.""
Consider health care, says Goldstaub. ""Men and women have different symptoms when having a heart attack — imagine if you trained an AI to only recognize male symptoms,"" she says. ""You'd have half the population dying from heart attacks unnecessarily."" It's happened before: crash test dummies for cars were designed after men; female drivers were 47% more likely to be seriously hurt in accidents. Regulators only started to require car makers to use dummies based on female bodies in 2011.
""It's a good example of what happens if we don't have diversity in our training sets,"" says Goldstaub. ""When it comes to health care, it's life or death — not getting a job is awful, but health care is life or death.""
Educating women
There's one obvious way to encourage better systems, says Richardson: ""we need more women in robots and AI.""
Right now that's not happening. According to the AAUW, only 26% of computing professionals in the U.S. are women — and there used to be more. Back in the 1990s, more than a third of those working in tech were female. According to Google's own diversity figures in 2017, 31 percent of its workforce is women, but only 20 percent of its technical roles are filled by women. And, only 1 percent of all their tech employees (of any gender) are black; 3 percent are hispanic. For AI in particular, Goldstaub suggests about 13 percent of those working in AI are women.
""I believe as a feminist the more women we can get into roles, the more diverse the output will be — and fewer shockers will get through,"" Goldstaub says.
Thankfully, groups such as AI4ALL have sprung up to help women step into careers in AI by encouraging high school students to take science, technology, engineering, and math (STEM) subjects.  ""When we look at the research about underrepresented populations and why they don’t go into the field, a lot of the research shows this actually stems back in high school at around age 15, which is when folks get discouraged or lose interest in STEM fields,"" says Posner.
Why is that? Posner points to a lack of role models, no exposure to technical subjects or innovation, and a general lack of encouragement. To fight back, AI4All shows high school students — in particular girls, those from low-income families, and from different ethnic backgrounds — the path to an AI career, offering educational camps and mentorships with industry leaders. ""And then we’re supporting you throughout your career path and into your career, if this is the path that you choose,"" she says.
To help push selected students toward creating ethical AI, the camps work on projects under the AI for Good banner, which designs systems specifically for humanitarian causes such as computer vision for hospitals or natural-language processing for disaster relief efforts. ""We've seen that it's actually really effective to teach rigorous AI concepts in the context of societal impact,"" she says.
And some of the projects AI4All students have made have been incredible, Posner says — by not including a diverse range of people in AI development, we not only risk biases but also miss out on better ideas. ""When we give access to more people incredible things happen and things that we could have never imagined before,"" she says. ""That’s why it’s especially critical to let’s not miss out on the potential inventions and talents of all these amazing underutilized groups.""
Being heard
Companies need to remember that there's more to diversity than hiring a token lady for the team, and women shouldn't be made to feel they need to represent their entire gender or race. ""A lot of women go into science and the last thing they talk about is sexism or gender or differences like that,"" Richardson says. ""When they enter these fields, the last thing they want to do is make an issue out of being a woman, if you know what I mean.""
That means it's not just women's responsibility to encourage their female colleagues to feel comfortable speaking up. ""What tends to happen is when the most powerful groups let in other people with less power, is the people with less power go along with the people with the most power,"" Richardson says. ""I've done it myself.""
Simply having women in the room isn't enough; they need to be heard — and often enough, that means we need to stand up and make people listen. ""You have to be brave and courageous to come in and challenge people with authority and power,"" Richardson says.
Degendering AI
One way to make AI less problematic for women is to take gender out of the equation. Alexa and Siri have something in common: they're both clearly female characters and female voices. That's taken further with virtual girlfriends such as Gatebox in Japan — and that's before we start talking about sex robots. But Alexa and Siri are a good place to start.
""What they tend to do is keep reproducing this idea of women as sexual objects to be used, to be appropriated,"" Richardson says, explaining that giving objects female personas cements existing power dynamics. ""Women are expected to give away power, to acknowledge and look after men, to laugh at their jokes, flatter their ego — these kinds of things. So when you've got men then creating models of relationships [with AI assistants], they're creating a model of relationship that is very egocentric, not very neutral… I think that's what's underlying a lot of robots and AI.""
Because of that, such tools should be gender neutral, Goldstaub argues. ""We should degender our AI, so it's like a washing machine rather than a Tamagotchi. Things that are meant to stay as tools should stay as tools.""
She adds: ""If I was in the room [when the decision that Alexa would be a woman was made], I would have suggested we try some other voices,"" says Goldstaub. ""Clearly that didn't happen.""
Algorithmic accountability
Even if we flood the labs and offices developing AI with women, and in particular with women of color — which we should do — there will still be abuses of this technology as well as unintended consequences. And we need to be able to spot both.
That's why some researchers are arguing for algorithmic accountability. As it stands, many machine-learning and AI-based systems are essentially black boxes to end users: put data in, magic happens, and we get an answer. That's problematic when the data being pulled in is demographic, and the output is whether or not to keep an individual in jail pending trial.
We need to see how algorithms work to make sure that they do. That could be through companies that make AI systems opening them up to researchers and regulators, or by forcing developers to publish their methods. Others suggest ethics boards that oversee such projects.
It also means the rest of us need to understand how AI works — and not see it as dark magic. ""It's not just developers that need to understand — it's also healthcare workers, law enforcement, criminal justice, policy makers. You wouldn't think that they would have to deal with the impacts of AI, but they absolutely will,"" Posner says. ""So demystifying it so the average person knows this is just a math tool, a technology tool, is important.""
No single answer
Such a complicated problem requires multiple solutions: we need to encourage more women into tech and AI development, and support them when they get there; companies need to stop conflating women and objects, and remove gender from AI; and we need transparency around the algorithms we use and not be intimidated or confused by them.
If we don't get this right, there's a risk beyond the immediate damage: we may refuse to use it all, missing out on the potential benefits. ""The technology itself also has tremendous potential for good and for creating benefits to human society,"" Posner says. ""But we have to make sure that the ability to create with it and shape it is in the hands of as many people as possible that represent the diverse general population.""
Related: It’s Not Just Facebook — Google Has Your Info, Too",,"[{'@type': 'ListItem', 'position': 1, 'name': 'My Life', 'item': 'https://www.teenvogue.com/my-life'}, {'@type': 'ListItem', 'position': 2, 'name': 'artificial intelligence', 'item': 'https://www.teenvogue.com/tag/artificial-intelligence'}, {'@type': 'ListItem', 'position': 3, 'name': ""Artificial Intelligence Isn't Good for Women, But We Can Fix It""}]"
https://news.google.com/rss/articles/CBMiMGh0dHBzOi8vbmV3c3Jvb20udW5sLmVkdS9hbm5vdW5jZS9jc2UvNzg5My80NTE5NtIBAA?oc=5,Attend AI and Employment interdisciplinary lecture | Announce | University of Nebraska-Lincoln - News | University Communication | Nebraska,2018-04-04,News | University Communication | Nebraska,https://newsroom.unl.edu,N/A,N/A,N/A,N/A,,,,,,,,,,,,N/A,N/A,"
AI and Employment

    On April 4 at 6 p.m., a lecture about artificial intelligence and how it may affect the future of employment, will be delivered by four university professors from the departments of Computer Science, Economics, Law, and Philosophy. This lecture is the first lecture of the Broader Considerations of Technology lecture series. This lecture is tailored for a broad audience including students studying STEM, Arts, Humanities, and the general public.
The development of technology impacts society in many ways. We, as engaged citizens, should be informed about recent developments in technology so we can most effectively shape a more favorable future. This lecture series aims to foster conversation about the development of technology, the ethics therein, and projections for the future. We intend to achieve this by presenting different disciplinary perspectives to best identify the social/societal implications of the development of technologies. 
Event Details:
Lecture Series: Broader Considerations of Technology
Title: Artificial Intelligence and Employment
Date: Wednesday, April 4, 2018 
Time: 6–7:30 p.m.
Location: Avery Hall 106, City Campus
Other: Open to all students and public. Free admittance, no ticket needed.
Reception: 5:30 p.m. Avery Hall 348, refreshments (Pastries, Coffee, etc,) will be served.
For questions or concerns, please reply to this email or contact Vy Doan: vdoan98@gmail.com.    
",,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifmh0dHBzOi8vd3d3LmF6Y29tbWVyY2UuY29tL3RlY2gtY29ubmVjdC90ZWNobm9sb2d5L2pvYmluZ2NvbS1mb3VuZGVyLWZpbmRpbmctc3VjY2Vzcy13aXRoLW5ldy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1wbGF0Zm9ybdIBAA?oc=5,Arizona Innovation Challenge Fall '17: Jobing.com Founder Finding Success with New Artificial-Intelligence Platform - Arizona Commerce Authority,2018-04-05,Arizona Commerce Authority,https://www.azcommerce.com,,,N/A,N/A,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWWh0dHBzOi8vZW5nbGlzaC52YXJ0aGFiaGFyYXRpLmluL2d1aWRlL2V4cGxvcmluZy1vcHBvcnR1bml0aWVzLWluLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNl0gEA?oc=5,Exploring opportunities in Artificial Intelligence - Vartha Bharati,2018-04-05,Vartha Bharati,https://english.varthabharati.in,"In the field of AI, it is important to focus more on improving the skills of the workforce, than be worried about the growing technology.",opportunities in Artificial Intelligence,"In the field of AI, it is important to focus more on improving the skills of the workforce, than be worried about the growing technology.",N/A,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,
