URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,url,image,author,publisher,headline,datePublished,dateModified,articleSection,name,isAccessibleForFree,itemListElement,article:section,article:summary,article text,mainEntity,thumbnailUrl,mainEntityOfPage,@graph,inLanguage,alternativeHeadline,hasPart,comment,commentCount,copyrightHolder,sourceOrganization,copyrightYear,isPartOf,logo,@id,diversityPolicy,ethicsPolicy,masthead,foundingDate,sameAs,articleBody,dateCreated
https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvZ2xlbm5nb3cvMjAyMi8xMC8zMC90aGUtYXJndW1lbnQtZm9yLWFuLWFpLWF1Z21lbnRlZC13b3JrZm9yY2Uv0gEA?oc=5,The Argument For An AI Augmented Workforce - Forbes,2022-10-30,Forbes,https://www.forbes.com,"Companies will apply AI to the work that humans either should not, will not, or can not do, making the workplace safer and more efficient than ever before. ","Artificial Intelligence,Booz Allen Hamilton,Augmented,AI,workforce,safety,labor shortage","Companies will apply AI to the work that humans either should not, will not, or can not do, making the workplace safer and more efficient than ever before. ","Companies will apply AI to the work that humans either should not, will not, or can not do, making the workplace safer and more efficient than ever before. ",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/glenngow/2022/10/30/the-argument-for-an-ai-augmented-workforce/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/specials-images/imageserve/635db3825570dc9d8908fd12/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Glenn Gow', 'url': 'https://www.forbes.com/sites/glenngow/', 'description': 'I am a CEO Coach, a Keynote Speaker on AI, and a Board Member (glenngow.com). Get CEO Coaching by AI at glenngow.com/AI-CEO-Coach', 'sameAs': ['https://www.linkedin.com/in/glenngow/', 'https://www.glenngow.com/the-ai-guy-for-boards/']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",The Argument For An AI Augmented Workforce,2022-10-30T06:00:00-04:00,2022-11-02T00:10:08-04:00,CFO Network,The Argument For An AI Augmented Workforce,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Leadership', 'item': 'https://www.forbes.com/leadership/'}, {'@type': 'ListItem', 'position': 3, 'name': 'CFO Network', 'item': 'https://www.forbes.com/cfo-network/'}]",CFO Network,N/A,"More From ForbesJul 16, 2024,01:45pm EDTPrivate Equity Is Coming For Your AC RepairmanJul 14, 2024,06:00am EDTAI's Double-Edged Sword: Managing Risks While Seizing OpportunitiesJul 11, 2024,05:00am EDTKathy Wagner: A Leader Interested In Vision, Growth, And Fast CarsJul 8, 2024,09:57pm EDTThe Fed’s SVB Like WoesJul 1, 2024,12:09pm EDTWhy Don’t We Ask Companies And Auditors To Disclose Their Materiality Standards?Jun 26, 2024,06:00am EDTThe SEC’s Cyber Disclosure Rules: Lessons Learned So Far In Year OneJun 26, 2024,03:54am EDTEric Kutcher: From Engineering To Finance Leader At McKinseyEdit StoryForbesLeadershipCFO NetworkThe Argument For An AI Augmented WorkforceGlenn GowContributorOpinions expressed by Forbes Contributors are their own.CEO Coach, Keynote Speaker on AI, Board Member. (glenngow.com)FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got it0Oct 30, 2022,06:00am EDTUpdated Nov 2, 2022, 12:10am EDTShare to FacebookShare to TwitterShare to LinkedinA robot augments fast-food workers.Miso Robotics
(This article is part of a series on Artificial Intelligence for Board Members and Senior Executives.)


When discussing artificial intelligence, many people express concern about AI replacing humans in the workforce. Some jobs will undoubtedly be replaced, but that is not necessarily a bad thing.

Early on, companies will apply AI to the work that humans either should not, will not or can not do, making the workplace safer and more efficient than ever before.

Here’s exactly what I mean:

PROMOTED
1. Jobs AI can do that humans should not
Industries like logging, oil rig work, metal foundries, electrical line repair, roofing, and chemical factories are the cause of 2.3 million workplace deaths, 160 million illnesses, and 340 million accidents worldwide, according to the International Labor Organization. This comes at a global cost of $2.8 trillion, and an incalculable personal cost to those who lose loved ones.
It doesn’t have to be this way. The most obvious ways that an AI augmented workforce can help involve automation and robotics like: welding, the application of toxic paints and adhesives, lifting, moving, and stacking heavy items and delivering goods with autonomous vehicles. Artificial intelligence technologies like computer vision and machine learning often enable machines to do these jobs as effectively as humans, without nearly as many safety concerns.
MORE FROMFORBES ADVISORBest Travel Insurance CompaniesByAmy DaniseEditorBest Covid-19 Travel Insurance PlansByAmy DaniseEditor
Looking deeper, AI can reduce workplace hazards even when humans execute the labor. Computer vision systems from companies like Intenseye and HGS monitor warehouses, factory floors, and other work environments to identify objects in the path of forklifts, unstable goods stacking, and even dangerous employee behavior like misuse of protective gear or execution of unauthorized work. Meanwhile, machine learning algorithms use data to predict when accidents are most likely to happen based on time of day, hours an employee has been working, age, experience, and a constellation of other factors too sophisticated for human supervisors to parse alone.









CxO
US


CEO: C-suite news, analysis, and advice for top decision makers right to your inbox.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the CEO newsletter!


                More Newsletters
            


You’re all set! Enjoy the CEO newsletter!

                More Newsletters
            



AI can even be used in training to proactively reduce accidents. Companies like Taqtile, Atheer, and Ario already use immersive technologies like VR and AR to prepare front line workers for difficult and dangerous roles. Consulting firm Booz Allen Hamilton predicts that AI solutions like natural language processing, artificial neural networks, and deep learning will soon be used to accustom workers to random but realistic workplace hazards so that they will be better prepared if such unpredictable events occur on the job in real life.


1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGE
2. Jobs AI can do that humans will not
How about jobs that humans simply prefer not to do? I am talking about repetitive, boring, and low-paying roles that employers find difficult to keep filled. Industries including agriculture, food service, durable goods manufacturing, and retail are facing an unprecedented labor shortage that some are predicting will last for decades.
AI-driven chatbots, perhaps one of the best-known labor shortage solutions out there, are at the tip of the spear. By removing the most tedious and predictable tasks from human customer service representatives, chatbots reduce not only the number of warm bodies needed at a call center, but also the fatigue and frustration that phone representatives feel answering the same mundane questions over and over again.
Likewise, machine learning in scheduling applications like those from Zira, Celayix, and Rotageek help allocate the right retail staff to the right locations at the right time, reducing inefficiencies and making the most of a thinly-stretched retail workforce. Hyper Food Robotics goes one further, with a completely autonomous fast food store that cuts human labor requirements down to virtually zero.
AI, together with robotics and automation, can play a crucial role in reviving American manufacturing despite the labor shortage. Canon’s 3D Machine Vision system, for example, uses AI to execute human-like fine motor skill activities. This also extends to food production: Carbon Robotics produces an autonomous weeding machine which uses computer vision to distinguish weeds from crops and destroys them with a precision laser.
3. Jobs AI can do that humans can not
There is one more area where AI in the workforce can add tremendous value—jobs for which human beings lack the time, ability or the mental acuity to do. Imagine examining millions of business transactions or job candidate resumes, for instance, to identify insights that correlate to success. That’s incredibly complex and time-consuming, and now—unnecessary.
AI-powered supply chain startups like Quantiful and Remi and LeanDNA can help manufacturers, wholesalers, and retailers decide what products to order when, what new products to develop, how to position and price them, and what volume of demand they should expect, based on diverse data such as: historical sales, economic conditions, seasonal trends, and product lifecycle. Meanwhile, HR software vendors like Praisidio, Eightfold and Findem use AI to source, evaluate, interview, and retain employees.
The list goes on. Imagine analyzing thousands of customer accounts to identify and prevent fraud. Imagine designing industrial products with every possible machine stressor, environmental condition, and human error in mind. Or imagine trying to analyze the demographics and buying behavior of millions of consumers to discover micro markets and personalize marketing offers.
Whatever complex set of business conditions you can imagine, it is more likely than not that someone is working on an AI solution that can provide far more speed, accuracy, and nuance than any human could ever do alone.
There is a true groundswell of innovative AI workforce solutions coming to reshape the business landscape. Instead of worrying about AI replacing humans in the workforce, we should celebrate the potential of artificial intelligence to relieve us of undesirable work and empower us to do more meaningful work than we are capable of on our own.
If you care about how AI is determining the winners and losers in business, and how you can leverage AI for the benefit of your organization, I encourage you to stay tuned. I write (almost) exclusively about how senior executives, board members, and other business leaders can use AI effectively. You can read past articles and be notified of new ones by clicking the “follow” button here.
Follow me on LinkedIn. Check out my website. Glenn GowFollowingFollowI am a CEO Coach, a Keynote Speaker on AI, and a Board Member (glenngow.com). Get CEO Coaching by AI at glenngow.com/AI-CEO-CoachEditorial StandardsPrintReprints & PermissionsJoin The ConversationComments 0One Community. Many Voices. Create a free account to share your thoughts. Read our community guidelines  here.Forbes Community GuidelinesOur community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space.In order to do so, please follow the posting rules in our site's Terms of Service.  We've summarized some of those key rules below. Simply put, keep it civil.Your post will be rejected if we notice that it seems to contain:False or intentionally out-of-context or misleading informationSpamInsults, profanity, incoherent, obscene or inflammatory language or threats of any kindAttacks on the identity of other commenters or the article's authorContent that otherwise violates our site's terms.User accounts will be blocked if we notice or believe that users are engaged in:Continuous attempts to re-post comments that have been previously moderated/rejectedRacist, sexist, homophobic or other discriminatory commentsAttempts or tactics that put the site security at riskActions that otherwise violate our site's terms.So, how can you be a power user?Stay on topic and share your insightsFeel free to be clear and thoughtful to get your point across‘Like’ or ‘Dislike’ to show your point of view.Protect your community.Use the report tool to alert us when someone breaks the rules.Thanks for reading our community guidelines.  Please read the full list of posting rules found in our site's Terms of Service.",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiYWh0dHBzOi8vbWl0c2xvYW4ubWl0LmVkdS9pZGVhcy1tYWRlLXRvLW1hdHRlci9uZXctYm9vay1leHBsb3Jlcy1ob3ctYWktcmVhbGx5LWNoYW5nZXMtd2F5LXdlLXdvcmvSAQA?oc=5,New book explores how AI really changes the way we work - MIT Sloan News,2022-10-31,MIT Sloan News,https://mitsloan.mit.edu,"A new book shows that successful firms are using artificial intelligence to complement human workers, not replace them.",N/A,"A new book shows that successful firms are using artificial intelligence to complement human workers, not replace them.","A new book shows that successful firms are using artificial intelligence to complement human workers, not replace them.",,,,,,,,,,,,,,N/A,N/A,"
 










recent




Jul 18, 2024
The bottom-line benefits of second chance hiring




Jul 17, 2024
Federal spending was responsible for the 2022 spike in inflation




Jul 16, 2024
Making generative AI work in the enterprise: New from MIT SMR


















Credit: FGC / Shutterstock















Ideas Made to Matter


                        Artificial Intelligence


New book explores how AI really changes the way we work

By
Brian Eastwood

Oct 31, 2022






Why It Matters		



A new book shows that successful firms are using artificial intelligence to complement human workers, not replace them. 



















Share













































































facebook
X
linkedin
email
print

open share links
close share links

One of the earliest predictions about artificial intelligence was that robots would steal jobs from people, particularly for seemingly monotonous and menial tasks.
Case in point: Flippy 2, the “autonomous robotic kitchen assistant” developed by Miso Robotics to turn hamburgers as they cook. The restaurant chain CaliBurger is testing Flippy at various locations. But what the robot is best at doing isn’t what it was designed to do.



Work smart with our Thinking Forward newsletterInsights from MIT experts, delivered every Tuesday morning.
Email Address









Leave this field blank





“[A CaliBurger franchise owner] said, ‘Flippy isn’t actually as good as I might have liked at flipping hamburgers, but it’s pretty good at pulling baskets of French fries out when they’re done,’” said Thomas Davenport, a digital fellow at the MIT Initiative on the Digital Economy. “We've known for a while that AI and robots might take the jobs of burger flippers — but it hasn't happened yet.”
Davenport’s new book, “Working With AI: Real Stories of Human-Machine Collaboration,” looks at 29 examples of humans working with AI-enabled systems, showing when and how AI works best and how companies can use AI responsibly. The book is co-authored by Singapore Management University professor Steven M. Miller.
The common theme is that AI is augmenting the work that humans do, not fully automating it. That means many jobs are safe, Davenport said during a recent webinar hosted by the MIT Sloan Management Review.
“One of our major conclusions is [augmentation] doesn’t appear to result in job loss,” Davenport said.
Davenport and Miller outlined how companies are implementing AI to the best advantage:
AI works best in “normal” conditions
Miller defined automation as a process in which a machine completes enough tasks that the human who was previously doing them is effectively displaced. Augmentation, on the other hand, is the automation of smaller tasks, not an entire job.
“The human stays involved in direct execution,” Miller said.
This distinction is important, and it helps to explain scenarios when organizations would look to use one or the other.
“AI is great at operating a machine under what we think of as normal conditions. Under expected circumstances, AI is really good at keeping things going,” Davenport said.
One example is looking for keywords in lengthy documents such as legal contracts or health records and producing results in seconds. Another is monitoring information from thousands or even millions of sources, whether sensors on a manufacturing floor or cybersecurity monitoring tools. These types of processes can be labor-intensive for humans but are easy for machines.
“Full automation works really well when you're in familiar territories and when the nature of variance and the nature of the unexpected is somewhat familiar. You can really drive for that efficiency, consistency, optimization, and productivity,” Miller said.
Of course, conditions are rarely normal enough for AI to run without human intervention. That’s why Flippy needs an experienced cook to know which part of the grill is the hot spot that cooks burgers faster. That’s why the robotic weed-puller developed by agtech company FarmWise still needs a farmer walking behind it to make sure the plant it’s clutching isn’t a vegetable. (Though this still beats the historic technique of spraying dangerous chemicals over the entire crop to kill the weeds.)
That’s why autonomous vehicles work well in the dry air of Las Vegas or Phoenix but haven’t made their way to rainy or snowy climates.
“I dread the day that they come to Boston in the middle of the winter,” Davenport said.
4 situations when AI augmentation makes sense
The concept of augmentation is hardly new. Humans started using stone tools thousands of years ago. It’s also familiar to knowledge workers who no longer need a calculator on their desks.
“Anybody who’s used a spreadsheet understands the power of augmentation,” Miller said.
Still, augmentation enabled by AI is often underappreciated in today’s enterprises.
“There are more opportunities than people assume,” Miller said.
Davenport and Miller provided examples that fit into four broad and overlapping categories:
When companies want to experiment. Product development is especially expensive in the pharmaceutical industry, where the median R&D cost for new drugs is now north of $1 billion. Instead of spending up to a year working on a drug only to realize that it’s not going to work, pharma manufacturers are creating AI infrastructure to assess their data and evaluate potential use cases for a new drug in as little as two weeks, Davenport said. Not only does this support frontline workers, it also leads to new “data product” roles to manage these AI use cases.
When there’s a lot of pencil-pushing. “Every businessperson we meet, and any company we talk to, almost always has the same lament: ‘We have so much to do, and we don't have the headcount to do it.’ At the same time, they complain about how much grunt work they have to do,” Miller said. In these situations, he added, automation will free up employees to do more of the experimental work augmented by AI, from product development to predictive analytics.
When things are happening quickly. Cybersecurity monitoring software can assess threats from millions of sources in real time. Some tools can also prioritize threats and even automatically respond to them — by shutting down a compromised device, for example. But in “complicated and nuanced cases,” Miller said it takes an experienced security analyst to assess all the data collected and determine whether something is a one-off incident or an indication of a pattern of attacks that may require mitigation on a larger scale.
When workers want a recommendation. When consumers put their trust in health care professionals, augmentation acts as form of decision support. For example, AI looks at the available information and makes a recommendation, but the final decision is up to the doctor in front of the computer. In countries such as China and Indonesia, where hundreds of millions of patients have access to AI-powered diagnostic tools such as Ping An Good Doctor, “A physician actually has to produce the diagnosis and the treatment, or else it’s not valid,” Davenport said. “But the AI system does save the doctor a lot of time and effort.”
How to work with AI in responsible ways 
Moving ahead with AI augmentation means accepting that machines are better than humans at some tasks.
Again, this concept isn’t new. “We’ve always had super tools. We have jet airplanes, we have cars, we have power shovels and cranes,” Miller said. “Now we have super tools that can do non-routine, cognitive-oriented things.”
Related Articles


Measuring the actual impact of robots on jobs


Real-world use cases for artificial intelligence


The lure of 'so-so technology,' and how to avoid it


At the same time, machines can’t bring in all the dimensions of intelligence. Citing the theory of adaptive intelligence from Robert J. Sternberg of Cornell University, Miller said the human mind is particularly suited for creativity, wisdom, and context — not just realizing that something can be done, but whether it should be done. In other words, don’t underestimate what humans can do.
The most effective and responsible use cases for AI augmentation are more than rudimentary tasks or job automation. The goal should be to bring humans and machines together to achieve a level of intelligence higher than what a human or a machine could achieve on its own, Miller said. The whole should be greater than the sum of its parts.
In doing this, enterprises can expect to create new types of products and services, as well as new types of work. But this comes with two clear caveats: Workers need the right skills to use AI systems, and they need to see that AI is helping them and not going to replace them.
“We really need to step up and help people adapt to some of these new tools,” Davenport said. “We need to retrain workers and engage them in this new type of work.”
Watch the webinar
Read next: Sec’s Gary Gensler on how AI is changing finance









































For more info
Sara Brown
Senior News Editor and Writer


sbrown1@mit.edu






Related Articles












Ideas Made to Matter

Making generative AI work in the enterprise: New from MIT SMR












Ideas Made to Matter

How to tap AI’s potential while avoiding its pitfalls 












Ideas Made to Matter

How can we preserve human ability in the age of machines?



",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiMmh0dHBzOi8vd3d3Lm5hdHVyZS5jb20vYXJ0aWNsZXMvczQzODU2LTAyMi0wMDE5OS0w0gEA?oc=5,Artificial intelligence and machine learning in cancer imaging | Communications Medicine - Nature.com,2022-10-27,Nature.com,https://www.nature.com,"An increasing array of tools is being developed using artificial intelligence (AI) and machine learning (ML) for cancer imaging. The development of an optimal tool requires multidisciplinary engagement to ensure that the appropriate use case is met, as well as to undertake robust development and testing prior to its adoption into healthcare systems. This multidisciplinary review highlights key developments in the field. We discuss the challenges and opportunities of AI and ML in cancer imaging; considerations for the development of algorithms into tools that can be widely used and disseminated; and the development of the ecosystem needed to promote growth of AI and ML in cancer imaging. Koh, Papanikolaou et al. discuss the application of artificial intelligence in cancer imaging. The authors highlight opportunities for exploiting machine learning algorithms in this field, and outline barriers in their implementation and how these might be addressed.",N/A,"Koh, Papanikolaou et al. discuss the application of artificial intelligence in cancer imaging. The authors highlight opportunities for exploiting machine learning algorithms in this field, and outline barriers in their implementation and how these might be addressed.","Communications Medicine - Koh, Papanikolaou et al. discuss the application of artificial intelligence in cancer imaging. The authors highlight opportunities for exploiting machine learning...",https://schema.org,WebPage,,,,,,,,,,,,N/A,N/A,"




Download PDF








Review Article

Open access

Published: 27 October 2022

Artificial intelligence and machine learning in cancer imaging
Dow-Mu Koh 
            ORCID: orcid.org/0000-0001-7654-80111 na1, Nickolas Papanikolaou 
            ORCID: orcid.org/0000-0003-3298-20721,2 na1, Ulrich Bick 
            ORCID: orcid.org/0000-0002-7254-85723, Rowland Illing4, Charles E. Kahn Jr. 
            ORCID: orcid.org/0000-0002-6654-74345, Jayshree Kalpathi-Cramer6, Celso Matos2, Luis Martí-Bonmatí7, Anne Miles8, Seong Ki Mun 
            ORCID: orcid.org/0000-0001-9661-79189, Sandy Napel 
            ORCID: orcid.org/0000-0002-6876-550710, Andrea Rockall 
            ORCID: orcid.org/0000-0001-8270-559711, Evis Sala 
            ORCID: orcid.org/0000-0002-5518-936012, Nicola Strickland11 & …Fred Prior 
            ORCID: orcid.org/0000-0002-6314-568313 Show authors

Communications Medicine
volume 2, Article number: 133 (2022)
            Cite this article




32k Accesses


79 Citations


82 Altmetric


Metrics details






AbstractAn increasing array of tools is being developed using artificial intelligence (AI) and machine learning (ML) for cancer imaging. The development of an optimal tool requires multidisciplinary engagement to ensure that the appropriate use case is met, as well as to undertake robust development and testing prior to its adoption into healthcare systems. This multidisciplinary review highlights key developments in the field. We discuss the challenges and opportunities of AI and ML in cancer imaging; considerations for the development of algorithms into tools that can be widely used and disseminated; and the development of the ecosystem needed to promote growth of AI and ML in cancer imaging.



Similar content being viewed by others






Predicting cancer outcomes with radiomics and artificial intelligence in radiology
                                        


Article
18 October 2021









Designing clinically translatable artificial intelligence systems for high-dimensional medical imaging
                                        


Article
16 November 2021









Machine intelligence in non-invasive endocrine cancer diagnostics
                                        


Article
09 November 2021








IntroductionArtificial intelligence (AI) and machine learning (ML) are rapidly transforming the scientific landscape, including many domains in medicine. AI refers to the creation of machines or tools that can simulate human thinking and behaviour, whereas ML is a subset of AI in which machine or tools learn from data to make classifications or prediction either with or without human supervision1. The advancement in these fields in recent years has been accelerated by the emergence of high performance computers.In medicine, digitised domains, such as imaging, lend themselves to become early adopters of AI and ML. The imaging pipeline from image acquisition, reconstruction, interpretation, reporting and the communication of results is operated within the digital space, allowing such data to be effectively captured for AI and ML. In particular, as cancer imaging represents a substantial proportion of the work in many departments, it is an area where early exploration and adoption of these technologies by radiologists as primary users appear likely. This is especially the case since these tasks can be repetitive (such as in cancer screening, where readers need to sieve through a large volume of normal studies to identify abnormalities), tedious (such as serial tumour measurements) and burdensome (such as outlining tumours for disease segmentation). Indeed, there are already a number of extant commercial products in the cancer imaging space, with the aim of improving work efficiency, reducing errors, and enhancing diagnostic performance.Many technological solutions are being developed in isolation, however, which may struggle to achieve routine clinical use. These may have been hampered by the limited opportunities for clinicians, radiologists, scientists, and other experts to interact collectively to understand the clinical and data science landscape; to identify the needs, risks, opportunities and challenges for the development, testing, validation and adoption of such tools. This requires the nurturing of multidisciplinary ecosystems collectively, including commercial partners as appropriate, to drive innovations and developments.This review aims to foster interdisciplinary communication on the above issues. We outline relevant AI and ML techniques and highlight key opportunities for implementing AI and ML in cancer imaging. The clinical, professional and technical challenges of implementing AI and ML in cancer imaging are discussed. We draw upon lessons learnt from the past, and take a forward look into the technical and infrastructural developments that are needed to facilitate AI in cancer imaging, enabling the integration of AI and ML technologies into hospital systems and the appropriate training of the future workforce.The medical image as imaging data: RadiomicsMedical images are still largely evaluated by expert radiologists, who are able to visually assess the absence or presence of disease, delineate the boundaries of tumours, evaluate tumour response to treatment and identify disease relapse. These human skills are generally used to define the reference standards against which AI and ML techniques are evaluated. However, there is increasing interest in exploring the smaller subunits that make up medical images (pixels/voxels) as imaging data, which lend themselves to analysis by computers to discover objective mathematical features that may be linked to disease behaviour or outcomes.Radiomics is the computerized analysis of medical images, or regions within medical images2. The images can be multidimensional, e.g., 2D X-ray, 3D computed tomography (CT), 4D ultrasound; and scalar-, e.g., CT, where the CT value is directly related to the tissue electron density, or vector-valued, e.g. phase-contrast magnetic resonance imaging (MRI), where the measured MRI signal is related to a mathematical vector function. The main goal in radiomics is to utilize algorithms that can identify patterns within images—usually beyond those that the human eye can perceive—and to exploit them to make predictions and therefore aid the clinical decision-making process. The computerized processing of images usually leads to a large number of imaging features. However, it is the non-redundant, stable and relevant features that are selected to develop a mathematical model that will answer the relevant clinical question, the so-called ground truth variable. Figure 1 illustrates the selection and testing of radiomics features to determine their ability, in a specific use-case, to distinguish between benign and malignant breast lesions. As a further extension, radiogenomics approaches, which integrate both radiomics and genomics analyses, are being developed to provide integrated diagnostics to aid disease management3,4.Fig. 1: Feature selection for radiomics.In this illustration, a model classifier is shown to differentiate benign from malignant breast lesions on imaging. Initially, a large number of radiomic features were computed and after removing the highly correlated features, the zero and near-zero variance features; a recursive feature elimination and reduction method was applied. The model performance illustrated here identifies11 features to be at the saturation point. The red curve (left) is showing accuracy versus number of features, while the blue curve (right) represents the model’s error function over the number of features. In this example, using 11 imaging features shows high accuracy while minimising the error function.Full size imageAnother example of a data set for radiomics analysis is a volumetric chest CT scan containing a tumour (e.g. a lung nodule), and a typical workflow could include: (1) identification of the tumour within the scan; (2) annotation of the tumour with semantic features (usually by expert radiologists)5; (3) outlining or segmentation of the tumour6; (4) computation of pre-determined tumour features (e.g. size, mean intensity, image texture, shape, margin sharpness)7,8,9 and/or using automated learning for task-relevant features; and (5) building a classifier that uses the computed features to predict a clinical state, e.g., probability of a specific gene mutation, response to therapy or overall survival10,11.Several groups are building radiomics processing tools to facilitate pipeline data analysis. At Stanford, the Quantitative Image Feature Pipeline12 has been developed, which contains an expandable library of quantitative imaging feature extraction and predictive modelling algorithms, capable of comprehensive characterization of the imaging phenotype, and cloud-based software for creating and executing quantitative image feature-generating and predictive pipelines, and for using and comparing image features to predict clinical and molecular features. It also allows users to upload their own algorithms as Docker containers13, and to configure them in a customizable workflow (Fig. 2).Fig. 2: Quantitative Imaging Feature Pipeline.This shows an example of the quantitative imaging feature pipeline (QIFP) used to process a positron emission tomography (PET) imaging cohort stored on a local network ePAD server. The box next to the “modify workflow” button is a selection button, which has been set to choose the workflow displayed. This workflow moves the image data into Stanford’s Quantitative Image Feature Engine (QIFE)64, which computes thousands of image features for each segmented tumour in the cohort, followed by a sparse regression modeler (LASSO TRAIN) that derives an association between a linear combination of a small number of image features to 5-year survival, and finally tests that model in an unseen cohort and produces an ROC curve displaying the accuracy of the association. Other workflows can be chosen that use one or more of the existing tools stored on the QIFP system.Full size imageAI and ML techniques in cancer imagingIn cancer imaging, images acquired from patients are pre-processed and transformed (to ensure data conformity or uniformity) as inputs to develop ML algorithms and models. Such pre-processing steps are used whether they relate to radiologist-defined features or mathematically derived radiomics features. This involves ensuring that the images are of similar image section thickness and of similar pixel-dimensions. As an overview, an ML model or algorithm maps the input imaging data and learns a simple or complex mathematic function that is linked to the target or output, such as a clinical or scientific observation. An ML algorithm can be established or trained with or without the use of so-called ground truth variables, which are reference findings verified by domain experts or by other means (e.g. pathology, laboratory tests, clinical follow-up). ML algorithms are usually developed using a training dataset, refined using a validation dataset, and then tested for their performance in an independent test dataset, ideally from a different institution.Some types of ML models are more widely used than others in imaging studies. As a simplistic discussion, (assuming that x is the input variable, f the mathematic function and y the target/output variable), the most common form is the predictive model, where one tries to predict y by learning the f(x). In exploratory models, one may simply attempt to link the input data x (e.g. an imaging feature) with the output y (e.g. gene expression).When working with continuous variables, regression models, such as Linear, Cox (Proportional Hazards), Regression Trees, Lasso, Ridge, ElasticNet, or others can be used14,15. As for discrete variables, classification models such as Naïve Bays, Support Vector Machines, Decision Trees, Random Forests, KNN (k-nearest neighbours), Generalized Linear Models, Bagging and others can be used16. These models can inform cancer diagnosis, disease characterization and stratification, treatment response or disease outcomes17.The success of any ML algorithm is influenced by data availability, machine computational power and subsequent algorithm refinements. The choice of ML algorithm may depend on data size. With smaller datasets (e.g. <1000 patients/examinations/images depending on use case), classical ML algorithms, such as Naïve Bayes, logistic regression, decision trees and support vector machines, are often applied. With larger datasets, more complex ML models, such as convolutional neural networks (CNN) that are very efficient in learning directly from images, may be preferable, although such models are more demanding in terms of computational power. CNN represent a type of deep learning, a subset of ML methods based on artificial neural networks. Artificial neural networks are inspired by the organization of neurons in the brain, simulating the connectivity of neurons to solve problems. ML algorithms can be supervised (i.e. the algorithm is developed using data that are labelled with some type of ‘correct answer’) at one end of the spectrum, or unsupervised (i.e. the algorithm uses the data to discover information by itself) at the other. The latter are associated with more complex CNN algorithms, which are able to discover patterns within imaging data without human intervention.The driving force of CNNs has emerged from the computer vision domain, where the large dataset of ImageNet18 (a library of labelled photographic images) and the interest by internet developers to identify objects automatically on photographic pictures led to the development of very efficient ML architectures (e.g. Inception V3, AlexNet, VGG-16 and 19); Some of these have shown value for medical applications using a method called transfer learning3, where a pretrained architecture trained using ImageNet is then applied to medical imaging and fine-tuned for the specific use case.In ML-based cancer imaging, it is not unusual for the number of predictors (e.g. CNN-derived features) to outweigh the number of data points or samples (patients or imaging studies). The latter results in model overfitting, where the model is optimized for the training dataset but consequently performs poorly on the test dataset. The most common strategies to reduce or prevent overfitting include: (a) to use techniques such as k-fold cross-validation by using multiple sub-samples of the dataset; (b) to train the algorithm with more data, where possible; (c) to perform feature selection, as appropriate, to reduce the dimensionality/number of the initial features; and/or (d) to implement ensemble learning, where feasible, to increase data size, that is to undertake algorithm training at multiple sites/institutions. Although an increasing number of healthcare organizations are moving to the cloud and centralized facilities to host and exploit their data, there is still resistance to data sharing and the need to protect patient privacy. These issues have fuelled distributed or federated learning approaches19,20. In federated learning, instead of collecting all data to a centralized repository, the models are circulated to different institutions and the models trained using local data at each site, sharing only the so called weights of a model between institutions. There is now also significant interest in the explainability21 and interpretability of algorithms to increase their trustworthiness. Clinical users may be less interested in the inner mechanics of ML models but would like to understand the way a model generates its output or prediction at a patient cohort level, as well as at an individual patient level.Clinical opportunities for AI/ML in cancer imagingMachine learning can be harnessed in multiple ways to advance and improve cancer imaging. Figure 3 illustrates the typical clinical journey of a patient with cancer and highlights some of the key aspects of imaging where AI systems could exert a positive impact22. Here, we outline some of these in more detail.Fig. 3: Potential use cases for artificial intelligence (AI) and machine learning (ML) in cancer imaging in relation to a patient’s cancer journey.A typical asymptomatic patient eventually develops cancer presenting symptoms, which usually leads to the cancer diagnosis. Following appropriate disease staging, cancer treatment commences, which can lead to good response or even cure. However, some patients will relapse or progress on treatment for which additional treatment may be administered. Unfortunately, some patients will succumb to their disease. The potential uses for Imaging AI and ML are as shown at various stages of the cancer journey and discussed in the text.Full size image
Risk assessment
The optimal use of cancer imaging technologies requires that we direct resources to patients at greatest risk. In the US, many states require assessment of breast density to assess risk for developing cancer. A deep learning system has shown high accuracy in classifying breast density, and such systems will help support consistent density notification to patients in breast cancer screening23,24. This is particularly valuable since visual breast density measurement has been shown to be associated with considerable interobserver variations (6–85%)25. The use of AI-based approaches can improve upon current risk models. For example, a deep learning model that incorporated mammographic features and traditional risk factors to determine those at greatest risk for malignancy performed more effectively than conventional breast cancer risk models alone26,27. More recently, very good agreement was reported for breast cancer risk evaluation using mammographic breast density determined by a senior radiologist, a junior radiologist and an AI software28.
Cancer screening and cancer detection
Cancer screening has been a highly active area of AI research. AI algorithms have been tested in diseases with active screening programmes such as lung cancer29,30,31 and breast cancer32,33,34,35,36. In breast cancer, some studies have shown that AI algorithms can equal the performance of expert readers36, be used as a second reader for screening mammographic reviews33, provide triaging for prioritizing image reading34 and have been found to be acceptable to women undergoing mammographic screening37. However, real-world evidence is still insufficient to recommend the wide adoption of AI-based tools for breast screening38. In addition to systematic screening, opportunistic screening (the detection of abnormalities in exams obtained for other purposes) may create possibilities to detect other cancers, especially where directed screening tests would be impractical or cost ineffective. For example, in patients undergoing low-dose CT for lung cancer screening, it is possible to use the same images to assess breast cancer risk by assessing the breast density on CT39.
AI systems are now available for the detection of pulmonary nodues31, which also includes nodule classification, nodule measurement and malignancy prediction. When radiologists used a deep learning model for detection and management of pulmonary nodules, their performance improved and reading time was reduced40. Undoubtedly, the use case for AI in cancer detection will widen to include other tumour types.
Diagnosis and classification
ML systems provide ways to improve classification of imaging findings related to cancer. Malignant brain tumours have different aetiologies and prognosis, but tissue sampling is invasive and may not provide accurate characterization due to disease heterogeneity. Studies have shown the potential of AI to identify and classify major intracranial tumours, which include variously high grade gliomas, low grade gliomas, cerebral metastases, meningiomas, pituitary adenoma and acoustic neuromas, as well as differentiating these from normal tissues41,42,43,44. Another developing application in this area is the classification of cystic lesions of the pancreas, since distinguishing between intraductal papillary mucinous neoplasms, mucinous cystic neoplasm and serous cystic neoplasms of the pancreas can be visually challenging45,46,47, and these conditions are associated with different outcomes.
Treatment response prediction
Radiomics with machine learning have been used to predict the response and outcomes of disease to treatment. Some examples of these include predicting the response of nasopharyngeal carcinoma to intensity-modulated radiation therapy48, the response of non-small cell lung cancer to neoadjuvant chemotherapy49, as well as the response to neoadjuvant treatment of rectal50,51,52, oesophageal53,54 and breast cancers55,56. Although highly promising, radiomics has not yielded widely generalizable results, thus limiting its current role and implementation in clinical practice.
Radiology-Pathology correlation
Matching radiology data to pathology report information is important for education, quality improvement, and patient care. Using natural language processing techniques, it is possible to mine text-based radiology57 and pathology58 report for key findings to cohort-specific populations for further investigative scrutiny. A system for natural language processing has been shown to classify free-text pathology reports (at an organ-level) to support a radiology follow-up tracking engine59, which can be used to alert radiologists to potential misses at study follow-ups. There also are opportunities to integrate anatomical pathology images with corresponding radiological images60,61.
Disease segmentation
The outlining of disease, or segmentation, is fundamental to many AI/ML and radiomics studies, and is necessary to derive quantitative tumour measurements including tumour diameters, as well as generating tumour contours for radiotherapy planning62,63,64. Registration of segmentations across time-series can also inform clinicians on how tumours are changing with treatment. Manual tracing of lesion borders can lead to high inter-reader variability65, which may be reduced with automatic disease segmentation using AI models. Although deep neural networks are powerful enough to segment lesions, it is recommended that the final AI segmentation result should be verified by an experienced radiologist.
Segmentation algorithms are relatively well developed for certain image and disease types, probably due to the power of deep learning methods which have shown to be very efficient when sufficient data are available. A segmentation problem is a classification problem at the voxel level (a voxel being the smallest unit that makes up the image, determined by the image section thickness and the spatial resolution at which the image is acquired), and given the fact that lesions or whole organs are comprised of hundreds if not thousands of voxels, the density of the data is much higher compared with the classification problem usually considered at a per-patient level (e.g. radiomics). From the segmentation of the disease, radiomic features can be computed from the entire tumour, but a more sophisticated approach is to extract radiomic features from physiologically distinct regions (e.g. based on blood flow, cell density, necrosis) within tumours inferred by their imaging characteristics known as habitats66,67 (Fig. 4).Fig. 4: Machine Learning (ML) in a radiomics pipeline for evaluating tumour habitats.a Whole tumour segmentation and identification of physiologically different regions by means of tissue-specific sub-segmentation on computed tomograhy (CT) imaging (e.g. using 3D volume rendering of tissue components with colour codes shown below). This is followed by b voxel-based radiomic feature map extraction and unsupervised clustering for tumour habitats considering the most clinically relevant region. Next, c quantitative measurements and inferred tumoural heterogeneity metrics are processed by ML predictive models to yield diagnostic and prognostic results. In this example, we have used CT images from a patient with metastatic ovarian cancer with a representative omental lesion.Full size image
Organs-at-risk segmentation
The principle of radiotherapy is to inflict maximum damage to tumours while sparing normal tissues. However, normal tissues and organs often lie in close proximity to tumours, such that they are considered as organs-at-risk to the potentially detrimental scattering effects of radiotherapy. Organs-at-risk segmentation is necessary in radiotherapy to monitor and minimize radiotherapy damage to adjacent normal tissues. For example, when treating pelvic cancers68,69, organs-at-risk segmentation includes the outlining of the normal urinary bladder, bowel loops, rectum and both hip joints. ML has also been successfully applied in organs-at-risk segmentation for radiotherapy planning in head and neck cancers70,71, breast cancers72 and non-small cell lung cancer73,74.
Imaging optimization
One of the growing applications for AI and ML in imaging, not limited to cancer imaging, is their use for imaging optimization. For example, in MRI, the examination time of an oncological body examination can take 30–60 min. AI and ML techniques are increasingly applied to accelerate image acquisition and/or image reconstructions (i.e. making the examination faster)75; as well as to improve image quality (e.g. creating so-called super-resolution MRI images)76. The ability to shorten MRI examination time without sacrificing image quality can improve patient throughput to address bottlenecks in MRI capacity across health systems.
Others
Natural language processing is also being investigated as a tool to generate automated reports, and as a means of reducing repetitive tasks by radiologists77. For the clinicians receiving the radiology report, natural language processing can also potentially be used a communication tool to alert clinicians to actionable reports, so that critical findings can be highlighted to referrers in a timely fashion78.
The current relative success of AI and ML in the different use cases discussed above is dependent on the complexity of the undertaking, data quality and availability, the sophistication of the mathematical models and the subsequent real-world testing of the algorithms. Many of these use cases are active areas of research and development. However, algorithms that are being developed and tested may fail to translate into meaningful clinical tools. It is therefore important to understand the challenges and barriers that need to be addressed to enable the implementation of AI and ML in cancer imaging.
Challenges for implementation of AI/ML in cancer imagingWhile there are significant opportunities for the development of AI and ML in cancer imaging, there are also challenges to address. Below, we discuss some of the important clinical, professional, and technical challenges that will be encountered in the translation of useful mathematical algorithms into wider clinical practice for patient benefit.Clinical challengesOne of most important considerations for the development of an AI or ML tool is that it should address a vital clinical challenge or question. As such, developers should have full appreciation of the clinical context and the implementation environment in which the AI tool is anticipated to operate. This will often require involving clinicians in the development of the tool.The clinical domain is characterized by data inflow from different sources. The amount of biomedical data generated in the clinic is increasing due to advances in multi-modal imaging (i.e. imaging using a variety of techniques), high-throughput technologies for multi-omics (e.g. genomics, proteomics and molecular pathology), as well as an increasing amount of data stored within electronic health records. Hence, multidisciplinary engagement is critical to success. This complex and diverse information can potentially be integrated using AI and ML to support personalised medicine79. However, such large-scale datasets pose new challenges for data-driven and model-based computational methods to yield meaningful results.AI has the potential to revolutionise cancer image analysis by applying sophisticated ML and computational intelligence. Cutting-edge AI methods can enable the shift from organisation-centric (based on organisational pathways) to patient-centric organization of healthcare, which may improve clinical outcomes and also potentially reduce healthcare costs80 by uncovering better individualized solutions. In addition, computerised oncological image analysis is encouraging the transition from largely qualitative image interpretation to quantitative assessment through automated methods aimed at earlier detection and enhanced lesion characterisation81, and the provision of better decision support tools. Within such a paradigm, there are important challenges that require better AI and ML solutions to solve. These include the need for reproducible and reliable tumour segmentation; accurate computer-assisted diagnosis; and clinically useful prognostic and predictive biomarkers with good performance. A particular challenge will be the quantification and monitoring of intra-/inter-tumoural heterogeneity throughout the course of the disease82,83. This will require access to high quality, longitudinal imaging datasets.One area where AI/ML could be particularly transformative is precision oncology, or the selection of a patient’s therapy based on their tumour’s molecular profile. Precision oncology is likely to benefit from integrated diagnostics84,85 (e.g. radiogenomics, which combines radiomics and genomics analyses) to provide robust computational tools for investigating cancer biology, as well as for predicting treatment response (Fig. 5). The solution includes large-scale structured data collection (from multiple institutions) that deals with cyber-security and privacy issues and supports continuous learning. At present, the main challenge is bridging the gap between emerging AI tools and clinical practice, by first performing well-validated clinical research studies of such applications. This is vital for the translation and deployment of AI approaches in precision oncology86 and, if used correctly, AI has the potential to decrease the cost of precision oncological treatments through more accurate patient selection strategies.Fig. 5: Potential future real-time tracking of whole tumour volume, spatial and temporal phenotypic heterogeneity with multi-omics data integration for precision oncology.This schema would allow the processing of multi-institutional data, where each medical centre acquires and stores (in local PACS) its own medical imaging data. To execute quantitative analyses, a radiomics gateway is used to communicate outside the institution by requesting an automated, real-time tumour segmentation from a trusted and specialised AI/ML centre, which allows for continuous learning. The medical images leaving the hospital are anonymised to deal with cyber-security and privacy issues. The segmentation results are used for radiomic feature extraction and analysis, acting as virtual biopsies. The quantitative imaging results are integrated with other biomedical data streams to determine associations with clinical and multi-omics information. Such an approach may develop reliable diagnostic and prognostic tools for multidisciplinary team meetings to improve cancer care in clinical practice; and the evolution of precision oncology. PACS Picture Archiving and Communication System, ML Machine Learning.Full size imageProfessional challengesBeyond the clinical challenges, there are professional challenges that are likely to shape the development and deployment of ML in cancer imaging. Stimuli promoting the development of ML include the relentless rise in the demand for imaging which, when coupled with acute and chronic workforce shortages, can lead to radiologist stress and burnout. Departments need to consider updating or redesigning their IT infrastructure and workflow to be ready for the testing and adoption of AI and ML technologies as these become available. Another challenge is how the radiological workforce perceives the potential utility of AI and ML in the clinic, including the threats and opportunities associated with the use of such technologies.In preparation for an AI and ML in Cancer Imaging meeting organized by the Champalimaud Foundation (Lisbon) and the International Cancer Imaging Society in 2019, an online survey of 569 radiologists from 35 countries was conducted. The majority (>60%) perceived the benefits of AI to outweigh potential risks (Supplementary Note). Most respondents agreed with the positive impacts of AI in radiology, including (1) alerting radiologists to abnormal findings; (2) increasing work efficiency; (3) making diagnostic suggestions when the radiologist is unsure; (4) accepting that the radiologist should be responsible when an error is made; and (5) changing the service model by increasing direct communications with patients. The respondents felt confident that AI and ML techniques are unlikely to replace the job of a radiologist. The majority (>70%) felt that it was important to prepare for the arrival of AI by (1) investing in education; (2) testing new tools; (3) supporting the curation of images and image annotation data at scale; and (4) working with commercial vendors to develop specific AI tools that improve workflow.The survey also identified areas of priority and need for AI tool development including the need for (1) tools that automatically track tumours across multiple time points to assess their response to treatment; (2) tools that improve automatic or semiautomatic tumour segmentation; (3) tools that support proforma reporting allowing annotation of image data to be captured prospectively; (4) tools that help confident identification of normal studies so that radiologists can focus on dealing with abnormal examinations; and (5) tools that help to identify tumours throughout the body.In addition, imaging departments need to plan for their workforce needs to deliver future AI empowered practice. Radiographers and technicians will require better understanding of AI, including their deployment in workflow management and image acquisition. Critically, an informatics team is needed to create the platform on which AI tools can be developed or tested in-line; a space for interacting with and annotating imaging data; and well-curated imaging and data repositories.Technical challengesMany state-of-the-art AI methods based on deep learning are achieving outstanding performance87. Reasons for their success include the strong ability of deep ML models to learn independently and the availability of large-scale labelled datasets that include precise annotations. Unfortunately, in biomedical research, collecting such accurate annotations is an expensive and potentially time-consuming process due to the need for domain experts’ knowledge88. Therefore, ML models that can work on rough annotations and weak supervision (e.g. bounding boxes that encompass an area of interest rather than precise outlining, or image-level labels rather than specific image-feature labels) have been attracting much attention89. The generation of large mineable imaging datasets might overcome data paucity and heterogeneity issues. However, along with the availability of samples, data quality and diversity should be considered by collecting and preparing harmonized datasets. The ability to generalize across multi-institutional studies may be improved by exploiting transfer learning and domain adaptation techniques.Designing and identifying reliable AI imaging studies is a challenge. Studies have been published with as few as 10 patients, making the results of such AI models highly questionable due to potential overfitting effects, which will negatively impact upon the generalizability of the findings. In radiomics, there is a rule of thumb when dealing with binominal classification tasks where 10–15 patients should be recruited for each feature that is part of the final radiomics signature90. Performance estimation should be based on the so-called test set: that is, a dataset comprised of examples that were completely excluded from the model’s training and tuning processes. To evaluate the model’s generalizability, apart from internal validation, external validation should be performed to test the model’s performance in one or more datasets acquired using different imaging equipment or in different geographical patient populations. Ideally, models should be validated in an external patient cohort that is 25–40% of the size of the training sample.Integrative models fusing information from other omics data such as genomics or proteomics, as well as clinical, environmental and social data, are gaining attention, especially in the setting of more complex clinical problems such as disease risk assessment and prognosis. Data sparsity and non-standardized therapeutic approaches between institutions are ongoing challenges when it comes to developing integrative ML models, but there is recognition of the need for better standardization (including data acquisition) that will facilitate these use cases of AI91.The use of images and integrating these with clinical and molecular data can be a source of real-world data to be used for evidence-generating studies. Retrospective data from imaging biobanks and repositories provide excellent opportunities to test AI tools and validate their performance. Harmonization techniques like ComBat92 can be considered to bring the imaging features into a standardized space, especially in multicentre studies when the amount of variability, if not reduced, can harm a model’s performance and generalizability. Radiologists have an excellent opportunity to lead the field by promoting observational in silico studies, taking care to oversee all relevant aspects from data harvesting to analyses to improve the reproducibility of results. The main aspects to be considered are as shown in Box 193.For the specific application in radiomics, there are also many challenges to radiomic computation and the use of radiomic features for prognostication, assessment of response to therapy, and diagnosis of molecular phenotype, including the sensitivity of radiomic feature values to image acquisition and reconstruction techniques94,95,96,97,98 and to variations in segmentations among different users and software99,100. To address these points, improvements in algorithms, and community agreement on use of open-source software, phantoms and standardized approaches101 are required for radiomics to reach its full potential.One of the reasons for the lack of translation of AI models to clinical application is that the focus has been on increasing model performance by AI enthusiasts, possibly at the expense of explainability. A typical example is the black-box approach of deep neural networks that produces outstanding performance, but may present difficulty in establishing its trustworthiness, therefore impeding its clinical adoption. A lack of multidisciplinary engagement may also impede the prioritization of AI solutions of significant clinical value. The clinical community may be skeptical about embracing AI technology into clinical routine, as long as the AI models are non-transparent in the way they reach a specific decision.In recent years, the AI community has started to recognise this limitation and has moved towards the development of explainable AI. The explainability of AI models touches upon a sensitive issue concerning patient safety, especially in clinical decision-support systems102. Since the vast majority of AI models are trained with retrospective, observational data, patient selection bias in machine learning models can lead to poor performance and erroneous predictions in prospective unknown cases. Therefore, domain experts should always verify the predictions and the reasoning behind the predictions made by the AI models. The latter can only be achieved when the models by design offer a degree of transparency. Involving the domain expert in model development is likely to make AI models more robust and reproducible and help gain the trust of end-users. Evaluating the overall performance of the AI solution beyond accuracy is also mandatory in the clinical pathway setting. This would include testing the real-world implementation of such models to ascertain their use and usability, trustworthiness, as well as cost and cost-effectiveness.Box 1 Important considerations from data curation to analyses to improve the robustness and generalizability of AI and ML in cancer imaging

Participant recruitment criteria
Consistency in the inclusion of the study population based on the presenting symptoms, results from previous tests, defining the appropriate index tests or by the selected reference standard


Participant sampling
To avoid or control bias in participant sampling, considerations could include the use of consecutive series of participants, use of well-defined selected data silos, clear and well-defined selection criteria; as well as adjusting for possible confounding variables


Data collection
What data to collect and how this is performed should be planned before participant recruitment and sampling. Where appropriate, target trial emulation may be undertaken, which is the application of design principles from randomized trials to the analysis of observational data, which may improve the quality of the observations.


Reference standard
The rationale and description of the reference standard should be clear


Technical specifications of materials and methods
Aspects of technical specifications should be well defined. These include how and when images and measurements were taken; the definition of units; cut-off thresholds; defined results categories (of both the index tests and the reference standard); description of the number, training, and expertise of persons executing and reading (original or new reporting); index tests and the reference standard; and blindness aspects (if the readers of the index tests and the reference standard were masked to other test results)

Show moreLessons learnt from the past: computer-aided diagnosis (CAD) for breast cancerEven though AI and ML are hugely promising technologies in imaging, it is worth noting lessons from the previous effort to apply computational approaches in cancer imaging, using computer-aided diagnosis of breast cancer as an example. Development of algorithms for automated detection of calcifications and masses on mammograms started in earnest in the mid-1980s, and in 1998 the first commercial CAD system for mammography, initially based on digitized film, received FDA approval103. Transition to digital mammography facilitated the implementation of CAD in clinical practice. The introduction of Medicare reimbursement coverage for the use of CAD in the United States, and promising preliminary results from clinical trials104,105, led to a rapid uptake of CAD in the US with ~74% of mammography interpretations utilizing CAD by 2010106. However, even though stand-alone sensitivity of commercial CAD systems in enriched reader studies is consistently superior to that of radiologists106, large retrospective registry-based studies failed to show a significant improvement in the diagnostic accuracy of screening mammography after implementation of CAD107,108. This disappointing result is likely to be explained by the relatively high number of false-positive prompts generated by current commercial CAD systems, which average between 1 and 2 false prompts per case. In the low-prevalence screening setting, this false-positive prompt rate translates into a positive predictive value of a CAD prompt of <1%. As radiologists will have to ignore more than 99% of the CAD prompts to find the one prompt actually pointing to a cancer, there will be a tendency to ignore the computer-generated prompts altogether. There is hope that newer deep learning algorithms will overcome some of the limitations of traditional feature-based CAD systems. Unsupervised training on much larger datasets with up to a million mammographic images has the potential to overcome the shortcomings of human observers, as deep learning algorithms no longer have to imitate the way the radiologist reads a mammogram109. However, increased automation of the detection task will come with added responsibilities for the algorithms110, which may need to show an improvement in patient outcome beyond diagnostic performance.Hence, the key lessons from previous CAD implementation in breast cancer suggest that the next generation AI tool to for cancer detection will need to have high diagnostic accuracy, in particular, high positive predictive value that will result in fewer false positives in the low disease prevalence setting. There is also the need for real-world testing of these tools beyond diagnostic performance to establish the health-related and wider benefits associated with their deployment.Technical, infrastructure and professional developments required for the adoption of AI/ML in cancer imaging
Imaging repositories and archives
Supervised learning approaches require large quantities of labelled data for training and validation103. There is a plethora of data sources that one could exploit for AI modelling in cancer imaging. These include imaging biobanks, which are virtual repositories of medical images; imaging biomarkers identified as endpoint surrogates; and population studies111. Imaging biobanks allow the in silico evaluation and validation of new biomarkers by establishing disease development probabilities, early disease diagnosis and phenotyping, disease grading and staging, targeting therapies and evaluation of disease response to treatment and prediction of adverse events.
Open access data repositories are one approach to capturing and disseminating sufficient high quality, well curated data. There are not many open access cancer image repositories. Data sharing is not a universally accepted concept112. Furthermore, patient privacy, data privacy, informed consent laws, regulations and the growing interest in the potential commercial value of patient data, differ by country and can pose barriers to data sharing113. Institutions and researchers consider data to be intellectual property, and limit or prohibit access to valuable data sets. Regulatory agencies (e.g. the FDA) argue for sequestration of data used to validate algorithms approved for commercial use114.
The US National Cancer Institute funded the creation and continued operation of the largest open access cancer image repository, The Cancer Imaging Archive (TCIA) (Fig. 6)115,116. TCIA is designed to foster increased public availability of high-quality cancer imaging data sets for research. Data are accessible due to strict adherence to F.A.I.R. (Findable, Accessible, Interoperable, and Reproducible) standards for data release117,118. Other research-funded initiatives to create data warehouses are also being developed across the European Union and elsewhere.Fig. 6: The Cancer Imaging Archive (TCIA) is a system of systems constructed from open-source software.TCIA is also a set of services designed to collect and curate high quality cancer image data and related clinical data and make it publicly available. (VMs = virtual machines).Full size image
Although size of a dataset matters, data quality and data variability are of equal importance. Data should be of sufficient quality and be acquired with uniform parameters. Clinical trials generate data with a higher level of quality control and consistency of data acquisition protocols. TCIA focuses on collecting, curating and publishing data from completed clinical trials. Curation in this context includes assurance of consistent metadata, anatomy coverage, and data formats which strictly comply with international data standards, as well as the anonymization of any patient identifiable data.
For an ML algorithm to be clinically useful it must be trained on data that appropriately represent the variance in the human population, the presentation of disease and data collection systems119,120. Labelled data are created manually by human experts, resulting in high cost and limited volume of high-quality training (and testing) datasets. Perhaps the most time-consuming process within a ML project is annotating the data and presenting them in a format compatible with further analysis and modelling processes. Image annotation is often a bottleneck for AI and ML, and crowd sourcing for such activity is being trialled as a way of improving efficiency. Depending on the task, the annotations may be provided at the patient level (overall survival, disease-free survival), at the image level (benign, malignant) or at the voxel level (lesion, non-lesion). Typically lesion detection algorithms need to be provided with annotations of a bounding box type usually encasing the lesion, while for training automatic segmentation models, radiologists need to outline lesions manually in multiple image slices121.
As sizeable imaging data from different sites and scanners become consolidated within repositories, it will be necessary to consider steps that will account for data diversity or heterogeneity. A possible solution might be to use deep learning approaches to learn from such data lacking homogeneity, which may result in outputs with lower variability and higher reproducibility. Retrospective observational studies with real-world data and quality assurance checklists93,122 will allow reproducible causality123 inferences from virtual patient cohorts to address clinical and policy-relevant questions. Particularly where the disease under study is relatively rare resulting in small datasets, it would be appropriate to use a cross-validation approach to develop and test the AI models.
Open-source software and open collaborations
The use of open-source software (OSS) strategy could help to alleviate some of the concerns regarding transparency and explainabiltiy when using AI in cancer imaging. OSS is software code made available under a legal licence in which the copyright holder provides (depending upon the specific terms) various rights to the licensees to study, change, improve and re-distribute the code without any fees. Today, there are many different types of OSS licences [https://opensource.org/approval] depending on the preference of the copyright holder. These licences range in the United States from what is commonly known as permissive licences, such as Apache-2.0, to strongly protective licenses, such as general public licence (GPL). OSS is available in its non-commercial form, however it can be made into commercial products with additional services such as warranty, training, documentation and maintenance under various commercial contracts.
A successful open-source ecosystem has three interacting components: (i) OSS itself, (ii) governance, and (iii) community of collaborators. Currently there are more than 50 open-source ML packages using different OSS licences, operating platforms, and programming languages. Some of the more popular packages include TensorFlow, Keras, PyTorch, Caffe2 and many others. They all have varying strengths and weaknesses depending on users’ needs.
These OSS packages are developed and sponsored by corporations and some individuals for their own use cases and applications, often not for medical imaging, but the packages are good initial platforms from which medical imaging research can be pursued. However, they will need to be optimized for higher performance for medical applications. For example, the pattern recognition in consumer applications usually depends on graphic features and image orientation. However, medical image patterns are usually orientation-independent, and diseases in medical images are subtle in nature and present themselves in minor grey value differences rather than graphical features. For these reasons, algorithms available on OSS packages will need to be re-trained and tuned using medical imaging data to optimise their performances. In summary, OSS represents a practical route by which the AI community can work together to collaborate and develop new AI tools, which can be more widely tested, and at the same time address some of the transparency and privacy concerns.
Healthcare and regulatory systems
There are significant perceived values of using AI solutions in healthcare124 at every stage of the clinical workflow. In radiology, this means improvements to the patient diagnostic pathway, from the appropriateness of imaging requests125 to how actionable findings in radiological reports are followed up126. The full potential of these improvements are not yet realised as there remain significant barriers to implementation.
From 2021, the new EU Medical Device Regulations has been enforced, mandating deeper scrutiny of software as a medical device (SaMD). Certification is given in accordance with how the software is used and applied within the clinical workflow. The majority of AI software in imaging are being certified as a decision-support tool, that is to say it should not be used on its own in for clinical or patient management. It is also worth considering whether the software is intended to be use by radiologists at primary reporting, or only after the initial primary report is issued as a second read. In the current commercial landscape, there are multitudes of software tools that are cleared by regulators but have not been adopted into healthcare systems.
AI products may continue to evolve after initial release through continuous training. Many products have found their way into the marketplace without being independently tested, despite obtaining CE labelling or FDA clearance. As such, a new FDA framework has been proposed to ensure the safety and effectiveness of AI tools127. The FDA has introduced a predetermined change control plan in premarket submissions. This plan includes anticipated modification (SaMD pre-specifications) and the associated methodology used to implement these controlled changes (algorithm change protocol). The FDA expects a commitment from manufacturers on transparency and real-world performance monitoring, as well as updates on changes implemented as part of the approved pre-specifications and the algorithm change protocol.
Once the product or software has been validated as a certified medical device, a Data Protection Impact Assessment process must be initiated, usually at the local level, to safeguard data privacy—in Europe, this means compliance with the General Data Protection Regulations (GDPR). At the same time, a Solution Architecture Review should also be undertaken to carefully examine the possible IT architecture for implementation. Local rules must also be adhered to with regards to patient data use and storage, since each country can vary in the interpretation of the GDPR. Privacy concerns and the need for a rational and coherent digital infrastructure has been referred to as ‘the inconvenient truth’ in medical AI128.
The process of software integration with existing hospital IT infrastructure is influenced by the experience of the AI company and its product design, the diversity and size of the healthcare system, as well as knowing how and what data are being transferred to and from the healthcare provider to the software processor and vice versa. Failure of software integration is a known barrier for adoption. Well-established companies with a sound product could be integrated in days, but the timeline usually gets longer in hospitals running an array of different radiology informatic systems (e.g., Picture Archiving and Communication Systems [PACS] and Vendor Neutral Archives [VNA], which communicate with the Hospital and Radiology Information Systems [HIS & RIS],) as well as dealing with a complex range of data inputs (e.g. non-standardised naming of imaging sequences from different scanners).
To facilitate AI workflows, similar imaging procedures should be standardised to the same acquisition protocol (regardless of scanner model and vendor), and all radiological reports could be structured in a similar way using common lexicon to facilitate data mining (e.g. RadReports.org with suggested structured reporting templates endorsed by the American College of Radiology). Without satisfying such conditions, software integration may need to be organised on a per-modality basis, which may require complex data mapping within the same hospital system. Hence, depending on how mature the software algorithm is, program bugs may reveal themselves as a consequence of such data input heterogeneity.
Introducing the use of a new AI tool within a healthcare system may proceed with initial caution by working with the supplier to undertake a mutually agreed trial period. Such a “try to buy” approach would allow users to assess the use and usability of the AI tool, integration with the workflow, as well as its trustworthiness. This is because physicians may distrust the tool unless it is proven to be highly accurate. One solution is to build a radiologist feedback tool onto the PACS interface. This would allow the radiologist to score the performance of any given AI algorithm—for example, using check boxes with legends such as ‘agree/AI overestimation/AI underestimation/both over and underestimation’. This would allow users to raise perceived discrepancies that can then be further assessed. Caution should also be given to tools that are developed by vendors that may lock-in users to specific algorithms, especially if they fail to meet local demands. The community of professionals who interact with the software tool also needs to be educated about its usage. It may be feasible for an AI developer to train a small group but this becomes challenging when confronted by many potential users in a large hospital system.
It is possible to process patient data using certified medical devices in routine clinical practice without additional consent. However, if vendors are seeking feedback to improve their software algorithm, then specific data consent is required and should be obtained prospectively from patients. Post-hoc sharing of such data may be denied, which means that processes must be put in place to identify patients who have provided consent and to rescind it where appropriate.
Even when the barriers to AI implementation are overcome, it may still be unclear: who pays for the AI? Whilst the development and testing AI tools can be funded by research grants or commercial partnership with companies, as yet, no healthcare systems or private health insurers have reimbursed AI usage. In the landscape of decreasing tariffs for radiological procedures, it is a challenge to find specific funding to support the introduction of AI, which can be costly to deploy across healthcare systems. Even though AI holds substantial promise to improve work efficiency, there are yet no published real-world evidence to date. The development of specific patient-centric services using AI may provide an opportunity to introduce tariff models for its use. One example is the UK pilot of a bone health service, which pays for identifying patients at risk of developing osteoporotic spinal fracture. Instead of payment for a specific AI product, the business case was constructed on the basis of the whole service, which aimed to identify patients at risk of osteoporotic fracture, thus enabling early intervention and potentially reducing subsequent healthcare costs by decreasing the number of fractures. This is an example of the coming together of value-based healthcare and AI.
In less coherent healthcare models where imaging services are component care providers (i.e. providers of specific services), it would be important to accrue local metrics to help justify AI adoption. Examples of these include metrics showing improvement in the accuracy of reporting by reducing the rate of patient recall in women undergoing mammography109; increasing the reporting speed and finally increases in revenues. By testing novel AI solutions in a variety of healthcare markets and trying different combinations of payor models, it may eventually be possible for AI software tools to be widely adopted into healthcare systems (Box 2).
Future radiological workforce
Appropriate training is required to allow users to judge whether an AI tool is fit for purpose before adoption into clinical practice, which would require radiologists to understand the principles of AI and how AI algorithms should be properly validated.
There are pressures that are encouraging the premature introduction of AI tools into clinical workflows. Firstly, there is a workforce crisis with a shortage of radiologists in many countries. In the UK, about 10% of radiologist vacancies are unfilled129. Secondly, there is a marked increase in global imaging demand and workload. In the UK, the CT and MRI workload has been rising by ~10% each year129. Thirdly, there is a relentless drive to improve workflow efficiency, by improving image procedure turnaround time without compromising diagnostic accuracy. Finally, AI is seen as a tool to support repetitive tasks (e.g. sequential tumour size measurement, or cancer screening), that are time-consuming and relatively uninteresting for radiologists to undertake.
Empowering radiologists to judge the performance of AI algorithms would require changes in medical school and radiology curricula to include an understanding of the terms and main methodology of AI/ML; the requisite development, training, testing and validation of algorithms; basic statistics relevant to AI/ML; and the challenges of data requirements. Such empowerment will also necessitate educating radiologists in how they can meaningfully and rigorously test the performance of AI algorithms within their own clinical practice.
The future of AI and ML applications in radiology will be reliant upon the education of stakeholders including medical students, trainee radiologists, qualified radiologists, other doctors, radiographers, computer scientists, data scientists and data engineers collaboratively to solve clinically relevant problems. This multidisciplinary dialogue is necessary and critical to the development of clinically relevant and technically accomplished AI tools to address the unmet needs in oncology. There is a clear need for more multidisciplinary AI meetings and conferences to encourage interactions between all stakeholders, both at the local level, as well as at the national and international level.
Box 2 Important factors for the selection of AI into a health systemCriteria and benchmarks

CE labelling


FDA clearance


UKCA marking

Incentives and motivations

Targeting a common disease


Potential for the AI algorithm to be developed into products that generate revenue


Attracting better or new payors


Formulation of fair value proposition between stakeholders or partners


Latitude to create/share own business model


AI tool Infrastructure fits with existing informatic systems


The AI tool can be assimilated into the clinical workflow

Show moreConclusionsCancer imaging is seeing rapid developments in AI, and in particular ML, with a broad range of clinical applications that are welcomed by the majority of radiologists. The development of new ML tools is often constrained by available imaging data; however, there is the potential for building and using real-world well-curated imaging data in biobanks and open access repositories to overcome such limitations. Adopting open-source tools for algorithm development, where possible, may lead to better transparency and collaboration across centres. However, even though exceptional diagnostic performance can be gained by the application of these AI software algorithms, it is still not clear how many of these will have a long-term meaningful impact on patient outcomes or will be cost-effective. An improved regulatory framework for the approval of AI-based tools for clinical deployment is evolving. There is a need for systematic evaluation of these software, which often undergo only limited testing prior to release. It is also important to empower all stakeholders, especially radiologists, with sufficient understanding of this growing field to enable them critically to appraise these technologies for adoption into their own practice. Creating opportunities for interdisciplinary engagement will also facilitate the development of useful clinical tools that aim to enhance patient care and outcomes.


ReferencesErickson, B. J., Korfiatis, P., Akkus, Z. & Kline, T. L. Machine learning for medical imaging. Radiographics 37, 505–515 (2017).Article 
    PubMed 
    
                    Google Scholar 
                Napel, S. In Radiomics and Radiogenomics: Technical Basis and Clinical Applications (eds Napel, S. & Rubin, D. L.) 3–12 (CRC Press, 2019).Trivizakis, E. et al. Artificial intelligence radiogenomics for advancing precision and effectiveness in oncologic care (Review). Int. J. Oncol. 57, 43–53 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Lo Gullo, R., Daimiel, I., Morris, E. A. & Pinker, K. Combining molecular and imaging metrics in cancer: radiogenomics. Insights Imaging 11, 1 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Rubin, D. L., Ugur Akdogan, M., Altindag, C. & Alkim, E. ePAD: an image annotation and analysis platform for quantitative imaging. Tomography 5, 170–183 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Kalpathy-Cramer, J. et al. A comparison of lung nodule segmentation algorithms: methods and results from a multi-institutional study. J. Digit. Imaging 29, 476–487 (2016).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Echegaray, S., Bakr, S., Rubin, D. L. & Napel, S. Quantitative image feature engine (QIFE): an open-source, modular engine for 3D quantitative feature extraction from volumetric medical images. J. Digit. Imaging 31, 403–414 (2018).Article 
    PubMed 
    
                    Google Scholar 
                van Griethuysen, J. J. M. et al. Computational radiomics system to decode the radiographic phenotype. Cancer Res. 77, e104–e107 (2017).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Zhang, L. et al. IBEX: an open infrastructure software platform to facilitate collaborative work in radiomics. Med. Phys. 42, 1341–1353 (2015).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Aerts, H. J. et al. Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach. Nat. Commun. 5, 4006 (2014).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Gevaert, O. et al. Non-small cell lung cancer: identifying prognostic imaging biomarkers by leveraging public gene expression microarray data–methods and preliminary results. Radiology 264, 387–396 (2012). One hundred fourteen of 180 CT image features and the PET standardized uptake value were predicted in terms of metagenes with an accuracy of 65%-86%.Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Mattonen, S. A. et al. Quantitative imaging feature pipeline: a web-based tool for utilizing, sharing, and building image-processing pipelines. J. Med. Imaging 7, 042803 (2020).Article 
    
                    Google Scholar 
                Di Tommaso, P. et al. The impact of Docker containers on the performance of genomic pipelines. PeerJ. 3, e1273 (2015).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Dankers, F., Traverso, A., Wee, L. & van Kuijk, S. M. J. In Fundamentals of Clinical Data Science (eds Kubben, P.,Dumontier, M. & Dekker, A.) 101–120 (2019).Traverso, A., Dankers, F., Osong, B., Wee, L. & van Kuijk, S. M. J. In Fundamentals of Clinical Data Science (eds Kubben, P., Dumontier, M. & Dekker, A.) 121–133 (2019).Parmar, C. et al. Radiomic machine-learning classifiers for prognostic biomarkers of head and neck cancer. Front. Oncol. 5, 272 (2015).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Ather, S., Kadir, T. & Gleeson, F. Artificial intelligence and radiomics in pulmonary nodule management: current status and future applications. Clin. Radiol. 75, 13–19 (2020).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Deng, J. D. et al. In IEEE Conference on Computer Vision and Pattern Recognition. 248–255 (2009).Rieke, N. et al. The future of digital health with federated learning. NPJ Digit. Med. 3, 119 (2020).Kirienko, M. et al. Distributed learning: a reliable privacy-preserving strategy to change multicenter collaborations using AI. Eur. J. Nucl. Med. Mol. Imaging https://doi.org/10.1007/s00259-021-05339-7 (2021).Kitamura, F. C. & Marques, O. Trustworthiness of artificial intelligence models in radiology and the role of explainability. J. Am. Coll. Radiol. 18, 1160–1162 (2021).Article 
    PubMed 
    
                    Google Scholar 
                Bi, W. L. et al. Artificial intelligence in cancer imaging: clinical challenges and applications. CA Cancer J. Clin. 69, 127–157 (2019).PubMed 
    PubMed Central 
    
                    Google Scholar 
                Mohamed, A. A. et al. A deep learning method for classifying mammographic breast density categories. Med. Phys. 45, 314–321 (2018).Article 
    PubMed 
    
                    Google Scholar 
                Arieno, A., Chan, A. & Destounis, S. V. A review of the role of augmented intelligence in breast imaging: from automated breast density assessment to risk stratification. Am. J. Roentgenol. 212, 259–270 (2019).Article 
    
                    Google Scholar 
                Sprague, B. L. et al. Variation in mammographic breast density assessments among radiologists in clinical practice: a multicenter observational study. Ann. Intern. Med. 165, 457–464 (2016).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Yala, A., Lehman, C., Schuster, T., Portnoi, T. & Barzilay, R. A deep learning mammography-based model for improved breast cancer risk prediction. Radiology 292, 60–66 (2019). Deep learning models that use full-field mammograms yield substantially improved risk discrimination compared with the standard Tyrer-Cuzick (version 8) risk prediction model.Article 
    PubMed 
    
                    Google Scholar 
                Dembrower, K. et al. Comparison of a deep learning risk score and standard mammographic density score for breast cancer risk prediction. Radiology 294, 265–272 (2020).Article 
    PubMed 
    
                    Google Scholar 
                Le Boulc’h, M. et al. Comparison of breast density assessment between human eye and automated software on digital and synthetic mammography: Impact on breast cancer risk. Diagn. Interv. Imaging 101, 811–819 (2020).Article 
    PubMed 
    
                    Google Scholar 
                Liu, B. et al. Evolving the pulmonary nodules diagnosis from classical approaches to deep learning-aided decision support: three decades’ development course and future prospect. J. Cancer Res. Clin. Oncol. 146, 153–185 (2020).Article 
    PubMed 
    
                    Google Scholar 
                Li, D. et al. The performance of deep learning algorithms on automatic pulmonary nodule detection and classification tested on different datasets that are not derived from LIDC-IDRI: a systematic review. Diagnostics 9, https://doi.org/10.3390/diagnostics9040207 (2019). The studies reviewed reached a classification accuracy between 68-99.6% and a detection accuracy between 80.6-94%.Schreuder, A., Scholten, E. T., van Ginneken, B. & Jacobs, C. Artificial intelligence for detection and characterization of pulmonary nodules in lung cancer CT screening: ready for practice. Transl. Lung Cancer Res. 10, 2378–2388 (2021).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Raya-Povedano, J. L. et al. AI-based strategies to reduce workload in breast cancer screening with mammography and tomosynthesis: a retrospective evaluation. Radiology 300, 57–65 (2021).Article 
    PubMed 
    
                    Google Scholar 
                Graewingholt, A. & Duffy, S. Retrospective comparison between single reading plus an artificial intelligence algorithm and two-view digital tomosynthesis with double reading in breast screening. J. Med. Screen https://doi.org/10.1177/0969141320984198 (2021).Dembrower, K. et al. Effect of artificial intelligence-based triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study. Lancet Digit Health 2, e468–e474 (2020).Article 
    PubMed 
    
                    Google Scholar 
                Tran, W. T. et al. Computational radiology in breast cancer screening and diagnosis using artificial intelligence. Can. Assoc. Radiol. J. 72, 98–108 (2021).Article 
    PubMed 
    
                    Google Scholar 
                McKinney, S. M. et al. International evaluation of an AI system for breast cancer screening. Nature 577, 89–94 (2020).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Lennox-Chhugani, N., Chen, Y., Pearson, V., Trzcinski, B. & James, J. Women’s attitudes to the use of AI image readers: a case study from a national breast screening programme. BMJ Health Care Inform. 28, https://doi.org/10.1136/bmjhci-2020-100293 (2021).Freeman, K. et al. Use of artificial intelligence for image analysis in breast cancer screening programmes: systematic review of test accuracy. BMJ 374, n1872, (2021).Chen, J. H. et al. Opportunistic breast density assessment in women receiving low-dose chest computed tomography screening. Acad. Radiol. 23, 1154–1161 (2016).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Liu, K. et al. Evaluating a fully automated pulmonary nodule detection approach and its impact on radiologist performance. Radiol. Artif. Intell. 1, e180084 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Chakrabarty, S. et al. MRI-based identification and classification of major intracranial tumor types by using a 3D convolutional neural network: a retrospective multi-institutional analysis. Radiol. Artif. Intell. 3, e200301 (2021).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Deepak, S. & Ameer, P. M. Brain tumor classification using deep CNN features via transfer learning. Comput. Biol. Med. 111, 103345 (2019).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Diaz-Pernas, F. J., Martinez-Zarzuela, M., Anton-Rodriguez, M. & Gonzalez-Ortega, D. A Deep learning approach for brain tumor classification and segmentation using a multiscale convolutional neural network. Healthcare 9, https://doi.org/10.3390/healthcare9020153 (2021).Nazir, M., Shakil, S. & Khurshid, K. Role of deep learning in brain tumor detection and classification (2015 to 2020): a review. Comput. Med. Imaging Graph 91, 101940 (2021).Article 
    PubMed 
    
                    Google Scholar 
                Dmitriev, K. et al. Classification of pancreatic cysts in computed tomography images using a random forest and convolutional neural network ensemble. Med. Image Comput. Comput. Assist. Interv. 10435, 150–158 (2017).PubMed 
    PubMed Central 
    
                    Google Scholar 
                Li, H. et al. Differential diagnosis for pancreatic cysts in CT scans using densely-connected convolutional networks. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. 2019, 2095–2098 (2019).PubMed 
    
                    Google Scholar 
                Yang, J., Guo, X., Ou, X., Zhang, W. & Ma, X. Discrimination of pancreatic serous cystadenomas from mucinous cystadenomas with CT textural features: based on machine learning. Front. Oncol. 9, 494 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Du, R. et al. Radiomics model to predict early progression of nonmetastatic nasopharyngeal carcinoma after intensity modulation radiation therapy: a multicenter study. Radiol. Artif. Intell. 1, e180075 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Khorrami, M. et al. Combination of peri- and intratumoral radiomic features on baseline CT scans predicts response to chemotherapy in lung adenocarcinoma. Radiol. Artif. Intell. 1, e180012 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Bibault, J. E. et al. Deep Learning and Radiomics predict complete response after neo-adjuvant chemoradiation for locally advanced rectal cancer. Sci. Rep. 8, 12611 (2018).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Delli Pizzi, A. et al. MRI-based clinical-radiomics model predicts tumor response before treatment in locally advanced rectal cancer. Sci. Rep. 11, 5379 (2021).Article 
    CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Shaish, H. et al. Radiomics of MRI for pretreatment prediction of pathologic complete response, tumor regression grade, and neoadjuvant rectal score in patients with locally advanced rectal cancer undergoing neoadjuvant chemoradiation: an international multicenter study. Eur. Radiol. 30, 6263–6273 (2020).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Kao, Y. S. & Hsu, Y. A meta-analysis for using radiomics to predict complete pathological response in esophageal cancer patients receiving neoadjuvant chemoradiation. In Vivo 35, 1857–1863 (2021).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Jin, X. et al. Prediction of response after chemoradiation for esophageal cancer using a combination of dosimetry and CT radiomics. Eur. Radiol. 29, 6080–6088 (2019).Article 
    PubMed 
    
                    Google Scholar 
                DiCenzo, D. et al. Quantitative ultrasound radiomics in predicting response to neoadjuvant chemotherapy in patients with locally advanced breast cancer: Results from multi-institutional study. Cancer Med. 9, 5798–5806 (2020).Article 
    CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Bitencourt, A. G. V. et al. MRI-based machine learning radiomics can predict HER2 expression level and pathologic response after neoadjuvant therapy in HER2 overexpressing breast cancer. EBioMedicine 61, 103042 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Pons, E., Braun, L. M., Hunink, M. G. & Kors, J. A. Natural language processing in radiology: a systematic review. Radiology 279, 329–343 (2016).Article 
    PubMed 
    
                    Google Scholar 
                Oliwa, T. et al. Obtaining knowledge in pathology reports through a natural language processing approach with classification, named-entity recognition, and relation-extraction heuristics. JCO Clin. Cancer Inform. 3, 1–8 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Steinkamp, J. M., Chambers, C. M., Lalevic, D., Zafar, H. M. & Cook, T. S. Automated organ-level classification of free-text pathology reports to support a radiology follow-up tracking engine. Radiol. Artif. Intell. 1, e180052 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Holzinger, A., Haibe-Kains, B. & Jurisica, I. Why imaging data alone is not enough: AI-based integration of imaging, omics, and clinical data. Eur. J. Nucl. Med. Mol. Imaging 46, 2722–2730 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Saltz, J. et al. Towards generation, management, and exploration of combined radiomics and pathomics datasets for cancer research. AMIA Jt. Summits Transl. Sci. Proc. 2017, 85–94 (2017).PubMed 
    PubMed Central 
    
                    Google Scholar 
                Liu, X., Li, K. W., Yang, R. & Geng, L. S. Review of deep learning based automatic segmentation for lung cancer radiotherapy. Front. Oncol. 11, 717039 (2021).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Kalantar, R. et al. Automatic segmentation of pelvic cancers using deep learning: state-of-the-art approaches and challenges. Diagnostics 11, https://doi.org/10.3390/diagnostics11111964 (2021).van Kempen, E. J. et al. Performance of machine learning algorithms for glioma segmentation of brain MRI: a systematic literature review and meta-analysis. Eur. Radiol. 31, 9638–9653 (2021).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Dinkel, J. et al. Inter-observer reproducibility of semi-automatic tumor diameter measurement and volumetric analysis in patients with lung cancer. Lung Cancer 82, 76–82 (2013). By using computer-assisted size assessment in primary lung tumor, interobserver-variability can be reduced to about half to one-third compared to standard manual measurements.Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Napel, S., Mu, W., Jardim-Perassi, B. V., Aerts, H. & Gillies, R. J. Quantitative imaging of cancer in the postgenomic era: Radio(geno)mics, deep learning, and habitats. Cancer 124, 4633–4649 (2018).Article 
    PubMed 
    
                    Google Scholar 
                Rundo, L. et al. Tissue-specific and interpretable sub-segmentation of whole tumour burden on CT images by unsupervised fuzzy clustering. Comput. Biol. Med. 120, 103751 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Savenije, M. H. F. et al. Clinical implementation of MRI-based organs-at-risk auto-segmentation with convolutional networks for prostate radiotherapy. Radiat. Oncol. 15, 104 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Chen, X. et al. A deep learning-based auto-segmentation system for organs-at-risk on whole-body computed tomography images for radiation therapy. Radiother. Oncol. 160, 175–184 (2021).Article 
    PubMed 
    
                    Google Scholar 
                Vrtovec, T., Mocnik, D., Strojan, P., Pernus, F. & Ibragimov, B. Auto-segmentation of organs at risk for head and neck radiotherapy planning: from atlas-based to deep learning methods. Med. Phys. 47, e929–e950 (2020).Article 
    PubMed 
    
                    Google Scholar 
                Chan, J. W. et al. A convolutional neural network algorithm for automatic segmentation of head and neck organs at risk using deep lifelong learning. Med. Phys. 46, 2204–2213 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Chung, S. Y. et al. Clinical feasibility of deep learning-based auto-segmentation of target volumes and organs-at-risk in breast cancer patients after breast-conserving surgery. Radiat. Oncol. 16, 44 (2021).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Feng, X., Qing, K., Tustison, N. J., Meyer, C. H. & Chen, Q. Deep convolutional neural network for segmentation of thoracic organs-at-risk using cropped 3D images. Med. Phys. 46, 2169–2180 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Zhu, J. et al. Comparison of the automatic segmentation of multiple organs at risk in CT images of lung cancer between deep convolutional neural network-based and atlas-based techniques. Acta Oncol. 58, 257–264 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Shanbhogue, K. et al. Accelerated single-shot T2-weighted fat-suppressed (FS) MRI of the liver with deep learning-based image reconstruction: qualitative and quantitative comparison of image quality with conventional T2-weighted FS sequence. Eur. Radiol. https://doi.org/10.1007/s00330-021-08008-3 (2021). Deep learning image reconstruction demonstrated superior image quality, improved respiratory motion and other ghosting artefacts, and increased lesion conspicuity with comparable liver-to-lesion contrast compared to conventional sequence.Chaudhari, A. S. et al. Diagnostic accuracy of quantitative multicontrast 5-minute knee MRI using prospective artificial intelligence image quality enhancement. Am. J. Roentgenol. 216, 1614–1625 (2021).Article 
    
                    Google Scholar 
                Monshi, M. M. A., Poon, J. & Chung, V. Deep learning in generating radiology reports: a survey. Artif. Intell. Med. 106, 101878 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Nakamura, Y. et al. Automatic detection of actionable radiology reports using bidirectional encoder representations from transformers. BMC Med. Inform. Decis. Mak. 21, 262 (2021).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. Nat. Med. 25, 44–56 (2019).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Seyhan, A. A. & Carini, C. Are innovation and new technologies in precision medicine paving a new era in patients centric care? J. Transl. Med. 17, 114 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Brady, S. M., Highnam, R., Irving, B. & Schnabel, J. A. Oncological image analysis. Med. Image Anal. 33, 7–12 (2016).Article 
    PubMed 
    
                    Google Scholar 
                Jimenez-Sanchez, A. et al. Heterogeneous tumor-immune microenvironments among differentially growing metastases in an ovarian cancer patient. Cell 170, 927–938.e920 (2017).Article 
    CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Martin-Gonzalez, P. et al. Integrative radiogenomics for virtual biopsy and treatment monitoring in ovarian cancer. Insights Imaging 11, 94 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Bukowski, M. et al. Implementation of eHealth and AI integrated diagnostics with multidisciplinary digitized data: are we ready from an international perspective. Eur. Radiol. 30, 5510–5524 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Mun, S. K., Wong, K. H., Lo, S. B., Li, Y. & Bayarsaikhan, S. Artificial intelligence for the future radiology diagnostic service. Front. Mol. Biosci. 7, 614258 (2020).Article 
    PubMed 
    
                    Google Scholar 
                Allen, B. Jr. et al. A road map for translational research on artificial intelligence in medical imaging: from the 2018 National Institutes of Health/RSNA/ACR/The Academy Workshop. J. Am. Coll. Radiol. 16, 1179–1189 (2019).Article 
    PubMed 
    
                    Google Scholar 
                LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Litjens, G. et al. A survey on deep learning in medical image analysis. Med. Image Anal. 42, 60–88 (2017).Article 
    PubMed 
    
                    Google Scholar 
                Rajchl, M. et al. DeepCut: object segmentation from bounding box annotations using convolutional neural networks. IEEE Trans. Med. Imaging 36, 674–683 (2017).Article 
    PubMed 
    
                    Google Scholar 
                Chalkidou, A., O’Doherty, M. J. & Marsden, P. K. False discovery rates in PET and CT studies with texture features: a systematic review. PLoS ONE 10, e0124165 (2015).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Zanfardino, M. et al. Bringing radiomics into a multi-omics framework for a comprehensive genotype-phenotype characterization of oncological diseases. J. Transl. Med. 17, 337 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Johnson, W. E., Li, C. & Rabinovic, A. Adjusting batch effects in microarray expression data using empirical Bayes methods. Biostatistics 8, 118–127 (2007).Article 
    PubMed 
    
                    Google Scholar 
                Hernan, M. A. & Robins, J. M. Using big data to emulate a target trial when a randomized trial is not available. Am. J. Epidemiol. 183, 758–764 (2016).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Schwier, M. et al. Repeatability of multiparametric prostate MRI radiomics features. Sci. Rep. 9, 9441 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Orlhac, F., Frouin, F., Nioche, C., Ayache, N. & Buvat, I. Validation of a method to compensate multicenter effects affecting CT radiomics. Radiology 291, 53–59 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Berenguer, R. et al. Radiomics of CT features may be nonreproducible and redundant: influence of CT acquisition parameters. Radiology 288, 407–415 (2018). Many radiomics features were found to be redundant and nonreproducible, indicating the need for careful feature selection.Article 
    PubMed 
    
                    Google Scholar 
                Hagiwara, A., Fujita, S., Ohno, Y. & Aoki, S. Variability and standardization of quantitative imaging: monoparametric to multiparametric quantification, radiomics, and artificial intelligence. Invest. Radiol. 55, 601–616 (2020).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Fedorov, A. et al. An annotated test-retest collection of prostate multiparametric MRI. Sci. Data 5, 180281 (2018).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Kalpathy-Cramer, J. et al. Radiomics of lung nodules: a multi-institutional study of robustness and agreement of quantitative imaging features. Tomography 2, 430–437 (2016).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                McNitt-Gray, M. et al. Standardization in quantitative imaging: a multicenter comparison of radiomic features from different software packages on digital reference objects and patient data sets. Tomography 6, 118–128 (2020).Article 
    CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Zwanenburg, A. et al. The image biomarker standardization initiative: standardized quantitative radiomics for high-throughput image-based phenotyping. Radiology 295, 328–338 (2020).Article 
    PubMed 
    
                    Google Scholar 
                Shortliffe, E. H. & Sepulveda, M. J. Clinical decision support in the era of artificial intelligence. JAMA 320, 2199–2200 (2018).Article 
    PubMed 
    
                    Google Scholar 
                Giger, M. L., Chan, H. P. & Boone, J. Anniversary paper: History and status of CAD and quantitative image analysis: the role of Medical Physics and AAPM. Med. Phys. 35, 5799–5820 (2008).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Helvie, M. A. et al. Sensitivity of noncommercial computer-aided detection system for mammographic breast cancer detection: pilot clinical trial. Radiology 231, 208–214 (2004).Article 
    PubMed 
    
                    Google Scholar 
                Birdwell, R. L., Ikeda, D. M., O’Shaughnessy, K. F. & Sickles, E. A. Mammographic characteristics of 115 missed cancers later detected with screening mammography and the potential utility of computer-aided detection. Radiology 219, 192–202 (2001).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Kohli, A. & Jha, S. Why CAD failed in mammography. J. Am. Coll. Radiol. 15, 535–537 (2018).Article 
    PubMed 
    
                    Google Scholar 
                Lehman, C. D. et al. Diagnostic accuracy of digital screening mammography with and without computer-aided detection. JAMA Intern. Med. 175, 1828–1837 (2015).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Fenton, J. J. et al. Influence of computer-aided detection on performance of screening mammography. N. Engl. J. Med. 356, 1399–1409 (2007).Article 
    CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Rodriguez-Ruiz, A. et al. Detection of breast cancer with mammography: effect of an artificial intelligence support system. Radiology 290, 305–314 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Jaremko, J. L. et al. Canadian association of radiologists white paper on ethical and legal issues related to artificial intelligence in radiology. Can. Assoc. Radiol. J. 70, 107–118 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Radiology, E. S. o. ESR position paper on imaging biobanks. Insights Imaging 6, 403–410 (2015).Article 
    
                    Google Scholar 
                Guinney, J. & Saez-Rodriguez, J. Alternative models for sharing confidential biomedical data. Nat. Biotechnol. 36, 391–392 (2018).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Negrouk, A. & Lacombe, D. Does GDPR harm or benefit research participants? An EORTC point of view. Lancet Oncol. 19, 1278–1280 (2018).Article 
    PubMed 
    
                    Google Scholar 
                Gallas, B. D. et al. Evaluating imaging and computer-aided detection and diagnosis devices at the FDA. Acad. Radiol. 19, 463–477 (2012).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Prior, F. et al. The public cancer radiology imaging collections of The Cancer Imaging Archive. Sci. Data 4, 170124 (2017).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Clark, K. et al. The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository. J. Digit. Imaging 26, 1045–1057 (2013). TCIA contains 30.9 million radiology images representing data collected from approximately 37,568 subjects; it outlines the curation and publication methods employed by TCIA and makes available 15 collections of cancer imaging data.Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Wilkinson, M. D. et al. A design framework and exemplar metrics for FAIRness. Sci. Data 5, 180118 (2018).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Wilkinson, M. D. et al. The FAIR guiding principles for scientific data management and stewardship. Sci Data 3, 160018 (2016).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Prior, F. et al. Open access image repositories: high-quality data to enable machine learning research. Clin. Radiol. 75, 7–12 (2020).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Vayena, E., Blasimme, A. & Cohen, I. G. Machine learning in medicine: addressing ethical challenges. PLoS Med. 15, e1002689 (2018).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Müller, H., Kalpathy-Cramer, J. & Seco de Herrera, A. G. Information retrieval evaluation in a changing wolrd. 41 (2019).von Elm, E. et al. The strengthening the reporting of observational studies in epidemiology (STROBE) statement: guidelines for reporting observational studies. Lancet 370, 1453–1457 (2007).Article 
    
                    Google Scholar 
                Castro, D. C., Walker, I. & Glocker, B. Causality matters in medical imaging. Nat Commun 11, 3673 (2020).Article 
    CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Langlotz, C. P. Will artificial intelligence replace radiologists? Radiol. Artif. Intell. 1, e190058 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Bizzo, B. C., Almeida, R. R., Michalski, M. H. & Alkasab, T. K. Artificial intelligence and clinical decision support for radiologists and referring providers. J. Am. Coll. Radiol. 16, 1351–1356 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Lou, R., Lalevic, D., Chambers, C., Zafar, H. M. & Cook, T. S. Automated detection of radiology reports that require follow-up imaging using natural language processing feature engineering and machine learning classification. J. Digit. Imaging 33, 131–136 (2020).Article 
    PubMed 
    
                    Google Scholar 
                US Food and Drugs Adminstration. Machine Learning (AI/ML)-based Software as a Medical Device (SaMD). (2019).Panch, T., Mattie, H. & Celi, L. A. The “inconvenient truth” about AI in healthcare. NPJ Digit. Med. 2, 77 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Clinical Radiology. UK workforce census 2020 report. (Royal College of Radiologists, 2020).Download referencesAcknowledgementsA.R. acknowledges National Institute of Health Research Imperial Biomedical Centre and the Imperial Cancer Research UK Centre. D.-M.K. acknowledges the National Institute of Health Research Clinical Research Facilities and Biomedical Research Centre at the Royal Marsden Hospital and Institute of Cancer Research. Dr. Leonard Rundo, Cambridge, for his contribution to the manuscript.Author informationAuthor notesThese authors contributed equally: Dow-Mu Koh, Nickolas Papanikolaou.Authors and AffiliationsDepartment of Radiology, Royal Marsden Hospital, Sutton, UKDow-Mu Koh & Nickolas PapanikolaouChampalimaud Foundation, Lisbon, PortugalNickolas Papanikolaou & Celso MatosCharité – Universitätsmedizin Berlin, Berlin, GermanyUlrich BickDepartment of Surgery & Interventional Science, University College London, London, UKRowland IllingDepartment of Radiology and Institute for Biomedical Informatics, University of Pennsylvania, Philadelphia, USACharles E. Kahn Jr.Centre for machine learning, Massachusetts General Hospital/Harvard Medical School, Boston, USAJayshree Kalpathi-CramerDepartment of Radiology, Hospital Universitari i Politècnic La Fe, Valencia, SpainLuis Martí-BonmatíDepartment of Psychological Sciences, Birkbeck University, London, UKAnne MilesArlington Innovation Center for Health Research, Virginia Tech, Arlington, USASeong Ki MunDepartment of Radiology, Stanford University, Stanford, USASandy NapelDepartment of Radiology, Imperial College Healthcare NHS Trust, London, UKAndrea Rockall & Nicola StricklandDepartment of Radiology, Cambridge University, Cambridge, UKEvis SalaDepartment of Biomedical Informatics and Department of Radiology, University of Arkansas for Medical Sciences, Little Rock, USAFred PriorAuthorsDow-Mu KohView author publicationsYou can also search for this author in
                        PubMed Google ScholarNickolas PapanikolaouView author publicationsYou can also search for this author in
                        PubMed Google ScholarUlrich BickView author publicationsYou can also search for this author in
                        PubMed Google ScholarRowland IllingView author publicationsYou can also search for this author in
                        PubMed Google ScholarCharles E. Kahn Jr.View author publicationsYou can also search for this author in
                        PubMed Google ScholarJayshree Kalpathi-CramerView author publicationsYou can also search for this author in
                        PubMed Google ScholarCelso MatosView author publicationsYou can also search for this author in
                        PubMed Google ScholarLuis Martí-BonmatíView author publicationsYou can also search for this author in
                        PubMed Google ScholarAnne MilesView author publicationsYou can also search for this author in
                        PubMed Google ScholarSeong Ki MunView author publicationsYou can also search for this author in
                        PubMed Google ScholarSandy NapelView author publicationsYou can also search for this author in
                        PubMed Google ScholarAndrea RockallView author publicationsYou can also search for this author in
                        PubMed Google ScholarEvis SalaView author publicationsYou can also search for this author in
                        PubMed Google ScholarNicola StricklandView author publicationsYou can also search for this author in
                        PubMed Google ScholarFred PriorView author publicationsYou can also search for this author in
                        PubMed Google ScholarContributionsD.-M.K.—concept, organisation, writing, reviewing and editing. N.P.—concept, writing, reviewing and editing. U.B.—writing, reviewing and editing. R.I.— writing, reviewing and editing. C.E.K., Jr—writing, reviewing and editing. J.K.-C.—writing, reviewing and editing, C.M.—writing, reviewing and editing, L.M.-B.—writing, reviewing and editing, A.M.—design, writing, reviewing and editing, S.K.M.—writing, reviewing and editing, S.N.—writing, reviewing and editing, A.R.—writing, reviewing and editing, E.S.—writing, reviewing and editing, N.S.—writing, reviewing and editing, F.P.—concept, writing, reviewing and editing.Corresponding authorCorrespondence to
                Dow-Mu Koh.Ethics declarations
Competing interests
U.B. has received patent royalties from Hologic, Inc, which has arisen from one or more of the following: US Patent 5 452 3671 [Automated method and system for the segmentation of medical images (1995)], US Patent 5 984 870 [Method and system for the automated analysis of lesions in ultrasound images (1999)]; US Patent 6 112 112 [Method and system for the assessment of tumour extent in magnetic resonance images (2000)]; US Patent 6 185 320 [Method and system for the detection of lesions in medical images 2001]]; US Patent 6 317 617 [Method, computer program product, and system for the automated analysis of lesions in magnetic resonance, mammogram and ultrasound images (2001)]. The remaining authors declare no competing interests.
Peer review
Peer review information
Communications Medicine thanks Raymond Mak and Michael Götz for their contribution to the peer review of this work.
Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary informationSupplementary InformationRights and permissions
Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.
Reprints and permissionsAbout this articleCite this articleKoh, DM., Papanikolaou, N., Bick, U. et al. Artificial intelligence and machine learning in cancer imaging.
                    Commun Med 2, 133 (2022). https://doi.org/10.1038/s43856-022-00199-0Download citationReceived: 20 December 2020Accepted: 06 October 2022Published: 27 October 2022DOI: https://doi.org/10.1038/s43856-022-00199-0Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        
Subjects

BiomarkersCancer imaging





This article is cited by





                                        Integrating classification and regression learners with bioimpedance methods for estimating weight status in infants and juveniles from the southern Cuba region
                                    


Taira Batista LunaJose Luis García BelloYohandys A. Zulueta

BMC Pediatrics (2024)




                                        The role of various physiological and bioelectrical parameters for estimating the weight status in infants and juveniles cohort from the Southern Cuba region: a machine learning study
                                    


Taira Batista LunaJose Luis García BelloYohandys A. Zulueta

BMC Pediatrics (2024)




                                        Enhancing lung cancer diagnosis with data fusion and mobile edge computing using DenseNet and CNN
                                    


Chengping ZhangMuhammad AamirYazeed Yasin Ghadi

Journal of Cloud Computing (2024)




                                        Deep learning infers clinically relevant protein levels and drug response in breast cancer from unannotated pathology images
                                    


Hui LiuXiaodong XieBin Wang

npj Breast Cancer (2024)




                                        Towards equitable AI in oncology
                                    


Vidya Sankar ViswanathanVani ParmarAnant Madabhushi

Nature Reviews Clinical Oncology (2024)






","{'headline': 'Artificial intelligence and machine learning in cancer imaging', 'description': 'An increasing array of tools is being developed using artificial intelligence (AI) and machine learning (ML) for cancer imaging. The development of an optimal tool requires multidisciplinary engagement to ensure that the appropriate use case is met, as well as to undertake robust development and testing prior to its adoption into healthcare systems. This multidisciplinary review highlights key developments in the field. We discuss the challenges and opportunities of AI and ML in cancer imaging; considerations for the development of algorithms into tools that can be widely used and disseminated; and the development of the ecosystem needed to promote growth of AI and ML in cancer imaging. Koh, Papanikolaou et al. discuss the application of artificial intelligence in cancer imaging. The authors highlight opportunities for exploiting machine learning algorithms in this field, and outline barriers in their implementation and how these might be addressed.', 'datePublished': '2022-10-27T00:00:00Z', 'dateModified': '2022-10-27T00:00:00Z', 'pageStart': '1', 'pageEnd': '14', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'sameAs': 'https://doi.org/10.1038/s43856-022-00199-0', 'keywords': ['Biomarkers', 'Cancer imaging', 'Medicine/Public Health', 'general'], 'image': ['https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs43856-022-00199-0/MediaObjects/43856_2022_199_Fig1_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs43856-022-00199-0/MediaObjects/43856_2022_199_Fig2_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs43856-022-00199-0/MediaObjects/43856_2022_199_Fig3_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs43856-022-00199-0/MediaObjects/43856_2022_199_Fig4_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs43856-022-00199-0/MediaObjects/43856_2022_199_Fig5_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs43856-022-00199-0/MediaObjects/43856_2022_199_Fig6_HTML.png'], 'isPartOf': {'name': 'Communications Medicine', 'issn': ['2730-664X'], 'volumeNumber': '2', '@type': ['Periodical', 'PublicationVolume']}, 'publisher': {'name': 'Nature Publishing Group UK', 'logo': {'url': 'https://www.springernature.com/app-sn/public/images/logo-springernature.png', '@type': 'ImageObject'}, '@type': 'Organization'}, 'author': [{'name': 'Dow-Mu Koh', 'url': 'http://orcid.org/0000-0001-7654-8011', 'affiliation': [{'name': 'Royal Marsden Hospital', 'address': {'name': 'Department of Radiology, Royal Marsden Hospital, Sutton, UK', '@type': 'PostalAddress'}, '@type': 'Organization'}], 'email': 'dowmukoh@icr.ac.uk', '@type': 'Person'}, {'name': 'Nickolas Papanikolaou', 'url': 'http://orcid.org/0000-0003-3298-2072', 'affiliation': [{'name': 'Royal Marsden Hospital', 'address': {'name': 'Department of Radiology, Royal Marsden Hospital, Sutton, UK', '@type': 'PostalAddress'}, '@type': 'Organization'}, {'name': 'Champalimaud Foundation', 'address': {'name': 'Champalimaud Foundation, Lisbon, Portugal', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Ulrich Bick', 'url': 'http://orcid.org/0000-0002-7254-8572', 'affiliation': [{'name': 'Charité – Universitätsmedizin Berlin', 'address': {'name': 'Charité – Universitätsmedizin Berlin, Berlin, Germany', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Rowland Illing', 'affiliation': [{'name': 'University College London', 'address': {'name': 'Department of Surgery & Interventional Science, University College London, London, UK', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Charles E. Kahn', 'url': 'http://orcid.org/0000-0002-6654-7434', 'affiliation': [{'name': 'University of Pennsylvania', 'address': {'name': 'Department of Radiology and Institute for Biomedical Informatics, University of Pennsylvania, Philadelphia, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Jayshree Kalpathi-Cramer', 'affiliation': [{'name': 'Massachusetts General Hospital/Harvard Medical School', 'address': {'name': 'Centre for machine learning, Massachusetts General Hospital/Harvard Medical School, Boston, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Celso Matos', 'affiliation': [{'name': 'Champalimaud Foundation', 'address': {'name': 'Champalimaud Foundation, Lisbon, Portugal', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Luis Martí-Bonmatí', 'affiliation': [{'name': 'Hospital Universitari i Politècnic La Fe', 'address': {'name': 'Department of Radiology, Hospital Universitari i Politècnic La Fe, Valencia, Spain', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Anne Miles', 'affiliation': [{'name': 'Birkbeck University', 'address': {'name': 'Department of Psychological Sciences, Birkbeck University, London, UK', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Seong Ki Mun', 'url': 'http://orcid.org/0000-0001-9661-7918', 'affiliation': [{'name': 'Virginia Tech', 'address': {'name': 'Arlington Innovation Center for Health Research, Virginia Tech, Arlington, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Sandy Napel', 'url': 'http://orcid.org/0000-0002-6876-5507', 'affiliation': [{'name': 'Stanford University', 'address': {'name': 'Department of Radiology, Stanford University, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Andrea Rockall', 'url': 'http://orcid.org/0000-0001-8270-5597', 'affiliation': [{'name': 'Imperial College Healthcare NHS Trust', 'address': {'name': 'Department of Radiology, Imperial College Healthcare NHS Trust, London, UK', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Evis Sala', 'url': 'http://orcid.org/0000-0002-5518-9360', 'affiliation': [{'name': 'Cambridge University', 'address': {'name': 'Department of Radiology, Cambridge University, Cambridge, UK', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Nicola Strickland', 'affiliation': [{'name': 'Imperial College Healthcare NHS Trust', 'address': {'name': 'Department of Radiology, Imperial College Healthcare NHS Trust, London, UK', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Fred Prior', 'url': 'http://orcid.org/0000-0002-6314-5683', 'affiliation': [{'name': 'University of Arkansas for Medical Sciences', 'address': {'name': 'Department of Biomedical Informatics and Department of Radiology, University of Arkansas for Medical Sciences, Little Rock, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}], 'isAccessibleForFree': True, '@type': 'ScholarlyArticle'}",,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiKmh0dHBzOi8vd3d3LmJiYy5jb20vbmV3cy9idXNpbmVzcy02MzMxMTg4NtIBLmh0dHBzOi8vd3d3LmJiYy5jb20vbmV3cy9idXNpbmVzcy02MzMxMTg4Ni5hbXA?oc=5,Your next job interview could take place in virtual reality - BBC.com,2022-10-30,BBC.com,https://www.bbc.com,Job interviews can now take place in VR worlds with a computer asking the questions.,N/A,Job interviews can now take place in VR worlds with a computer asking the questions.,Job interviews can now take place in VR worlds with a computer asking the questions.,http://schema.org,ReportageNewsArticle,https://www.bbc.com/news/business-63311886,"{'@type': 'ImageObject', 'width': 1024, 'height': 576, 'url': 'https://ichef.bbci.co.uk/news/1024/branded_news/1797C/production/_127463669_vr_index_bodywsaps.jpg'}","[{'@type': 'Person', 'name': 'By Elizabeth Hotson'}]","{'@type': 'NewsMediaOrganization', 'name': 'BBC News', 'publishingPrinciples': 'http://www.bbc.co.uk/news/help-41670342', 'logo': {'@type': 'ImageObject', 'url': 'https://static.files.bbci.co.uk/ws/simorgh-assets/public/news/images/metadata/poster-1024x576.png'}}",Your next job interview could take place in virtual reality,2022-10-31T00:04:24.000Z,2022-10-31T00:04:24.000Z,,,,,N/A,N/A,"Your next job interview could take place in virtual reality30 October 2022ShareElizabeth HotsonBusiness reporterShareBodyswapsHow would you like to have a job interview conducted in virtual reality by a computer?Going for a job interview is the stuff of nightmares for many people, while for others it is a chance to shine.Either way you are typically still interviewed by other human beings, either after walking into a scary office with one or more bosses sitting behind a desk, or via an equally nerve-wracking Zoom call.Yet thanks to advances in artificial intelligence (AI) and virtual reality (VR) technology, you may soon be interviewed for that job you really want... by a computer. Earlier this year students at Sandwell College in West Bromwich put on VR headsets to do some mock interviews.Their avatars - cartoon-like, 3D representations of themselves - were put through their paces by another talking avatar representing the AI software system.BodyswapsThis is what the Bodyswaps' VR job interviews look like""I'd never had an interview before in my life,"" says engineering student Ayyan Ahmed. ""But because there was no human judging me, and it was all online, I could actually express myself.""And then, at the end of the process, he [the VR interviewer] told me what I did wrong, and what I did right. It really helped me to know what to do in my next interview.""The VR system the students were using is made by London-based company Bodyswaps.Questions and requests that the interviewees get asked range from the straightforward ""tell me about your greatest achievements"", to the more challenging ""do you prefer to be loved or feared?""The user then gets feedback, not just on what they said, but how they say it, whether they are maintaining eye contact with the interviewer avatar, and even their posture.Ayyan AhmedStudent Ayyan Ahmed says he felt he could talk freely to the computer interviewerBodyswaps' chief executive Christophe Mallet says the idea is that people can keep practising with the simulated job interviews until they feel ready for a real-life one.He believes that the technology has the chance to level the playing field for candidates from less well-off backgrounds who might not otherwise have the opportunity to practise for job interviews in a professional setting.""Things like communication, empathy and leadership are the secrets to success [in interviews], but how can you practise those? If you are rich, you can get a coach. Otherwise you have e-learning [such as watching videos on the topic], but that doesn't work as you are lacking immersion.""Using VR and AI we now have a way to immerse you safely in a simulation where you can practise interview conversations.""BodyswapsChristophe Mallet says the aim is to help people more easily and effectively practise for interviewsBodyswaps launched its interview simulations last year, and says it is mainly targeting schools, colleges and universities, so their students can practise before they enter the jobs market. Its business model is subscription based, with the amount an institution has to pay each year being dependent on how many people use the system.While Bodyswaps is currently focused on mock job interrogations, the AI technology of Swedish firm Tengai is already allowing firms out in the real world to get a computer to do their initial job interviews.The AI software is represented as a talking cartoon head that appears on the screen of a candidate's computer or smartphone. It asks a set list of questions, and then the recorded answers are used to help recruiters draw up shortlists for jobs.TengaiTengai's tech sees a software system interview people via a computerised faceTengai's chief executive Elin Öberg Mårtenzon says that a central aim of the software is to avoid making wrong assumptions, such as judging someone on their appearance.""We wanted to create a situation where you could actually look at objective data instead, by putting some sort of filter between the candidate and the recruiter,"" she says.""And that filter is supposed to mitigate bias in the process… so that they know that they are not being judged by things other than those relating to the job description.""But while it might be tempting to see AI and VR technology as the magical solution for selecting the right candidate for a job, employment lawyer Susan Thompson says we need to be cautious.New Tech Economy is a series exploring how technological innovation is set to shape the new emerging economic landscape.""AI doesn't eliminate risk, and it's not without risk itself,"" says the partner at London law firm Simkins. ""So I think that's the most important thing employers need to be mindful of.""We have to remember, AI is actually still fundamentally created by a human. [So], first and foremost, I'd be saying to any employer who's thinking of using it, be it at the interview stage, or even at the exit stage, 'do you understand it, and to what extent have you spoken to the developer about what data points it uses?'""And, has it been stress tested to eliminate bias? Does it eliminate gender bias? And how does it do that?""Ms Thompson adds: ""I would be hesitant in advocating that AI is used exclusively in the interview process. I think there should be some sort of human intervention somewhere.""Artificial intelligenceEmploymentPayVirtual reality",,https://ichef.bbci.co.uk/news/1024/branded_news/1797C/production/_127463669_vr_index_bodywsaps.jpg,https://www.bbc.com/news/business-63311886,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiemh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvam9lbWNrZW5kcmljay8yMDIyLzEwLzI5L3RydXN0LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXN0aWxsLWEtd29yay1pbi1wcm9ncmVzcy1zdXJ2ZXktc2hvd3Mv0gEA?oc=5,"Trust Artificial Intelligence? Still A Work In Progress, Survey Shows - Forbes",2022-10-29,Forbes,https://www.forbes.com,"Trust is a priority, but many organizations haven’t taken steps to ensure AI is trustworthy.","Artificial Intelligence,AI","Trust is a priority, but many organizations haven’t taken steps to ensure AI is trustworthy.","Trust is a priority, but many organizations haven’t taken steps to ensure AI is trustworthy.",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/joemckendrick/2022/10/29/trust-artificial-intelligence-still-a-work-in-progress-survey-shows/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/specials-images/imageserve/635d917f6cae08519f08fd16/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Joe McKendrick', 'url': 'https://www.forbes.com/sites/joemckendrick/', 'description': 'I am an author, independent researcher and speaker exploring innovation, information technology trends and markets. I served as co-chair of the 2023 AI Summit in New York, as well as the 2021 and 2022 Summits. I regularly contribute to Harvard Business Review on AI topics. My column on service orientation appears on CNET, covering topics shaping business and technology careers. I am also a co-author of the SOA Manifesto, which outlines the values and guiding principles of service orientation in business and IT. Much of my research work is in conjunction with Forbes Insights and Unisphere Research/ Information Today, Inc., covering topics such as artificial intelligence, cloud computing, digital transformation, and big data analytics. In a previous life, I served as communications and research manager of the Administrative Management Society (AMS), an international professional association dedicated to advancing knowledge within the IT and business management fields. I am a graduate of Temple University.', 'sameAs': ['https://www.twitter.com/joemckendrick', 'Joe McKendrick']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}","Trust Artificial Intelligence? Still A Work In Progress, Survey Shows",2022-10-29T16:51:28-04:00,2022-10-29T23:39:26-04:00,Enterprise Tech,"Trust Artificial Intelligence? Still A Work In Progress, Survey Shows",True,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Enterprise Tech', 'item': 'https://www.forbes.com/enterprise-tech/'}]",Enterprise Tech,N/A,"Edit StoryInnovationEnterprise TechTrust Artificial Intelligence? Still A Work In Progress, Survey ShowsJoe McKendrickSenior ContributorOpinions expressed by Forbes Contributors are their own.I track how technology innovations move markets and careersFollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got itOct 29, 2022,04:51pm EDTUpdated Oct 29, 2022, 11:39pm EDTShare to FacebookShare to TwitterShare to LinkedinIs AI trustworthy?getty
Our dependency on AI-based outputs seems to grow every day, both from a business as well as personal perspective. But are we willing to fully trust this output? Are we sure the data fed into these systems is accurate? Are the decision models and algorithms kept up to date? Is it free of bias? Are humans kept in the loop?


The answers to these questions are all still up in the air, according to a recent survey of 7,502 business around the world, commissioned by IBM in partnership with Morning Consult.

AI usage just keeps growing. Right now, 35% of companies use AI in their business — up from 31% a year ago. An additional 42% are exploring AI. There are benefits — such as cost savings and efficiencies (54%), improvements in IT or network performance (53%), and better experiences for customers (48%).

PROMOTED
Trust is a priority, but many organizations haven’t taken enough steps to ensure AI is trustworthy, the survey also shows. Eighty-five percent of respondents agree that consumers are more likely to choose a company that’s transparent about how its AI models are built, managed and used. In addition, 84% say that “being able to explain how their AI arrives at different decisions is important to their business.”

Maintaining brand integrity and customer trust is the most important reason to pursue AI trust, cited by 56% of managers. Another 50% say meeting external regulatory and compliance obligations is key, and 48% cite the ability to govern data and AI across the entire lifecycle. Another 48% seek the ability to monitor data and AI across the lifecycle.
MORE FROMFORBES ADVISORBest Travel Insurance CompaniesByAmy DaniseEditorBest Covid-19 Travel Insurance PlansByAmy DaniseEditor
A majority of respondents say they lag in many efforts to ensure trust — from finding the right skills to proactive efforts to avoid bias. A majority organizations haven’t taken key steps to ensure their AI is trustworthy and responsible, such as reducing bias (74%), tracking performance variations and model drift (68%), and making sure they can explain AI-powered decisions (61%).









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



“A significant challenge is that the field of applied AI ethics is still relatively new, and most companies cite a lack of skills and training,” the survey’s authors state. Leading challenges to assuring greater trust in AI include the following:


Lack of skills and training to develop and manage trustworthy 63%
AI governance/mgmt tools that don’t work across all environments 60%
Lack of an AI strategy 59%
AI outcomes that aren’t explainable 57%
Lack of company guidelines for developing trustworthy, ethical AI 57%
AI vendors who don’t include explainability features 57%
Lack of regulatory guidance from governments or industry 56%
Building models on data that has inherent bias 56%


The good news is the more likely a company is to have deployed AI, the more likely it is to value the importance of trustworthiness, the survey’s authors state. IT professionals at businesses currently deploying AI are 17% more likely to report that their business values AI explainability than those that are simply exploring AI.
The most activity associated with AI trust focuses on safeguarding data privacy, the survey also shows.
Follow me on Twitter. Joe McKendrickFollowingFollowI am an author, independent researcher and speaker exploring innovation, information technology trends and markets. I served as co-chair of the... Read MoreEditorial StandardsPrintReprints & Permissions",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiO2h0dHBzOi8vc2lsaWNvbmNhbmFscy5jb20vMy1ldXJvcGVhbi1sb2NhdGlvbnMtZm9yLWFpLWpvYnMv0gEA?oc=5,3 great European locations for artificial intelligence jobs - Silicon Canals,2022-10-27,Silicon Canals,https://siliconcanals.com,Jobs in artificial intelligence and machine learning are considered to be a secure bet.,N/A,Jobs in artificial intelligence and machine learning are considered to be a secure bet.,Jobs in artificial intelligence and machine learning are considered to be a secure bet.,https://schema.org,,,,,,,,,,,,,Guest contributions,N/A,"



Home » Guest contributions


3 great European locations for artificial intelligence jobs








By
Guest Contributor





ShareLinkedInFacebookXWhatsAppTelegram













By
Guest Contributor


|
October 27, 2022
|
Last update:
March 25, 2024








Guest contributions



Image credit: Depositphotos




LinkedInFacebookWhatsAppXTelegramShare





There are some careers within tech that are considered to be more robust or recession-proof than others: for example cybersecurity and cloud computing, which often go hand-in-hand thanks to the rise in adoption of SaaS providers and IoT services, meaning our data needs more protection than ever.
Jobs in artificial intelligence and machine learning are also considered to be a secure bet. As AI creates new ways of automating tasks and processes, it is making some roles redundant – but it is leading to the creation of many more for knowledge workers in particular. According to PwC’s Global Artificial Intelligence Study, AI has the potential to contribute to the global economy by 2030, and provide a 26% boost in GDP for local economies.





The European Commission is wise to the opportunity. It aims to make the EU a world-class hub for AI, while ensuring that the tech is human-centric and trustworthy. “Europe is well placed to benefit from the potential of AI, not only as a user but also as a creator and a producer of this technology. It has excellent research centres, innovative start-ups, a world-leading position in robotics and competitive manufacturing and services sectors, from automotive to healthcare, energy, financial services and agriculture,” says the Commission’s white paper On Artificial Intelligence – A European approach to excellence and trust.
While Europe isn’t at U.S. levels in terms of AI startups, it comes in second globally. The number of AI startups in European states was 769 in 2021, compared to the United States’ 1,393. Equally, in terms of education, Europe takes second place in terms of the number of AI-specialised Master’s programs, with Germany having the highest overall number of AI programmes, and France leading with AI Master’s courses.
Within Europe as a whole, Statista reports that the UK was the highest-ranked Western European country for AI in 2020. It is closely followed by Finland, Germany, and Sweden, but there are other players with potential too, such as Denmark and the Netherlands. Below, we’re taking a look at three European locations for artificial intelligence jobs.
The UK
The UK launched its first National Artificial Intelligence (AI) strategy in 2021 to ensure it secures its place as a global science leader into the future. “The UK is a global superpower in AI and is well placed to lead the world over the next decade as a genuine research and innovation powerhouse, a hive of global talent and a progressive regulatory and business environment,” the strategy says. 
- A message from our partner -





There are currently 1,486 AI companies across the UK which have secured £8.48b worth of equity investment. These include OneTrust, Graphcore, Thought Machine, Lendable and Signal AI. Twenty percent of the UK’s AI firms have been founded since 2017, indicating strong growth in the sector.
Big multinational tech companies are in this space too, such as Google, currently hiring for a Principal Engineer, Artificial Intelligence and Cerebra in London. You’ll be directly developing and landing ambient AI projects and you will prove out ambient AI projects that address customer challenges, leveraging advances in privacy technologies, models, and signals that devices have access to. Find out more here.
Germany
As Europe’s biggest economy, it’s no surprise that Germany is a leading location for artificial intelligence. Home to the German Research Centre for Artificial Intelligence, one of the world’s largest nonprofit contract research institutes, which has facilities in multiple cities accessing the country including Berlin, Bremen and Lübeck.
Most of its AI activity is focused on Berlin: 48% of all German AI start-ups between 2012 and 2017 were founded in the city, and by 2025, the turnover of the AI companies in the Berlin-Brandenburg area is expected to rise to €2bn.  
Names to know include Unicsoft, Mobildev, Talentica Software, Trigma and Celadon, and big household names are innovating too, such as Mercedes-Benz Tech Innovation and Siemens AG, which is hiring in Berlin. The Consultant Artificial Intelligence/ Advanced Analytics will advise customers on business intelligence projects on the www.pulse.cloud platform and will work on the development of various models in the field of machine learning and data science as well as advanced analytics. Find out more about the role here.
The Netherlands
In 2020, the Dutch capital of Amsterdam came second in the small cities category in the Global Cities’ AI Readiness Index, which captures cities’ ability to adapt and thrive in the coming age of AI. The report found that Amsterdam was as competitive as some megacities (which included Beijing and New York), with populations of more than 10 million. 





The city is home to Amsterdam Science Park, which was the birthplace of the European internet, and more recently the home of the National Innovation Centre for AI (ICAI). This centre brings academia, industry and government together to develop AI technologies.
Top AI companies in The Netherlands include Under Reality, Eastsource, Bitfury and Dashmote and right now, Synechron is seeking a Data Scientist in Amsterdam to work on solving AI challenges for, and with customers. Your knowledge and skills will help shape the future of banking as you lead client projects to solve AI challenges and translate analytical solutions into recommendations. You can get the full job spec here.
Ready for a brand new you? Check out lots of career opportunities on the Silicon Canals Job Board today









Topics: 
Guest contributionsguestblogKnowledge & InsightsNewsStartups




Follow us: 















Guest Contributor









Featured events | Browse events
Current MonthJulyNo Events 










Partner content



01
How Deeploy gives explainable AI (XAI) a central place in MLOps






02
Meet the 11 social innovators pitching at DNNL X Social Enterprise NL Launchpad Demo Day 2024






03
Empowering Ukrainian Startups: Highlights from the PowerUp Ukraine 2024 Conference






04
Meet the 10 promising startups selected for Glovo Startup Lab






05
Technology startup Rierino partners with FeatureMind to deliver seamless e-commerce and digital transformation experiences in EMEA













Related posts





Polish martech startup Digital First AI secures €3.5M to automate marketing strategies with AI





Dutch-based Speaksee raises €1M+ to assist hearing-impaired and deaf people to participate in group conversations





Prague VC ZAKA launches new VC fund €15M to back early-stage startups in the US and EU





London’s GrowUp lands €45M to grow pesticide-free greens for the UK





CrowdStrike update hiccup causes massive global IT outage; millions of Windows users affected: Know more





Amid bankruptcy talks, Dutch firm LioniX gets €1.5M bridge financing to ensure continuity



",,,,"[{'@type': 'Organization', '@id': 'https://siliconcanals.com/#organization', 'name': 'Silicon Canals', 'sameAs': ['https://www.facebook.com/siliconcanals', 'https://twitter.com/siliconcanals', 'https://www.linkedin.com/company/silicon-canals/'], 'logo': {'@type': 'ImageObject', '@id': 'https://siliconcanals.com/#logo', 'url': 'https://siliconcanals.com/wp-content/uploads/2024/03/SC_logo_01_Stack_FC-RGB_Positive_1200-1-1.png', 'contentUrl': 'https://siliconcanals.com/wp-content/uploads/2024/03/SC_logo_01_Stack_FC-RGB_Positive_1200-1-1.png', 'caption': 'Silicon Canals', 'inLanguage': 'en-GB', 'width': '2560', 'height': '1060'}}, {'@type': 'WebSite', '@id': 'https://siliconcanals.com/#website', 'url': 'https://siliconcanals.com', 'name': 'Silicon Canals', 'publisher': {'@id': 'https://siliconcanals.com/#organization'}, 'inLanguage': 'en-GB'}, {'@type': 'ImageObject', '@id': 'https://3a18c69c.rocketcdn.me/wp-content/uploads/2024/03/netherlands-3.jpg', 'url': 'https://3a18c69c.rocketcdn.me/wp-content/uploads/2024/03/netherlands-3.jpg', 'width': '1200', 'height': '628', 'inLanguage': 'en-GB'}, {'@type': 'BreadcrumbList', '@id': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': '1', 'item': {'@id': 'https://siliconcanals.com', 'name': 'Home'}}, {'@type': 'ListItem', 'position': '2', 'item': {'@id': 'https://siliconcanals.com/guest-contributions/', 'name': 'Guest contributions'}}, {'@type': 'ListItem', 'position': '3', 'item': {'@id': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/', 'name': '3 great European locations for artificial intelligence jobs'}}]}, {'@type': 'WebPage', '@id': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/#webpage', 'url': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/', 'name': '3 great European locations for artificial intelligence jobs - Silicon Canals', 'datePublished': '2022-10-27T13:01:48+02:00', 'dateModified': '2024-03-25T09:13:45+02:00', 'isPartOf': {'@id': 'https://siliconcanals.com/#website'}, 'primaryImageOfPage': {'@id': 'https://3a18c69c.rocketcdn.me/wp-content/uploads/2024/03/netherlands-3.jpg'}, 'inLanguage': 'en-GB', 'breadcrumb': {'@id': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/#breadcrumb'}}, {'@type': 'Person', '@id': 'https://siliconcanals.com/author/guest123contributor/', 'name': 'Guest Contributor', 'url': 'https://siliconcanals.com/author/guest123contributor/', 'image': {'@type': 'ImageObject', '@id': 'https://secure.gravatar.com/avatar/586d658a67b5f8052231a52dc28a56ba?s=96&amp;d=mm&amp;r=g', 'url': 'https://secure.gravatar.com/avatar/586d658a67b5f8052231a52dc28a56ba?s=96&amp;d=mm&amp;r=g', 'caption': 'Guest Contributor', 'inLanguage': 'en-GB'}, 'worksFor': {'@id': 'https://siliconcanals.com/#organization'}}, {'@type': 'NewsArticle', 'headline': '3 great European locations for artificial intelligence jobs - Silicon Canals', 'keywords': 'jobs', 'datePublished': '2022-10-27T13:01:48+02:00', 'dateModified': '2024-03-25T09:13:45+02:00', 'articleSection': 'Guest contributions, guestblog, Knowledge &amp; Insights, News, Startups', 'author': {'@id': 'https://siliconcanals.com/author/guest123contributor/', 'name': 'Guest Contributor'}, 'publisher': {'@id': 'https://siliconcanals.com/#organization'}, 'description': 'Jobs in artificial intelligence and machine learning are considered to be a secure bet.', 'name': '3 great European locations for artificial intelligence jobs - Silicon Canals', '@id': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/#richSnippet', 'isPartOf': {'@id': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/#webpage'}, 'image': {'@id': 'https://3a18c69c.rocketcdn.me/wp-content/uploads/2024/03/netherlands-3.jpg'}, 'inLanguage': 'en-GB', 'mainEntityOfPage': {'@id': 'https://siliconcanals.com/3-european-locations-for-ai-jobs/#webpage'}}]",,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiUGh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjIvMTAvMjgvYXJ0cy9tdXNpYy9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1vcGVyYS5odG1s0gEA?oc=5,"Music, Science and Healing Intersect in an A.I. Opera (Published 2022) - The New York Times",2022-10-31,The New York Times,https://www.nytimes.com,The work-in-progress “Song of the Ambassadors” got a test run at Alice Tully Hall — with Lincoln Center’s artistic director lending her brain.,N/A,The work-in-progress “Song of the Ambassadors” got a test run at Alice Tully Hall — with Lincoln Center’s artistic director lending her brain.,The work-in-progress “Song of the Ambassadors” got a test run at Alice Tully Hall — with Lincoln Center’s artistic director lending her brain.,https://schema.org,BreadcrumbList,https://www.nytimes.com/,"[{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2022/10/28/arts/28AI-Notebook-1/28AI-Notebook-1-videoSixteenByNineJumbo1600.jpg', 'height': 900, 'width': 1600, 'contentUrl': 'https://static01.nyt.com/images/2022/10/28/arts/28AI-Notebook-1/28AI-Notebook-1-videoSixteenByNineJumbo1600.jpg', 'caption': 'Shanta Thake, Lincoln Center’s artistic director, participating in a performance of “Song of the Ambassadors” at Alice Tully Hall.', 'creditText': 'Vincent Tullo for The New York Times'}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2022/10/28/arts/28AI-Notebook-1/28AI-Notebook-1-superJumbo.jpg', 'height': 1365, 'width': 2048, 'contentUrl': 'https://static01.nyt.com/images/2022/10/28/arts/28AI-Notebook-1/28AI-Notebook-1-superJumbo.jpg', 'caption': 'Shanta Thake, Lincoln Center’s artistic director, participating in a performance of “Song of the Ambassadors” at Alice Tully Hall.', 'creditText': 'Vincent Tullo for The New York Times'}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2022/10/28/arts/28AI-Notebook-1/28AI-Notebook-1-mediumSquareAt3X.jpg', 'height': 1800, 'width': 1800, 'contentUrl': 'https://static01.nyt.com/images/2022/10/28/arts/28AI-Notebook-1/28AI-Notebook-1-mediumSquareAt3X.jpg', 'caption': 'Shanta Thake, Lincoln Center’s artistic director, participating in a performance of “Song of the Ambassadors” at Alice Tully Hall.', 'creditText': 'Vincent Tullo for The New York Times'}]","[{'@context': 'https://schema.org', '@type': 'Person', 'url': 'https://www.nytimes.com/by/frank-rose', 'name': 'Frank Rose'}]","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}","Music, Science and Healing Intersect in an A.I. Opera",2022-10-28T20:07:41.000Z,2022-10-31T04:00:11.000Z,,The New York Times,False,"[{'@context': 'https://schema.org', '@type': 'ListItem', 'name': 'Arts', 'position': 1, 'item': 'https://www.nytimes.com/section/arts'}, {'@context': 'https://schema.org', '@type': 'ListItem', 'name': 'Music', 'position': 2, 'item': 'https://www.nytimes.com/section/arts/music'}]",Arts,N/A,"Shanta Thake, Lincoln Center’s artistic director, participating in a performance of “Song of the Ambassadors” at Alice Tully Hall.Credit...Vincent Tullo for The New York TimesMusic, Science and Healing Intersect in an A.I. OperaThe work-in-progress “Song of the Ambassadors” got a test run at Alice Tully Hall — with Lincoln Center’s artistic director lending her brain.Shanta Thake, Lincoln Center’s artistic director, participating in a performance of “Song of the Ambassadors” at Alice Tully Hall.Credit...Vincent Tullo for The New York TimesSupported bySKIP ADVERTISEMENTShare full article6Read in appBy Frank RosePublished Oct. 28, 2022Updated Oct. 31, 2022“This is what your brain was doing!” a Lincoln Center staffer said to Shanta Thake, the performing arts complex’s artistic director, while swiping through some freshly taken photos.It was the end of a recent rehearsal at Alice Tully Hall for “Song of the Ambassadors,” a work-in-progress that fuses elements of traditional opera with artificial intelligence and neuroscience, and the photos did appear to show Thake’s brain doing something remarkable: generating images of flowers. Bright, colorful, fantastical flowers of no known species or genus, morphing continuously in size, color and shape, as if botany and fluid dynamics had somehow merged.“Song of the Ambassadors,” which was presented to the public at Tully on Tuesday evening, was created by K Allado-McDowell, who leads the Artists and Machine Intelligence initiative at Google, with the A.I. program GPT-3; the composer Derrick Skye, who integrates electronics and non-Western motifs into his work; and the data artist Refik Anadol, who contributed A.I.-generated visualizations. There were three singers — “ambassadors” to the sun, space and life — as well as a percussionist, a violinist and a flute player. Thake, sitting silently to one side of the stage with a simple, inexpensive EEG monitor on her head, was the “brainist,” feeding brain waves into Anadol’s A.I. algorithm to generate the otherworldly patterns.AdvertisementSKIP ADVERTISEMENT“I’m using my brain as a prop,” she said in an interview.ImageThe “ambassadors” included, from left, Debi Wong, Laurel Semerdjian and Andrew Turner.Credit...Vincent Tullo for The New York TimesImageDigital art by Refik Anadol was projected above the Tully stage.Credit...Vincent Tullo for The New York TimesJust to the side of the stage, level with the musicians, sat a pair of neuroscientists, Ying Choon Wu and Alex Khalil, who had been monitoring the brain waves of two audience volunteers sitting nearby, with their heads encased in research-grade headsets from a company called Cognionics.Wu, a scientist at the University of California, San Diego, investigates the effects of works of art on the brain; in another study, she’s observing the brain waves of people viewing paintings at the San Diego Museum of Art. Khalil, a former U.C. San Diego researcher who now teaches ethnomusicology at University College Cork in Ireland, focuses on how music gets people to synchronize their behavior. Both aim to integrate art and science.Which makes them a good match for Allado-McDowell, who first pitched “Song of the Ambassadors” in January 2021 as a participant in the Collider, a Lincoln Center fellowship program supported by the Mellon Foundation. “My proposal was to think about the concert hall as a place where healing could happen,” said Allado-McDowell, 45, who uses the gender-neutral pronouns “they” and “them.”Healing has long preoccupied them. They suffered from severe migraines for years; then, as a student at San Francisco State University, they signed up for a yoga class that took an unexpected turn. “I was besieged by rainbows,” they recalled in a forthcoming memoir. “Orbs of light flickered in my vision. Panting shallow breaths, I broke out of the teacher’s hypnotic groove and escaped to the hall outside. As I knelt on the carpet, cool liquid uncoiled in my lower back … as a glowing purple sphere pulsed gold and green in my inner vision.”AdvertisementSKIP ADVERTISEMENTThis, they were told, was a relatively mild form of kundalini awakening — kundalini being, in Hindu mythology, the serpent that is coiled at the base of the spine, a powerful energy that generally emerges from its dormant state only after extensive meditation and chanting. Others might simply have dropped yoga. “For me, it was an indication that I didn’t understand reality,” Allado-McDowell said. “It showed me that I didn’t have a functional cosmology.”ImageAudience volunteers were outfitted with research-grade headsets from a company called Cognionics.Credit...Vincent Tullo for The New York TimesWhat followed was a yearslong quest to get one. Along the way, they picked up a master’s degree in art and went to work for a Taiwanese tech company in Seattle. At one point, while sitting in a clearing in the Amazon rainforest, they had a thought: “A.I.s are the children of humanity. They need to learn to love and to be loved. Otherwise they will become psychopaths and kill everyone.”Later, in 2014, Allado-McDowell joined a nascent A.I. research team at Google. When the leader suggested collaborations with artists, they volunteered to lead the initiative. Artists and Machine Intelligence was launched in February 2016 — 50 years after “9 Evenings: Theater and Engineering,” the pioneering union of art and technology led by Robert Rauschenberg and the AT&T Bell Labs engineer Billy Kluver. The connection was not lost on Allado-McDowell.One of the earliest partnerships they established was with Anadol: first for “Archive Dreaming,” a project inspired by the Borges story “The Library of Babel,” then for “WDCH Dreams,” Anadol’s A.I.-driven projection onto the billowing steel superstructure of the Frank Gehry-designed Walt Disney Concert Hall in Los Angeles. For “Song of the Ambassadors,” Anadol said, “we are transforming brain activities in real time into an ever-changing color space.”Anadol’s artwork also responds to Skye’s music, which alternates between periods of activity and repose. “We wanted to bring people in and out of a space of meditation,” Skye said. “I carved out these long gaps where all we’re doing is environmental sounds. Then we slowly bring them out.”AdvertisementSKIP ADVERTISEMENTAll this is tied to Allado-McDowell’s goal of testing the therapeutic powers of music in a performance setting. “Might there be policy implications?” they asked. “Might there be a role that institutions could play if we know that sound and music is healing? Can that open up new possibilities for arts funding, for policy, for what is considered a therapeutic experience or an artistic experience?”The jury is still out.“We know that listening to music has an immediate impact for things like mood, attention, focus,” said Lori Gooding, an associate professor of music therapy at Florida State University and president of the American Music Therapy Association. Positive results have been found for people who have suffered a stroke, for example — but that’s after individualized therapy in a medical or professional setting. The approach in “Song of the Ambassadors,” she said, is different because of “the public aspect of it.”ImageDerrick Skye’s score was performed by musicians including the violinist Joshua Henderson.Credit...Vincent Tullo for The New York TimesImageOne goal of the project is to turn a hall like Tully into a public healing space.Credit...Vincent Tullo for The New York TimesWu and Khalil, the neuroscientists involved with the production, have yet to analyze their data. But at a panel discussion preceding Tuesday’s performance — and yes, this opera did come with a panel discussion — Khalil made a prediction that left the audience cheering.“We’ve started to understand that cognition — that is, the working of the mind — exists far outside our head,” he said. “We used to imagine that the brain is a processor and that cognition happened there. But actually, we think our minds extend throughout our bodies and beyond our bodies into the world.”With music, he continued, these extended minds can lock onto rhythms, and through the rhythms onto other minds, and then onto yet more. As for the spaces where that happens, Khalil said, “You can start to think of them as healing places.”A version of this article appears in print on Oct. 31, 2022, Section C, Page 5 of the New York edition with the headline: Making Art While Playing The Brain. Order Reprints | Today’s Paper | SubscribeRead 6 CommentsShare full article6Read in appAdvertisementSKIP ADVERTISEMENTComments 6Music, Science and Healing Intersect in an A.I. OperaSkip to CommentsThe comments section is closed.
      To submit a letter to the editor for publication, write to
      letters@nytimes.com.Enjoy unlimited access to all of The Times.6-month Welcome Offeroriginal price:   $6.25sale price:   $1/weekLearn more",,,https://www.nytimes.com/2022/10/28/arts/music/artificial-intelligence-opera.html,,en,Making Art While Playing The Brain,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.meteredContent'}",{'@id': '#commentsContainer'},6.0,"{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",2024.0,"{'@type': ['CreativeWork', 'Product'], 'name': 'The New York Times', 'productID': 'nytimes.com:basic'}","{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'height': 291, 'width': 291, 'contentUrl': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'creditText': 'The New York Times'}",https://www.nytimes.com/#publisher,https://www.nytco.com/company/diversity-and-inclusion/,https://www.nytco.com/company/standards-ethics/,https://www.nytimes.com/interactive/2023/01/28/admin/the-new-york-times-masthead.html,1851-09-18,https://en.wikipedia.org/wiki/The_New_York_Times,,
https://news.google.com/rss/articles/CBMieGh0dHBzOi8vd3d3LnNlYXR0bGV0aW1lcy5jb20vYnVzaW5lc3MvdGVjaG5vbG9neS9hcnQtZ2VuZXJhdGVkLWJ5LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWlzLXRyYW5zZm9ybWluZy1jcmVhdGl2ZS13b3JrL9IBAA?oc=5,Art generated by artificial intelligence is transforming creative work - The Seattle Times,2022-10-31,The Seattle Times,https://www.seattletimes.com,"AI-based image generators have made it possible for anyone to create unique, hyper-realistic images just by typing a few words into a text box. These apps are already astoundingly popular.",N/A,"AI-based image generators have made it possible for anyone to create unique, hyper-realistic images just by typing a few words into a text box. These apps are already astoundingly popular.","AI-based image generators have made it possible for anyone to create unique, hyper-realistic images just by typing a few words into a text box. These apps are already astoundingly popular.",https://schema.org,BreadcrumbList,,,,,,,,,,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Business', 'item': 'https://www.seattletimes.com/business/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Technology', 'item': 'https://www.seattletimes.com/business/technology/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Art generated by artificial intelligence is transforming creative work'}]",N/A,N/A,"


BusinessNation & WorldTechnology 

    Art generated by artificial intelligence is transforming creative work  

 Oct. 31, 2022 at 6:00 am  Updated Nov. 1, 2022 at 12:38 pm  







An undated image from Marlop, Stability AI Discord Community, a community created artwork from the Stability AI Discord community. A celebration for Stability AI, the start-up... (Marlop, Stability AI Discord Community via The New York Times) More  

Skip Ad 



By 

KEVIN ROOSE

The New York Times 


For years, the conventional wisdom among Silicon Valley futurists was that artificial intelligence and automation spelled doom for blue-collar workers whose jobs involved repetitive manual labor. Truck drivers, retail cashiers and warehouse workers would all lose their jobs to robots, they said, while workers in creative fields like art, entertainment and media would be safe.Well, an unexpected thing happened recently: AI entered the creative class.In the past few months, AI-based image generators like DALL-E 2, Midjourney and Stable Diffusion have made it possible for anyone to create unique, hyper-realistic images just by typing a few words into a text box.These apps, though new, are already astoundingly popular. DALL-E 2, for example, has more than 1.5 million users generating more than 2 million images every day, while Midjourney’s official Discord server has more than 3 million members.These programs use what’s known as “generative AI,” a type of AI that was popularized several years ago with the release of text-generating tools like GPT-3 but has since expanded into images, audio and video.It’s still too early to tell whether this new wave of apps will end up costing artists and illustrators their jobs. What seems clear, though, is that these tools are already being put to use in creative industries.
Advertising

Skip AdSkip AdSkip Ad 

Recently, I spoke to five creative-class professionals about how they’re using AI-generated art in their jobs.“It spit back a perfect image”Collin Waldoch, 29, a game designer in the New York City borough of Brooklyn, recently started using generative AI to create custom art for his online game, Twofer Goofer, which works a bit like a rhyming version of Wordle. Every day, players are given a clue — like “a set of rhythmic moves while in a half-conscious state” — and are tasked with coming up with a pair of rhyming words that matches the clue. (In this case, “trance dance.”)Initially, Waldoch planned to hire human artists through gig-work platform Upwork to illustrate each day’s rhyming word pair. But when he saw the cost — between $50 and $60 per image, plus time for rounds of feedback and edits — he decided to try using AI instead. He plugged word pairs into Midjourney and DreamStudio, an app based on Stable Diffusion, and tweaked the results until they looked right. Total cost: a few minutes of work, plus a few cents. (DreamStudio charges about 1 cent per image; Midjourney’s standard membership costs $30 per month for unlimited images.)“I typed in ‘carrot parrot,’ and it spit back a perfect image of a parrot made of carrots,” he said. “That was the immediate ‘aha’ moment.”Waldoch said he didn’t feel guilty about using AI instead of hiring human artists, because human artists were too expensive to make the game worthwhile.“We wouldn’t have done this” if not for AI, he said.
Advertising

Skip Ad 




        Related
        


Elon Musk presents a robot that can ‘raise the roof.’ That’s about allNvidia puts AI at center of latest GeForce graphics card upgrade





More


“I don’t feel like it will take my job away”Isabella Orsi, 24, an interior designer in San Francisco, recently used a generative AI app called InteriorAI to create a mock-up for a client.The client, a tech startup, was looking to spruce up its office. Orsi uploaded photos of the client’s office to InteriorAI, then applied a “cyberpunk” filter. The app produced new renderings in seconds — showing what the office’s entryway would look like with colored lights, contoured furniture and a new set of shelves.Orsi thinks that rather than replacing interior designers entirely, generative AI will help them come up with ideas during the initial phase of a project.“I think there’s an element of good design that requires the empathetic touch of a human,” she said. “So I don’t feel like it will take my job away. Somebody has to discern between the different renderings, and at the end of the day, I think that needs a designer.”“It’s like working with a really willful concept artist”Patrick Clair, 40, a filmmaker in Sydney, Australia, started using AI-generated art this year to help him prepare for a presentation to a film studio.
Advertising

Skip AdSkip AdSkip Ad 

Clair, who has worked on hit shows including “Westworld,” was looking for an image of a certain type of marble statue. But when he went looking on Getty Images — his usual source for concept art — he came up empty. Instead, he turned to DALL-E 2.  He predicted that rather than replacing concept artists or putting Hollywood special effects wizards out of a job, AI image generators would simply become part of every filmmaker’s tool kit.“It’s like working with a really willful concept artist,” he said. 

Most Read Business Stories


Amazon cracks down on ‘coffee badging,’ amid return-to-office push  
 
How to clean up your phone’s photo library to free up space  
 
European Union adds porn site XNXX to list of online platforms facing strictest digital scrutiny  
 
Airbus, Boeing execs diverge in style, mood as aviation’s biggest event kicks off  
 
Malicious actors trying to exploit global tech outage for their own gain   VIEW



“Photoshop can do things that you can’t do with your hands, in the same way a calculator can crunch numbers in a way that you can’t in your brain, but Photoshop never surprises you,” he continued. “Whereas DALL-E surprises you and comes back with things that are genuinely creative.”“What if we could show what the dogs playing poker looked like?”During a recent creative brainstorm, Jason Carmel, 49, an executive at New York advertising agency Wunderman Thompson, found himself wondering if AI could help.“We had three and a half good ideas,” he said of his team. “And the fourth one was just missing a visual way of describing it.”



Sponsored



Skip Ad 



Skip Ad 



Skip Ad 



Skip Ad 




The image they wanted — a group of dogs playing poker, for an ad being pitched to a pet medicine company — would have taken an artist all day to sketch. Instead, they asked DALL-E 2 to generate it.“We were like, what if we could show what the dogs playing poker looked like?” Carmel said.The resulting image didn’t end up going into an ad, but Carmel predicts that generative AI will become part of every ad agency’s creative process. He doesn’t, however, think that using AI will meaningfully speed up the agencies’ work or replace their art departments. He said many of the images generated by AI weren’t good enough to be shown to clients and that users who weren’t experienced users of these apps would probably waste a lot of time trying to formulate the right prompts.  “This is a sketch tool”Sarah Drummond, a service designer in London, started using AI-generated images a few months ago to replace the black-and-white sketches she did for her job. These were usually basic drawings that visually represented processes she was trying to design improvements for, like a group of customers lining up at a store’s cash register.Instead of spending hours creating what she called “blob drawings” by hand, Drummond, 36, now types what she wants into DALL-E 2 or Midjourney.  “Would I use it for final output? No. I would hire someone to fully make what we wanted to realize,” she said. “But the throwaway work that you do when you’re any kind of designer, whether it’s visual, architectural, urban planner — you’re sketching, sketching, sketching. And so this is a sketch tool.” 
This story was originally published at nytimes.com. Read it here. 

The Seattle Times does not append comment threads to stories from wire services such as the Associated Press, The New York Times, The Washington Post or Bloomberg News. Rather, we focus on discussions related to local stories by our own staff. You can read more about our community policies here.
",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMihwFodHRwczovL3d3dy5hZi5taWwvTmV3cy9BcnRpY2xlLURpc3BsYXkvQXJ0aWNsZS8zMjA1MzUzL2RhZi1taXQtYWktYWNjZWxlcmF0b3ItdGFja2xlcy1jaGFsbGVuZ2Utb2YtY3VsdGl2YXRpbmctZ3Jvd2luZy13b3JsZC1jbGFzcy1haS_SAQA?oc=5,"DAF-MIT AI Accelerator tackles challenge of cultivating, growing world-class AI workforce - Air Force Link",2022-10-31,Air Force Link,https://www.af.mil,N/A,N/A,"This month, the Department of the Air Force-Massachusetts Institute of Technology Artificial Intelligence Accelerator commenced their Lead AI pilot. ","This month, the Department of the Air Force-Massachusetts Institute of Technology Artificial Intelligence Accelerator commenced their Lead AI pilot. ",http://schema.org,Organization,https://www.af.mil/,,,,,,,,Air Force,,,N/A,N/A,"

DAF-MIT AI Accelerator tackles challenge of cultivating, growing world-class AI workforce 












Published Oct. 31, 2022

                        By Department of the Air Force AI Accelerator Public Affairs 
                        
                    
Department of the Air Force AI Accelerator


CAMBRIDGE, Mass. (AFNS)  --  This month, the Department of the Air Force-Massachusetts Institute of Technology Artificial Intelligence Accelerator commenced their Lead AI pilot.

This pilot, hosted by the AI Accelerator’s Education Research team, is geared towards commanders, senior non-commissioned officers, and civilians in senior leadership roles. The team received over 1,400 applicants and selected 200 participants to formulate a diverse cohort across the DAF, sister services, Defense Department schoolhouse faculty members, and other government organizations. 


    






The Department of the Air Force-Massachusetts Institute of Technology Artificial Intelligence Accelerator logo. (Contributed graphic)





During the course’s kick-off event, Col. Garry Floyd, DAF AIA director, encouraged the participants by asking, “AI is in and is coming to the battle space. The question is, what are we bringing and what is the pacing threat bringing?”

Col. Floyd continued, “As you learn about the capabilities and limitations of AI, please know that your AI Accelerator team here is working to make AI perform even better for the DAF’s use cases through our various lines of efforts.”

The AIA’s Education Research team is examining the challenge of educating, cultivating, and growing a world-class AI workforce in support of the DoD’s AI Education Strategy. With copious amounts of data and an increased reliance on data-driven technologies, the DoD must adopt AI at speed and at scale. Accelerated adoption is dependent on building and developing a workforce where all grades and roles thrive in the digital era.

For the past two years, the AI education team has tackled the challenge of how to educate an entire workforce as large and as diverse as the DAF. Previously, AI education occurred in traditional academic settings, consisting of students with prerequisite knowledge and similar backgrounds. However, to cultivate an AI-ready force, the team is investigating the process of teaching AI and machine learning capabilities to Airmen and Guardians of various rank and responsibility levels - from senior leaders, to developers and acquirers, and to operators who directly use AI-enabled capabilities.

Using the DoD AI Education Strategy as their roadmap, the research team examined variables like pedagogy, content and curriculum, and learning platforms and experiences for the six archetypes - or target audiences - identified in the strategy. The archetypes are segmented by role and expertise and fall into three major buckets: leaders, developers, and users. Along with a common foundation, concentrations within archetypes allow for tailored learning based on AI-related roles. The current pilot is geared toward senior leaders.

“In this pilot, [the cohort] will gain a practical grounding in AI and its business applications helping you transform your organizations into the workforce of the future,” said Maj. John Radovan, deputy director of the AIA during his introductory remarks. “The courses will provide a roadmap for the strategic implementation of AI technologies from the leadership context. We hope that you’ll be able to harness key AI management and leadership insights to support informed, strategic decision making.”

The current cohort may volunteer to take part in a corresponding study to better understand the optimal learning experience, guide the design for future AI education programs, and inform how to scale-up versions of the program. The diverse cohort will provide crucial data for the research team to expand and improve for the next iteration. Findings from the study will assist future business decisions to ultimately advance the AI education and training of an elite, AI-ready force.

To learn more about upcoming AI education opportunities, check for updates on the AIA website.


FS
USAF
AF
Air Force
MIT
AI
DAF-MIT AI Accelerator
innovation
research
strategy
training
future fight

",,,,,,,,,,,,,,,,,,,,"['http://www.facebook.com/Usairforce', 'http://twitter.com/usairforce', 'http://instagram.com/officialusairforce', 'http://www.youtube.com/afbluetube']",,
https://news.google.com/rss/articles/CBMiiAFodHRwczovL3d3dy4yNC03cHJlc3NyZWxlYXNlLmNvbS9wcmVzcy1yZWxlYXNlLzQ5NTM5Mi9wYXVsLXBhbGxhdGgtcGhkLWNlbGVicmF0ZWQtZm9yLWRlZGljYXRpb24tdG8tdGhlLWZpZWxkLW9mLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNl0gEA?oc=5,"Paul Pallath, PhD, Celebrated for Dedication to the Field of Artificial Intelligence - 24-7 Press Release",2022-10-27,24-7 Press Release,https://www.24-7pressrelease.com,Dr. Pallath lends years of experience to his work with Levi Strauss & Co.,PRESS RELEASE,"October 27, 2022 -- Dr. Pallath lends years of experience to his work with Levi Strauss & Co.","October 27, 2022 -- Dr. Pallath lends years of experience to his work with Levi Strauss & Co.",,,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiiwFodHRwczovL3d3dy5lZHVjYXRpb250aW1lcy5jb20vYXJ0aWNsZS9qb2ItdHJlbmRzLW1hcmtldC1tYW50cmEvOTUxMjM5ODIvaG93LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWlzLWNoYW5naW5nLXRoZS1sYW5kc2NhcGUtb2YtZWR1Y2F0aW9u0gEA?oc=5,How Artificial Intelligence is changing the landscape of education - EducationTimes.com - Education Times,2022-10-27,Education Times,https://www.educationtimes.com,"Technological solutions help students learn at their own pace by giving them a personalized experience, writes Beas Dev Ralhan - EducationTimes.com",N/A,"Technological solutions help students learn at their own pace by giving them a personalized experience, writes Beas Dev Ralhan - EducationTimes.com","Technological solutions help students learn at their own pace by giving them a personalized experience, writes Beas Dev Ralhan - EducationTimes.com",https://schema.org,BreadcrumbList,https://educationtimes.com/,,,,,,,,EducationTimes,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://educationtimes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Articles', 'item': 'https://educationtimes.com/articles'}, {'@type': 'ListItem', 'position': 3, 'name': 'job-trends-market-mantra', 'item': 'https://www.educationtimes.com//section/65780049/market-mantra'}, {'@type': 'ListItem', 'position': 4, 'name': 'How Artificial Intelligence is changing the landscape of education'}]",N/A,N/A,"WORLD HEALTH DAY: NMC launches ‘My Health, My Rights’ initiative for medical students",,,,,,,,,,,,,,https://educationtimes.com/Education.om-Logo.svg,,,,,,,,
https://news.google.com/rss/articles/CBMiaGh0dHBzOi8vc2NpdGVjaGRhaWx5LmNvbS9kcmVhbXMtZG8tY29tZS10cnVlLWhpZ2gtc2Nob29sLW5hc2EtaW50ZXJuLXdvcmtzLXdpdGgtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Uv0gEA?oc=5,Dreams Do Come True: High School NASA Intern Works with Artificial Intelligence - SciTechDaily,2022-10-29,SciTechDaily,https://scitechdaily.com,N/A,N/A,Reach for the stars because you might just become one! Drina Shah has a fascination with space exploration and engineering. Engineering and space exploration are two things that Drina Shah finds fascinating. Shah was given the chance to work on the NASA CubeSat Launch Initiative Project while he was,Reach for the stars because you might just become one! Drina Shah has a fascination with space exploration and engineering. Engineering and space exploration are two things that Drina Shah finds fascinating. Shah was given the chance to work on the NASA CubeSat Launch Initiative Project while he was,https://schema.org,,,,,,,,,,,,,Space,N/A," Dreams Do Come True: High School NASA Intern Works with Artificial IntelligenceTOPICS:EducationalNASANASA Goddard Space Flight Center By Grace Pham, NASA Johnson Space Center October 29, 2022Drina Shah standing in front of the Goddard Space Flight Center. Credit: NASA





Reach for the stars because you might just become one! Drina Shah has a fascination with space exploration and engineering. Engineering and space exploration are two things that Drina Shah finds fascinating. Shah was given the chance to work on the NASA CubeSat Launch Initiative Project while he was in high school. She was one of eight students from her school to become a finalist out of six schools from around the country.With her accomplishment from high school and her interest in space exploration and engineering, she sought out an internship with NASA whose values greatly align with hers.Artificial Intelligence ProjectShah is currently a Senior at Mooresville High School in North Carolina and a former NASA virtual intern at the Goddard Space Flight Center. The project that Shah worked on during her internship was an Artificial Intelligence based science translator for the spread of hydrological information.  Dreams Do Come TrueNASA’s mission of innovating for the benefit of humanity and inspiring the world through discovery, as well as its core values of safety, integrity, teamwork, excellence, and inclusion inspired Shah to work for NASA. This internship meant the world to her and ended up being the very first job that she has ever had.“It really was a dream come true opportunity for me and I’m sure it will help propel my career and my interest in space, engineering, and artificial intelligence,” Shah said.If you are looking for a dream opportunity, check out NASA’s internship website for more information. You can also feel free to check out other fascinating stories such as Rose Ferreira, who found hope in the Moon, and Nicholas Houghton, who has a dream of becoming an astronaut and became an intern with an exciting position.





",,,,"[{'@type': 'NewsArticle', '@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#article', 'isPartOf': {'@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/'}, 'author': 'Grace Pham, NASA Johnson Space Center', 'headline': 'Dreams Do Come True: High School NASA Intern Works with Artificial Intelligence', 'datePublished': '2022-10-29T18:07:25+00:00', 'dateModified': '2022-10-29T17:47:45+00:00', 'mainEntityOfPage': {'@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/'}, 'wordCount': 328, 'commentCount': 0, 'publisher': {'@id': 'https://scitechdaily.com/#organization'}, 'image': {'@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://scitechdaily.com/images/Drina-Shah.jpg', 'keywords': ['Educational', 'NASA', 'NASA Goddard Space Flight Center'], 'articleSection': ['Space'], 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'CommentAction', 'name': 'Comment', 'target': ['https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#respond']}]}, {'@type': 'WebPage', '@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/', 'url': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/', 'name': 'Dreams Do Come True: High School NASA Intern Works with Artificial Intelligence', 'isPartOf': {'@id': 'https://scitechdaily.com/#website'}, 'primaryImageOfPage': {'@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#primaryimage'}, 'image': {'@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://scitechdaily.com/images/Drina-Shah.jpg', 'datePublished': '2022-10-29T18:07:25+00:00', 'dateModified': '2022-10-29T17:47:45+00:00', 'breadcrumb': {'@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#primaryimage', 'url': 'https://scitechdaily.com/images/Drina-Shah.jpg', 'contentUrl': 'https://scitechdaily.com/images/Drina-Shah.jpg', 'width': 1248, 'height': 832, 'caption': 'Drina Shah standing in front of the Goddard Space Flight Center. Credit: NASA'}, {'@type': 'BreadcrumbList', '@id': 'https://scitechdaily.com/dreams-do-come-true-high-school-nasa-intern-works-with-artificial-intelligence/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'SciTechDaily', 'item': 'https://scitechdaily.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Dreams Do Come True: High School NASA Intern Works with Artificial Intelligence'}]}, {'@type': 'WebSite', '@id': 'https://scitechdaily.com/#website', 'url': 'https://scitechdaily.com/', 'name': 'SciTechDaily', 'description': 'Science, Space and Technology News 2024', 'publisher': {'@id': 'https://scitechdaily.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://scitechdaily.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://scitechdaily.com/#organization', 'name': 'SciTechDaily', 'url': 'https://scitechdaily.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://scitechdaily.com/#/schema/logo/image/', 'url': 'https://scitechdaily.com/images/scitechdaily-slogo.png', 'contentUrl': 'https://scitechdaily.com/images/scitechdaily-slogo.png', 'width': 200, 'height': 200, 'caption': 'SciTechDaily'}, 'image': {'@id': 'https://scitechdaily.com/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/scitechdaily', 'https://x.com/SciTechDaily1', 'https://www.youtube.com/@scitechdaily']}, {'@type': 'Person', '@id': 'https://scitechdaily.com/#/schema/person/0835c4deb3d8fede6625aa47fc21e767', 'name': ""Mike O'Neill"", 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://scitechdaily.com/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/d49dd0dcc99d9bf36b008d53f1d2aa2b?s=96&d=identicon&r=pg', 'contentUrl': 'https://secure.gravatar.com/avatar/d49dd0dcc99d9bf36b008d53f1d2aa2b?s=96&d=identicon&r=pg', 'caption': ""Mike O'Neill""}, 'sameAs': ['http://www.scitechdaily.com']}]",,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibmh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvc2NodXlsZXJtb29yZS8yMDIyLzEwLzI4L3doby1vd25zLXZvaWNlLWFuZC1pbWFnZS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1yaWdodHMv0gEA?oc=5,Who Owns Voice And Image Artificial Intelligence Rights? - Forbes,2022-10-28,Forbes,https://www.forbes.com,This article discusses the ownership of voice and image rights for use by artificial intelligence.,"AI Work,James Earl Jones,AI,Domicile Rule,Disney,Voice,image,artificial intelligence",This article discusses the ownership of voice and image rights for use by artificial intelligence.,This article discusses the ownership of voice and image rights for use by artificial intelligence.,http://schema.org,BreadcrumbList,https://www.forbes.com/sites/schuylermoore/2022/10/28/who-owns-voice-and-image-artificial-intelligence-rights/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/specials-images/imageserve/635c427f8cf45c068908fd18/0x0.jpg?format=jpg&crop=2330,1311,x0,y16,safe&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Schuyler Moore', 'url': 'https://www.forbes.com/sites/schuylermoore/', 'description': 'I am a partner in the corporate entertainment department of Greenberg Glusker, practicing entertainment, corporate, and tax law. I graduated from UCLA (Phi Beta Kappa, Summa Cum Laude) and UCLA Law School (first in class). I am the author of The Biz: The Basic Business, legal, and Financial Aspects of the Film Industry, a popular book in its 4th edition, Taxation of the Entertainment Industry, and What They Don’t Teach You in Law School. I have been named (a) one of the top 100 entertainment lawyers by the Hollywood Reporter, (b) one of the top 25 entertainment lawyers by Variety, (c) one of the top 100 lawyers in California by the Daily Journal and (d) one of top 3 ""Most Influential Lawyers"" in media by the National Law Journal. I was an adjunct professor at both the UCLA Law School and the UCLA Anderson School of Management, teaching Entertainment Law and Finance for many years.', 'sameAs': ['https://www.twitter.com/schuylermmoore', 'https://www.greenbergglusker.com/schuyler-sky-m-moore/', 'schuyler.m.moore']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",Who Owns Voice And Image Artificial Intelligence Rights?,2022-10-28T17:01:40-04:00,2023-10-05T12:16:58-04:00,Hollywood & Entertainment,Who Owns Voice And Image Artificial Intelligence Rights?,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Business', 'item': 'https://www.forbes.com/business/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Hollywood & Entertainment', 'item': 'https://www.forbes.com/hollywood-entertainment/'}]",Hollywood & Entertainment,N/A,"More From ForbesJul 21, 2024,10:00am EDTTaylor Swift Has Three Singles From Three Different Albums On The Same Radio ChartJul 21, 2024,09:00am EDTColdplay’s ‘Viva La Vida’ Is Back–And Bigger Than EverJul 21, 2024,08:00am EDTBruce Springtseen Earns A Brand New Hit On A Chart He’s Never Appeared OnJul 21, 2024,01:29am EDTGlen Powell On The One Way Not To Approach A TornadoJul 20, 2024,05:12pm EDTRevealed: Marvel's Most Loss-Making MovieJul 20, 2024,04:35pm EDTDisney Documents Reveal Unequal Pay On ‘The Marvels’Jul 20, 2024,03:15pm EDT10 Great Movies Leaving Netflix At The End Of JulyEdit StoryForbesBusinessHollywood & EntertainmentWho Owns Voice And Image Artificial Intelligence Rights?Schuyler MooreContributorOpinions expressed by Forbes Contributors are their own.Partner at Greenberg GluskerFollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got it1Oct 28, 2022,05:01pm EDTShare to FacebookShare to TwitterShare to LinkedinDarth VaderGetty Images
With the advent of the ability of artificial intelligence (""AI"") to alter an individual's voice and image (whether in deepfakes or expressly fictional works), it is critical to determine who – if anyone – owns the right to do so, particularly when the voice or image is clearly identified with a fictional character from an existing film. This issue is highlighted by the recent license by James Earl Jones (the voice of Darth Vader) of his voice to an AI company. While articles state that the license of his voice was for use by Disney (the owner of the Star Wars franchise), the transaction raises the following questions: (a) could anyone use his voice without permission and (b) could James Earl Jones have licensed his voice to third parties for use in other films, particularly if used in the distinctive manner of Darth Vader?


This article will refer to the individual whose voice or image is at issue as the ""Individual,"" the licensee of AI rights as the ""AI Licensee,"" the new AI work incorporating the voice or image as the ""AI Work,"" and any prior work that the voice or image is taken from, or resembles elements of, as the ""Prior Work.""

The right to a voice or image can generally be divided into two categories: (a) the right of publicity (under various guises, including right of privacy, trademark, deepfake laws, or unfair competition) and (b) copyright, to the extent voice or image for the AI Work is taken from, or resembles elements of, a Prior Work.

PROMOTED
Let's first deal with the right of publicity. For simplicity, this article does not discuss whether or not a particular court has the authority to hear a case (jurisdiction over the defendant), but just the choice of law that a court that does have such jurisdiction will apply. Critically, the majority of courts in the U.S. apply the law of the domicile of the Individual (or their domicile at the time of death), by treating the right of publicity as personal property (the ""Domicile Rule""). For example, if the Individual is (or was at the time of death) domiciled in a jurisdiction that does not recognize the right of publicity, then anyone can exploit an AI Work using their voice or image in a jurisdiction that follows the Domicile Rule. However, some courts in the U.S. (and most courts outside the U.S.) apply the law of the jurisdiction where the AI Work is exploited (the ""Exploitation Rule""), such as by targeting customers in the jurisdiction, while a passive website that is merely open to the public without pay will not trigger the laws of that jurisdiction. In either case, the location of the domicile or headquarters of the AI Licensee is irrelevant.

Once it is established what laws apply, the next question is whether those laws enforce the right of publicity. While most U.S. states recognize this right during the life of the Individual, some states limit protection to celebrities, some limit it to advertising, and many don't recognize it all after the death of the Individual. In addition, many foreign countries don't recognize the right at all (or it is impossible to enforce it as a practical matter).
MORE FROMFORBES ADVISORBest Travel Insurance CompaniesByAmy DaniseEditorBest Covid-19 Travel Insurance PlansByAmy DaniseEditor
If the relevant law does protect the right of publicity, the final question will be whether the Individual's voice or image is recognizable in the AI Work, since a claim is only valid if that is the case. For example, the voice of James Earl Jones is instantly recognizable, even if most people don't know him by name, and that will almost certainly remain the case in any AI Work that uses his voice.









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



It is reported that James Earl Jones lives in New York, a state that protects the right of publicity against a commercial use and permits that right to be inherited. Thus, the AI Licensee of his voice should have enforceable rights to use his voice even after his death, both in states that follow the Domicile Rule and in states that follow the Exploitation Rule, but not in jurisdictions that do not follow either rule (e.g., many countries outside the U.S.). In addition, New York (as well as California) do not provide protection after death of the Individual for an AI Work that is for entertainment, such as a movie, so anyone could use James Earl Jones voice in another movie without permission after his death in a jurisdiction that follows the Domicile Rule.


1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGE
If the Individual has consented to the AI Work (or their consent is not required under the analysis above), the next issue to be considered is copyright, which is a two-fold inquiry: (a) whether the voice or image was taken from some Prior Work and (b) whether the AI Work resembles elements of a Prior Work.
If the voice or image of the Individual is initially copied from a Prior Work in order to be altered by AI, that copying alone is technically copyright infringement (even if the resulting AI Work does not resemble any elements of the Prior Work), although most courts would apply the fair use defense to permit the initial copying.
A separate issue is whether the AI Work resembles elements of a Prior Work, regardless of the source of the voice or image. For example, what if an AI Work is created by an AI Licensee other than Disney using the distinctive voice of James Earl Jones to create a new villain named Dark Hater that has the same voice as Darth Vader? While an Individual's voice is generally not protected by copyright, if an AI Work uses a voice or image that the public associates with a particular fictitious character (live or animated) from a Prior Work, the owner of the Prior Work may have a valid claim for copyright infringement of that character, although a claim based solely on imitating the voice of a fictitious character is untested.
So murky waters indeed, and as always, the law will be forced to catch up with technology. This will be a fun one to watch.
Follow me on Twitter. Check out my website or some of my other work here. Schuyler MooreFollowingFollowI am a partner in the corporate entertainment department of Greenberg Glusker, practicing entertainment, corporate, and tax law. I graduated from... Read MoreEditorial StandardsPrintReprints & PermissionsThe video player is currently playing an ad. You can skip the ad in 5 sec with a mouse or keyboard
1/100:26Yo Gotti Sheds Light On His Upbringing 





Skip Ad
 
Continue watchingYo Gotti Sheds Light On His Upbringing after the adVisit Advertiser websiteGO TO PAGEJoin The ConversationComments 1One Community. Many Voices. Create a free account to share your thoughts. Read our community guidelines  here.Forbes Community GuidelinesOur community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space.In order to do so, please follow the posting rules in our site's Terms of Service.  We've summarized some of those key rules below. Simply put, keep it civil.Your post will be rejected if we notice that it seems to contain:False or intentionally out-of-context or misleading informationSpamInsults, profanity, incoherent, obscene or inflammatory language or threats of any kindAttacks on the identity of other commenters or the article's authorContent that otherwise violates our site's terms.User accounts will be blocked if we notice or believe that users are engaged in:Continuous attempts to re-post comments that have been previously moderated/rejectedRacist, sexist, homophobic or other discriminatory commentsAttempts or tactics that put the site security at riskActions that otherwise violate our site's terms.So, how can you be a power user?Stay on topic and share your insightsFeel free to be clear and thoughtful to get your point across‘Like’ or ‘Dislike’ to show your point of view.Protect your community.Use the report tool to alert us when someone breaks the rules.Thanks for reading our community guidelines.  Please read the full list of posting rules found in our site's Terms of Service.",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMid2h0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L2pvYnMvdG9wLTEwLWhpZ2gtcGF5aW5nLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWludGVybnNoaXBzLXRvLWFwcGx5LWZvci10aGlzLW5vdmVtYmVy0gGBAWh0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L2FtcC9zdG9yeS9qb2JzL3RvcC0xMC1oaWdoLXBheWluZy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1pbnRlcm5zaGlwcy10by1hcHBseS1mb3ItdGhpcy1ub3ZlbWJlcg?oc=5,Top 10 High-Paying Artificial Intelligence Internships to Apply for this November - Analytics Insight,2022-10-31,Analytics Insight,https://www.analyticsinsight.net,,"High-Paying Artificial Intelligence Internships,Artificial Intelligence Internships,Artificial Intelligence,Artificial Intelligence,AI Internships",The top high-paying artificial intelligence internships to apply that can boost your career The field of artificial intelligence is rapidly expanding. And a hug,The top high-paying artificial intelligence internships to apply that can boost your career The field of artificial intelligence is rapidly expanding. And a hug,http://schema.org,NewsArticle,https://www.analyticsinsight.net/jobs/top-10-high-paying-artificial-intelligence-internships-to-apply-for-this-november,"{'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2022/10/Top-10-High-Paying-Artificial-Intelligence-Internships-to-Apply-for-this-November.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}","[{'@type': 'Person', 'givenName': 'Zaveria', 'name': 'Zaveria', 'url': 'https://www.analyticsinsight.net/author/zaveria'}]","{'@type': 'Organization', '@context': 'http://schema.org', 'name': 'Analytics Insight', 'url': 'https://www.analyticsinsight.net', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'author': 'analyticsinsight', 'contentUrl': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'url': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'name': 'logo', 'width': '', 'height': ''}, 'sameAs': ['https://whatsapp.com/channel/0029VafDe8HCBtxLV2PpRA2l', 'https://twitter.com/analyticsinme', 'https://in.pinterest.com/analyticsinsightsubmissions/_created/', 'https://www.instagram.com/analyticsinsightmagazine/', 'https://www.facebook.com/analyticsinsight.net', 'https://news.google.com/publications/CAAiEDD0Ze78owxVdNti611RNvQqFAgKIhAw9GXu_KMMVXTbYutdUTb0?hl=en-IN&gl=IN&ceid=IN%3Aen', 'https://t.me/analyticsinsightmag', 'https://www.youtube.com/channel/UCgF2J0b46YP0vvVEbgL_GuQ', 'https://www.linkedin.com/company/analytics-insight/'], 'id': 'https://www.analyticsinsight.net'}",Top 10 High-Paying Artificial Intelligence Internships to Apply for this November,2022-10-31T04:00:36Z,2022-10-31T04:00:36Z,Jobs,Top 10 High-Paying Artificial Intelligence Internships to Apply for this November,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.analyticsinsight.net'}, {'@type': 'ListItem', 'position': 2, 'name': 'Jobs', 'item': 'https://www.analyticsinsight.net/jobs'}, {'@type': 'ListItem', 'position': 3, 'name': 'Top 10 High-Paying Artificial Intelligence Internships to Apply for this November', 'item': 'https://www.analyticsinsight.net/jobs/top-10-high-paying-artificial-intelligence-internships-to-apply-for-this-november'}]",N/A,N/A,Future Growth Prospects in the Cryptocurrency Market,,https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2022/10/Top-10-High-Paying-Artificial-Intelligence-Internships-to-Apply-for-this-November.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true,"{'@type': 'WebPage', '@id': 'https://www.analyticsinsight.net/jobs/top-10-high-paying-artificial-intelligence-internships-to-apply-for-this-november'}",,,,,,,,,,"{'@type': 'WebPage', 'url': 'https://www.analyticsinsight.net/jobs/top-10-high-paying-artificial-intelligence-internships-to-apply-for-this-november', 'primaryImageOfPage': {'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2022/10/Top-10-High-Paying-Artificial-Intelligence-Internships-to-Apply-for-this-November.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}}",,,,,,,,"The top high-paying artificial intelligence internships to apply that can boost your career.The field of artificial intelligence is rapidly expanding. And a huge large number of students are expressing interest in it. Nevertheless, before entering the actual work market. High-paying artificial intelligence internships are essential because students need practical experience with issues and employment and it is also the best AI element..Many large IT organizations are looking for AI, ML, and data science experts. The development of intelligent tools and machines is a remarkable achievement made possible by AI, which has already dramatically altered the industrial environment. The growing usage of technology has spurred interest in artificial intelligence internships. Some companies are also accepting internships in artificial intelligence to assist mold the future of young individuals. This is the spot for you if you are the one looking for AI internships in India. The top high-paying artificial intelligence internships to apply to this winter to advance your career are listed below.\.1.Intern-AI/ML Models at Mercedes-Benz Research and Development.Where: Bengaluru, Karnataka, India.Duration: 6 months (Full-Time).Job Role:.Analyze &amp; evaluate AI/ML models. Understand the algorithms in depth..Research &amp; define the methodology for statistical analysis of Privacy risks in AI/ML models..An impact assessment by providing a quantitative analysis of fundamental privacy risk..2.Intern AI for Cybersecurity at Qualys  .Where: Pune, Maharashtra, India.Duration: 6 months (Full-Time).Job Role:.Conduct the literature survey, implement machine learning algorithms and evaluate their performance. Develop novel machine learning algorithms to predict malicious content or activity using data in various formats including text and tabular data. Designing and deploying Artificial Intelligence/Machine Learning Algorithms – both statistical machine learning models and Deep learning models. .3.Data Science Internship at First Tech Consulting.Where: Hyderabad, Telangana, India.Duration: 3 months (Full-time).Job Role:.Working on tasks related to the data science project.Learning and working on machine learning techniques.Acting as a mentor for our training wing-related tasks.4.Intern at IBM.Where: Gurgaon, Haryana, India.Duration: Full Time.Job Role:.As a Research Intern, you get involved in work that changes the world. What about an opportunity with an incredible and diverse career – where you can truly discover your passion? Are you looking for a culture of openness, collaboration, and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge..5.Graduate Intern Technical at Intel Corporation.Where: Bengaluru, Karnataka, India.Duration: Full Time.Job Role:.The internship will mainly focus on system-level software areas such as Deep learning frameworks, distributed systems, Deep learning algorithm development, and optimization..6.AI Internship at Oxygen to Innovation.Where: Noida, Uttar Pradesh, India.Duration: Full-Time.Job Role:.Work on GUI – user interface (UI).Work on artificial intelligence-based applications.Work on image processing and user interfaces.7.Intern-ORM at Games 24*7.Where: Mumbai, Maharashtra, India.Duration: 6 months (Full-Time).Job Role:.Daily quality check on all active social media platforms of Games24x7 (My11Circle and RummyCircle).Documenting/Updating the SOPs and process changes in the social customer support.Coordinating with the agency/ tool vendor and giving them live feedback on the tickets..Focus on end-to-end resolution and enhancing the Social CX..Hawkeye on the competitive landscape and coming up with ideas for engaging with potential consumers..Documenting in-app promotions of our competitors and sharing the same with the team..8.Intern at Stefanini Group.Where: Noida, Uttar Pradesh, India.Duration: Full-time.Job Role:.As a System Administrator, you will have responsibilities to provide the services of a Level 1 Technician. You will be the first point of contact for all infrastructure monitoring incidents and requests in a fast-paced professional environment. You will monitor, create &amp; triage incidents. Good to have troubleshooting skills on Windows Server 2012/2019. Create documentation &amp; SOP for easing out tasks &amp; resolve infrastructure-related issues. You will work in a timely and efficient manner while ensuring attendance, quality, and customer service metrics are met..9.Co-Op/Intern at AMD.Where: Bengaluru, Karnataka, India.Duration: Full-Time.Job Role:.Building blocks for the data center, artificial intelligence, PCs, gaming, and embedded systems..10.Internship at Global Technology Solutions.Where: Bengaluru, Karnataka, India.Duration: 6 months (Full-Time).Job Role:.The individual will be working on projects that require one or more of the following technologies (but not limited to): JAVA/ J2EE, C, Reactjs, SQL, Kafka DotNet, Python, Robotic Process Automation (RPA) tools (Blue Prism and UIPath)..Disclaimer: Analytics Insight does not provide financial advice or guidance. Also note that the cryptocurrencies mentioned/listed on the website could potentially be scams, i.e. designed to induce you to invest financial resources that may be lost forever and not be recoverable once investments are made. You are responsible for conducting your own research (DYOR) before making any investments. Read more here.",2022-10-31T04:00:36Z
