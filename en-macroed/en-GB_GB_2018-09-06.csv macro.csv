URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,article:section,article:summary,article text,@context,@type,headline,datePublished,dateModified,thumbnailUrl,author,publisher,image,url,articleBody,isPartOf,dateline,mainEntityOfPage,dateCreated,creator,copyrightHolder,copyrightYear,articleSection,inLanguage,isAccessibleForFree,name,foundingDate,logo,sameAs,itemListElement,@graph,potentialAction,uploadDate,contentUrl,embedUrl,duration,speakable,timeRequired,wordCount,video,hasPart,identifier,alternativeHeadline
https://news.google.com/rss/articles/CBMihwFodHRwczovL3d3dy50aGVndWFyZGlhbi5jb20vdGVjaG5vbG9neS8yMDE4L3NlcC8wOC9qb3NlcGgtc3RpZ2xpdHotb24tYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utd2VyZS1nb2luZy10b3dhcmRzLWEtbW9yZS1kaXZpZGVkLXNvY2lldHnSAYcBaHR0cHM6Ly9hbXAudGhlZ3VhcmRpYW4uY29tL3RlY2hub2xvZ3kvMjAxOC9zZXAvMDgvam9zZXBoLXN0aWdsaXR6LW9uLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXdlcmUtZ29pbmctdG93YXJkcy1hLW1vcmUtZGl2aWRlZC1zb2NpZXR5?oc=5,Joseph Stiglitz on artificial intelligence: 'We’re going towards a more divided society' - The Guardian,2018-09-08,The Guardian,https://www.theguardian.com,"The technology could vastly improve lives, the economist says – but only if the tech titans that control it are properly regulated. ‘What we have now is totally inadequate’",N/A,"The technology could vastly improve lives, the economist says – but only if the tech titans that control it are properly regulated. ‘What we have now is totally inadequate’",N/A,Technology,N/A,"‘All the worst tendencies of the private sector in taking advantage of people are heightened by these new technologies’ ... Joseph Stiglitz. Photograph: Alexandre Isard/Paris Match/Contour/Getty ImagesThe technology could vastly improve lives, the economist says – but only if the tech titans that control it are properly regulated. ‘What we have now is totally inadequate’By Ian Sample Science editorSat 8 Sep 2018 02.00 EDTLast modified on Mon 10 Sep 2018 05.10 EDTShare274274It must be hard for Joseph Stiglitz to remain an optimist in the face of the grim future he fears may be coming. The Nobel laureate and former chief economist at the World Bank has thought carefully about how artificial intelligence will affect our lives. On the back of the technology, we could build ourselves a richer society and perhaps enjoy a shorter working week, he says. But there are countless pitfalls to avoid on the way. The ones Stiglitz has in mind are hardly trivial. He worries about hamfisted moves that lead to routine exploitation in our daily lives, that leave society more divided than ever and threaten the fundamentals of democracy.“Artificial intelligence and robotisation have the potential to increase the productivity of the economy and, in principle, that could make everybody better off,” he says. “But only if they are well managed.”On 11 September, the Columbia University professor will be in London to deliver the latest lecture in the Royal Society’s You and AI series. Stiglitz will talk about the future of work, an area where predictions have been frequent, contradictory and unnerving. Last month, the Bank of England’s chief economist, Andy Haldane, warned that “large swathes” of Britain’s workforce face unemployment as AI and other technologies automate more jobs. He had less to say about the new positions AI may create. A report from PricewaterhouseCoopers in July argued that AI may create as many jobs as it destroys – perhaps even more. As with the Industrial Revolution, the misery would come not from a lack of work, but the difficulty in switching from one job to another.A distinction Stiglitz makes is between AI that replaces workers and AI that helps people to do their jobs better. It already helps doctors to work more efficiently. At Addenbrooke’s hospital in Cambridge, for example, cancer consultants spend less time than they used to planning radiotherapy for men with prostate cancer, because an AI system called InnerEye automatically marks up the gland on the patients’ scans. The doctors process patients faster, the men start treatment sooner and the radiotherapy is delivered with more precision.View image in fullscreenMicrosoft’s InnerEye project uses AI to make treatment for prostate cancer more efficient. Photograph: Microsoft Project InnerEye StudyFor other specialists, the technology is more of a threat. Well-trained AIs are now better at spotting breast tumours and other cancers than radiologists. Does that mean widespread unemployment for radiologists? It is not so straightforward, says Stiglitz. “Reading an MRI scan is only part of the job that person performs, but you can’t easily separate that task from the others.”And yet some jobs may be fully replaced. Mostly these are low-skilled roles: truck drivers, cashiers, call centre workers and more. Again, though, Stiglitz sees reasons to be cautious about what that will mean for overall unemployment. There is a strong demand for unskilled workers in education, the health service and care for older people. “If we care about our children, if we care about our aged, if we care about the sick, we have ample room to spend more on those,” Stiglitz says. If AI takes over certain unskilled jobs, the blow could be softened by hiring more people into health, education and care work and paying them a decent wage, he says.Stiglitz won the Nobel prize for economics in 2001 for his analyses of imperfect information in markets. A year later, he published Globalisation and Its Discontents, a book that laid bare his disillusion with the International Monetary Fund – the World Bank’s sister organisation – and, by extension, the US Treasury. Trade negotiations, he argued, were driven by multinationals at the expense of workers and ordinary citizens. “What I want to emphasise is that it is time to focus on the public-policy issues surrounding AI, because the concerns are a continuation of the concerns that globalisation and innovation have brought us. We were slow to grasp what they were doing and we shouldn’t make that mistake again.”Beyond the impact of AI on work, Stiglitz sees more insidious forces at play. Armed with AI, tech firms can extract meaning from the data we hand over when we search, buy and message our friends. It is used ostensibly to deliver a more personalised service. That is one perspective. Another is that our data is used against us.“These new tech giants are raising very deep issues about privacy and the ability to exploit ordinary people that were never present in earlier eras of monopoly power,” says Stiglitz. “Beforehand, you could raise the price. Now you can target particular individuals by exploiting their information.”We’ve gone from a 60-hour working week to a 45-hour week and we could go to 30 or 25Joseph StiglitzIt is the potential for datasets to be combined that most worries Stiglitz. For example, retailers can now track customers via their smartphones as they move around stores and can gather data on what catches their eye and which displays they walk straight past.“In your interactions with Google, Facebook, Twitter and others, they gather an awful lot of data about you. If that data is combined with other data, then companies have a great deal of information about you as an individual – more information than you have on yourself,” he says.“They know, for example, that people who search this way are willing to pay more. They know every store you’ve visited. That means that life is going to be increasingly unpleasant, because your decision to shop in a certain store may result in you paying more money. To the extent that people are aware of this game, it distorts their behaviour. What is clear is that it introduces a level of anxiety in everything we do and it increases inequality even more.”Stiglitz poses a question that he suspects tech firms have faced internally. “Which is the easier way to make a buck: figuring out a better way to exploit somebody, or making a better product? With the new AI, it looks like the answer is finding a better way to exploit somebody.”Grim revelations about how Russia turned to Facebook, Twitter and Google to interfere with the 2016 US election brought home how effectively people can be targeted with bespoke messages. Stiglitz is concerned that companies are using, or will use, similar tactics to exploit their customers, in particular those who are vulnerable, such as compulsive shoppers. “As opposed to a doctor who might help us manage our frailties, their objective is to take as much advantage of you as they can,” he says. “All the worst tendencies of the private sector in taking advantage of people are heightened by these new technologies.”So far, Stiglitz argues, neither governments nor tech firms have done enough to prevent such abuses. “What we have now is totally inadequate,” he says. “There is nothing to circumscribe that kind of bad behaviour and we have enough evidence that there are people who are willing to do it, who have no moral compunction.”In the US in particular, there has been a willingness to leave tech firms to thrash out decent rules of behaviour and adhere to them, Stiglitz believes. One of the many reasons is that the complexity of the technology can make it intimidating. “It overwhelms a lot of people and their response is: ‘We can’t do it, the government can’t do it, we have to leave it to the tech giants.’”View image in fullscreen‘When you have so much wealth concentrated in the hands of relatively few, you have a more unequal society and that is bad for our democracy’ ... a warehouse operated by Amazon, which is now worth more than $1tn. Photograph: Nick Ansell/PABut Stiglitz thinks that view is changing. There is a growing awareness of how companies can use data to target customers, he believes. “Initially, a lot of young people took the view that I have nothing to hide: if you behave well, what are you afraid of? People thought: ‘What harm is there to it?’ And now they realise there can be a lot of harm. I think a large fraction of Americans no longer give the tech firms the benefit of the doubt.”So, how do we get back on track? The measures Stiglitz proposes are broad and it is hard to see how they could be brought in swiftly. The regulatory structure has to be decided publicly, he says. This would include what data the tech firms can store; what data they can use; whether they can merge different datasets; the purposes for which they can use that data; and what degree of transparency they must provide about what they do with the data. “These are all issues that have to be decided,” he says. “You can’t allow the tech giants to do it. It has to be done publicly with an awareness of the danger that the tech firms represent.”Fresh policies are needed to curb monopoly powers and redistribute the immense wealth that is concentrated in the leading AI firms, he adds. This month, Amazon became the second company, after Apple, to reach a market valuation of $1tn. The pair are now worth more than the top 10 oil companies combined. “When you have so much wealth concentrated in the hands of relatively few, you have a more unequal society and that is bad for our democracy,” says Stiglitz.Taxes are not enough. To Stiglitz, this is about labour bargaining power, intellectual property rights, redefining and enforcing competition laws, corporate governance laws and the way the financial system operates. “It’s a much broader agenda than just redistribution,” he says.He is not a fan of universal basic income, a proposal under which everyone receives a no-strings handout to cover the costs of living. Advocates argue that, as tech firms gather ever more wealth, UBI could help to redistribute the proceeds and ensure that everyone benefits. But, to Stiglitz, UBI is a cop-out. He does not believe it is what most people want.“If we don’t change our overall economic and policy framework, what we’re going towards is greater wage inequality, greater income and wealth inequality and probably more unemployment and a more divided society. But none of this is inevitable,” he says. “By changing the rules, we could wind up with a richer society, with the fruits more equally divided, and quite possibly where people have a shorter working week. We’ve gone from a 60-hour working week to a 45-hour week and we could go to 30 or 25.”None of this will happen overnight, he warns. A more robust public debate around AI and work is needed to throw up new ideas, for a start. “Silicon Valley may hire a disproportionate fraction [of people who work in AI], but it may not take that many people to figure it out, including people from Silicon Valley who have become disgruntled with what has been going on,” he says. “People will, and have already begun to, think about new ideas. There will be people with skills who try to work out solutions.”Explore more on these topicsSaturday interviewArtificial intelligence (AI)ConsciousnessComputingFacebookXGooglefeaturesShareReuse this content",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDE4LzkvOC8xNzgzMzE2MC9wZW50YWdvbi1kYXJwYS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1haS1pbnZlc3RtZW500gEA?oc=5,The Pentagon plans to spend $2 billion to put more artificial intelligence into its weaponry - The Verge,2018-09-08,The Verge,https://www.theverge.com,"The Defense Department’s cutting-edge research arm has promised to make the military’s largest investment to date in artificial intelligence systems for U.S. weaponry, committing to spend up to $2 billion over the next five years in what it depicted as a new effort to make such systems more trusted and accepted by military commanders.",N/A,Officials say they want computers to be capable of explaining their decisions to military commanders,N/A,N/A,N/A,"Tech/US & WorldThe Pentagon plans to spend $2 billion to put more artificial intelligence into its weaponryThe Pentagon plans to spend $2 billion to put more artificial intelligence into its weaponry / Officials say they want computers to be capable of explaining their decisions to military commandersBy  Zachary Fryer-BiggsCenter for Public Integrity Sep 8, 2018, 6:00 AM EDTShare this story0 Comments / 0 New Photo by John Moore/Getty ImagesThe Defense Department’s cutting-edge research arm has promised to make the military’s largest investment to date in artificial intelligence (AI) systems for U.S. weaponry, committing to spend up to $2 billion over the next five years in what it depicted as a new effort to make such systems more trusted and accepted by military commanders.The director of the Defense Advanced Research Projects Agency (DARPA) announced the spending spree on the final day of a conference in Washington celebrating its sixty-year history, including its storied role in birthing the internet. The agency sees its primary role as pushing forward new technological solutions to military problems, and the Trump administration’s technical chieftains have strongly backed injecting artificial intelligence into more of America’s weaponry as a means of competing better with Russian and Chinese military forces.The DARPA investment is small by Pentagon spending standardsThe DARPA investment is small by Pentagon spending standards, where the cost of buying and maintaining new F-35 warplanes is expected to exceed a trillion dollars. But it is larger than AI programs have historically been funded and roughly what the United States spent on the Manhattan Project that produced nuclear weapons in the 1940’s, although that figure would be worth about $28 billion today due to inflation.In July defense contractor Booz Allen Hamilton received an $885 million contract to work on undescribed artificial intelligence programs over the next five years. And Project Maven, the single largest military AI project, which is meant to improve computers’ ability to pick out objects in pictures for military use, is due to get $93 million in 2019.Turning more military analytical work – and potentially some key decision-making – over to computers and algorithms installed in weapons capable of acting violently against humans is controversial.Google had been leading the Project Maven project for the department, but after an organized protest by Google employees who didn’t want to work on software that could help pick out targets for the military to kill, the company said in June it would discontinue its work after its current contract expires.While Maven and other AI initiatives have helped Pentagon weapons systems become better at recognizing targets and doing things like flying drones more effectively, fielding computer-driven systems that take lethal action on their own hasn’t been approved to date.A Pentagon strategy document released in August says advances in technology will soon make such weapons possible. “DoD does not currently have an autonomous weapon system that can search for, identify, track, select, and engage targets independent of a human operator’s input,” said the report, which was signed by top Pentagon acquisition and research officials Kevin Fahey and Mary Miller. But “technologies underpinning unmanned systems would make it possible to develop and deploy autonomous systems that could independently select and attack targets with lethal force,” the report predicted. while AI systems are technically capable of choosing targets and firing weapons, commanders have been hesitant about surrendering control The report noted that while AI systems are already technically capable of choosing targets and firing weapons, commanders have been hesitant about surrendering control to weapons platforms partly because of a lack of confidence in machine reasoning, especially on the battlefield where variables could emerge that a machine and its designers haven’t previously encountered.Right now, for example, if a soldier asks an AI system like a target identification platform to explain its selection, it can only provide the confidence estimate for its decision, DARPA’s director Steven Walker told reporters after a speech announcing the new investment – an estimate often given in percentage terms, as in the fractional likelihood that an object the system has singled out is actually what the operator was looking for.“What we’re trying to do with explainable AI is have the machine tell the human ‘here’s the answer, and here’s why I think this is the right answer’ and explain to the human being how it got to that answer,” Walker said.DARPA officials have been opaque about exactly how its newly-financed research will result in computers being able to explain key decisions to humans on the battlefield, amidst all the clamor and urgency of a conflict, but the officials said that being able to do so is critical to AI’s future in the military.Human decision-making and rationality depend on a lot more than just following rulesVaulting over that hurdle, by explaining AI reasoning to operators in real time, could be a major challenge. Human decision-making and rationality depend on a lot more than just following rules, which machines are good at. It takes years for humans to build a moral compass and commonsense thinking abilities, characteristics that technologists are still struggling to design into digital machines.“We probably need some gigantic Manhattan Project to create an AI system that has the competence of a three year old,” Ron Brachman, who spent three years managing DARPA’s AI programs ending in 2005, said earlier during the DARPA conference. “We’ve had expert systems in the past, we’ve had very robust robotic systems to a degree, we know how to recognize images in giant databases of photographs, but the aggregate, including what people have called commonsense from time to time, it’s still quite elusive in the field.”Michael Horowitz, who worked on artificial intelligence issues for Pentagon as a fellow in the Office of the Secretary of Defense in 2013 and is now a professor at the University of Pennsylvania, explained in an interview that “there’s a lot of concern about AI safety – [about] algorithms that are unable to adapt to complex reality and thus malfunction in unpredictable ways. It’s one thing if what you’re talking about is a Google search, but it’s another thing if what you’re talking about is a weapons system.”Horowitz added that if AI systems could prove they were using common sense, ”it would make it more likely that senior leaders and end users would want to use them.” An expansion of AI’s use by the military was endorsed by the Defense Science Board in 2016, which noted that machines can act more swiftly than humans in military conflicts. But with those quick decisions, it added, come doubts from those who have to rely on the machines on the battlefield.“While commanders understand they could benefit from better, organized, more current, and more accurate information enabled by application of autonomy to warfighting, they also voice significant concerns,” the report said.DARPA isn’t the only Pentagon unit sponsoring AI research. The Trump administration is now in the process of creating a new Joint Artificial Intelligence Center in that building to help coordinate all the AI-related programs across the Defense Department.But DARPA’s planned investment stands out for its scope.DARPA currently has about 25 programs focused on AI researchDARPA currently has about 25 programs focused on AI research, according to the agency, but plans to funnel some of the new money through its new Artificial Intelligence Exploration Program. That program, announced in July, will give grants up to $1 million each for research into how AI systems can be taught to understand context, allowing them to more effectively operate in complex environments.Walker said that enabling AI systems to make decisions even when distractions are all around, and to then explain those decisions to their operators will be “critically important…in a warfighting scenario.”The Center for Public Integrity is a nonprofit investigative news organization in Washington, DC.Comments0 Comments / 0 NewFeatured Videos From The VergeThe next next thing in AI and AR | The Vergecast
1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGEThe Verge's Nilay Patel, Alex Cranz, and Alex Heath discuss Apple's Vision Pro team reportedly refocusing on a cheaper headset, Meta launching a new ""Wearables"" organization, a new AI company startup from former OpenAI chief scientist, and a whole lot more tech news..Most PopularMost PopularGoogle is reportedly planning its biggest startup acquisition everAT&T reportedly gave $370,000 to a hacker to delete its stolen customer dataHere’s how much Valve pays its staff — and how few people it employsAmazon’s press-to-order Dash buttons are officially discontinuedComplaints about crashing 13th, 14th Gen Intel CPUs now have data to back them upVerge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly.Email (required)Sign upBy submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.From our sponsorAdvertiser Content From",http://schema.org/,NewsArticle,The Pentagon plans to spend $2 billion to put more artificial intelligence into its weaponry,2018-09-08T10:00:01.000Z,2018-09-08T10:00:01.000Z,https://cdn.vox-cdn.com/thumbor/XBtOye5UNIpOc8umTkVkqm5AAP4=/0x0:5184x3456/1400x788/filters:focal(2560x1414:2561x1415)/cdn.vox-cdn.com/uploads/chorus_asset/file/12869301/503879780.jpg.jpg,"[{'@type': 'Person', 'name': 'Zachary Fryer-Biggs', 'url': 'https://www.theverge.com/authors/zachary-fryer-biggs'}]","{'@type': 'Organization', 'name': 'The Verge', 'logo': {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/uploads/chorus_asset/file/24015294/verge_duet_google_news.png', 'width': 250, 'height': 50}}","[{'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/XBtOye5UNIpOc8umTkVkqm5AAP4=/0x0:5184x3456/1400x788/filters:focal(2560x1414:2561x1415)/cdn.vox-cdn.com/uploads/chorus_asset/file/12869301/503879780.jpg.jpg', 'width': 1400, 'height': 788}, {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/Rk_vT8GFV9w2xIL4SnOCbN-7sBg=/0x0:5184x3456/1400x1050/filters:focal(2560x1414:2561x1415)/cdn.vox-cdn.com/uploads/chorus_asset/file/12869301/503879780.jpg.jpg', 'width': 1400, 'height': 1050}, {'@type': 'ImageObject', 'url': 'https://cdn.vox-cdn.com/thumbor/D07y7ghEuZlANCdVSguIswHeiYc=/0x0:5184x3456/1400x1400/filters:focal(2560x1414:2561x1415)/cdn.vox-cdn.com/uploads/chorus_asset/file/12869301/503879780.jpg.jpg', 'width': 1400, 'height': 1400}]",https://www.theverge.com/2018/9/8/17833160/pentagon-darpa-artificial-intelligence-ai-investment,"The Defense Department’s cutting-edge research arm has promised to make the military’s largest investment to date in artificial intelligence (AI) systems for U.S. weaponry, committing to spend up to $2 billion over the next five years in what it depicted as a new effort to make such systems more trusted and accepted by military commanders.

The director of the Defense Advanced Research Projects Agency (DARPA) announced the spending spree on the final day of a conference in Washington celebrating its sixty-year history, including its storied role in birthing the internet. 

The agency sees its primary role as pushing forward new technological solutions to military problems, and the Trump administration’s technical chieftains have strongly backed injecting artificial intelligence into more of America’s weaponry as a means of competing better with Russian and Chinese military forces.

""The DARPA investment is small by Pentagon spending standards""

The DARPA investment is small by Pentagon spending standards, where the cost of buying and maintaining new F-35 warplanes is expected to exceed a trillion dollars. But it is larger than AI programs have historically been funded and roughly what the United States spent on the Manhattan Project that produced nuclear weapons in the 1940’s, although that figure would be worth about $28 billion today due to inflation.

In July defense contractor Booz Allen Hamilton received an $885 million contract to work on undescribed artificial intelligence programs over the next five years. And Project Maven, the single largest military AI project, which is meant to improve computers’ ability to pick out objects in pictures for military use, is due to get $93 million in 2019.

Turning more military analytical work – and potentially some key decision-making – over to computers and algorithms installed in weapons capable of acting violently against humans is controversial.

Google had been leading the Project Maven project for the department, but after an organized protest by Google employees who didn’t want to work on software that could help pick out targets for the military to kill, the company said in June it would discontinue its work after its current contract expires.

While Maven and other AI initiatives have helped Pentagon weapons systems become better at recognizing targets and doing things like flying drones more effectively, fielding computer-driven systems that take lethal action on their own hasn’t been approved to date.

A Pentagon strategy document released in August says advances in technology will soon make such weapons possible. “DoD does not currently have an autonomous weapon system that can search for, identify, track, select, and engage targets independent of a human operator’s input,” said the report, which was signed by top Pentagon acquisition and research officials Kevin Fahey and Mary Miller. 

But “technologies underpinning unmanned systems would make it possible to develop and deploy autonomous systems that could independently select and attack targets with lethal force,” the report predicted. 

""while AI systems are technically capable of choosing targets and firing weapons, commanders have been hesitant about surrendering control ""

The report noted that while AI systems are already technically capable of choosing targets and firing weapons, commanders have been hesitant about surrendering control to weapons platforms partly because of a lack of confidence in machine reasoning, especially on the battlefield where variables could emerge that a machine and its designers haven’t previously encountered.

Right now, for example, if a soldier asks an AI system like a target identification platform to explain its selection, it can only provide the confidence estimate for its decision, DARPA’s director Steven Walker told reporters after a speech announcing the new investment – an estimate often given in percentage terms, as in the fractional likelihood that an object the system has singled out is actually what the operator was looking for.

“What we’re trying to do with explainable AI is have the machine tell the human ‘here’s the answer, and here’s why I think this is the right answer’ and explain to the human being how it got to that answer,” Walker said.

DARPA officials have been opaque about exactly how its newly-financed research will result in computers being able to explain key decisions to humans on the battlefield, amidst all the clamor and urgency of a conflict, but the officials said that being able to do so is critical to AI’s future in the military.

""Human decision-making and rationality depend on a lot more than just following rules""

Vaulting over that hurdle, by explaining AI reasoning to operators in real time, could be a major challenge. Human decision-making and rationality depend on a lot more than just following rules, which machines are good at. It takes years for humans to build a moral compass and commonsense thinking abilities, characteristics that technologists are still struggling to design into digital machines.

“We probably need some gigantic Manhattan Project to create an AI system that has the competence of a three year old,” Ron Brachman, who spent three years managing DARPA’s AI programs ending in 2005, said earlier during the DARPA conference. “We’ve had expert systems in the past, we’ve had very robust robotic systems to a degree, we know how to recognize images in giant databases of photographs, but the aggregate, including what people have called commonsense from time to time, it’s still quite elusive in the field.”

Michael Horowitz, who worked on artificial intelligence issues for Pentagon as a fellow in the Office of the Secretary of Defense in 2013 and is now a professor at the University of Pennsylvania, explained in an interview that “there’s a lot of concern about AI safety – [about] algorithms that are unable to adapt to complex reality and thus malfunction in unpredictable ways. It’s one thing if what you’re talking about is a Google search, but it’s another thing if what you’re talking about is a weapons system.”

Horowitz added that if AI systems could prove they were using common sense, ”it would make it more likely that senior leaders and end users would want to use them.” 

An expansion of AI’s use by the military was endorsed by the Defense Science Board in 2016, which noted that machines can act more swiftly than humans in military conflicts. But with those quick decisions, it added, come doubts from those who have to rely on the machines on the battlefield.

“While commanders understand they could benefit from better, organized, more current, and more accurate information enabled by application of autonomy to warfighting, they also voice significant concerns,” the report said.

DARPA isn’t the only Pentagon unit sponsoring AI research. The Trump administration is now in the process of creating a new Joint Artificial Intelligence Center in that building to help coordinate all the AI-related programs across the Defense Department.

But DARPA’s planned investment stands out for its scope.

""DARPA currently has about 25 programs focused on AI research""

DARPA currently has about 25 programs focused on AI research, according to the agency, but plans to funnel some of the new money through its new Artificial Intelligence Exploration Program. That program, announced in July, will give grants up to $1 million each for research into how AI systems can be taught to understand context, allowing them to more effectively operate in complex environments.

Walker said that enabling AI systems to make decisions even when distractions are all around, and to then explain those decisions to their operators will be “critically important…in a warfighting scenario.”

The Center for Public Integrity is a nonprofit investigative news organization in Washington, DC.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibmh0dHBzOi8vd3d3LmVjb25vbWlzdC5jb20vb3Blbi1mdXR1cmUvMjAxOC8wOS8xMC9kby10aGUtYmVuZWZpdHMtb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utb3V0d2VpZ2gtdGhlLXJpc2tz0gEA?oc=5,Do the benefits of artificial intelligence outweigh the risks? - The Economist,2018-09-10,The Economist,https://www.economist.com,"We need to develop AI that aligns with human values, says Frank L. Ruta",[],"We need to develop AI that aligns with human values, says Frank L. Ruta","We need to develop AI that aligns with human values, says Frank L. Ruta",N/A,N/A,"Open Future | Open Progress: Essay competition winnerDo the benefits of artificial intelligence outweigh the risks?We need to develop AI that aligns with human values, says Frank L. Ruta Sep 10th 2018ShareBy FRANK L. RUTAThis essay is the winner of The Economist’s Open Future essay competition in the category of Open Progress, responding to the question: “Do the benefits of artificial intelligence outweigh the risks?” The winner is Frank L. Ruta, 24 years old, from America. * * * Towards the end of the second world war, a group of scientists in America working to develop an atomic bomb for the Manhattan Project warned that using the weapon would inevitably lead to a geopolitical landscape characterised by a nuclear arms race. This would force America, they said, to outpace other nations in building up nuclear armaments. They recommended that if the military did choose to use the weapon, an international effort for nuclear non-proliferation should promptly be established.Already have an account?Log inContinue with a free trialExplore all our independent journalism for free for one month. Cancel any timeFree trialOr continue reading this articleRegister nowShareReuse this contentThe Economist todayHandpicked stories, in your inboxA daily newsletter with the best of our journalismSign upYes, I agree to receive exclusive content, offers and updates to products and services from The Economist Group. I can change these preferences at any time.More from Open Future“Making real the ideals of our country”Cory Booker, a Democratic senator from New Jersey, on racial justice, fixing racial income inequality—and optimismHow society can overcome covid-19Countries can test, quarantine and prepare for the post-coronavirus world, says Larry Brilliant, an epidemiologistTelemedicine is essential amid the covid-19 crisis and after itOnline health care helps patients and medical workers—and will be a legacy of combating the novel coronavirus, says Eric Topol of Scripps ResearchMore from Open Future“Making real the ideals of our country”Cory Booker, a Democratic senator from New Jersey, on racial justice, fixing racial income inequality—and optimismHow society can overcome covid-19Countries can test, quarantine and prepare for the post-coronavirus world, says Larry Brilliant, an epidemiologistTelemedicine is essential amid the covid-19 crisis and after itOnline health care helps patients and medical workers—and will be a legacy of combating the novel coronavirus, says Eric Topol of Scripps ResearchWe can harness peer pressure to uphold social valuesPeople are natural followers, so use “behavioural contagion” to improve lives, says Robert Frank of Cornell UniversityEven noxious ideas need airing—censorship only makes them strongerRestricting free speech in the name of liberty fuels illiberalism, says Jacob Mchangama of Justitia, a Danish think-tankSociety doesn't think ahead but we can trick ourselves into doing betterThere are mental techniques to bypass our natural short-termism—and to defend our liberty, says Steven Johnson, author of ""Farsighted""",https://schema.org,BreadcrumbList,Do the benefits of artificial intelligence outweigh the risks?,2018-09-10T09:03:14Z,2018-09-10T21:46:48Z,https://www.economist.com/sites/default/files/20180414_OFP006_2.jpg,"{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}, 'url': 'https://www.economist.com/'}","{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}",https://www.economist.com/sites/default/files/20180414_OFP006_2.jpg,https://www.economist.com,,"{'@type': ['NewsArticle', 'Product'], 'name': 'The Economist', 'productID': 'economist.com:showcase'}",,https://www.economist.com/open-future/2018/09/10/do-the-benefits-of-artificial-intelligence-outweigh-the-risks,2018-09-10T09:05:29Z,"{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}","{'@type': 'NewsMediaOrganization', 'name': 'The Economist', 'logo': {'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}}",2018.0,Open Future,en,False,The Economist,1843,"{'@type': 'ImageObject', 'url': 'https://www.economist.com/engassets/google-search-logo.png'}","['https://www.facebook.com/theeconomist', 'https://www.instagram.com/theeconomist', 'https://www.twitter.com/theeconomist', 'https://www.linkedin.com/company/the-economist', 'https://www.youtube.com/user/economistmagazine', 'https://en.wikipedia.org/wiki/The_Economist']","[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://www.economist.com/open-future', 'name': 'Open Future'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://www.economist.com/open-future/2018/09/10/do-the-benefits-of-artificial-intelligence-outweigh-the-risks', 'name': 'Do the benefits of artificial intelligence outweigh the risks?'}}]",,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMieWh0dHBzOi8vd2Fyb250aGVyb2Nrcy5jb20vMjAxOC8wOS9iZXlvbmQta2lsbGVyLXJvYm90cy1ob3ctYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtY2FuLWltcHJvdmUtcmVzaWxpZW5jZS1pbi1jeWJlci1zcGFjZS_SAQA?oc=5,Beyond Killer Robots: How Artificial Intelligence Can Improve Resilience in Cyber Space - War On The Rocks,2018-09-06,War On The Rocks,https://warontherocks.com,"Recently, one of us spent a week in China discussing the future of war with a group of American and Chinese academics. Everyone speculated about the role",N/A,"Recently, one of us spent a week in China discussing the future of war with a group of American and Chinese academics. Everyone speculated about the role",N/A,N/A,N/A,N/A,https://schema.org,,,,,,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'WebSite', '@id': 'https://warontherocks.com/#website', 'url': 'https://warontherocks.com/', 'name': 'War on the Rocks', 'description': '', 'potentialAction': [{'@type': 'SearchAction', 'target': 'https://warontherocks.com/?s={search_term_string}', 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'ImageObject', '@id': 'https://warontherocks.com/2018/09/beyond-killer-robots-how-artificial-intelligence-can-improve-resilience-in-cyber-space/#primaryimage', 'inLanguage': 'en-US', 'url': 'https://warontherocks.com/wp-content/uploads/2018/09/sulmeyerdura.png', 'width': 1330, 'height': 825}, {'@type': 'WebPage', '@id': 'https://warontherocks.com/2018/09/beyond-killer-robots-how-artificial-intelligence-can-improve-resilience-in-cyber-space/#webpage', 'url': 'https://warontherocks.com/2018/09/beyond-killer-robots-how-artificial-intelligence-can-improve-resilience-in-cyber-space/', 'name': 'Beyond Killer Robots: How Artificial Intelligence Can Improve Resilience in Cyber Space \xa0 - War on the Rocks', 'isPartOf': {'@id': 'https://warontherocks.com/#website'}, 'primaryImageOfPage': {'@id': 'https://warontherocks.com/2018/09/beyond-killer-robots-how-artificial-intelligence-can-improve-resilience-in-cyber-space/#primaryimage'}, 'datePublished': '2018-09-06T07:55:20+00:00', 'dateModified': '2018-09-05T22:48:20+00:00', 'author': {'@id': 'https://warontherocks.com/#/schema/person/0d2e59fb4494488f4beea9d9eace38ff'}, 'description': 'Recently, one of us spent a week in China discussing the future of war with a group of American and Chinese academics. Everyone speculated about the role', 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://warontherocks.com/2018/09/beyond-killer-robots-how-artificial-intelligence-can-improve-resilience-in-cyber-space/']}]}, {'@type': ['Person'], '@id': 'https://warontherocks.com/#/schema/person/0d2e59fb4494488f4beea9d9eace38ff', 'name': 'Usha Sahay', 'image': {'@type': 'ImageObject', '@id': 'https://warontherocks.com/#personlogo', 'inLanguage': 'en-US', 'url': 'https://secure.gravatar.com/avatar/81f0e1389cb8a21e5a00c885069e0b6b?s=96&d=identicon&r=g', 'caption': 'Usha Sahay'}}]",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiaGh0dHBzOi8vZGF0YWNvbm9teS5jb20vMjAxOC8wOS8wNy8xNS1nbG9iYWwtaW5mbHVlbmNlcnMtaW4tYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtYW5kLW1hY2hpbmUtbGVhcm5pbmcv0gEA?oc=5,15 GLOBAL INFLUENCERS IN ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING - Dataconomy,2018-09-07,Dataconomy,https://dataconomy.com,,N/A,"It's easier to get a job if you are an Artificial Intelligence (AI) or Machine Learning (ML) ‘expert’, and investors","It's easier to get a job if you are an Artificial Intelligence (AI) or Machine Learning (ML) ‘expert’, and investors",Artificial Intelligence,N/A,"





Google’s Gemini AI experiment with robots

 July 12, 2024

",https://schema.org,VideoObject,,,,https://cdn.jwplayer.com/v2/media/dKvaFFd3/poster.jpg?width=720,,,https://dataconomy.com/wp-content/uploads/2018/09/app-bird-close-up-545332-620x396.jpg,https://dataconomy.com/wp-content/uploads/2018/09/app-bird-close-up-545332-620x396.jpg,,,,,,,,,,,,Tech and Gaming,,,,,"[{'@type': 'Organization', '@id': 'https://dataconomy.com/#organization', 'name': 'Dataconomy Media', 'sameAs': ['https://www.facebook.com/DataconomyMedia/', 'https://twitter.com/DataconomyMedia'], 'logo': {'@type': 'ImageObject', '@id': 'https://dataconomy.com/#logo', 'url': 'https://dataconomy.com/wp-content/uploads/2022/12/DC-logo-full_multicolor.png', 'contentUrl': 'https://dataconomy.com/wp-content/uploads/2022/12/DC-logo-full_multicolor.png', 'caption': 'Dataconomy', 'inLanguage': 'en-US', 'width': '326', 'height': '90'}}, {'@type': 'WebSite', '@id': 'https://dataconomy.com/#website', 'url': 'https://dataconomy.com', 'name': 'Dataconomy', 'publisher': {'@id': 'https://dataconomy.com/#organization'}, 'inLanguage': 'en-US'}, {'@type': 'ImageObject', '@id': 'https://dataconomy.com/wp-content/uploads/2018/09/app-bird-close-up-545332.jpg', 'url': 'https://dataconomy.com/wp-content/uploads/2018/09/app-bird-close-up-545332.jpg', 'width': '5781', 'height': '3697', 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://dataconomy.com/2018/09/07/15-global-influencers-in-artificial-intelligence-and-machine-learning/#webpage', 'url': 'https://dataconomy.com/2018/09/07/15-global-influencers-in-artificial-intelligence-and-machine-learning/', 'name': '15 GLOBAL INFLUENCERS IN ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING - Dataconomy', 'datePublished': '2018-09-07T10:58:45+01:00', 'dateModified': '2018-09-07T16:19:55+01:00', 'isPartOf': {'@id': 'https://dataconomy.com/#website'}, 'primaryImageOfPage': {'@id': 'https://dataconomy.com/wp-content/uploads/2018/09/app-bird-close-up-545332.jpg'}, 'inLanguage': 'en-US'}, {'@type': 'Person', '@id': 'https://dataconomy.com/author/diksha-dutta/', 'name': 'Diksha Dutta', 'url': 'https://dataconomy.com/author/diksha-dutta/', 'image': {'@type': 'ImageObject', '@id': 'https://secure.gravatar.com/avatar/18b6a3aa0d570cb48ee235a9a5d07297?s=96&amp;d=mm&amp;r=g', 'url': 'https://secure.gravatar.com/avatar/18b6a3aa0d570cb48ee235a9a5d07297?s=96&amp;d=mm&amp;r=g', 'caption': 'Diksha Dutta', 'inLanguage': 'en-US'}, 'sameAs': ['http://www.dikshadutta.com'], 'worksFor': {'@id': 'https://dataconomy.com/#organization'}}, {'@type': 'NewsArticle', 'headline': '15 GLOBAL INFLUENCERS IN ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING - Dataconomy', 'datePublished': '2018-09-07T10:58:45+01:00', 'dateModified': '2018-09-07T16:19:55+01:00', 'articleSection': 'Artificial Intelligence, Big Data, Data Science, Machine Learning, Tech Trends', 'author': {'@id': 'https://dataconomy.com/author/diksha-dutta/', 'name': 'Diksha Dutta'}, 'publisher': {'@id': 'https://dataconomy.com/#organization'}, 'description': 'It&#039;s easier to get a job if you are an Artificial Intelligence (AI) or Machine Learning (ML) ‘expert’, and investors want to bet on companies which use the', 'name': '15 GLOBAL INFLUENCERS IN ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING - Dataconomy', '@id': 'https://dataconomy.com/2018/09/07/15-global-influencers-in-artificial-intelligence-and-machine-learning/#richSnippet', 'isPartOf': {'@id': 'https://dataconomy.com/2018/09/07/15-global-influencers-in-artificial-intelligence-and-machine-learning/#webpage'}, 'image': {'@id': 'https://dataconomy.com/wp-content/uploads/2018/09/app-bird-close-up-545332.jpg'}, 'inLanguage': 'en-US', 'mainEntityOfPage': {'@id': 'https://dataconomy.com/2018/09/07/15-global-influencers-in-artificial-intelligence-and-machine-learning/#webpage'}}]","{'@type': 'SeekToAction', 'target': 'https://dataconomy.com/2018/09/07/15-global-influencers-in-artificial-intelligence-and-machine-learning/?jw_start=%7Bseek_to_second_number%7D', 'startOffset-input': 'required name=seek_to_second_number'}",2022-03-29T19:22:00.000Z,https://cdn.jwplayer.com/manifests/dKvaFFd3.m3u8?max_resolution=1280,https://dataconomy.com/2018/09/07/15-global-influencers-in-artificial-intelligence-and-machine-learning/,PT0M34S,,,,,,,
https://news.google.com/rss/articles/CBMiSGh0dHBzOi8vYWJvdXQuZmIuY29tL25ld3MvMjAxOC8wOS9pbnNpZGUtZmVlZC1zdWljaWRlLXByZXZlbnRpb24tYW5kLWFpL9IBTGh0dHBzOi8vYWJvdXQuZmIuY29tL25ld3MvMjAxOC8wOS9pbnNpZGUtZmVlZC1zdWljaWRlLXByZXZlbnRpb24tYW5kLWFpL2FtcC8?oc=5,How Facebook AI Helps Suicide Prevention - Meta,2018-09-10,Meta,https://about.fb.com,"Facebook is using machine learning to get more help to more people, faster.",N/A,"Facebook is using machine learning to get more help to more people, faster.",N/A,N/A,N/A,"


By Catherine Card, Director of Product Management
Update on September 29, 2021 at 11:50AM PT:
Information in this article may be outdated. For current information about our suicide and self-injury content detection technology, please visit our Safety Center. As described in the Safety Center, our algorithms are intended to help identify potential suicide and self-injury content and are not intended to diagnose or treat any mental health or other condition.
Originally published on September 10, 2018 at 6:00AM PT:
Around the world, a death by suicide occurs every 40 seconds and is the second-leading cause of death for 15- to 29-year-olds. In the US, nearly 45,000 people take their life every year.
When someone is expressing thoughts of suicide, it’s important to get them help as quickly as possible. Because friends and family are connected through Facebook, we can help a person in distress get in touch with people who can support them. For years, people have had the ability to report Facebook posts that they feel indicate someone is thinking about suicide. This flags the posts for review by trained members of our Community Operations team, who can connect the poster with support resources if needed.
Last year, we began to use machine learning to expand our ability to get timely help to people in need. This tool uses signals to identify posts from people who might be at risk, such as phrases in posts and concerned comments from friends and family.
Actually getting a computer to recognize suicidal expression, however, was a complex exercise in analyzing human nuance. One of the biggest challenges the team faced was that so many phrases that might indicate suicidal intent — “kill,” “die,” “goodbye” — are commonly used in other contexts. A human being might recognize that “I have so much homework I want to kill myself” is not a genuine cry of distress, but how do you teach a computer that kind of contextual understanding? Until the team managed to solve that problem, the machine learning model caught too many harmless false positives to be a useful filter for the human review team.
To train a machine learning classifier, you need to feed it tons of examples, both of what you’re trying to identify (positive examples) as well as what you’re not trying to identify (negative examples), so that the classifier learns to distinguish patterns between the two. (Check out the first video on this page for more on this concept.) Usually, you want thousands or even millions of examples in both categories. But when it comes to Facebook posts that contain suicidal expressions, the team had, thankfully, relatively few positive examples to work from. That sparse data set was dwarfed by the negative examples — that is, the entire universe of Facebook text posts that were not suicidal expressions. This imbalance only made the context challenge harder.
The team’s big breakthrough was the realization that they had a smaller and more precise set of negative examples: the set of Facebook posts that people had flagged as potentially containing thoughts of suicide, but which the trained Community Operations reviewers determined did not demonstrate a person at risk of committing self-harm. This set of negative examples contained a lot of the “I have so much homework I want to kill myself” type, which led to more precise training of the classifiers on accurate suicidal expressions.
“The smaller data set helped us develop a much more nuanced understanding of what is a suicidal pattern and what isn’t,” says Dan Muriello, an engineer on the team that produced the tools.
The text of the post is only one factor the algorithm examines to determine whether a post should be flagged for review. It also looks at comments left on the post. Here, too, there is linguistic nuance to consider — posts that reviewers determined were serious cases of people in imminent harm tended to have comments like, “Tell me where you are” or “Has anyone heard from him/her?” while potentially less-urgent posts had comments more along the lines of “Call anytime” or “I’m here for you.” Day and time of the original posting are factors, as well, as experts say the early hours of the morning and Sundays, when the workweek looms, can be common times for contemplating suicide.
Even with the introduction of these AI-fueled detection efforts, people are still core to Facebook’s success around suicide prevention. That’s why anyone who flags a potential cry for help is shown support options, including resources for help and ways to connect with loved ones.  And whether a post is reported by a concerned friend or family member or identified via machine learning, the next steps in the process remain the same. A trained member of Facebook’s Community Operations team reviews it to determine if the person is at risk — and if so, the original poster is shown support options, such as prompts to reach out to a friend and help-line phone numbers. In serious cases, when it’s determined that there may be imminent danger of self harm, Facebook may contact local authorities. Since these efforts began last year, we’ve worked with first responders on over 1,000 wellness checks based on reports we’ve received from our proactive detection efforts.
Technology can’t replace people in the process, but it can be an aid to connect more people in need with compassionate help.
“We’re not doctors, and we’re not trying to make a mental health diagnosis,” says Muriello. “We’re trying to get information to the right people quickly.”

(1) Various combinations of words can have positive or negative impact on the classifier’s confidence that the content includes a suicidal thought.
(2) The classifiers score the content based on how closely correlated the main text and comments are to previously confirmed cases of suicidal expression.
(3) The classifier scores, combined with other numerical details (e.g. time of day, day of week) are inputted into a “random forest learning algorithm,” a type of machine learning that specializes in numerical data. It uses many decision trees and outputs the mean prediction of the individual trees.
For more information:

Visit our Suicide Prevention hub on the Facebook Safety Center







 Categories: 			

Meta


Safety and Expression



 Tags: 


Artificial Intelligence and Machine Learning
 
Suicide Prevention
 




 










",https://schema.org,,,,,,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'Organization', '@id': 'https://about.fb.com/#organization', 'name': 'Meta', 'url': 'https://about.fb.com/', 'sameAs': [], 'logo': {'@type': 'ImageObject', '@id': 'https://about.fb.com/#logo', 'inLanguage': 'en-US', 'url': 'https://about.fb.com/wp-content/uploads/2023/09/Meta-logo-PNG-1.png?fit=2048%2C883', 'width': 2048, 'height': 883, 'caption': 'Meta'}, 'image': {'@id': 'https://about.fb.com/#logo'}}, {'@type': 'WebSite', '@id': 'https://about.fb.com/#website', 'url': 'https://about.fb.com/', 'name': 'Meta', 'description': '', 'publisher': {'@id': 'https://about.fb.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': 'https://about.fb.com/?s={search_term_string}', 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'ImageObject', '@id': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/#primaryimage', 'inLanguage': 'en-US', 'url': 'https://about.fb.com/wp-content/uploads/2018/09/41250458_293857231204323_1943756568713494528_n.png?fit=3557%2C2001', 'width': 3557, 'height': 2001}, {'@type': 'WebPage', '@id': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/#webpage', 'url': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/', 'name': 'How Facebook AI Helps Suicide Prevention | Meta', 'isPartOf': {'@id': 'https://about.fb.com/#website'}, 'primaryImageOfPage': {'@id': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/#primaryimage'}, 'datePublished': '2018-09-10T06:00:31+00:00', 'dateModified': '2021-09-29T18:49:22+00:00', 'description': 'Facebook is using machine learning to get more help to more people, faster.', 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/']}], 'author': 'Meta'}, {'@type': 'NewsArticle', '@id': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/#article', 'isPartOf': {'@id': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/#webpage'}, 'author': 'Catherine Card, Director of Product Management', 'headline': 'How Facebook AI Helps Suicide Prevention', 'datePublished': '2018-09-10T06:00:31+00:00', 'dateModified': '2021-09-29T18:49:22+00:00', 'mainEntityOfPage': {'@id': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/#webpage'}, 'publisher': {'@id': 'https://about.fb.com/#organization'}, 'image': {'@id': 'https://about.fb.com/news/2018/09/inside-feed-suicide-prevention-and-ai/#primaryimage'}, 'keywords': 'Artificial Intelligence and Machine Learning,Suicide Prevention', 'articleSection': 'Homepage - Hidden,Inside Feed Archive,Meta,News - Hidden,Safety and Expression', 'inLanguage': 'en-US', 'thumbnailUrl': 'https://about.fb.com/wp-content/uploads/2018/09/41250458_293857231204323_1943756568713494528_n.png?w=1200', 'articleBody': 'By\xa0Catherine Card, Director of Product Management\r\n\r\nUpdate on September 29, 2021 at 11:50AM PT:\r\n\r\nInformation in this article may be outdated. For current information about our suicide and self-injury content detection technology, please visit our Safety Center. As described in the Safety Center, our algorithms are intended to help identify potential suicide and self-injury content and are not intended to diagnose or treat any mental health or other condition.\r\n\r\nOriginally published on September 10, 2018 at 6:00AM PT:\r\n\r\nAround the world, a death by suicide occurs every 40 seconds and is the second-leading cause of death for 15- to 29-year-olds. In the US, nearly 45,000 people take their life every year.\r\n\r\nWhen someone is expressing thoughts of suicide, it’s important to get them help as quickly as possible. Because friends and family are connected through Facebook, we can help a person in distress get in touch with people who can support them. For years, people have had the ability to report Facebook posts that they feel indicate someone is thinking about suicide. This flags the posts for review by trained members of our Community Operations team, who can connect the poster with support resources if needed.\r\n\r\nLast year, we began to use machine learning to expand our ability to get timely help to people in need. This tool uses signals to identify posts from people who might be at risk, such as phrases in posts and concerned comments from friends and family.\r\n\r\nActually getting a computer to recognize suicidal expression, however, was a complex exercise in analyzing human nuance. One of the biggest challenges the team faced was that so many phrases that might indicate suicidal intent — “kill,” “die,” “goodbye” — are commonly used in other contexts. A human being might recognize that “I have so much homework I want to kill myself” is not a genuine cry of distress, but how do you teach a computer that kind of contextual understanding? Until the team managed to solve that problem, the machine learning model caught too many harmless false positives to be a useful filter for the human review team.\r\n\r\nTo train a machine learning classifier, you need to feed it tons of examples, both of what you\'re trying to identify (positive examples) as well as what you\'re not trying to identify (negative examples), so that the classifier learns to distinguish patterns between the two. (Check out the first video on this page for more on this concept.) Usually, you want thousands or even millions of examples in both categories. But when it comes to Facebook posts that contain suicidal expressions, the team had, thankfully, relatively few positive examples to work from. That sparse data set was dwarfed by the negative examples — that is, the entire universe of Facebook text posts that were not suicidal expressions. This imbalance only made the context challenge harder.\r\n\r\nThe team\'s big breakthrough was the realization that they had a smaller and more precise set of negative examples: the set of Facebook posts that people had flagged as potentially containing thoughts of suicide, but which the trained Community Operations reviewers determined did not demonstrate a person at risk of committing self-harm. This set of negative examples contained a lot of the ""I have so much homework I want to kill myself"" type, which led to more precise training of the classifiers on accurate suicidal expressions.\r\n\r\n“The smaller data set helped us develop a much more nuanced understanding of what is a suicidal pattern and what isn’t,” says Dan Muriello, an engineer on the team that produced the tools.\r\n\r\nThe text of the post is only one factor the algorithm examines to determine whether a post should be flagged for review. It also looks at comments left on the post. Here, too, there is linguistic nuance to consider — posts that reviewers determined were serious cases of people in imminent harm tended to have comments like, “Tell me where you are” or “Has anyone heard from him/her?” while potentially less-urgent posts had comments more along the lines of “Call anytime” or “I’m here for you.” Day and time of the original posting are factors, as well, as experts say the early hours of the morning and Sundays, when the workweek looms, can be common times for contemplating suicide.\r\n\r\nEven with the introduction of these AI-fueled detection efforts, people are still core to Facebook\'s success around suicide prevention. That’s why anyone who flags a potential cry for help is shown support options, including resources for help and ways to connect with loved ones.  And whether a post is reported by a concerned friend or family member or identified via machine learning, the next steps in the process remain the same. A trained member of Facebook\'s Community Operations team reviews it to determine if the person is at risk — and if so, the original poster is shown support options, such as prompts to reach out to a friend and help-line phone numbers. In serious cases, when it\'s determined that there may be imminent danger of self harm, Facebook may contact local authorities. Since these efforts began last year, we\'ve worked with first responders on over 1,000 wellness checks based on reports we\'ve received from our proactive detection efforts.\r\n\r\nTechnology can’t replace people in the process, but it can be an aid to connect more people in need with compassionate help.\r\n\r\n“We’re not doctors, and we’re not trying to make a mental health diagnosis,” says Muriello. “We’re trying to get information to the right people quickly.”\r\n\r\n\r\n\r\n(1) Various combinations of words can have positive or negative impact on the classifier’s confidence that the content includes a suicidal thought.\r\n(2) The classifiers score the content based on how closely correlated the main text and comments are to previously confirmed cases of suicidal expression.\r\n(3) The classifier scores, combined with other numerical details (e.g. time of day, day of week) are inputted into a “random forest learning algorithm,” a type of machine learning that specializes in numerical data. It uses many decision trees and outputs the mean prediction of the individual trees.\r\n\r\nFor more information:\r\n\r\n \tVisit our Suicide Prevention hub on the Facebook Safety Center', 'description': 'Facebook is using machine learning to get more help to more people, faster.', 'isAccessibleForFree': True}]",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiUmh0dHBzOi8vcGh5cy5vcmcvbmV3cy8yMDE4LTA5LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXRyYWNrLW15c3RlcmlvdXMtY29zbWljLmh0bWzSAVFodHRwczovL3BoeXMub3JnL25ld3MvMjAxOC0wOS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS10cmFjay1teXN0ZXJpb3VzLWNvc21pYy5hbXA?oc=5,Artificial intelligence helps track down mysterious cosmic radio bursts - Phys.org,2018-09-10,Phys.org,https://phys.org,"Artificial intelligence is invading many fields, most recently astronomy and the search for intelligent life in the universe, or SETI.","Science, Physics News, Science news, Technology News, Physics, Materials, Nanotech, Technology, Science","Artificial intelligence is invading many fields, most recently astronomy and the search for intelligent life in the universe, or SETI.","Artificial intelligence is invading many fields, most recently astronomy and the search for intelligent life in the universe, or SETI.",N/A,N/A,"










														September 10, 2018
														
													






Artificial intelligence helps track down mysterious cosmic radio bursts

										 by 										 University of California - Berkeley







                Breakthrough Listen researchers used artificial intelligence to search through radio signals recorded from a fast radio burst, capturing many more than humans could. They are using a similar algorithm to search for new kinds of candidate signals from extraterrestrial civilizations. Credit: Breakthrough Listen image
             

Artificial intelligence is invading many fields, most recently astronomy and the search for intelligent life in the universe, or SETI.





Researchers at Breakthrough Listen, a SETI project led by the University of California, Berkeley, have now used machine learning to discover 72 new fast radio bursts from a mysterious source some 3 billion light years from Earth.
Fast radio bursts are bright pulses of radio emission mere milliseconds in duration, thought to originate from distant galaxies. The source of these emissions is still unclear, however. Theories range from highly magnetized neutron stars blasted by gas streams from a nearby supermassive black hole, to suggestions that the burst properties are consistent with signatures of technology developed by an advanced civilization.
""This work is exciting not just because it helps us understand the dynamic behavior of fast radio bursts in more detail, but also because of the promise it shows for using machine learning to detect signals missed by classical algorithms,"" said Andrew Siemion, director of the Berkeley SETI Research Center and principal investigator for Breakthrough Listen, the initiative to find signs of intelligent life in the universe.
Breakthrough Listen is also applying the successful machine-learning algorithm to find new kinds of signals that could be coming from extraterrestrial civilizations.
While most fast radio bursts are one-offs, the source here, FRB 121102, is unique in emitting repeated bursts. This behavior has drawn the attention of many astronomers hoping to pin down the cause and the extreme physics involved in fast radio bursts.






The AI algorithms dredged up the radio signals from data were recorded over a five-hour period on Aug. 26, 2017, by the Green Bank Telescope in West Virginia. An earlier analysis of the 400 terabytes of data employed standard computer algorithms to identify 21 bursts during that period. All were seen within one hour, suggesting that the source alternates between periods of quiescence and frenzied activity, said Berkeley SETI postdoctoral researcher Vishal Gajjar.
UC Berkeley Ph.D. student Gerry Zhang and collaborators subsequently developed a new, powerful machine-learning algorithm and reanalyzed the 2017 data, finding an additional 72 bursts not detected originally. This brings the total number of detected bursts from FRB 121102 to around 300 since it was discovered in 2012.
""This work is only the beginning of using these powerful methods to find radio transients,"" said Zhang. ""We hope our success may inspire other serious endeavors in applying machine learning to radio astronomy.""
Zhang's team used some of the same techniques that internet technology companies use to optimize search results and classify images. They trained an algorithm known as a convolutional neural network to recognize bursts found by the classical search method used by Gajjar and collaborators, and then set it loose on the dataset to find bursts that the classical approach missed.
The results have helped put new constraints on the periodicity of the pulses from FRB 121102, suggesting that the pulses are not received with a regular pattern, at least if the period of that pattern is longer than about 10 milliseconds. Just as the patterns of pulses from pulsars have helped astronomers constrain computer models of the extreme physical conditions in such objects, the new measurements of FRBs will help figure out what powers these enigmatic sources, Siemion said.
""Whether or not FRBs themselves eventually turn out to be signatures of extraterrestrial technology, Breakthrough Listen is helping to push the frontiers of a new and rapidly growing area of our understanding of the Universe around us,"" he added.
The new results are described in an article accepted for publication in The Astrophysical Journal and available for download from the Breakthrough Listen website.



Journal information:
Astrophysical Journal










													Provided by
																											University of California - Berkeley










Citation:
												Artificial intelligence helps track down mysterious cosmic radio bursts (2018, September 10)
												retrieved 15 July 2024
												from https://phys.org/news/2018-09-artificial-intelligence-track-mysterious-cosmic.html
											 

											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 


",https://schema.org,BreadcrumbList,Artificial intelligence helps track down mysterious cosmic radio bursts,2018-09-10T15:32:45-04:00,2018-09-10T15:32:45-04:00,,"{'@type': 'Organization', 'name': 'Science X'}","{'@type': 'Organization', 'name': 'Phys.org', 'logo': {'@type': 'ImageObject', 'url': 'https://phys.b-cdn.net/tmpl/v6/img/logo.amp.png', 'width': 204, 'height': 60}}","{'@type': 'ImageObject', 'url': 'https://scx2.b-cdn.net/gfx/news/hires/2018/29-artificialin.jpg', 'width': 1772, 'height': 1240}",,,,,"{'@type': 'WebPage', '@id': 'https://phys.org/news/2018-09-artificial-intelligence-track-mysterious-cosmic.html'}",,,,,,,,,,,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://phys.org/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Astronomy &amp; Space', 'item': 'https://phys.org/space-news/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Astronomy', 'item': 'https://phys.org/space-news/astronomy/'}]",,,,,,,"{'@type': 'SpeakableSpecification', 'xpath': ['/html/head/title', ""/html/head/meta[@name='Description']/@content""]}",,,,,,
https://news.google.com/rss/articles/CBMiY2h0dHBzOi8vaGFja2Vybm9vbi5jb20vc2l4LXVzZWZ1bC1tZXRhcGhvcnMtZm9yLXRoaW5raW5nLWFib3V0LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWM3NDY4YjE1NTFmYdIBAA?oc=5,Six useful metaphors for thinking about artificial intelligence - hackernoon.com,2018-09-06,hackernoon.com,https://hackernoon.com,"The way AI is talked about today is mostly relating what is actually machine learning. Cassie Kozyrkov, Googles Chief Decision Scientist, calls machine learning a <a href=""https://www.youtube.com/watch?v=iLu9XyZ55oI"" target=""_blank"">fancy labeling machine</a>. And thats a very grounding and comforting way to think about it. We teach the machine to label things like apples and pears by showing many examples of fruit to train on. Thats very graspable. And if you only show it green apples and read pears thats all that it will be able to label so it’s important to have good examples.",N/A,"The way AI is talked about today is mostly relating what is actually machine learning. Cassie Kozyrkov, Googles Chief Decision Scientist, calls machine learning a <a href=""https://www.youtube.com/watch?v=iLu9XyZ55oI"" target=""_blank"">fancy labeling machine</a>. And thats a very grounding and comforting way to think about it. We teach the machine to label things like apples and pears by showing many examples of fruit to train on. Thats very graspable. And if you only show it green apples and read pears thats all that it will be able to label so it’s important to have good examples.","The way AI is talked about today is mostly relating what is actually machine learning. Cassie Kozyrkov, Googles Chief Decision Scientist, calls machine learning a <a href=""https://www.youtube.com/watch?v=iLu9XyZ55oI"" target=""_blank"">fancy labeling machine</a>. And thats a very grounding and comforting way to think about it. We teach the machine to label things like apples and pears by showing many examples of fruit to train on. Thats very graspable. And if you only show it green apples and read pears thats all that it will be able to label so it’s important to have good examples.",N/A,N/A,"🔥Enter the #ethereum Writing Contest and Compete for $1,000 in Prizes🔥Today at 2:01 PMWhich HackerNoon Tech Category Do You Read The Most? Let Us Know in This Week's Poll! 06/24/2024Did You Know? You can win $$$ by participating in HackerNoon's Writing Contests!💰 Check out our running contests here06/17/2024📊 Got Ideas for a Poll? We'd Love to Hear Them! 📊 Suggest your own poll and join the conversation today!06/17/2024🌈 Celebrate Pride Month with HackerNoon! 🌈 Dive into our dazzling new rainbow theme and spread love and inclusivity 💚06/17/2024Did you know: On average, HackerNoon Editors mark 3.9% of all published stories as top? Run, don't walk to your drafts page now! 06/03/2024Hundreds of Templates, Editable Story Drafts & Improved Speed - All in HackerNoon’s Latest Version of Mobile App. Download Now!04/30/2024NEW🔥Create your Pixelated Avatar with Hackernoon Today - Try it now! 🏃04/12/2024New alert! 🚨 One-tap Google login and signup is now live on HackerNoon! Give it a try! 🚀03/19/2024This is a reminder to drink more water, and maybe get off your bed and exercise a bit? 😂 Whatever you do, we support you. Stay hydrated while browsing HackerNoon 💚03/08/2024You can now WRITE ANYWHERE with the HackerNoon Mobile App! Experience today 💚03/08/2024Time to update all your call-to-actions to gain more traffic 🚗🛵🚌03/08/2024What's trending in the HackerNoon Community today? 03/08/2024What's new on HackerNoon this week? Find out now!03/08/2024New! Pixelated Icons Open Sourced by HackerNoon01/30/2024HackerNoon's original documentary is officially here! Watch Web 2.5 Today!01/30/2024",http://schema.org,Article,Six useful metaphors for thinking about artificial intelligence,2018-09-06,,,"{'@type': 'Person', 'name': 'Martin Willers'}",,https://hackernoon.imgix.net/fallback-feat.png,,"“SUR-FAKE” Antoine Geiger To be able to think creatively, purposefully and responsibly about a new technology like AI we need helpful comparisons that makes it possible to relate to its inherent characteristics. My background is in design thinking, not data science, and these six analogies have been helpful for me when think about AI in a meaningful way. A fancy thing labeler machine The way AI is talked about today is mostly relating what is actually machine learning. Cassie Kozyrkov, Googles Chief Decision Scientist, calls machine learning a . And thats a very grounding and comforting way to think about it. We teach the machine to label things like apples and pears by showing many examples of fruit to train on. Thats very graspable. And if you only show it green apples and read pears thats all that it will be able to label so it’s important to have good examples. fancy labeling machine Computer excel sheets replaced endless rows of people sitting on typewriters doing manual data entry (and putting physical paper into binders and cabinets). Machine learning will replace all labeling work inte the same way. No more need to manually and repeatedly identifying, categorizing and sorting things. We can expect to uptake of this technology happen rapidly because most human don’t prefer to do repetitive work. A took 7000 pictures of cucumbers that his mother had manually sorted and built and trained a machine to sort them automatically based on this technology. Japanese farmer And Island of drunk people Because we rarely understand the underlying mechanics of how the fancy thing labeling machine works we have to be good at testing that the decided outcomes are what we expect. recently spoke about this at and had a another great analogy for how to think about this. Cassie Kozyrkov SthlmTechFest You have a friend that has and island with numerous of drunk inhabitants with connected laptops. They have all the free time in the world and are all very willing to do things but don’t respond to any detailed written instructions so you have to give them examples. This means it’s a pain to spend time teaching them if you have a one-off task, so you focus instead on the repetitive drudgery you’d like to cut out of your life. But wait! Before you offload all your work to this island, consider: How drunk are these people? Can they even do your task? Don’t just blindly trust them with important work. Force them to earn your trust by checking that they actually perform your task well enough. According to Cassie you’re not ready to dive into a serious machine learning project until you are in possession of a document that outlines: What does it mean to do your task correctly? Which mistakes are worse than which other mistakes? Of the 1000 units of drunk replies you got — some imperfectly — how do you give that pile of work a score? : A open-source robotics platform from with evolutionary computation that can teach itself to walk and explore with out examples or sensors. Aracna A useful pet or spider web is working to bring human-centered on-device AI to Hardware and was recently talking at in Stockholm about AI as tiny minds in octopus, hawks and spiders. We are so obsessed that AI will be some sort of human form interaction like the voice in the movie , but basically we are already seeing big change because the technology is small and cheap and working in decentralized processors. So Matt sugest we think of AI as companion species with different intelligence than ours. This distributed team work suggest other sorts of relationships. Sentient AI spiders could be offloading their cognition to a web that would be intuitively graspable for us in its surrounding. The web and its color could be indicating things like concentrated air pollution in urban environments. Matt Jones Google Frontiers Her is director at microsoft AI research and was speaking at the about bringing crafted humanity in to tech. She also compared early Artificial general intelligence (AGI) as companion such as an innate little animal and showed picture of the robot dog. Ana Arriola design me-Convention The village idiot vs Einstein says that the percentage of intelligence that is non-human is increasing and eventually we will represent a very small percentage of intelligence. In the movie Swedish philosopher reaches out his arms as far as he can and says that we perceive the intelligence gap between the village idiot and Einstein as far apart as we can imagine. It’s the entire spectrum in which we judge intelligence. He then holds his fingers together and says, it’s really this: a very very small distance on the axel of intelligence. That is why we wont see AI coming and be really surprised when artificial general intelligence just swooshes by. Elon Musks more human than human Nick Bostrom Max Tegmark rising sea visualization of Hans Moravecs landscape of knowledge. The rising sea is the co-founder of the future of life institute and had a great in Swedish radio where he compared the increased capabilities of AI as that of a rising sea. In the abstract landscape different task will have different elevations. The sea level represents the current level of AI capabilities, so you might want to avoid careers in the waterfront that will soon be taken over by the water elevation. How high will the water be raising? When will it cover every mountain top of human knowledge so that we have a artificial general intelligence? Most AI researchers believes this will happens in decades, meaning during the lifetime of most people reading this text. How do I prepare my kids to coexist with a super intelligence? Max Tegmark summer talk The only way I’m ever gonna trust AI is if I can understand it on some levelMax Tegmark From movie poster More Human Than Human The strange loop mirror When I think of AI I think about the machine looking back at you, seeing you much like a distorted mirror does. AI-first companies like facebook and google is looking back at you with years of self images, questions and thousands of data points in terms of likes and other interactions. It’s a strange mirror with a kind of memory that gives you an x-ray into your self image. When the AI in this mirror is making me feel seen, it’s doing me a service in the maintenance of being human, which is to tell my story. The more limbic resonance in us, the more engagement. We really like being mirrored! There is a loopiness to this because it’s self reinforcing and therefor giving rise to phenomenons such as and saturated in our egos. filter bubbles biases In the end, we are self-perceiving, self-inventing, locked-in mirages that are little miracles of self-reference.Douglas Hofstadter, p.363 I Am a Strange Loop I Am a Strange Loop is a 2007 book by , examining in depth the concept of a to explain the sense of “I”. Basically stating that analogy as the core of cognition and understanding. Any sufficiently complex system of analogy such as number theory can give rise to a self mirroring/referencing “strange loop” effect. Douglas Hofstadter strange loop The key thought experiment in the mirror analogy is to think about what distorsion the AI mirror possesses so that we can understand the subsequent strange loops it will cause in our self image. In extension this is the idea that “I” is distributed over numerous system, rather than being limited to precisely one brain. If these other systems are distorted by limited data from only men in western religions we will se a great bias in the system and as a consequence in our “I”. The normative psychological effect high and low involvement mediums is well documented since stated that the medium is the message in the 1960s. The company illegally used big data from 87 million facebook user to create relevant confirmation bias content (strange loops) and had a big influence in the Donald Trumps presidential campaign 2016 and leave.eu success in the UK Brexit vote 2017. Marshall McLuhan cambridge analytica , documentary about robots for the elderly. Ik ben Alice To me, this mirror idea also explains the obsession of making robots in our own image. In order to meet future care demands for elderly who are lonely and suffering from dementia, has been developed. In the documentary Alice cares (Ik ben Alice) we see lonely elderly in Netherlands quickly get attached to the robot and form a very human interaction even though the robots capabilities of dialoge is limited and mostly work by using the . The realness of the attachment is made apparent when the old ladies get sad and depressed when the robots are removed from their home. AI is here to stay but it’s up to us to craft the best and most human way for it to play. carebot Alice old mirroring trick",,,,,,,,artificial-intelligence,,,Six useful metaphors for thinking about artificial intelligence,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiS2h0dHBzOi8vd3d3LmNubi5jb20vc3R5bGUvYXJ0aWNsZS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1haS1hcnQvaW5kZXguaHRtbNIBRGh0dHBzOi8vYW1wLmNubi5jb20vY25uL3N0eWxlL2FydGljbGUvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtYWktYXJ0?oc=5,"AI can produce pictures, but can it create art for itself? - CNN",2018-09-10,CNN,https://www.cnn.com,"With digital technology now enabling machines to recognize, learn from and respond to humans and the world, an inevitable question follows: Can machines be creative? And will artificial intelligence ever be able to make art?","artificial intelligence, arts and entertainment, business and industry sectors, business, economy and trade, computer science and information technology, technology, visual arts, broadcasting industry, digital and streaming music, internet and www, internet broadcasting, media industry, music industry, streaming media","With digital technology now enabling machines to recognize, learn from and respond to humans and the world, an inevitable question follows: Can machines be creative? And will artificial intelligence ever be able to make art?","With digital technology now enabling machines to recognize, learn from and respond to humans and the world, an inevitable question follows: Can machines be creative? And will artificial intelligence ever be able to make art?",N/A,N/A,"






Editor’s Note: Photographer and artist Chen Man is CNN Style’s guest editor. She has commissioned a series of features on visual language and imagining the future. JJ Charlesworth is an art critic and senior editor for ArtReview.




CNN
         — 
    


            Some of the earliest surviving cave art depicts images in the purest sense: pigment marks around the stencil of an outstretched human hand. 
    

            Examples like this have been found across the world – at sites in Indonesia, Argentina and France, among others – dating back as far as 39,000 B.C. They’re the breathtaking first examples of what we humans, even in the 21st century, still think of as art – acts of material-shaping, image-making and self-representation.
    




Ad Feedback





            Creativity is something we closely associate with what it means to be human. But with digital technology now enabling machines to recognize, learn from and respond to humans and the world – from digital assistants to driverless cars – an inevitable question follows: Can machines be creative? And will artificial intelligence ever be able to make art?
    






""Untitled"" (1972) by Vera Molnár, an early example of computer art. 

Courtesy The Mayor Gallery and Vera Molnar



            Artists since the 1960s have persistently explored how computers might produce art independently of humans. Pioneers like the Hungarian Vera Molnár created the first code-based drawing programs. In the 80s and 90s, the British artist William Latham applied principles from genetics and evolution in simulators that would ‘evolve’ animated, organic sculptural forms without human direction.
    




How to put a price tag on virtual reality artworks 




            But with the accelerating pace of research into artificial cognition backed up by vast computing power, and science’s growing interest in neural networks and deep machine learning, the idea that computers might truly “create” works of art has gained currency. 
    

            After Spotify stirred controversy by promoting obscure or pseudonymous artists who create generic music for playlists such as “Peaceful Piano” or “Ambient Chill,” technology writer David Pogue asked, “Why couldn’t Spotify, or any music service, start using AI to generate free music to save itself money?” Pogue noted Spotify’s telling hire of AI researcher François Pachet, who, at Sony, had been working on AI software that writes music.
    

            Elsewhere, the art market website Artnet reported earlier in the year that a Parisian art collector had bought an image “made by artificial intelligence” – or rather, created using a computer program by Obvious, a French art collective whose strapline is “creativity is not only for humans.” The software “trained” itself using a set of historical paintings for reference, before producing an image that resembles an 18th-century portrait, though it is itself entirely new.
    



La Comtesse de Belamy






            What lies behind this – and many other – recent art experiments is the use of “generative adversarial networks” (GANs). GANs are “neural networks” that teach themselves through their own experimentation, rather than being programmed by humans. Put simply, these systems involve two programs, one of which holds a database of something real (in this instance, pictures of old paintings) while the other produces images that try to “fool” the first into thinking that they constitute an example of what it knows. Gradually, the second program develops ever-more convincing (but completely invented) images in the “style” of the database examples.
    




Google Tilt Brush: Impossible now a reality?




            In an important example of GANs’ use in art, researchers at New Jersey’s Rutgers University last year published results of a study in which their machines tried to create images that humans would think of as plausible enough to be real artworks from history. (The most convincing results were, perhaps unsurprisingly, images that looked like contemporary abstract paintings.)
    

        Defining originality


            It could be argued that the ability of machines to learn what things look like, and then make convincing new examples, marks the advent of “creative” AI. After all, if we enjoy a piece of music or artwork, why should it be disturbing to know that it was entirely machine-made? Isn’t the pleasure or the interest we derive from it proof enough?
    






""Digitalização do retrato de Fabiana"" (1970) by Waldemar Cordeiro

Courtesy The Mayor Gallery / Waldemar Cordeiro



            In George Orwell’s classic dystopian novel “1984,” the masses are entertained with songs made by a machine called a versificator that are “composed without any human intervention whatever.” Perhaps not dissimilarly, big data is today used to analyze what viewers and listeners most want to see and hear.
    

            Netflix famously commissioned “House of Cards” based on its own data, which suggested that viewers who liked the original UK series also liked films directed by David Fincher and those starring Kevin Spacey. YouTubers busily churn out videos in direct response to the search patterns of Internet users. And with companies like Microsoft developing programs that can generate realistic images based on users’ descriptions of what they want to see, the idea of virtual creators making culture that humans want to consume seems to be edging ever-closer.
    






A work by British artist William Latham.

(C) William Latham



            Some commentators and futurists have concluded that, eventually, we’ll no longer need human artists. But the current excitement around “creative” machines – those making artworks that could once only be made by humans – barely gets to grips with the question of how artistic originality really works.
    


Play



Video Ad Feedback



Step into virtual reality art with Google's Tilt Brush
                


                                01:01
                            
 - Source:
                CNN





            Machines have, it is true, been able to invent new works. Nothing exactly like them exists, so they’re “new” in some basic sense. But what machines produce is only really a sophisticated variation on an established corpus of pre-existing art. Moreover, this approach to inventiveness has been based in human-centered evaluation and judgment – it’s us who confirm whether the work is “good” or “bad.” Yes, captcha, this is an image of a bridge.
    




This new virtual reality tool could transform how we design cars 




            And yet, originality in art – real creativity – isn’t about confirming what has already been done, but about doing something differently and for a reason. What characterizes human originality is intentional difference. Human artists have always had reasons for trying to make something new – thoughts, criticisms, frustrations, passions, insights, hopes, ideals and all kinds of other motives. Newness, if not merely some random output we just happen to like, is the result of an artist intending to do something differently. And what is critical in that intention are the reasons for it.
    

        Art needs a reason


            A century ago, artist Marcel Duchamp decided it was no longer interesting to make artworks the way they had always been made. Instead he chose an ordinary object (in this case, a urinal) and presented it as “Fountain.” His reasons for doing so have vexed and inspired critics, artists and the public ever since.
    




Virtual reality just got (more) interesting 




            Duchamp’s reasons were, in part, to do with abandoning the tired, conventional wisdom that assumed art had to look like a certain kind of object. He decided to think differently about what an artwork could be, though his rationale took another 50 years to be accepted. Even today, people still profoundly disagree with his motives. Originality lies in questioning the reasons for what has become commonplace, and Duchamp was not trying to generate the most “likes” for his work. No machine has thus far chosen not to make art.
    






1964 replica of Marcel Duchamp's ""Fountain"" (1917). 

© Succession Marcel Duchamp/ADAGP, Paris and DACS, London 2018



            What Duchamp’s originality outlines is the way human creativity works as part of a culture’s conversation with itself about what things are and the way we give them value. An artist presents a work and says, “What do you think of this?” The audience may like it or not like it, but more than this, humans argue over the value of what the artist has made. That’s how all art criticism starts out. Liking something isn’t enough to make it good. With the development of machine learning, it’s as if machines are getting better at asking, “Do you like this?” and yet, machines cannot yet answer why we like what they make – and have no reason to make it for themselves.
    

            The outline of a hand on a cave seems to say, “We are here.” Looking at their own hand-prints on the rock, early humans must have been aware that they had creatively changed their world, and by making a mark on it, were aware of themselves by doing so. If machines achieve the same thing, it probably won’t be through making images for humans to like. It will be the moment that an algorithm, or an artificial neural network, shows to itself that it exists.
    

            What will the digital equivalent of the hand’s outline on the rock look like? Well, like nothing we’ve ever seen.
    






",https://schema.org,NewsArticle,"AI can produce pictures, but can it create art for itself?",2018-09-06T00:19:24Z,2018-09-10T09:24:23Z,"https://media.cnn.com/api/v1/images/stellar/prod/180903153312-comtesse-de-belamy.jpg?q=w_1900,h_1900,x_0,y_0,c_fill","[{'@type': 'Person', 'name': 'JJ Charlesworth', 'url': ''}]","{'@type': 'NewsMediaOrganization', 'name': 'CNN', 'logo': 'https://media.cnn.com/api/v1/images/stellar/prod/cnnlogo.png?q=w_60,h_61', 'foundingDate': '1980-06-01', 'url': 'https://www.cnn.com', 'sameAs': ['https://www.facebook.com/cnn/', 'https://twitter.com/CNN', 'https://www.instagram.com/cnn/', 'https://www.youtube.com/cnn']}","[{'@type': 'ImageObject', 'contentUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/180903153312-comtesse-de-belamy.jpg?q=w_1900,h_1900,x_0,y_0,c_fill', 'caption': 'Software by French art collective Obvious ""trained"" itself using a set of historical paintings for reference, before producing an image that resembles an 18th-century portrait, though it is itself entirely new.', 'sourceOrganization': {'@type': 'Organization', 'name': 'Obvious'}, 'width': '1900', 'height': '1900', 'creditText': 'Obvious ', 'dateCreated': '2018-09-03T07:51:20Z', 'dateModified': '2018-09-03T07:51:21Z'}, {'@type': 'ImageObject', 'contentUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/180625112105-vera-molnr-computer-art.jpg?q=w_664,h_900,x_0,y_0,c_fill', 'caption': '""Untitled"" (1972) by Vera Molnár, an early example of computer art. ', 'sourceOrganization': {'@type': 'Organization', 'name': 'The Mayor Gallery'}, 'width': '664', 'height': '900', 'creditText': 'Courtesy The Mayor Gallery and Vera Molnar', 'dateCreated': '2018-06-25T03:22:14Z', 'dateModified': '2018-06-25T03:22:15Z'}, {'@type': 'ImageObject', 'contentUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/180625112520-waldemar-cordeiro-digitalizao-do-retrato-de-fabiana.jpg?q=w_1023,h_900,x_0,y_0,c_fill', 'caption': '""Digitalização do retrato de Fabiana"" (1970) by Waldemar Cordeiro', 'sourceOrganization': {'@type': 'Organization', 'name': 'The Mayor Gallery'}, 'width': '1023', 'height': '900', 'creditText': 'Courtesy The Mayor Gallery / Waldemar Cordeiro', 'dateCreated': '2018-06-25T03:26:36Z', 'dateModified': '2018-06-25T03:26:38Z'}, {'@type': 'ImageObject', 'contentUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/180704102914-william-latham-1.jpg?q=w_2000,h_1125,x_0,y_0,c_fill', 'caption': 'A work by British artist William Latham.', 'sourceOrganization': {'@type': 'Organization', 'name': '(C) William Latham'}, 'width': '2000', 'height': '1125', 'creditText': '(C) William Latham', 'dateCreated': '2018-07-04T02:29:48Z', 'dateModified': '2018-07-04T02:29:48Z'}, {'@type': 'ImageObject', 'contentUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/180904174533-fountain-duchamp-replica.jpg?q=w_1536,h_1343,x_0,y_0,c_fill', 'caption': '1964 replica of Marcel Duchamp\'s ""Fountain"" (1917). ', 'sourceOrganization': {'@type': 'Organization', 'name': '© Succession Marcel Duchamp/ADAGP, Paris and DACS, London 2018'}, 'width': '1536', 'height': '1343', 'creditText': '© Succession Marcel Duchamp/ADAGP, Paris and DACS, London 2018', 'dateCreated': '2018-09-04T09:47:22Z', 'dateModified': '2018-09-04T09:47:23Z'}, {'@type': 'ImageObject', 'contentUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/180903153312-comtesse-de-belamy.jpg?q=w_1900,h_1900,x_0,y_0,c_fill', 'sourceOrganization': {'@type': 'Organization', 'name': 'Obvious'}, 'width': '1900', 'height': '1900', 'creditText': 'Obvious ', 'dateCreated': '2018-09-03T07:51:20Z', 'dateModified': '2018-09-03T07:51:21Z'}]",,"Some of the earliest surviving cave art depicts images in the purest sense: pigment marks around the stencil of an outstretched human hand.  Examples like this have been found across the world – at sites in Indonesia, Argentina and France, among others – dating back as far as 39,000 B.C. They’re the breathtaking first examples of what we humans, even in the 21st century, still think of as art – acts of material-shaping, image-making and self-representation. Creativity is something we closely associate with what it means to be human. But with digital technology now enabling machines to recognize, learn from and respond to humans and the world – from digital assistants to driverless cars – an inevitable question follows: Can machines be creative? And will artificial intelligence ever be able to make art?  Artists since the 1960s have persistently explored how computers might produce art independently of humans. Pioneers like the Hungarian Vera Molnár created the first code-based drawing programs. In the 80s and 90s, the British artist William Latham applied principles from genetics and evolution in simulators that would ‘evolve’ animated, organic sculptural forms without human direction. But with the accelerating pace of research into artificial cognition backed up by vast computing power, and science’s growing interest in neural networks and deep machine learning, the idea that computers might truly “create” works of art has gained currency.  After Spotify stirred controversy by promoting obscure or pseudonymous artists who create generic music for playlists such as “Peaceful Piano” or “Ambient Chill,” technology writer David Pogue asked, “Why couldn’t Spotify, or any music service, start using AI to generate free music to save itself money?” Pogue noted Spotify’s telling hire of AI researcher François Pachet, who, at Sony, had been working on AI software that writes music. Elsewhere, the art market website Artnet reported earlier in the year that a Parisian art collector had bought an image “made by artificial intelligence” – or rather, created using a computer program by Obvious, a French art collective whose strapline is “creativity is not only for humans.” The software “trained” itself using a set of historical paintings for reference, before producing an image that resembles an 18th-century portrait, though it is itself entirely new.  What lies behind this – and many other – recent art experiments is the use of “generative adversarial networks” (GANs). GANs are “neural networks” that teach themselves through their own experimentation, rather than being programmed by humans. Put simply, these systems involve two programs, one of which holds a database of something real (in this instance, pictures of old paintings) while the other produces images that try to “fool” the first into thinking that they constitute an example of what it knows. Gradually, the second program develops ever-more convincing (but completely invented) images in the “style” of the database examples.  In an important example of GANs’ use in art, researchers at New Jersey’s Rutgers University last year published results of a study in which their machines tried to create images that humans would think of as plausible enough to be real artworks from history. (The most convincing results were, perhaps unsurprisingly, images that looked like contemporary abstract paintings.) Defining originality It could be argued that the ability of machines to learn what things look like, and then make convincing new examples, marks the advent of “creative” AI. After all, if we enjoy a piece of music or artwork, why should it be disturbing to know that it was entirely machine-made? Isn’t the pleasure or the interest we derive from it proof enough?   In George Orwell’s classic dystopian novel “1984,” the masses are entertained with songs made by a machine called a versificator that are “composed without any human intervention whatever.” Perhaps not dissimilarly, big data is today used to analyze what viewers and listeners most want to see and hear. Netflix famously commissioned “House of Cards” based on its own data, which suggested that viewers who liked the original UK series also liked films directed by David Fincher and those starring Kevin Spacey. YouTubers busily churn out videos in direct response to the search patterns of Internet users. And with companies like Microsoft developing programs that can generate realistic images based on users’ descriptions of what they want to see, the idea of virtual creators making culture that humans want to consume seems to be edging ever-closer.  Some commentators and futurists have concluded that, eventually, we’ll no longer need human artists. But the current excitement around “creative” machines – those making artworks that could once only be made by humans – barely gets to grips with the question of how artistic originality really works. Machines have, it is true, been able to invent new works. Nothing exactly like them exists, so they’re “new” in some basic sense. But what machines produce is only really a sophisticated variation on an established corpus of pre-existing art. Moreover, this approach to inventiveness has been based in human-centered evaluation and judgment – it’s us who confirm whether the work is “good” or “bad.” Yes, captcha, this is an image of a bridge. And yet, originality in art – real creativity – isn’t about confirming what has already been done, but about doing something differently and for a reason. What characterizes human originality is intentional difference. Human artists have always had reasons for trying to make something new – thoughts, criticisms, frustrations, passions, insights, hopes, ideals and all kinds of other motives. Newness, if not merely some random output we just happen to like, is the result of an artist intending to do something differently. And what is critical in that intention are the reasons for it. Art needs a reason A century ago, artist Marcel Duchamp decided it was no longer interesting to make artworks the way they had always been made. Instead he chose an ordinary object (in this case, a urinal) and presented it as “Fountain.” His reasons for doing so have vexed and inspired critics, artists and the public ever since.  Duchamp’s reasons were, in part, to do with abandoning the tired, conventional wisdom that assumed art had to look like a certain kind of object. He decided to think differently about what an artwork could be, though his rationale took another 50 years to be accepted. Even today, people still profoundly disagree with his motives. Originality lies in questioning the reasons for what has become commonplace, and Duchamp was not trying to generate the most “likes” for his work. No machine has thus far chosen not to make art. What Duchamp’s originality outlines is the way human creativity works as part of a culture’s conversation with itself about what things are and the way we give them value. An artist presents a work and says, “What do you think of this?” The audience may like it or not like it, but more than this, humans argue over the value of what the artist has made. That’s how all art criticism starts out. Liking something isn’t enough to make it good. With the development of machine learning, it’s as if machines are getting better at asking, “Do you like this?” and yet, machines cannot yet answer why we like what they make – and have no reason to make it for themselves. The outline of a hand on a cave seems to say, “We are here.” Looking at their own hand-prints on the rock, early humans must have been aware that they had creatively changed their world, and by making a mark on it, were aware of themselves by doing so. If machines achieve the same thing, it probably won’t be through making images for humans to like. It will be the moment that an algorithm, or an artificial neural network, shows to itself that it exists. What will the digital equivalent of the hand’s outline on the rock look like? Well, like nothing we’ve ever seen.",,,"{'@type': 'WebPage', '@context': 'https://schema.org', 'url': 'https://www.cnn.com/style/article/artificial-intelligence-ai-art/index.html', 'dateModified': '2018-09-10T09:24:23Z', 'inLanguage': 'en', 'additionalType': 'article_feature', 'publisher': {'@type': 'NewsMediaOrganization', 'name': 'CNN', 'logo': 'https://media.cnn.com/api/v1/images/stellar/prod/cnnlogo.png?q=w_60,h_61', 'foundingDate': '1980-06-01', 'url': 'https://www.cnn.com', 'sameAs': ['https://www.facebook.com/cnn/', 'https://twitter.com/CNN', 'https://www.instagram.com/cnn/', 'https://www.youtube.com/cnn']}, 'name': 'AI can produce pictures, but can it create art for itself?', 'headline': 'AI can produce pictures, but can it create art for itself?', 'description': 'With digital technology now enabling machines to recognize, learn from and respond to humans and the world, an inevitable question follows: Can machines be creative? And will artificial intelligence ever be able to make art?', 'datePublished': '2018-09-06T00:19:24Z'}",,,,,"['style', 'arts']",en,False,,,,,,,,,,,,,P0Y0M0DT0H5M49S,1310.0,"[{'@context': 'https://schema.org', '@type': 'VideoObject', 'contentUrl': '/arts/2016/05/09/google-tilt-brush.cnn_507855_ios_650.mp4', 'dateModified': '2016-05-09T13:10:25Z', 'uploadDate': '2016-05-09T13:01:01Z', 'embedUrl': 'https://fave.api.cnn.io/v1/fav/?video=arts/2016/05/09/google-tilt-brush.cnn&stellarUri=cms.cnn.com/_components/video-resource/instances/h_cc888f3d682aa69c4c7d492887c70333@published&stellarUdk=rn0624cy&customer=cnn&edition=domestic&env=prod', 'duration': 'PT00H01M01S', 'inLanguage': 'en', 'name': ""Step into virtual reality art with Google's Tilt Brush"", 'headline': ""Step into virtual reality art with Google's Tilt Brush"", 'description': 'Google has revealed a brand new Tilt Brush product, which can be used to created immersive 3D paintings, all you need is a virtual reality head set.', 'thumbnail': {'@type': 'ImageObject', 'contentUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/160509172153-google-tilt-brush-teaser-2.jpg?q=w_1600,h_900,x_0,y_0,c_fill', 'sourceOrganization': {'@type': 'Organization', 'name': 'courtesy google'}, 'width': '1600', 'height': '900', 'creditText': 'courtesy google', 'dateCreated': '2016-05-09T09:26:16Z', 'dateModified': '2016-07-27T02:47:57Z'}, 'thumbnailUrl': 'https://media.cnn.com/api/v1/images/stellar/prod/160509172153-google-tilt-brush-teaser-2.jpg?q=w_1600,h_900,x_0,y_0,c_fill', 'mainEntityOfPage': 'https://www.cnn.com/videos/arts/2016/05/09/google-tilt-brush.cnn'}]","{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.article__content'}",,
https://news.google.com/rss/articles/CBMiP2h0dHBzOi8vd3d3LmF4aW9zLmNvbS8yMDE4LzA5LzA2L2FjYWRlbWlhLWNvcnBvcmF0ZS1yZXNlYXJjaC1hadIBAA?oc=5,Academia vs. tech industry in battle for artificial intelligence - Axios,2018-09-06,Axios,https://www.axios.com,Tech industry poaching academics for AI research,"Uber,Facebook",Tech industry poaching academics for AI research,Tech industry poaching academics for AI research,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMidWh0dHBzOi8vd3d3Lm1pbGl0YXJ5LmNvbS9kYWlseS1uZXdzLzIwMTgvMDkvMDcvNS1zY2FyeS10aGluZ3MtYWJvdXQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utd29ycnktbWlsaXRhcnktYnJhc3MuaHRtbNIBeWh0dHBzOi8vd3d3Lm1pbGl0YXJ5LmNvbS9kYWlseS1uZXdzLzIwMTgvMDkvMDcvNS1zY2FyeS10aGluZ3MtYWJvdXQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utd29ycnktbWlsaXRhcnktYnJhc3MuaHRtbD9hbXA?oc=5,5 Scary Things About Artificial Intelligence That Worry Military Brass - Military.com,2018-09-07,Military.com,https://www.military.com,Exactly how artificial intelligence will play out on the battlefield remains to be seen.,N/A,Exactly how artificial intelligence will play out on the battlefield remains to be seen.,Exactly how artificial intelligence will play out on the battlefield remains to be seen.,Daily News,N/A,"



 







Lance Cpl. Timothy Knaggs (center), a team leader with India Company, 3rd Battalion, 3rd Marine Regiment, walks ahead of the Legged Squad Support System, acting as the ""follow"" for the machine at Marine Corps Training Area Bellows, June 19, 2014. (U.S. Marine Corps/Matthew Callahan) 




Military.com
|
By                 
          
                        Gina Harkins



Published September 07, 2018




Robots with minds of their own, bad guys manipulating data, and getting sorely outpaced by the Chinese. Those are just a few concerns shared by leaders tasked with weaving machine learning into military operations.
Exactly how artificial intelligence will play out on the battlefield remains to be seen. Defense leaders and researchers are on the hunt for ways to free up troops' time so they can address more pressing issues.Advertisement
""How can we lighten the load for soldiers, sailors, airmen and Marines?"" Maj. Gen. William Cooley, head of the Air Force Research Laboratory, asked during a Wednesday conference hosted by Defense News. ""How can AI make us more efficient, effective and capable?""
Think more computers sifting through complex data instead of an officer having to assess it, or technology that can recognize preventative maintenance needs on troops' equipment.
There are plenty of challenges that come along with using artificial intelligence, though. Here are just a few worries they shared about machines doing the thinking for troops.Advertisement
 
1. Killer robots.
We might be a ways off from a ""Terminator""-style nightmare in which a self-thinking computer wages war on the planet. But as the military experiments with more autonomous vehicles and robots, experts are thinking about ways to keep them in check.
Balancing the desire to make life easier for the warfighter while protecting humans from machines is an ""incredibly difficult"" topic, said Rear Adm. David Hahn, chief of naval research and director of Innovation, Technology Requirements and Test and Evaluation. What's important to remember, he added, is that accepting some risk and knowingly taking a gamble are two very different things.
 
2. More machines, fewer humans making decisions.
Warfare will always involve human beings, said Alexander Kott, chief scientist with the Army Research Laboratory. One of his lab's key efforts is thinking about how humans and artificial intelligence can best fight as a team.
Researchers first need to think about what's best done by a machine versus a human, Cooley added.
""How can we apply these tools to a more unpredictable environment to safeguard humans so they're in control, but they have help?"" he said.
 
3. Bad or tainted data.
A lot of machine-based-learning software used in the civilian world is open source, which means lots of different people can affect the artificial intelligence. In the military where national security risks are at stake, that approach isn't likely to work.
Kott called data the ""greatest challenge in terms of applying artificial intelligence in the military.""
""There's not enough of it, it's distorted, it's dynamic, it's rapidly changing,"" he said. ""Data can and will be used against you.""
That not only means they need to be clever about how they teach artificial intelligence to work using limited data, but also how to protect that data being manipulated by an adversary.
 
4. Limited platforms.
Another limitation the military faces is having to integrate artificial intelligence capabilities into some of its complex existing platforms. It's not always feasible for the U.S. military to design entirely new systems built just for AI capabilities -- especially something with complicated security needs.
""The legacy systems we start with are significantly different than those our adversaries start with,"" Hahn said.
 
5. The next space race.
There's a global power competition happening in artificial intelligence. China has openly declared AI to be the next space race, Hahn said. And the country plans on winning that race.
That means the military must look to industry and academic experts who can help the services build capabilities rapidly, Cooley said. Countries like China don't deal with the rigid contract structures and other rules that can slow the developmental process significantly.
""We must be innovative and change the dynamic of how we're going to get at this or we're not going to get the capability for the U.S. military,"" he said.
--Gina Harkins can be reached at gina.harkins@military.com. Follow her on Twitter at @ginaaharkins.
Story Continues

 Related Topics: Military Headlines
Artificial Intelligence - AI
Gear and Equipment


 


Gina Harkins




 

 





Gina Harkins previously covered the Marine Corps, Navy and Coast Guard for Military.com. She has covered troops and their families for nearly a decade, reporting from the U.S., Central and South America, and at sea. Before joining Military.com, Gina was a longtime reporter and managing editor for the independent Marine Corps Times. Her story on two recruits trapped at Marine Corps Recruit Depot Parris Island in South Carolina was runner up for the 2013 Military Reporters and Editors award for domestic military coverage. She holds a master’s degree from Northwestern University’s Medill School of Journalism and studied political science and international relations in her undergrad at Northeastern Illinois University.
        Read Full Bio






    © Copyright 2024 Military.com. All rights reserved. This article may not be republished, rebroadcast, rewritten or otherwise distributed without written permission. To reprint or license this article or any content from Military.com, please submit your request here.




Advertisement





",https://schema.org,,,,,,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'NewsArticle', 'headline': '5 Scary Things About Artificial Intelligence That Worry Military Brass', 'image': {'@type': 'ImageObject', 'url': 'https://images02.military.com/sites/default/files/styles/full/public/2018-09/Legged-Squad-Support-System-1800.jpg'}, 'datePublished': 'September 7, 2018', 'dateModified': 'September 7, 2018', 'author': {'@type': 'Organization', 'name': 'Military.com'}, 'publisher': {'@type': 'Organization', 'name': 'Military.com', 'url': 'https://www.military.com/', 'logo': {'@type': 'ImageObject', 'url': 'https://images.military.com/themes/military/logo.png'}}, 'mainEntityOfPage': 'https://www.military.com/daily-news/2018/09/07/5-scary-things-about-artificial-intelligence-worry-military-brass.html'}, {'@type': 'WebPage', 'breadcrumb': {'@type': 'BreadcrumbList', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Military News', 'item': 'https://www.military.com/daily-news'}]}}, {'@type': 'WebSite', 'name': 'Military.com', 'url': 'https://www.military.com/'}]",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiTWh0dHBzOi8vYmVjb21pbmdodW1hbi5haS9hLWRlbW9ub2xvZ3ktb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtN2RlNzYyODRjNzE50gEA?oc=5,A Demonology of Artificial Intelligence | by Jonathan Cook | Becoming Human - Becoming Human: Artificial Intelligence Magazine,2018-09-10,Becoming Human: Artificial Intelligence Magazine,https://becominghuman.ai,"A great fuss was made this summer about Google’s removal of its famous motto, Don’t Be Evil, from several places in its code of conduct. In fact, Google has retained the motto, but only in one place…",N/A,How to get Past the Fear and Enjoy the Shadowy Embrace of AI,How to get Past the Fear and Enjoy the Shadowy Embrace of AI,N/A,N/A,"A Demonology of Artificial IntelligenceHow to get Past the Fear and Enjoy the Shadowy Embrace of AIJonathan Cook·FollowPublished inBecoming Human: Artificial Intelligence Magazine·8 min read·Sep 10, 2018170ListenShareA great fuss was made this summer about Google’s removal of its famous motto, Don’t Be Evil, from several places in its code of conduct. In fact, Google has retained the motto, but only in one place, at the very end of its code.We can quibble about the details, but what is clear is that not being evil has become a lower level priority at the algorithmic powerhouse. Revelations that Google is lending its artificial intelligence services to enhance the targeting capabilities of the military’s flying killer robots gave support to these concerns. Even Silicon Valley elites have to admit that the phrase “flying killer robots” does have a bit of an evilish tone to it.This week, Google’s un-evil pretensions further eroded, when the tech giant’s CEO Larry Page refused to show up to answer questions from the U.S. Senate about digital industry abuses. This act of indifference to American democracy followed news that Google is arranging, through a nefarious partnership with Mastercard, to spy on users’ activities even when they’re completely offline.Google’s new unofficial motto seems to be: Who Is To Say What Is Evil, Really?In ordinary conditions, it’s not necessary for companies to openly declare that they will not be evil. We take it for granted that businesses, though they may be amoral, will at least try not to be truly wicked. The fact that Google ever even had to make it an explicit long-term goal not to be evil lets us know that moral standards in Big Tech aren’t quite as tight as they are outside of Silicon Valley.To Fear Artificial Intelligence - So Plebeian!Still, deeply invested experts continue to pooh-pooh people’s concerns about Dominant Digital’s development of increasingly powerful AI. They advise us that our fears of artificial intelligence are inconvenient irrational obstacles, getting in the way of what could be a beautiful transhumanist revolution. When J. Bradford DeLong of the National Bureau of Economic Research warns that, “It is unhelpful to stoke fears about the rise of the robots,” one wonders exactly who he thinks is being helped by uncritical acceptance of the rosy forecasts issued by algorithmic juggernauts.Perhaps critical examination and regulation of artificial intelligence would be unhelpful to the upper echelon of people who tend to rub elbows at conferences on the digital future. They come from positions of entitlement in business and society in general that bias them in favor of the grand disruptive schemes of the tech industry. What financial elites regard as unhelpful, others will consider essential.It’s easy to forget, as predictions of the glorious future of transhumanist utopias flow freely from Silicon Valley, that nobody writing about the future of artificial intelligence really knows what’s going to happen. They’re just guessing, and whenever people write about the future, their biases emerge at their strongest. This truth applies even to tech gurus. So, if we’re going to talk about the future of AI, let’s agree to put the facts aside. When people talk about the future, they’re giving voice to their emotions — and there’s nothing wrong with that.Trending AI Articles:1. From Perceptron to Deep Neural Nets2. Neural networks for solving differential equations3. Turn your Raspberry Pi into homemade Google Home4. AI & NLP WorkshopAnalysts who consider only the immediately available data about the practical impact of artificial intelligence are missing the point that AI is as much a mythological innovation as it is a technological development. Outwardly, the fears people have about artificial intelligence take on a practical tone, because we all sense that practical concerns are more likely to be taken seriously. Functional apprehensions are, however, merely a socially-acceptable mask that conceals deeper, metaphorical concerns.Whether people’s fears about artificial intelligence are justified or not, the fears exist, and will therefore significantly impact the implementation of AI technology. History makes it plain that logical arguments are ineffective at dispelling fundamental cultural concerns. So, Instead of dismissing people’s fears as uninformed, the rational thing for Silicon Valley to do is to try learning from those fears.The time has come for the tech titans to learn to face their demons.Elon Musk’s DemonsThe comparison of artificial intelligence to a demon was famously made by Elon Musk, the CEO of Tesla. He warned, “I’m increasingly inclined to think that there should be some regulatory oversight, maybe at the national and international level, just to make sure that we don’t do something very foolish. With artificial intelligence we’re summoning the demon. You know those stories where there’s the guy with the pentagram, and the holy water, and he’s like — Yeah, he’s sure he can control the demon? Doesn’t work out.”Elon Musk is not alone in his apprehensions of the digital. Futurist Gerd Leonhard brings up the demonic metaphor for the digital world when he talks about the “faustian bargain between us and Facebook”. Nikarika Singh asks, “Are we creating a digital satan?” while Forbes asks, “Have marketers struck a devil’s bargain with AI?”The meme of artificial-intelligence-as-diabolical-fiend is too common to be random. So, let’s play around with this idea. What if AI were to be a demon? What kind of demon might it be?Literal DemonsThere is the simple through-and-through evil demon, of course, whose job is merely to torture human beings as much as possible, without relief.Perhaps artificial intelligence is a changeling, replacing what we thought was a human with an uncanny replica.Alternatively, AI might be a succubus, seducing us and crushing us in the night, paralyzing us by placing us into an in-between state of mind in which we are somewhat aware, but not fully conscious and in control of our own bodies… and distracting us with Internet porn.AI could be a djinn, tempting us with the dangerous fulfillment of our wishes, making us wish we had never rubbed that lamp.Maybe, though, AI could become like a daemon from Philip Pullman’s fantasy series of novels, His Dark Materials, serving as a kind of outward companion, to reflect our essential character, along with the thoughts and feelings that we ordinarily hide from outselves. That’s a more hopeful vision.Metaphorical DemonsAlong the lines of Philip Pullman’s daemons, it may be more helpful to think of the demon status of artificial intelligence metaphorically. As to what might the AI demon might be a metaphorical representation for, the most obvious choice is our own dark human nature.Carol and Dinah Mack, in their Field Guide to Demons, Vampires, Fallen Angels and Other Subversive Spirits, point out that the disturbing features we confront in the form of demons are often nothing other than exaggerations of our own flaws. “Demons are personifications of our own passions, impulses and desires, lurking about in the dark, hidden terrain of our unconscious, and their habits hold profund insights into the nature of our minds,” they write.From this perspective, the demons of AI reflect our human anxieties and ambitions, as much as the technological reality of what we’re likely to face in the near future. AI is a way for us to talk about ourselves, while appearing to talk about machines. As much as it is a technology, artificial intelligence is a cultural phenomenon.As a human construction, artificial intelligence ought to subject to the regulation of human governments, but such a practical, legal approach won’t get to the heart of the matter. Rational reassurance from tech experts will also be insufficient to allay our fears. AI apologists who only speak to the literal, functional concerns people express will satisfy themselves and justify industry insiders, but will convince few others of the validity of their positions. We need tools of cultural management to deal with the more subjective forms of terror artificial intelligence presents.Culturally, demons are often demoted and frustrated gods. They were once revered in their own right, until they came along and were replaced by cultural updates. Isn’t that just the kind of frustration that many people are feeling right now, about the status they’ve lost during the upheaval of the digital revolution, and it’s threatened next step? Economically, the advent of the Internet only enriched a small elite, leaving the rest of us struggling against each other for a smaller piece of the pie than what we had before.Expressing fear of artificial intelligence could therefore be a means for expressing fear of the human beings who are developing artificial intelligence. If a demon is a powerful being who seems to delight in torturing people, is it too far of a stretch to say that many people now place tech executives like Mark Zuckerberg and Jeff Bezos among the demonic hordes?AI exists in what Grant McCracken refers to as a zone of displaced meaning — a conceptual space where cultures locate the anxieties that they are incapable of dealing directly. It may be too emotionally challenging for us to grapple with the reality that we feel tortured by the very digital services we choose to interact with.The demonology of artificial intelligence is multivocal and ambiguous, which only amplifies our fear. When we speak of artificial intelligence, we aren’t typically talking about what exists right now. Rather, we refer to what we believe will soon exist. Of course, the future, even the near future, is unknowable.Anthropologist Victor Turner observed that, culturally, “The unclear is the unclean.” Artificial intelligence is not merely terrifying, but culturally contaminated when it takes place in a black box we cannot understand. If this is the case, as much as we need need regulation of artificial intelligence, we also need to develop rituals of decontamination.There are many ways to design decontamination rituals, but the one thing they all share is the creation of clear separation, of boundaries that unclean things cannot cross. Already, primitive forms of artificial intelligence such as Amazon’s Alexa are crossing boundaries in defiance of cultural norms, often without the permission of people using digital services.If the tycoons of Silicon Valley want to recast their artificial intelligence ambitious as more divine and less demonic, they’re going to need to accept the existence of AI-free zones, places where people can gain refuge from the swarm of “assistants” that are always listening, always watching, always helping themselves to whatever bits of our lives can be most profitable to their infernal masters.",http://schema.org,NewsArticle,A Demonology of Artificial Intelligence - Becoming Human: Artificial Intelligence Magazine,2018-09-10T13:46:13.535Z,2018-09-11T16:58:05.584Z,,"{'@type': 'Person', 'name': 'Jonathan Cook', 'url': 'https://jonathanccook.medium.com'}","{'@type': 'Organization', 'name': 'Becoming Human: Artificial Intelligence Magazine', 'url': 'becominghuman.ai', 'logo': {'@type': 'ImageObject', 'width': 146, 'height': 60, 'url': 'https://miro.medium.com/v2/resize:fit:292/1*1fYpRTTpKQNa0zuEPe3itg.png'}}",['https://miro.medium.com/v2/resize:fit:1200/1*rVNyArsssXtsLl8r7-JYog.jpeg'],https://becominghuman.ai/a-demonology-of-artificial-intelligence-7de76284c719,,,,https://becominghuman.ai/a-demonology-of-artificial-intelligence-7de76284c719,2018-09-10T13:46:13.535Z,['Jonathan Cook'],,,,,,A Demonology of Artificial Intelligence - Becoming Human: Artificial Intelligence Magazine,,,,,,,,,,,,,,,,7de76284c719,
https://news.google.com/rss/articles/CBMicWh0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L2N4by1pbnNpZ2h0cy9pcHNvZnQtZ2xvYmFsLWxlYWRlci1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1jb2duaXRpdmUtdGVjaC1zeXN0ZW1z0gF7aHR0cHM6Ly93d3cuYW5hbHl0aWNzaW5zaWdodC5uZXQvYW1wL3N0b3J5L2N4by1pbnNpZ2h0cy9pcHNvZnQtZ2xvYmFsLWxlYWRlci1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1jb2duaXRpdmUtdGVjaC1zeXN0ZW1z?oc=5,IPsoft: A Global Leader in Artificial Intelligence and Cognitive Tech Systems - Analytics Insight,2018-09-10,Analytics Insight,https://www.analyticsinsight.net,,"Analytics Insight Magazine,Deep Learning,IPsoft,Amelia,Chetan Dube","IPsoft is the world leader in Enterprise AI and the home of Amelia, the industry's most-human digital AI colleague. Amelia's ability to learn, interact and impr","IPsoft is the world leader in Enterprise AI and the home of Amelia, the industry's most-human digital AI colleague. Amelia's ability to learn, interact and impr",N/A,N/A,"How Macroeconomic Factors Influence Crypto Market Volatility
",http://schema.org,NewsArticle,IPsoft: A Global Leader in Artificial Intelligence and Cognitive Tech Systems,2018-09-10T08:16:31Z,2018-09-10T08:16:31Z,https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2018/09/AUG18-Rectangle5.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true,"[{'@type': 'Person', 'givenName': 'Ashish Sukhadeve', 'name': 'Ashish Sukhadeve', 'url': 'https://www.analyticsinsight.net/author/ashish-sukhadeve'}]","{'@type': 'Organization', '@context': 'http://schema.org', 'name': 'Analytics Insight', 'url': 'https://www.analyticsinsight.net', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'author': 'analyticsinsight', 'contentUrl': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'url': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'name': 'logo', 'width': '', 'height': ''}, 'sameAs': ['https://www.linkedin.com/company/analytics-insight/', 'https://www.facebook.com/analyticsinsight.net', 'https://twitter.com/analyticsinme', 'https://www.youtube.com/channel/UCgF2J0b46YP0vvVEbgL_GuQ', 'https://www.instagram.com/analyticsinsightmagazine/', 'https://in.pinterest.com/analyticsinsightsubmissions/_created/'], 'id': 'https://www.analyticsinsight.net'}","{'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2018/09/AUG18-Rectangle5.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}",https://www.analyticsinsight.net/cxo-insights/ipsoft-global-leader-artificial-intelligence-cognitive-tech-systems,"IPsoft is the world leader in Enterprise AI and the home of Amelia, the industry's most-human digital AI colleague. Amelia's ability to learn, interact and improve over time makes her the market's only AI that can fully understand user needs and intentions. Amelia can be trained to recognize words and phrases in more than 100 languages. She delivers real-life business benefits including lower operating costs, higher customer satisfaction and increased employee productivity. When a company hires Amelia (or several Amelias) they'll be able to design her to address a variety of company needs, roles, and verticals—all according to their specific business processes and procedures. She observes, learns, and remembers anything you ask. She can read emotions and context during conversations with colleagues and customers, on any communications channel. She can do the repetitive work of thousands while helping to improve the creativity and productivity of her human colleagues..IPsoft was the first company to launch an end-to-end digital platform, 1Desk, to deliver shared enterprise services. By connecting front-office conversations to back-end systems, IPsoft automates business processes that serve employees, customers and citizens, resulting in rapid resolutions, satisfied users and substantial organizational savings..IPsoft solutions such as Amelia Marketplace and 1Desk make it even easier for organizations to put Amelia to work. The Marketplace offers out of the box capabilities for Amelia across verticals such as banking, telecom, hospitality, insurance and healthcare, allowing Amelia to help perform tasks in a more efficient and affordable way. Marketplace provides faster and more immediate assistance to Amelia for customers, while eliminating minor, repetitive tasks from human employees, allowing them to focus on more complicated issues. Similarly, 1Desk helps companies scale their deployment of Amelia by connecting her cognitive abilities with internal systems to fully automate end to end processes in IT and HR. Both these products help integrate digital colleagues into the workforce to help employees focus on creative tasks that help accelerate innovation..The Mission to Power Industries with Intelligent Systems.IPsoft's mission is to power the world with intelligent systems, eliminate routine work and free human talent to focus on creating value through innovation..Within a few years of its founding, IPsoft established a presence in India providing remote IT infrastructure managed services and shortly thereafter opened its first European outpost in London. As growth continued to accelerate, new offices followed. First in California then Amsterdam and beyond. In 2009 alone, IPsoft opened its Frankfurt, Austin, Chicago and Stockholm offices and expanded its New York and Bangalore offices. More recently, IPsoft entered the Asia-Pacific region, opening offices in Singapore, Tokyo and Sydney..In 2014, following 15 years of development, IPsoft launched Amelia, the first cognitive agent to interact like a human. Amelia opened a new chapter in IPsoft's growth and placed IPsoft at the forefront of a technology-enabled revolution that promises to change how we live and work forever: the age of the Smart Machine..Exemplary Leadership.Chetan Dube, IPsoft's Founder and CEO, believed that the IT infrastructure and business operations of the future would be managed not by people but by expert systems. Twenty years ago, he left the world of academia, at New York University's Courant Institute of Mathematical Sciences, to pursue a career in business. He was haunted by the question; &quot;If machines could think, what kind of world would that make possible for and what higher forms of creative expression would that allow for in humans?&quot; Out of that idea, Amelia was born..Pioneering Innovation and Disruption in Cognitive AI.IPsoft's Amelia is constantly reinventing the user experience and defining the next generation of intelligent AI – when customers or employees interact with Amelia, they don't feel they are interacting with something artificial due to her advanced empathetic abilities, capacity to switch context and channels, and intelligent response. Amelia is the only digital colleague on the market that offers all of these features with expert-level accuracy and emotional intelligence..IPsoft has an industry expertise that has allowed 6 of top 10 global banks advance in cognitive AI. IPsoft serves more than 550 of the world's leading brands directly, including more than half of the world's largest IT services providers. To name a few, these brands including Allstate, Black Rock, SEB and Credit Suisse..Delivering Intelligent Automation with Super Efficiency .&quot;Every day we apply ourselves to transforming our clients' IT and business operations for competitive advantage. Through a combination of our technology platforms and our services, IPsoft guarantees predictable business benefits through intelligent automation,&quot; said Chetan..Both Marketplace and 1Desk use automation to help reduce inefficiencies and free employees' time and resources to focus their energy on more creative tasks to accelerate innovation. By using AI to handle repetitive tasks, employees are empowered to take on more creative work that adds overall business value and supports strategic goals..&quot;According to McKinsey, the impact of AI technologies on business is projected to increase labor productivity by up to 40% and enable people to make more efficient use of their time. Executives are even more optimistic – 80% believe artificial intelligence improves worker performance and creates jobs, he mentioned&quot;..Recognitions for Driving Business Excellence.Today, Amelia is helping Credit Suisse serve 76,000 users in 40 countries, by augmenting human intelligence. Whilst at Allstate, Amelia has been trained on more than 50 unique insurance topics and regulations across 50 states in the US. Amelia is reducing the complexity for customer service representatives who handle over 250,000 monthly calls by giving them better and quicker access to information policies and procedures – blending human intelligence and problem-solving with technology, as she also constantly learns from her human co-workers..In 2017, the customer-facing IPsoft Amelia implementation at SEB, leading corporate bank in the Nordics, was awarded with the Supernova Award by Constellation Research, highlighting the bank's cutting-edge use of cognitive tech for internal and external services. In Amelia's first three weeks, over 4,000 conversations were held with 700 employees, and she solved the majority of issues without delay. Amelia has also helped SEB avoid 544 hours of escalations to customer support with an average handle time of six minutes and an 80 to 85 percent accuracy in immediate recognition of user intent..From Challenges to Value Creation.The IPsoft mission is to challenge the widespread idea that &quot;robots&quot; will take over our human jobs and make us unnecessary, but in fact, the employment of digital colleagues will actually create new jobs that involve new skill sets such as roles that focus on the creation, maintenance and performance improvement of virtual agents. For instance, top performing human agents might become escalation managers, working alongside virtual agents to ensure KPIs are aligned across the digital and human workforce and steering teams towards new goals on an ongoing basis..The Future Ahead.Chetan said tomorrow's success is not going to be dependent on &quot;hard work&quot; but rather &quot;smart work&quot;, and it will be reliant on transdisciplinary collaboration and the ability to incorporate advanced technologies in differentiated ways. This human-AI collaboration will allow humans to gain more technical skills, while simultaneously improving the sophistication and efficiency of digital agents. IPsoft enables that journey for any company of any size, across the world..Disclaimer: Analytics Insight does not provide financial advice or guidance. Also note that the cryptocurrencies mentioned/listed on the website could potentially be scams, i.e. designed to induce you to invest financial resources that may be lost forever and not be recoverable once investments are made. You are responsible for conducting your own research (DYOR) before making any investments. Read more here.","{'@type': 'WebPage', 'url': 'https://www.analyticsinsight.net/cxo-insights/ipsoft-global-leader-artificial-intelligence-cognitive-tech-systems', 'primaryImageOfPage': {'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2018/09/AUG18-Rectangle5.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}}",,"{'@type': 'WebPage', '@id': 'https://www.analyticsinsight.net/cxo-insights/ipsoft-global-leader-artificial-intelligence-cognitive-tech-systems'}",2018-09-10T08:16:31Z,,,,CXO Insights,,,IPsoft: A Global Leader in Artificial Intelligence and Cognitive Tech Systems,,,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.analyticsinsight.net'}, {'@type': 'ListItem', 'position': 2, 'name': 'CXO Insights', 'item': 'https://www.analyticsinsight.net/cxo-insights'}, {'@type': 'ListItem', 'position': 3, 'name': 'IPsoft: A Global Leader in Artificial Intelligence and Cognitive Tech Systems', 'item': 'https://www.analyticsinsight.net/cxo-insights/ipsoft-global-leader-artificial-intelligence-cognitive-tech-systems'}]",,,,,,,,,,,,,
