URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,url,image,author,publisher,headline,datePublished,dateModified,articleSection,name,isAccessibleForFree,itemListElement,article:section,article:summary,article text,mainEntityOfPage,dateCreated,heading,articleBody,isBasedOn,thumbnailUrl,isPartOf,alternativeHeadline,@graph,identifier,creator,mentions,hasPart,address,diversityPolicy,email,legalName,leiCode,telephone,logo,brand,dateline,uploadDate,duration,potentialAction
https://news.google.com/rss/articles/CBMijQFodHRwczovL3d3dy5mb3JiZXMuY29tL3NpdGVzL2Jlcm5hcmRtYXJyLzIwMTgvMTEvMTIvdGhlLWFtYXppbmctd2F5cy1nb29nbGUtYW5kLWdyYW1tYXJseS11c2UtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtdG8taW1wcm92ZS1vdXItd3JpdGluZy_SAQA?oc=5,The Amazing Ways Google And Grammarly Use Artificial Intelligence To Improve Your Writing - Forbes,2018-11-12,Forbes,https://www.forbes.com,Grammarly and grammar suggestions from Google Docs use artificial intelligence technologies and algorithms to improve the way we write. Grammarly and Google have their sights set on developing their tech to provide even more editorial oversight to help humans communicate more effectively.,,Grammarly and grammar suggestions from Google Docs use artificial intelligence technologies and algorithms to improve the way we write. Grammarly and Google have their sights set on developing their tech to provide even more editorial oversight to help humans communicate more effectively.,Grammarly and grammar suggestions from Google Docs use artificial intelligence technologies and algorithms to improve the way we write. Grammarly and Google have their sights set on developing their tech to provide even more editorial oversight to help humans communicate more effectively.,http://schema.org,BreadcrumbList,https://www.forbes.com/sites/bernardmarr/2018/11/12/the-amazing-ways-google-and-grammarly-use-artificial-intelligence-to-improve-our-writing/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/blogs-images/bernardmarr/files/2018/11/AdobeStock_156936344-1200x900.jpeg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Bernard Marr', 'url': 'https://www.forbes.com/sites/bernardmarr/', 'description': 'Bernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society. He has written over 20 best-selling and award-winning books and advises and coaches many of the world’s best-known organisations. He has a combined following of 4 million people across his social media channels and newsletters and was ranked by LinkedIn as one of the top 5 business influencers in the world. Follow Bernard on LinkedIn, X (Twitter) or YouTube. Join his newsletter, check out his website and books.', 'sameAs': ['https://www.linkedin.com/in/bernardmarr/', 'https://www.twitter.com/BernardMarr', 'https://bernardmarr.com/']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",The Amazing Ways Google And Grammarly Use Artificial Intelligence To Improve Your Writing,2018-11-12T00:19:00-05:00,2018-11-15T05:55:10-05:00,Enterprise & Cloud,The Amazing Ways Google And Grammarly Use Artificial Intelligence To Improve Your Writing,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Enterprise Tech', 'item': 'https://www.forbes.com/enterprise-tech/'}]",Enterprise & Cloud,N/A,"More From ForbesJul 8, 2024,09:00am EDTSee The Future Data Center At The Israeli Quantum Computing CenterJun 30, 2024,09:00am EDTWar Can’t Stop Israeli Startups Determined To Thrive In The DesertJun 6, 2024,06:00am EDTIsraeli Startup Combines Software With Medicine To Transform $1.6 Trillion Pharma MarketMay 22, 2024,12:18pm EDT$20M Fund For Connecting Early-Stage Israeli Startups To New York CityMay 20, 2024,09:00am EDTBiomed 2024 Showcases Israel’s Resilient Entrepreneurial SpiritApr 30, 2024,09:00am EDTAI Is Moving Biology From Science To Engineering, Advancing MedicineApr 10, 2024,09:00am EDTThis Startup Wants To Be OpenAI Of Stem Cell Therapy, Targets $250B MarketEdit StoryForbesInnovationEnterprise TechThe Amazing Ways Google And Grammarly Use Artificial Intelligence To Improve Your WritingBernard MarrContributorOpinions expressed by Forbes Contributors are their own.FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got itNov 12, 2018,12:19am ESTUpdated Nov 15, 2018, 05:55am ESTThis article is more than 5 years old.Share to FacebookShare to TwitterShare to LinkedinWhile online editing tools such as Grammarly and grammar suggestions from Google Docs aren’t foolproof, the artificial intelligence and machine learning algorithms that power them are successfully improving the way many of us write. In the process, it saved millions of embarrassing errors caused by carelessness (there vs. they're) and, of course, caught mistakes that involved more challenging grammar rules. Whether we write an email, a text or something more formal, even professionals use these editing tools to detect errors before they mistakenly get broadcast. To really appreciate the technology that makes these editing tools possible, let's take a look at these services and the impressive ways they improve our writing.








Adobe Stock
Adobe Stock






Grammarly: The Leader of the Pack
Since its 2009 inception, cloud-based Grammarly has grown to more than 15 million daily active users and is often one of the top-ranked grammar checkers. Users can download the Grammarly Keyboard for their mobile devices, add an extension to Chrome, Firefox, Safari, Microsoft Edge, use the Grammarly Desktop App or download a plug-in for Microsoft Office so the algorithm can spell and grammar check on Word, social media and Outlook. Grammarly switched from a subscription-only model to a freemium model in 2015 to provide more access to users.
How does it work? Just like with other machine learning algorithms, Grammarly's artificial intelligence system was originally provided with a lot of high-quality training data to teach the algorithm by showing it examples of what proper grammar looks like. This text corpus—a huge compilation human researchers organized and labeled so the AI could understand it—showed, as an example, not only proper uses of punctuation, grammar and spelling, but incorrect applications so the machine could learn the difference. In addition, Grammarly’s system uses natural language processing to analyze every nuance of language down to the character level and all the way up to words and full paragraphs of text.
PROMOTED
The feedback the system gets through humans when they ignore a proposed suggestion helps the system get smarter and provides the human linguists working with the input of the machine on how to make the system better. The more text it is exposed to, the better it can make appropriate suggestions. That's one of the reasons the company switched in 2010 to a consumer service from targeting enterprise customers so it would have access to a larger data set and a more significant opportunity.

In 2017, investors General Catalyst, IVP and Spark Capital committed $110 million to the already profitable company to help it further enhance its capabilities. Although the company has made great strides in improving grammar, grammatically correct writing doesn't necessarily mean it is compelling or concise.  So, although the company has a history of adding new checks such as to identify vagueness or plagiarism to improve your writing, expect this new infusion of funds to allow the company to add staff in an effort to continue improvements to its algorithm and the editing it can do. It has adequately tackled the basic mechanics of writing from spelling, grammar, and sentence structure as well as being able to help with clarity and readability of text. The next frontier is to provide context-specific suggestions.
Grammar Suggestions: Google Docs’ AI Grammar Checker
Grammarly may have recently introduced an extension of their own to work with Google Docs, but Google wants to get their own skin in the game with its grammar suggestions product. Google is using machine translation, the same tech they use to translate from one language to another (and one it has said approached human levels of accuracy), to power its editing tool. Instead of language to language, it translates poorly written text into grammatically correct text. If the system identifies a grammar issue, it will highlight it similar to how the spell check functionality works, so you have a chance to review possible grammar errors before hitting “send” or “publish.” Currently, details are limited about the new service, and the grammar check feature is only available via an early adopter program.









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



These are undoubtedly intriguing developments, but we don't anticipate that they will take the place of a human writer anytime soon.
And according to the Grammarly Blog, “Our goal is to help you express yourself in the best way possible, whether you're applying for a job or texting a joke to your friends."" Ultimately, the company desires to become ""a true communication assistant that improves how people connect with and understand each other.""  It sounds like a brave mission any high school English teacher would support.Follow me on Twitter or LinkedIn. Check out my website or some of my other work here. Bernard MarrFollowingFollowBernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is... Read MoreEditorial StandardsPrintReprints & Permissions",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMieGh0dHBzOi8vd3d3Lm1ja2luc2V5LmNvbS9mZWF0dXJlZC1pbnNpZ2h0cy9hcnRpZmljaWFsLWludGVsbGlnZW5jZS9haS1hZG9wdGlvbi1hZHZhbmNlcy1idXQtZm91bmRhdGlvbmFsLWJhcnJpZXJzLXJlbWFpbtIBAA?oc=5,"Adoption of AI advances, but foundational barriers remain - McKinsey",2018-11-13,McKinsey,https://www.mckinsey.com,Survey respondents report the rapid adoption of AI and expect only a minimal effect on head count. Yet few companies have in place the foundational building blocks that enable AI to generate value at scale.,N/A,Survey respondents report the rapid adoption of AI and expect only a minimal effect on head count. Yet few companies have in place the foundational building blocks that enable AI to generate value at scale.,Survey respondents report the rapid adoption of AI and expect only a minimal effect on head count. Yet few companies have in place the foundational building blocks that enable AI to generate value at scale.,https://schema.org,Survey,https://www.mckinsey.com,https://www.mckinsey.com/~/media/mckinsey/featured%20insights/artificial%20intelligence/ai%20adoption%20advances%20but%20foundational%20barriers%20remain/ai-adoption-advances-1536x1536-100.jpg,,"{'@type': 'Organization', 'name': 'McKinsey & Company', 'logo': {'@type': 'ImageObject', 'url': 'https://www.mckinsey.com/~/media/Thumbnails/Mck_Logo'}}",,2018-11-13T00:00:00Z,2018-11-13T00:00:00Z,,,,,N/A,N/A,N/A,"{'@type': 'WebPage', '@id': 'https://www.mckinsey.com/featured-insights/artificial-intelligence/ai-adoption-advances-but-foundational-barriers-remain'}",2018-11-12T17:32:09Z,"AI adoption advances, but foundational barriers remain",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9ob3ctdG8tdGVhY2gtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtY29tbW9uLXNlbnNlL9IBAA?oc=5,How to Teach Artificial Intelligence Some Common Sense - WIRED,2018-11-13,WIRED,https://www.wired.com,"We’ve spent years teaching neural nets to think like human brains. They’re crazy-smart, but what if we’ve been doing it all wrong?","['business', 'the big story', 'ai hub', 'games', 'research', 'neural network', 'big company', 'magazine-26.12', 'cover story', 'longreads', 'artificial intelligence', 'ai', 'neural networks', '_syndication_noshow', 'magazine']","We’ve spent years teaching neural nets to think like human brains. They’re crazy-smart, but what if we’ve been doing it all wrong?","We’ve spent years teaching neural nets to think like human brains. They’re crazy-smart, but what if we’ve been doing it all wrong?",https://schema.org/,BreadcrumbList,https://www.wired.com/story/how-to-teach-artificial-intelligence-common-sense/,"['https://media.wired.com/photos/5beb0c088daf7470d1923b48/16:9/w_1200,h_675,c_limit/AI-GG3A0107-3w.gif', 'https://media.wired.com/photos/5beb0c088daf7470d1923b48/4:3/w_1064,h_798,c_limit/AI-GG3A0107-3w.gif', 'https://media.wired.com/photos/5beb0c088daf7470d1923b48/1:1/w_800,h_800,c_limit/AI-GG3A0107-3w.gif']","[{'@type': 'Person', 'name': 'Clive Thompson', 'sameAs': 'https://www.wired.com/author/clive-thompson/'}]","{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}",How to Teach Artificial Intelligence Some Common Sense,2018-11-13T06:00:00.000-05:00,2018-11-13T06:00:00.000-05:00,business,,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'Business', 'item': 'https://www.wired.com/business/'}, {'@type': 'ListItem', 'position': 2, 'name': 'magazine-26.12', 'item': 'https://www.wired.com/tag/magazine-2612/'}, {'@type': 'ListItem', 'position': 3, 'name': 'How to Teach Artificial Intelligence Some Common Sense'}]",tags,N/A,"Clive ThompsonBusinessNov 13, 2018 6:00 AMHow to Teach Artificial Intelligence Some Common SenseWe’ve spent years feeding neural nets vast amounts of data, teaching them to think like human brains. They’re crazy-smart, but they have absolutely no common sense. What if we’ve been doing it all wrong?We’ve spent years feeding neural nets vast amounts of data, teaching them to think like human brains. They’re crazy-smart, but they have absolutely no common sense. What if we’ve been doing it all wrong?Beth HolzerSave this storySaveSave this storySaveThe AI Database →ApplicationGamesEnd UserResearchBig companySectorResearchTechnologyNeural NetworkFive years ago, the coders at DeepMind, a London-based artificial intelligence company, watched excitedly as an AI taught itself to play a classic arcade game. They’d used the hot technique of the day, deep learning, on a seemingly whimsical task: mastering Breakout,1 the Atari game in which you bounce a ball at a wall of bricks, trying to make each one vanish.1 Steve Jobs was working at Atari when he was commissioned to create 1976’s Breakout, a job no other engineer wanted. He roped his friend Steve Wozniak, then at Hewlett-­Packard, into helping him.Deep learning is self-education for machines; you feed an AI huge amounts of data, and eventually it begins to discern patterns all by itself. In this case, the data was the activity on the screen—blocky pixels representing the bricks, the ball, and the player’s paddle. The DeepMind AI, a so-called neural network made up of layered algorithms, wasn’t programmed with any knowledge about how Breakout works, its rules, its goals, or even how to play it. The coders just let the neural net examine the results of each action, each bounce of the ball. Where would it lead?To some very impressive skills, it turns out. During the first few games, the AI flailed around. But after playing a few hundred times, it had begun accurately bouncing the ball. By the 600th game, the neural net was using a more expert move employed by human Breakout players, chipping through an entire column of bricks and setting the ball bouncing merrily along the top of the wall.Trending NowNeuroscientist Explains One Concept in 5 Levels of Difficulty“That was a big surprise for us,” Demis Hassabis, CEO of DeepMind, said at the time. “The strategy completely emerged from the underlying system.” The AI had shown itself capable of what seemed to be an unusually subtle piece of humanlike thinking, a grasping of the inherent concepts behind Breakout. Because neural nets loosely mirror the structure of the human brain, the theory was that they should mimic, in some respects, our own style of cognition. This moment seemed to serve as proof that the theory was right.December 2018. Subscribe to WIRED.
Illustration: Axis of StrengthThen, last year, computer scientists at Vicarious, an AI firm in San Francisco, offered an interesting reality check. They took an AI like the one used by DeepMind and trained it on Breakout. It played great. But then they slightly tweaked the layout of the game. They lifted the paddle up higher in one iteration; in another, they added an unbreakable area in the center of the blocks.A human player would be able to quickly adapt to these changes; the neural net couldn’t. The seemingly supersmart AI could play only the exact style of Breakout it had spent hundreds of games mastering. It couldn’t handle something new.“We humans are not just pattern recognizers,” Dileep George, a computer scientist who cofounded Vicarious, tells me. “We’re also building models about the things we see. And these are causal models—we understand about cause and effect.” Humans engage in reasoning, making logi­cal inferences about the world around us; we have a store of common-sense knowledge that helps us figure out new situations. When we see a game of Breakout that’s a little different from the one we just played, we realize it’s likely to have mostly the same rules and goals. The neural net, on the other hand, hadn’t understood anything about Breakout. All it could do was follow the pattern. When the pattern changed, it was helpless.The A.I. IssueThe A.I. IssueThe DIY Tinkerers Harnessing the Power of AITom SimoniteThe A.I. IssueFei-Fei Li's Quest to Make Machines Better for HumanityJessi HempelThe A.I. IssueThe Genius Neuroscientist Who Might Hold the Key to True AIShaun RavivDeep learning is the reigning monarch of AI. In the six years since it exploded into the mainstream, it has become the dominant way to help machines sense and perceive the world around them. It powers Alexa’s speech recognition, Waymo’s self-driving cars, and Google’s on-the-fly translations. Uber is in some respects a giant optimization problem, using machine learning to figure out where riders will need cars. Baidu, the Chinese tech giant, has more than 2,000 engineers cranking away on neural net AI. For years, it seemed as though deep learning would only keep getting better, leading inexorably to a machine with the fluid, supple intelligence of a person.But some heretics argue that deep learning is hitting a wall. They say that, on its own, it’ll never produce generalized intelligence, because truly humanlike intelligence isn’t just pattern recognition. We need to start figuring out how to imbue AI with everyday common sense, the stuff of human smarts. If we don’t, they warn, we’ll keep bumping up against the limits of deep learning, like visual-recognition systems that can be easily fooled by changing a few inputs, making a deep-learning model think a turtle is a gun. But if we succeed, they say, we’ll witness an explosion of safer, more useful devices—health care robots that navigate a cluttered home, fraud detection systems that don’t trip on false positives, medical breakthroughs powered by machines that ponder cause and effect in disease.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDBut what does true reasoning look like in a machine? And if deep learning can’t get us there, what can?Beth HolzerGary Marcus is a pensive, bespectacled 48-year-old professor of psychology and neuroscience at New York University, and he’s probably the most famous apostate of orthodox deep learning.Marcus first got interested in artificial intelligence in the 1980s and ’90s, when neural nets were still in their experimental phase, and he’s been making the same argument ever since. “It’s not like I came to this party late and want to pee on it,” Marcus told me when I met him at his apartment near NYU. (We are also personal friends.) “As soon as deep learning erupted, I said ‘This is the wrong direction, guys!’ ”SIGN UP TODAYGet the Backchannel newsletter for the best features and investigations on WIRED.Back then, the strategy behind deep learning was the same as it is today. Say you wanted a machine to teach itself to recognize daisies. First you’d code some algorithmic “neurons,” connecting them in layers like a sandwich (when you use several layers, the sandwich gets thicker or deep—hence “deep” learning). You’d show an image of a daisy to the first layer, and its neurons would fire or not fire based on whether the image resembled the examples of daisies it had seen before. The signal would move on to the next layer, where the process would be repeated. Eventually, the layers would winnow down to one final verdict.At first, the neural net is just guessing blindly; it starts life a blank slate, more or less. The key is to establish a useful feedback loop. Every time the AI misses a daisy, that set of neural connections weakens the links that led to an incorrect guess; if it’s successful, it strengthens them. Given enough time and enough daisies, the neural net gets more accurate. It learns to intuit some pattern of daisy-­ness that lets it detect the daisy (and not the sunflower or aster) each time. As the years went on, this core idea—start with a naive network and train by repetition—was improved upon and seemed useful nearly anywhere it was applied.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDBut Marcus was never convinced. For him, the problem is the blank slate: It assumes that humans build their intelligence purely by observing the world around them, and that machines can too. But Marcus doesn’t think that’s how humans work. He walks the intellectual path laid down by Noam Chomsky,2 who argued that humans are born wired to learn, programmed to master language and interpret the physical world.2 In 1975 the psycholo­gist Jean Piaget and the linguist Noam Chomsky met in France for what would prove to be a historic debate. Grossly simplified, Piaget argued that human brains are blank-slate self-­learning machines, and Chomsky that they are endowed with some preprogrammed smarts.For all their supposed braininess, he notes, neural nets don’t appear to work the way human brains do. For starters, they’re much too data-hungry. In most cases, each neural net requires thousands or millions of examples to learn from. Worse, each time you want a neural net to recognize a new type of item, you have to start from scratch. A neural net trained to recognize only canaries isn’t of any use in recognizing, say, birdsong or human speech.“We don’t need massive amounts of data to learn,” Marcus says. His kids didn’t need to see a million cars before they could recognize one. Better yet, they can generalize; when they see a tractor for the first time, they understand that it’s sort of like a car. They can also engage in counterfactuals. Google Translate can map the French equivalent of the English sentence “The glass was pushed, so it fell off the table.” But it doesn’t know what the words mean, so it couldn’t tell you what would happen if the glass weren’t pushed. Humans, Marcus notes, grasp not just the patterns of grammar but the logic behind it. You could give a young child a fake verb like pilk, and she’d likely be able to reason that the past tense would be pilked. She hasn’t seen that word before, of course. She hasn’t been “trained” on it. She has just intuited some logic about how language works and can apply it to a new situation.“These deep-learning systems don’t know how to integrate abstract knowledge,” says Marcus, who founded a company that created AI to learn with less data (and sold the company to Uber in 2016).Earlier this year, Marcus published a white paper on arXiv, arguing that, without some new approaches, deep learning might never get past its current limitations. What it needs is a boost—rules that supplement or are built in to help it reason about the world.Beth HolzerMost PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDOren Etzioni is a smiling bear of a guy. A computer scientist who runs the Allen Institute for Artificial Intelligence in Seattle, he greets me in his bright office wearing jeans and a salmon-­colored shirt, ushering me in past a whiteboard scrawled with musings about machine intelligence. (“DEFINE SUCCESS,” “WHAT’S THE TASK?”) Outside, in the sun-drenched main room of the institute, young AI researchers pad around sylphlike, headphones attached, quietly pecking at keyboards.Etzioni and his team are working on the common-sense problem. He defines it in the context of two legendary AI moments—the trouncing of the chess grandmaster Garry Kasparov3 by IBM’s Deep Blue in 1997 and the equally shocking defeat of the world’s top Go player by DeepMind’s AlphaGo last year. (Google bought DeepMind in 2014.)3 In 1996, Kasparov—then the best chess player in the world—beat Deep Blue. During a rematch a year later, Kasparov surrendered after 19 moves. He later told a reporter: “I’m a human being. When I see something that is well beyond my understanding, I’m afraid.”“With Deep Blue we had a program that would make a superhuman chess move—while the room was on fire,” Etzioni jokes. “Right? Completely lacking context. Fast-forward 20 years, we’ve got a computer that can make a superhuman Go move—while the room is on fire.” Humans, of course, do not have this limitation. His team plays weekly games of bughouse chess, and if a fire broke out the humans would pull the alarm and run for the doors.Humans, in other words, possess a base of knowledge about the world (fire burns things) mixed with the ability to reason about it (you should try to move away from an out-of-control fire). For AI to truly think like people, we need to teach it the stuff that everyone knows, like physics (balls tossed in the air will fall) or the relative sizes of things (an elephant can’t fit in a bathtub). Until AI possesses these basic concepts, Etzioni figures, it won’t be able to reason.With an infusion of hundreds of millions of dollars from Paul Allen,4 Etzioni and his team are trying to develop a layer of common-sense reasoning to work with the existing style of neural net. (The Allen Institute is a nonprofit, so everything they discover will be published, for anyone to use.)4 Microsoft cofounder and philanthropist Paul Allen donated billions to science, climate, and health research, as well as to Seattle causes. He died of complications from cancer on October 15 at age 65.The first problem they face is answering the question, What is common sense?Etzioni describes it as all the knowledge about the world that we take for granted but rarely state out loud. He and his colleagues have created a set of benchmark questions that a truly reasoning AI ought to be able to answer: If I put my socks in a drawer, will they be there tomorrow? If I stomp on someone’s toe, will they be mad?One way to get this knowledge is to extract it from people. Etzioni’s lab is paying crowdsourced humans on Amazon Mechanical Turk to help craft common-sense statements. The team then uses various machine-learning techniques—some old-school statistical analyses, some deep-learning neural nets—to draw lessons from those statements. If they do it right, Etzioni believes they can produce reusable Lego bricks of computer reasoning: One set that understands written words, one that grasps physics, and so on.Yejin Choi, one of Etzioni’s leading common-­sense scientists, has led several of these crowdsourced efforts. In one project, she wanted to develop an AI that would understand the intent or emotion implied by a person’s actions or statements. She started by examining thousands of online stories, blogs, and idiom entries in Wiktionary and extracting “phrasal events,” such as the sentence “Jeff punches Roger’s lights out.” Then she’d anonymize each phrase—“Person X punches Person Y’s lights out”—and ask the Turkers to describe the intent of Person X: Why did they do that? When she had gathered 25,000 of these marked-up sentences, she used them to train a machine-learning system to analyze sentences it had never seen before and infer the emotion or intent of the subject.LEARN MOREThe WIRED Guide to Artificial IntelligenceAt best, the new system worked only half the time. But when it did, it evinced some very humanlike perception: Given a sentence like “Oren cooked Thanksgiving dinner,” it predicted that Oren was trying to impress his family. “We can also reason about others’ reactions, even if they’re not mentioned,” Choi notes. “So X’s family probably feel impressed and loved.” Another system her team built used Turkers to mark up the psychological states of people in stories; the resulting system could also draw some sharp inferences when given a new situation. It was told, for instance, about a music instructor getting angry at his band’s lousy performance and that “the instructor was furious and threw his chair.” The AI predicted that the musicians would “feel fear afterwards,” even though the story doesn’t explicitly say so.Choi, Etzioni, and their colleagues aren’t abandoning deep learning. Indeed, they regard it as a very useful tool. But they don’t think there is a shortcut to the laborious task of coaxing people to explicitly state the weird, invisible, implied knowledge we all possess. Deep learning is garbage in, garbage out. Merely feeding a neural net tons of news articles isn’t enough, because it wouldn’t pick up on the unstated knowledge, the obvious stuff that writers didn’t bother to mention. As Choi puts it, “People don’t say ‘My house is bigger than me.’ ” To help tackle this problem, she had the Turkers analyze the physical relationships implied by 1,100 common verbs, such as “X threw Y.” That, in turn, allowed for a simple statistical model that could take the sentence “Oren threw the ball” and infer that the ball must be smaller than Oren.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDAnother challenge is visual reasoning. Aniruddha Kembhavi, another of Etzioni’s AI scientists, shows me a virtual robot wandering around an onscreen house. Other Allen Institute scientists built the Sims-like house, filling it with everyday items and realistic physics—kitchen cupboards full of dishes, couches that can be pushed around. Then they designed the robot, which looks like a dark gray garbage canister with arms, and told it to hunt down certain items. After thousands of tasks, the neural net gains a basic grounding in real-life facts.“What this agent has learned is, when you ask it ‘Do I have tomatoes?’ it doesn’t go and open all the cabinets. It prefers to open the fridge,” Kembhavi says. “Or if you say ‘Find me my keys,’ it doesn’t try to pick up the television. It just looks behind the television. It has learned that TVs aren’t usually picked up.”Etzioni and his colleagues hope that these various components—Choi’s language reasoning, the visual thinking, other work they’re doing on getting an AI to grasp textbook science information—can all eventually be combined. But how long will it take, and what will the final products look like? They don’t know. The common-sense systems they’re building still make mistakes, sometimes more than half the time. Choi estimates she’ll need around a million crowdsourced human statements as she trains her various language-parsing AIs. Building common sense, it would seem, is uncommonly hard.There are other pathways to making machines that reason, and they’re even more labor-intensive. For example, you could simply sit down and write out, by hand, all the rules that tell a machine how the world works. This is how Doug Lenat’s Cyc project works. For 34 years, Lenat has employed a team of engineers and philosophers to code 25 million rules of general common sense, like “water is wet” or “most people know the first names of their friends.” This lets Cyc deduce things: “Your shirt is wet, so you were probably in the rain.” The advantage is that Lenat has exquisite control over what goes into Cyc’s database; that isn’t true of crowdsourced knowledge.Brute-force, handcrafted AI has become unfashionable in the world of deep learning. That’s partly because it can be “brittle”: Without the right rules about the world, the AI can get flummoxed. This is why scripted chatbots are so frustrating; if they haven’t been explicitly told how to answer a question, they have no way to reason it out. Cyc is enormously more capable than a chatbot and has been licensed for use in health care systems, financial services, and military projects. But the work is achingly slow, and it’s expensive. Lenat says it has cost around $200 million to develop Cyc.But a bit of hand coding could be how you replicate some of the built-in knowledge that, according to the Chomskyite view, human brains possess. That’s what Dileep George and the Vicarious researchers did with Breakout. To create an AI that wouldn’t get stumped by changes to the layout of the game, they abandoned deep learning and built a system that included hard-coded basic assumptions. Without too much trouble, George tells me, their AI learned “that there are objects, and there are interactions between objects, and that the motion of one object can be causally explained between the object and something else.”Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDAs it played Breakout, the system developed the ability to weigh different courses of action and their likely outcomes. This worked in reverse too. If the AI wanted to break a block in the far left corner of the screen, it reasoned to put the paddle in the far right corner. Crucially, this meant that when Vicarious changed the layout of the game—adding new bricks or raising the paddle—the system compensated. It appeared to have extracted some general understanding about Breakout itself.Granted, there are trade-offs in this type of AI engineering. It’s arguably more painstaking to craft and takes careful planning to figure out precisely what foreordained logic to feed into the system. It’s also hard to strike the right balance of speed and accuracy when designing a new system. George says he looks for the minimum set of data “to put into the model so it can learn quickly.” The fewer assumptions you need, the more efficiently the machine will make decisions. Once you’ve trained a deep-learning model to recognize cats, you can show it a Russian blue it has never seen and it renders the verdict—it’s a cat!—almost instantaneously. Having processed millions of photos, it knows not only what makes a cat a cat but also the fastest way to identify one. In contrast, Vicarious’ style of AI is slower, because it’s actively making logical inferences as it goes.When the Vicarious AI works well, it can learn from much less data. George’s team created an AI to bust captchas,5 those “I’m not a robot” obstacles online, by recognizing characters in spite of their distorted, warped appearance.5 Captcha stands for “Completely Automated Public Turing test to tell Computers and Humans Apart.” It originated at Carnegie Mellon University in 2000; Yahoo was the first big company to make its use commonplace.Much as with the Breakout system, they endowed their AI with some abilities up front, such as knowledge that helps it discern the likely edges of characters. With that bootstrapping in place, they only needed to train the AI on 260 images before it learned to break captchas with 90.4 percent accuracy. In contrast, a neural net needed to be trained on more than 2.3 million images before it could break a captcha.Others are building common-sense-like structure into neural nets in different ways. Two researchers at DeepMind, for instance, recently created a hybrid system—part deep learning, part more traditional techniques—known as inductive logic programming. The goal was to produce something that could do mathematical reasoning.They trained it on the children’s game fizz-buzz, in which you count upward from 1, saying “fizz” if a number is divisible by 3 and “buzz” if it is divisible by 5. A regular neural net would be able to do this only for numbers it had seen before; train it up to 100 and it would know that 99 is “fizz” and 100 is “buzz.” But it wouldn’t know what to do with 105. In contrast, the hybrid DeepMind system seemed to understand the rule and went past 100 with no problem. Edward Grefenstette, one of the DeepMind coders who built the hybrid, says, “You can train systems that will generalize in a way that deep-learning networks simply couldn’t on their own.”Beth HolzerMost PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDYann LeCun, a deep-learning pioneer and the current head of Facebook’s AI research wing, agrees with many of the new critiques of the field. He acknowledges that it requires too much training data, that it can’t reason, that it doesn’t have common sense. “I’ve been basically saying this over and over again for the past four years,” he reminds me. But he remains steadfast that deep learning, properly crafted, can provide the answer. He disagrees with the Chomskyite vision of human intelligence. He thinks human brains develop the ability to reason solely through interaction, not built-in rules. “If you think about how animals and babies learn, there’s a lot of things that are learned in the first few minutes, hours, days of life that seem to be done so fast that it looks like they are hardwired,” he notes. “But in fact they don’t need to be hardwired, because they can be learned so quickly.” In this view, to figure out the physics of the world, a baby just moves its head around, data-crunches the incoming imagery, and concludes that, hey, depth of field is a thing.Still, LeCun admits it’s not yet clear which routes will help deep learning get past its humps. It might be “adversarial” neural nets, a relatively new technique in which one neural net tries to fool another neural net with fake data—forcing the second one to develop extremely ­subtle internal representations of pictures, sounds, and other inputs. The advantage here is that you don’t have the “data hungriness” problem. You don’t need to collect millions of data points on which to train the neural nets, because they’re learning by studying each other. (Apocalyptic side note: A similar method is being used to create those profoundly troubling “deepfake” videos in which someone appears to be saying or doing something they are not.)I met LeCun at the offices of Facebook’s AI lab in New York. Mark Zuckerberg recruited him in 2013, with the promise that the lab’s goal would be to push the limits of ambitious AI, not just produce minor tweaks for Facebook’s products. Like an academic lab, LeCun and his researchers publish their work for others to access.LeCun, who retains the rich accent of his native France and has a Bride of Frankenstein shock of white in his thick mass of dark hair, stood at a whiteboard energetically sketching out theories of possible deep-learning advances. On the facing wall was a set of gorgeous paintings from Stanley Kubrick’s 2001: A Space Odyssey—the main spaceship floating in deep space, the wheel-like ship orbiting Earth. “Oh, yes,” LeCun said, when I pointed them out; they were reprints of artwork Kubrick commissioned for the movie.It was weirdly unsettling to discuss humanlike AI with those images around, because of course HAL 9000,6 the humanlike AI in 2001, turns out to be a highly efficient murderer.6 HAL was originally supposed to be voiced by Martin Balsam, an actor with a thick Bronx accent. After recording, however, director Stanley Kubrick decided Balsam sounded “too colloquially American.” He was replaced by Canadian actor Douglas Rain.And this pointed to a deeper philosophical question that floats over the whole debate: Is smarter AI even a good idea? Vicarious’ system cracked captcha, but the whole point of captcha is to prevent bots from impersonating humans. Some AI thinkers worry that the ability to talk to humans and understand their psychology could make a rogue AI incredibly dangerous. Nick Bostrom7 at the University of Oxford has sounded the alarm about the dangers of creating a “superintelligence,” an AI that self-improves and rapidly outstrips humanity, able to outthink and outflank us in every way. (One way he suggests it might amass control is by manipulating people—something for which possessing a “theory of mind” would be quite useful.)7 In 2003, Bostrom published the now-famous paper-clip warning about superintelligence: “A well-meaning team of programmers [could] make a big mistake in designing its goal system. This could result … in a super­intelligence whose top goal is the manufacturing of paper clips, with the consequence that it starts transforming first all of Earth and then increasing portions of space into paper-clip manufacturing facilities.”Elon Musk is sufficiently convinced of this danger that he has funded OpenAI, an organization dedicated to the notion of safe AI.This future doesn’t keep Etzioni up at night. He’s not worried about AI becoming maliciously superintelligent. “We’re worried about something taking over the world,” he scoffs, “that can’t even on its own decide to play chess again.” It’s not clear how an AI would develop a desire to do so or what that desire would look like in software. Deep learning can conquer chess, but it has no inborn will to play.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDWhat does concern him is that current AI is woefully inept. So while we might not be creating HAL with a self-preserving intelligence, an “inept AI attached to deadly weapons can easily kill,” he says. This is partly why Etzioni is so determined to give AI some common sense. Ultimately, he argues, it will make AI safer; the idea that humanity shouldn’t be wholesale slaughtered is, of course, arguably a piece of common-­sense knowledge itself. (Part of the Allen Institute’s mandate is to make AI safer by making it more reasonable.)Related Storieswired25Researchers Call for More Humanity in Artificial IntelligenceTom SimoniteArtificial IntelligenceTo Break a Hate-Speech Algorithm, Try 'Love'Louise MatsakisWIRED Q&AEmmanuel Macron Talks to WIRED About France's AI StrategyNicholas ThompsonEtzioni notes that the dystopic sci-fi visions of AI are less risky than near-term economic displacement. The better AI gets at common sense, the more rapidly it’ll take over jobs that currently are too hard for mere pattern-­matching deep learning: drivers, cashiers, managers, analysts of all stripes, and even (alas) journalists. But truly reasoning AI could wreak havoc even beyond the economy. Imagine how good political disinformation bots would be if they could use common-­sense knowledge to appear indistinguishably human on Twitter or Facebook or in mass phone calls.Marcus agrees that reasoning AI will have dangers. But the upsides, he says, would be huge. AI that could reason and perceive like humans yet move at the speed of computers could revolutionize science, teasing out causal connections at a pace impossible for us alone. It could follow if-then chains and ponder counterfactuals, running mental experiments the way humans do, except with massive robotic knowledge. “We might finally be able to cure mental illness, for example,” Marcus adds. “AI might be able to understand these complex biological cascades of proteins that are involved in building brains and having them work correctly or not.”Sitting beneath the images from 2001, LeCun makes a bit of a heretical point himself. Sure, making artificial intelligence more humanlike helps AI to navigate our world. But directly replicating human styles of thought? It’s not clear that’d be useful. We already have humans who can think like humans; maybe the value of smart machines is that they are quite alien from us.“They will tend to be more useful if they have capabilities we don’t have,” he tells me. “Then they’ll become an amplifier for intelligence. So to some extent you want them to have a nonhuman form of intelligence ... You want them to be more rational than humans.” In other words, maybe it’s worth keeping artificial intelligence a little bit artificial.Clive Thompson (@pomeranian99) is a columnist for WIRED.This article appears in the December issue. Subscribe now.Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.More Great WIRED StoriesThis genius neuroscientist might hold the key to true AIHow to safely and securely dispose of your old gadgetsPHOTOS: When your baby is actually made of siliconeOnline conspiracy groups are a lot like cultsPipeline vandals are reinventing climate activismGet even more of our inside scoops with our weekly Backchannel newsletter","{'@type': 'WebPage', '@id': 'https://www.wired.com/story/how-to-teach-artificial-intelligence-common-sense/'}",,,"Deep learning is self-education for machines; you feed an AI huge amounts of data, and eventually it begins to discern patterns all by itself. In this case, the data was the activity on the screen—blocky pixels representing the bricks, the ball, and the player’s paddle. The DeepMind AI, a so-called neural network made up of layered algorithms, wasn’t programmed with any knowledge about how Breakout works, its rules, its goals, or even how to play it. The coders just let the neural net examine the results of each action, each bounce of the ball. Where would it lead?
To some very impressive skills, it turns out. During the first few games, the AI flailed around. But after playing a few hundred times, it had begun accurately bouncing the ball. By the 600th game, the neural net was using a more expert move employed by human Breakout players, chipping through an entire column of bricks and setting the ball bouncing merrily along the top of the wall.
“That was a big surprise for us,” Demis Hassabis, CEO of DeepMind, said at the time. “The strategy completely emerged from the underlying system.” The AI had shown itself capable of what seemed to be an unusually subtle piece of humanlike thinking, a grasping of the inherent concepts behind Breakout. Because neural nets loosely mirror the structure of the human brain, the theory was that they should mimic, in some respects, our own style of cognition. This moment seemed to serve as proof that the theory was right.
Then, last year, computer scientists at Vicarious, an AI firm in San Francisco, offered an interesting reality check. They took an AI like the one used by DeepMind and trained it on Breakout. It played great. But then they slightly tweaked the layout of the game. They lifted the paddle up higher in one iteration; in another, they added an unbreakable area in the center of the blocks.
A human player would be able to quickly adapt to these changes; the neural net couldn’t. The seemingly supersmart AI could play only the exact style of Breakout it had spent hundreds of games mastering. It couldn’t handle something new.
“We humans are not just pattern recognizers,” Dileep George, a computer scientist who cofounded Vicarious, tells me. “We’re also building models about the things we see. And these are causal models—we understand about cause and effect.” Humans engage in reasoning, making logi­cal inferences about the world around us; we have a store of common-sense knowledge that helps us figure out new situations. When we see a game of Breakout that’s a little different from the one we just played, we realize it’s likely to have mostly the same rules and goals. The neural net, on the other hand, hadn’t understood anything about Breakout. All it could do was follow the pattern. When the pattern changed, it was helpless.
Deep learning is the reigning monarch of AI. In the six years since it exploded into the mainstream, it has become the dominant way to help machines sense and perceive the world around them. It powers Alexa’s speech recognition, Waymo’s self-driving cars, and Google’s on-the-fly translations. Uber is in some respects a giant optimization problem, using machine learning to figure out where riders will need cars. Baidu, the Chinese tech giant, has more than 2,000 engineers cranking away on neural net AI. For years, it seemed as though deep learning would only keep getting better, leading inexorably to a machine with the fluid, supple intelligence of a person.
But some heretics argue that deep learning is hitting a wall. They say that, on its own, it’ll never produce generalized intelligence, because truly humanlike intelligence isn’t just pattern recognition. We need to start figuring out how to imbue AI with everyday common sense, the stuff of human smarts. If we don’t, they warn, we’ll keep bumping up against the limits of deep learning, like visual-recognition systems that can be easily fooled by changing a few inputs, making a deep-learning model think a turtle is a gun. But if we succeed, they say, we’ll witness an explosion of safer, more useful devices—health care robots that navigate a cluttered home, fraud detection systems that don’t trip on false positives, medical breakthroughs powered by machines that ponder cause and effect in disease.
But what does true reasoning look like in a machine? And if deep learning can’t get us there, what can?
Marcus first got interested in artificial intelligence in the 1980s and ’90s, when neural nets were still in their experimental phase, and he’s been making the same argument ever since. “It’s not like I came to this party late and want to pee on it,” Marcus told me when I met him at his apartment near NYU. (We are also personal friends.) “As soon as deep learning erupted, I said ‘This is the wrong direction, guys!’ ”
Back then, the strategy behind deep learning was the same as it is today. Say you wanted a machine to teach itself to recognize daisies. First you’d code some algorithmic “neurons,” connecting them in layers like a sandwich (when you use several layers, the sandwich gets thicker or deep—hence “deep” learning). You’d show an image of a daisy to the first layer, and its neurons would fire or not fire based on whether the image resembled the examples of daisies it had seen before. The signal would move on to the next layer, where the process would be repeated. Eventually, the layers would winnow down to one final verdict.
At first, the neural net is just guessing blindly; it starts life a blank slate, more or less. The key is to establish a useful feedback loop. Every time the AI misses a daisy, that set of neural connections weakens the links that led to an incorrect guess; if it’s successful, it strengthens them. Given enough time and enough daisies, the neural net gets more accurate. It learns to intuit some pattern of daisy-­ness that lets it detect the daisy (and not the sunflower or aster) each time. As the years went on, this core idea—start with a naive network and train by repetition—was improved upon and seemed useful nearly anywhere it was applied.
But Marcus was never convinced. For him, the problem is the blank slate: It assumes that humans build their intelligence purely by observing the world around them, and that machines can too. But Marcus doesn’t think that’s how humans work. He walks the intellectual path laid down by Noam Chomsky,2 who argued that humans are born wired to learn, programmed to master language and interpret the physical world.
For all their supposed braininess, he notes, neural nets don’t appear to work the way human brains do. For starters, they’re much too data-hungry. In most cases, each neural net requires thousands or millions of examples to learn from. Worse, each time you want a neural net to recognize a new type of item, you have to start from scratch. A neural net trained to recognize only canaries isn’t of any use in recognizing, say, birdsong or human speech.
“We don’t need massive amounts of data to learn,” Marcus says. His kids didn’t need to see a million cars before they could recognize one. Better yet, they can generalize; when they see a tractor for the first time, they understand that it’s sort of like a car. They can also engage in counterfactuals. Google Translate can map the French equivalent of the English sentence “The glass was pushed, so it fell off the table.” But it doesn’t know what the words mean, so it couldn’t tell you what would happen if the glass weren’t pushed. Humans, Marcus notes, grasp not just the patterns of grammar but the logic behind it. You could give a young child a fake verb like pilk, and she’d likely be able to reason that the past tense would be pilked. She hasn’t seen that word before, of course. She hasn’t been “trained” on it. She has just intuited some logic about how language works and can apply it to a new situation.
“These deep-learning systems don’t know how to integrate abstract knowledge,” says Marcus, who founded a company that created AI to learn with less data (and sold the company to Uber in 2016).
Earlier this year, Marcus published a white paper on arXiv, arguing that, without some new approaches, deep learning might never get past its current limitations. What it needs is a boost—rules that supplement or are built in to help it reason about the world.
Etzioni and his team are working on the common-sense problem. He defines it in the context of two legendary AI moments—the trouncing of the chess grandmaster Garry Kasparov3 by IBM’s Deep Blue in 1997 and the equally shocking defeat of the world’s top Go player by DeepMind’s AlphaGo last year. (Google bought DeepMind in 2014.)
“With Deep Blue we had a program that would make a superhuman chess move—while the room was on fire,” Etzioni jokes. “Right? Completely lacking context. Fast-forward 20 years, we’ve got a computer that can make a superhuman Go move—while the room is on fire.” Humans, of course, do not have this limitation. His team plays weekly games of bughouse chess, and if a fire broke out the humans would pull the alarm and run for the doors.
Humans, in other words, possess a base of knowledge about the world (fire burns things) mixed with the ability to reason about it (you should try to move away from an out-of-control fire). For AI to truly think like people, we need to teach it the stuff that everyone knows, like physics (balls tossed in the air will fall) or the relative sizes of things (an elephant can’t fit in a bathtub). Until AI possesses these basic concepts, Etzioni figures, it won’t be able to reason.
With an infusion of hundreds of millions of dollars from Paul Allen,4 Etzioni and his team are trying to develop a layer of common-sense reasoning to work with the existing style of neural net. (The Allen Institute is a nonprofit, so everything they discover will be published, for anyone to use.)
The first problem they face is answering the question, What is common sense?
Etzioni describes it as all the knowledge about the world that we take for granted but rarely state out loud. He and his colleagues have created a set of benchmark questions that a truly reasoning AI ought to be able to answer: If I put my socks in a drawer, will they be there tomorrow? If I stomp on someone’s toe, will they be mad?
One way to get this knowledge is to extract it from people. Etzioni’s lab is paying crowdsourced humans on Amazon Mechanical Turk to help craft common-sense statements. The team then uses various machine-learning techniques—some old-school statistical analyses, some deep-learning neural nets—to draw lessons from those statements. If they do it right, Etzioni believes they can produce reusable Lego bricks of computer reasoning: One set that understands written words, one that grasps physics, and so on.
Yejin Choi, one of Etzioni’s leading common-­sense scientists, has led several of these crowdsourced efforts. In one project, she wanted to develop an AI that would understand the intent or emotion implied by a person’s actions or statements. She started by examining thousands of online stories, blogs, and idiom entries in Wiktionary and extracting “phrasal events,” such as the sentence “Jeff punches Roger’s lights out.” Then she’d anonymize each phrase—“Person X punches Person Y’s lights out”—and ask the Turkers to describe the intent of Person X: Why did they do that? When she had gathered 25,000 of these marked-up sentences, she used them to train a machine-learning system to analyze sentences it had never seen before and infer the emotion or intent of the subject.
At best, the new system worked only half the time. But when it did, it evinced some very humanlike perception: Given a sentence like “Oren cooked Thanksgiving dinner,” it predicted that Oren was trying to impress his family. “We can also reason about others’ reactions, even if they’re not mentioned,” Choi notes. “So X’s family probably feel impressed and loved.” Another system her team built used Turkers to mark up the psychological states of people in stories; the resulting system could also draw some sharp inferences when given a new situation. It was told, for instance, about a music instructor getting angry at his band’s lousy performance and that “the instructor was furious and threw his chair.” The AI predicted that the musicians would “feel fear afterwards,” even though the story doesn’t explicitly say so.
Choi, Etzioni, and their colleagues aren’t abandoning deep learning. Indeed, they regard it as a very useful tool. But they don’t think there is a shortcut to the laborious task of coaxing people to explicitly state the weird, invisible, implied knowledge we all possess. Deep learning is garbage in, garbage out. Merely feeding a neural net tons of news articles isn’t enough, because it wouldn’t pick up on the unstated knowledge, the obvious stuff that writers didn’t bother to mention. As Choi puts it, “People don’t say ‘My house is bigger than me.’ ” To help tackle this problem, she had the Turkers analyze the physical relationships implied by 1,100 common verbs, such as “X threw Y.” That, in turn, allowed for a simple statistical model that could take the sentence “Oren threw the ball” and infer that the ball must be smaller than Oren.
Another challenge is visual reasoning. Aniruddha Kembhavi, another of Etzioni’s AI scientists, shows me a virtual robot wandering around an onscreen house. Other Allen Institute scientists built the Sims-like house, filling it with everyday items and realistic physics—kitchen cupboards full of dishes, couches that can be pushed around. Then they designed the robot, which looks like a dark gray garbage canister with arms, and told it to hunt down certain items. After thousands of tasks, the neural net gains a basic grounding in real-life facts.
“What this agent has learned is, when you ask it ‘Do I have tomatoes?’ it doesn’t go and open all the cabinets. It prefers to open the fridge,” Kembhavi says. “Or if you say ‘Find me my keys,’ it doesn’t try to pick up the television. It just looks behind the television. It has learned that TVs aren’t usually picked up.”
Etzioni and his colleagues hope that these various components—Choi’s language reasoning, the visual thinking, other work they’re doing on getting an AI to grasp textbook science information—can all eventually be combined. But how long will it take, and what will the final products look like? They don’t know. The common-sense systems they’re building still make mistakes, sometimes more than half the time. Choi estimates she’ll need around a million crowdsourced human statements as she trains her various language-parsing AIs. Building common sense, it would seem, is uncommonly hard.
Brute-force, handcrafted AI has become unfashionable in the world of deep learning. That’s partly because it can be “brittle”: Without the right rules about the world, the AI can get flummoxed. This is why scripted chatbots are so frustrating; if they haven’t been explicitly told how to answer a question, they have no way to reason it out. Cyc is enormously more capable than a chatbot and has been licensed for use in health care systems, financial services, and military projects. But the work is achingly slow, and it’s expensive. Lenat says it has cost around $200 million to develop Cyc.
But a bit of hand coding could be how you replicate some of the built-in knowledge that, according to the Chomskyite view, human brains possess. That’s what Dileep George and the Vicarious researchers did with Breakout. To create an AI that wouldn’t get stumped by changes to the layout of the game, they abandoned deep learning and built a system that included hard-coded basic assumptions. Without too much trouble, George tells me, their AI learned “that there are objects, and there are interactions between objects, and that the motion of one object can be causally explained between the object and something else.”
As it played Breakout, the system developed the ability to weigh different courses of action and their likely outcomes. This worked in reverse too. If the AI wanted to break a block in the far left corner of the screen, it reasoned to put the paddle in the far right corner. Crucially, this meant that when Vicarious changed the layout of the game—adding new bricks or raising the paddle—the system compensated. It appeared to have extracted some general understanding about Breakout itself.
Granted, there are trade-offs in this type of AI engineering. It’s arguably more painstaking to craft and takes careful planning to figure out precisely what foreordained logic to feed into the system. It’s also hard to strike the right balance of speed and accuracy when designing a new system. George says he looks for the minimum set of data “to put into the model so it can learn quickly.” The fewer assumptions you need, the more efficiently the machine will make decisions. Once you’ve trained a deep-learning model to recognize cats, you can show it a Russian blue it has never seen and it renders the verdict—it’s a cat!—almost instantaneously. Having processed millions of photos, it knows not only what makes a cat a cat but also the fastest way to identify one. In contrast, Vicarious’ style of AI is slower, because it’s actively making logical inferences as it goes.
When the Vicarious AI works well, it can learn from much less data. George’s team created an AI to bust captchas,5 those “I’m not a robot” obstacles online, by recognizing characters in spite of their distorted, warped appearance.
Much as with the Breakout system, they endowed their AI with some abilities up front, such as knowledge that helps it discern the likely edges of characters. With that bootstrapping in place, they only needed to train the AI on 260 images before it learned to break captchas with 90.4 percent accuracy. In contrast, a neural net needed to be trained on more than 2.3 million images before it could break a captcha.
Others are building common-sense-like structure into neural nets in different ways. Two researchers at DeepMind, for instance, recently created a hybrid system—part deep learning, part more traditional techniques—known as inductive logic programming. The goal was to produce something that could do mathematical reasoning.
They trained it on the children’s game fizz-buzz, in which you count upward from 1, saying “fizz” if a number is divisible by 3 and “buzz” if it is divisible by 5. A regular neural net would be able to do this only for numbers it had seen before; train it up to 100 and it would know that 99 is “fizz” and 100 is “buzz.” But it wouldn’t know what to do with 105. In contrast, the hybrid DeepMind system seemed to understand the rule and went past 100 with no problem. Edward Grefenstette, one of the DeepMind coders who built the hybrid, says, “You can train systems that will generalize in a way that deep-learning networks simply couldn’t on their own.”
Still, LeCun admits it’s not yet clear which routes will help deep learning get past its humps. It might be “adversarial” neural nets, a relatively new technique in which one neural net tries to fool another neural net with fake data—forcing the second one to develop extremely ­subtle internal representations of pictures, sounds, and other inputs. The advantage here is that you don’t have the “data hungriness” problem. You don’t need to collect millions of data points on which to train the neural nets, because they’re learning by studying each other. (Apocalyptic side note: A similar method is being used to create those profoundly troubling “deepfake” videos in which someone appears to be saying or doing something they are not.)
I met LeCun at the offices of Facebook’s AI lab in New York. Mark Zuckerberg recruited him in 2013, with the promise that the lab’s goal would be to push the limits of ambitious AI, not just produce minor tweaks for Facebook’s products. Like an academic lab, LeCun and his researchers publish their work for others to access.
LeCun, who retains the rich accent of his native France and has a Bride of Frankenstein shock of white in his thick mass of dark hair, stood at a whiteboard energetically sketching out theories of possible deep-learning advances. On the facing wall was a set of gorgeous paintings from Stanley Kubrick’s 2001: A Space Odyssey—the main spaceship floating in deep space, the wheel-like ship orbiting Earth. “Oh, yes,” LeCun said, when I pointed them out; they were reprints of artwork Kubrick commissioned for the movie.
It was weirdly unsettling to discuss humanlike AI with those images around, because of course HAL 9000,6 the humanlike AI in 2001, turns out to be a highly efficient murderer.
And this pointed to a deeper philosophical question that floats over the whole debate: Is smarter AI even a good idea? Vicarious’ system cracked captcha, but the whole point of captcha is to prevent bots from impersonating humans. Some AI thinkers worry that the ability to talk to humans and understand their psychology could make a rogue AI incredibly dangerous. Nick Bostrom7 at the University of Oxford has sounded the alarm about the dangers of creating a “superintelligence,” an AI that self-improves and rapidly outstrips humanity, able to outthink and outflank us in every way. (One way he suggests it might amass control is by manipulating people—something for which possessing a “theory of mind” would be quite useful.)
Elon Musk is sufficiently convinced of this danger that he has funded OpenAI, an organization dedicated to the notion of safe AI.
This future doesn’t keep Etzioni up at night. He’s not worried about AI becoming maliciously superintelligent. “We’re worried about something taking over the world,” he scoffs, “that can’t even on its own decide to play chess again.” It’s not clear how an AI would develop a desire to do so or what that desire would look like in software. Deep learning can conquer chess, but it has no inborn will to play.
What does concern him is that current AI is woefully inept. So while we might not be creating HAL with a self-preserving intelligence, an “inept AI attached to deadly weapons can easily kill,” he says. This is partly why Etzioni is so determined to give AI some common sense. Ultimately, he argues, it will make AI safer; the idea that humanity shouldn’t be wholesale slaughtered is, of course, arguably a piece of common-­sense knowledge itself. (Part of the Allen Institute’s mandate is to make AI safer by making it more reasonable.)
Etzioni notes that the dystopic sci-fi visions of AI are less risky than near-term economic displacement. The better AI gets at common sense, the more rapidly it’ll take over jobs that currently are too hard for mere pattern-­matching deep learning: drivers, cashiers, managers, analysts of all stripes, and even (alas) journalists. But truly reasoning AI could wreak havoc even beyond the economy. Imagine how good political disinformation bots would be if they could use common-­sense knowledge to appear indistinguishably human on Twitter or Facebook or in mass phone calls.
Marcus agrees that reasoning AI will have dangers. But the upsides, he says, would be huge. AI that could reason and perceive like humans yet move at the speed of computers could revolutionize science, teasing out causal connections at a pace impossible for us alone. It could follow if-then chains and ponder counterfactuals, running mental experiments the way humans do, except with massive robotic knowledge. “We might finally be able to cure mental illness, for example,” Marcus adds. “AI might be able to understand these complex biological cascades of proteins that are involved in building brains and having them work correctly or not.”
Sitting beneath the images from 2001, LeCun makes a bit of a heretical point himself. Sure, making artificial intelligence more humanlike helps AI to navigate our world. But directly replicating human styles of thought? It’s not clear that’d be useful. We already have humans who can think like humans; maybe the value of smart machines is that they are quite alien from us.
“They will tend to be more useful if they have capabilities we don’t have,” he tells me. “Then they’ll become an amplifier for intelligence. So to some extent you want them to have a nonhuman form of intelligence ... You want them to be more rational than humans.” In other words, maybe it’s worth keeping artificial intelligence a little bit artificial.

Clive Thompson (@pomeranian99) is a columnist for WIRED.
This article appears in the December issue. Subscribe now.
Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.

More Great WIRED Stories

This genius neuroscientist might hold the key to true AI
How to safely and securely dispose of your old gadgets
PHOTOS: When your baby is actually made of silicone
Online conspiracy groups are a lot like cults
Pipeline vandals are reinventing climate activism
Get even more of our inside scoops with our weekly Backchannel newsletter",,"https://media.wired.com/photos/5beb0c088daf7470d1923b48/1:1/w_800,h_800,c_limit/AI-GG3A0107-3w.gif","{'@type': 'CreativeWork', 'name': 'WIRED'}","We’ve spent years teaching neural nets to think like human brains. They’re crazy-smart, but what if we’ve been doing it all wrong?",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWGh0dHBzOi8vd3d3Lm5vdmFydGlzLmNvbS9zdG9yaWVzL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWRlY29kZXMtY2FuY2VyLXBhdGhvbG9neS1pbWFnZXPSAQA?oc=5,Artificial intelligence decodes cancer pathology images - Novartis,2018-11-12,Novartis,https://www.novartis.com,Novartis researchers are collaborating with tech startup PathAI to search for hidden information in pathology slides.,"Cancer, Emerging Technology, Innovation, Novartis Institutes for BioMedical Research, novartis, novartis pharmaceuticals, global healthcare companies,sandoz, sandoz pharmaceuticals, oncology drug development, novartis oncology pipeline, innovative healthcare, rare disease treatment, rare disease patients, innovative medicines, cell therapy, gene therapy",Novartis researchers are collaborating with tech startup PathAI to search for hidden information in pathology slides.,Novartis researchers are collaborating with tech startup PathAI to search for hidden information in pathology slides.,https://schema.org,,,,,,,,,,,,,N/A,N/A,"






Discovery



Artificial intelligence decodes cancer pathology images



Novartis researchers are collaborating with tech startup PathAI to search for hidden information in pathology slides.



By

Elizabeth Dougherty

|
Nov 12, 2018


 
For 150 years, pathologists have been looking through microscopes at tissue samples mounted on slides to diagnose cancer. Each assessment is weighty: Does this patient have cancer or not?
The job of a pathologist is daunting. A single slide could contain hundreds of thousands of cells. Only a handful might be cancer. Inaccurate diagnosis rates range from 3-9% of cases, according to a recent review(link is external).
Enter artificial intelligence (AI), an extra set of unbiased, indefatigable artificial eyes that could help catch errors. Many researchers are pursuing this possibility, but Novartis pathologists think AI might have an additional role to play. They hypothesize that pathology slides could contain information that helps explain why some patients respond to therapy when other seemingly similar patients do not.
To explore this idea, pathologists and data scientists from Novartis have joined forces with tech startup PathAI. They are training an AI system developed by PathAI to learn to see the same patterns pathologists see and then building on that to determine if the system can detect hidden but informative patterns too subtle or complex for pathologists to discern. The effort is part of a larger effort at Novartis to leverage data and digital technologies in ways that could help drug developers get the right drugs to the right patients faster.












View
1/2



View
2/2



<>01













A pathologist sees a field of cells on a slide and relies on years of training to find those that might be cancer.





1/2







The PathAI system also finds signs of cancer and overlays the slide with its assessment, showing cancer (red), surrounding cells (green) and dead cells (black).





2/2










Previous



Next











 
In a first phase of testing, the collaborative team has trained the PathAI system to look at slides from untreated patients and distinguish tumor from normal tissue. The system can also identify different cell types on a slide reliably. For a pathologist, these feats are akin to finding a needle in a haystack and then labeling every piece of straw.
The ability to label every cell is becoming increasingly important as cancer therapies evolve to include medicines that target not only cancer cells but also immune cells. If computers can analyze an entire slide at once and quantify cell types and locations, they could potentially reveal patterns that predict how well a patient might fare on a given therapy.
“Hopefully we can figure out which features correlate with survival or response to a drug,” says Meg McLaughlin, a pathologist and Director of the Oncology Pathology and Biomarkers group in the Oncology Translational Research team at the Novartis Institutes for BioMedical Research (NIBR).
With a recent explosion of experimental immuno-oncology options alongside therapies that target cancer-driving mutations, one of the biggest challenges for drug hunters is matching the most appropriate therapy to individual patients. While genomic information helps drive smart decisions, valuable clues in pathology slides could also help. “We want to create a platform that enables the field of pathology to support the accelerating pace of drug development,” says Andrew Beck, a pathologist, computer scientist and CEO of PathAI, located in Boston, Massachusetts, in the US.

We want to create a platform that enables the field of pathology to support the accelerating pace of drug development.
Andrew Beck, CEO of PathAI

Training the AI model
In collaboration with the Institute of Pathology at the University Hospital Basel in Switzerland, the Novartis team gained access to 400 pathology images from breast and lung cancer tissues along with anonymized information about the patients’ diagnoses and survival times.
The challenge for PathAI’s platform? Given an image, identify cancer, identify cell types and predict the patient’s probability of surviving five years.












One way to approach the challenge is to feed a set of untrained AI algorithms a subset of the data and see what it learns. Unlike a trained pathologist, the machine approaches the problem with no knowledge of cells or cancer.
“A human already has a lot of knowledge,” says NIBR data scientist Holger Hoefling, who is working on the project with PathAI and with an internal NIBR group aiming to use AI to assess safety concerns in pathology images. “Think about autonomous cars. To train a car to drive, the amount of time and data required for training is gigantic. In contrast, you put a human behind the wheel for 20 hours and let them drive.”
To give the untrained algorithms more knowledge about the training data, PathAI decided to feed them even more rich data. A team of consulting pathologists marks up the slides, giving the algorithms more information to work with. It’s a bit like annotations in a hefty piece of literature that highlight and explain critical passages.
For example, when training the algorithms to distinguish cell types, PathAI diced the training slides into about 10 000 smaller images and had pathologists label the cell types in each slice. “We had to think really hard about how we annotate the images,” says McLaughlin. “That step determines to a large extent what you get out of the AI model in the end.”
What is a black box?
AI experts refer to the trained algorithms as a “black box” because it’s difficult to know what the system has learned from the training data or how it makes decisions.
Inside the black box is a set of machine learning algorithms. These algorithms are a cascade of formulas that recognize features, such as the presence of a certain shape, and associate them with real-world data, such as how long a patient actually survived.
As the algorithms see more and more images, they adjust their understanding of the patterns they see in the data. Eventually they learn that certain shapes in a slide predict likely health outcomes, such as having a good chance of living one year or a poor chance of surviving six months.
The black box approach has the benefit of taking a fresh view of the data, so it can reveal unexpected biological patterns. But it can also discover patterns that have no biological meaning at all. Data scientists need to scrutinize the AI model’s output, identify the meaningless conclusions, and adjust the training data and algorithms in ways that weed them out.
Seeing through a machine’s eyes
After training, the PathAI platform lets users see pathology images through the machine’s eyes. Regions of the slides determined to be cancer glow bright red in a field of green surrounding tissue. Different cell types stand out in vivid colors like candies in a dish. The existing platform is for research use only, but PathAI aims to build applications that could be used by doctors in the future.












View
1/2



View
2/2



<>01













Pathologists use visual cues such as cell size and shape to differentiate cell types on a pathology slide.





1/2







The PathAI system has also learned to recognize cell types and overlays the slide with indications of five different kinds: lymphocyte (green), tumor cell (red), macrophage (yellow), plasma cell (black) and fibroblast (purple).





2/2










Previous



Next











 
Now that the researchers have shown that the PathAI system has the potential to see what pathologists see, they want to find out if there’s information in those images that isn’t obvious to pathologists.
For example, they wonder if the distribution and abundance of certain cells, such as immune cells, could hold clues about how well a patient might do on immune therapy. To find out using the human eye would require the painstaking scrutiny of tens of thousands of cells per slide, an implausible task. “I could potentially do it,” says McLaughlin. “But it would take forever.”
With AI, however, the task becomes feasible. McLaughlin and her team at Novartis are supplying PathAI with pathology images and data about survival times and response to therapy from a recent Novartis clinical trial for cancer. Gathering this data together and sharing it with PathAI is no small task. In addition to locating slides and verifying the consent of patients, the team must present orderly and consistent data from doctor’s visits, which can be a challenge when multiple doctors working across several clinics collected it.
“The era we’re heading into is more about data than it is about algorithms,” says Lee Cooper, assistant professor of biomedical informatics and biomedical engineering at Emory University in the US. Cooper specializes in using machine learning to understand pathology images and is collaborating with NIBR researchers. “The algorithms will continue to improve, but really the challenge we’re facing is how to produce the datasets we need to build the best algorithms we can.”
Once the data is in hand, PathAI will have the images annotated. After that, it will be up to the machines to find any hidden messages.
“If we can show that this method can take in this data and overlay information we haven’t seen before from pathologists, we’ll be onto something of potential value,” says Beck.
Video by PJ Kaszas.
Learn how Novartis and PathAI are using #artificialintelligence to decode #cancer pathology images.





Reimagine medicine with Novartis


Find career opportunities in research at Novartis.

Learn More







",,,,,,,,,"[{'@type': 'NewsArticle', 'headline': 'Artificial intelligence decodes cancer pathology images', 'name': 'Artificial intelligence decodes cancer pathology images', 'about': ['Discovery'], 'description': 'Novartis researchers are collaborating with tech startup PathAI to search for hidden information in pathology slides.', 'image': {'@type': 'ImageObject', 'url': 'https://www.novartis.com/sites/novartiscom/files/2021-06/pathai-hero-image.jpg'}, 'datePublished': 'Mon, 11/12/2018 - 11:03', 'dateModified': 'Thu, 02/10/2022 - 13:40', 'publisher': {'@type': 'Organization', '@id': 'https://www.novartis.com/', 'name': 'Novartis', 'url': 'https://www.novartis.com/', 'sameAs': ['https://www.twitter.com/novartis', 'https://www.linkedin.com/company/novartis', 'https://www.youtube.com/user/novartis', 'https://www.facebook.com/novartis', 'https://www.instagram.com/novartis'], 'logo': {'@type': 'ImageObject', 'url': 'https://www.novartis.com/sites/novartis_com/files/novartis-logo-open-graph.jpg'}}, 'mainEntityOfPage': 'https://www.novartis.com/stories/artificial-intelligence-decodes-cancer-pathology-images'}, {'@type': 'WebPage', '@id': 'https://www.novartis.com/', 'breadcrumb': {'@type': 'BreadcrumbList', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.novartis.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Stories', 'item': 'https://www.novartis.com/stories'}, {'@type': 'ListItem', 'position': 3, 'name': 'Artificial intelligence decodes cancer pathology images', 'item': 'https://www.novartis.com/stories/artificial-intelligence-decodes-cancer-pathology-images'}]}, 'description': 'Novartis researchers are collaborating with tech startup PathAI to search for hidden information in pathology slides.', 'publisher': {'@type': 'Organization', '@id': 'https://www.novartis.com/', 'name': 'Novartis', 'url': 'https://www.novartis.com/', 'sameAs': ['https://www.twitter.com/novartis', 'https://www.linkedin.com/company/novartis', 'https://www.youtube.com/user/novartis', 'https://www.facebook.com/novartis', 'https://www.instagram.com/novartis'], 'logo': {'@type': 'ImageObject', 'url': 'https://www.novartis.com/sites/novartis_com/files/novartis-logo-open-graph.jpg'}}}]",,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiYGh0dHBzOi8vdG93YXJkc2RhdGFzY2llbmNlLmNvbS9pcy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1yYWNpc3QtYW5kLW90aGVyLWNvbmNlcm5zLTgxN2ZhNjBkNzVlOdIBAA?oc=5,Is Artificial Intelligence Racist? (And Other Concerns) | by Mauro Comi - Towards Data Science,2018-11-11,Towards Data Science,https://towardsdatascience.com,"When we think of concerns in Artificial intelligence, two main obvious connections are job loss and lethal autonomous weapons. While killer robots might be an actual threat in the future, the…",N/A,"When we think of concerns in Artificial intelligence, the two main obvious connections are job loss and lethal autonomous weapons. While…","When we think of concerns in Artificial intelligence, the two main obvious connections are job loss and lethal autonomous weapons. While…",http://schema.org,NewsArticle,https://towardsdatascience.com/is-artificial-intelligence-racist-and-other-concerns-817fa60d75e9,['https://miro.medium.com/v2/resize:fit:1200/1*BqejDh9f63qSMa40RyZkdw.jpeg'],"{'@type': 'Person', 'name': 'Mauro Comi', 'url': 'https://towardsdatascience.com/@mauro_ai'}","{'@type': 'Organization', 'name': 'Towards Data Science', 'url': 'towardsdatascience.com', 'logo': {'@type': 'ImageObject', 'width': 192, 'height': 60, 'url': 'https://miro.medium.com/v2/resize:fit:384/1*cFFKn8rFH4ZndmaYeAs6iQ.png'}}",Is Artificial Intelligence Racist? (And Other Concerns),2018-11-12T00:56:53.970Z,2020-05-09T13:49:58.001Z,,Is Artificial Intelligence Racist? (And Other Concerns),,,N/A,N/A,"Is Artificial Intelligence Racist? (And Other Concerns)Mauro Comi·FollowPublished inTowards Data Science·6 min read·Nov 11, 20181742ListenShareWhen we think of concerns in Artificial intelligence, two main obvious connections are job loss and lethal autonomous weapons. While killer robots might be an actual threat in the future, the consequence of automation is a complicated phenomenon that experts are still actively analyzing. Very likely, as for any major Industrial revolution, the market will gradually stabilize. Advances in technology will create new types of jobs, inconceivable at the moment, which will be later disrupted by a new major technology takeover. We have seen this multiple times in modern history and we are probably going to see this again.A third major field of concern is the ethical impact of AI. Here the question falls: is Artificial Intelligence racist?Well, in short.. there is no short answer.What about a long answer? Tales of Google, Seals, and GorillasIn order to answer this question, we first need to define what Racism is.Racism: The belief that all members of each race possess characteristics, abilities, or qualities specific to that race, especially so as to distinguish it as inferior or superior to another race or races. ~ Oxford DictionariesRacism is related to the generalization of specific characteristics to all the members of a race. Generalization is a key concept in Machine learning and this is especially true in classification algorithms. Inductive learning is related to derive general concepts from specific examples. The majority of techniques in supervised learning try to approximate functions to predict the categories of input values with the highest possible accuracy.A function that fits our training set too closely generates overfitting. In practice, it is not able to derive a proper general function given different inputs. On the other hand, a function that doesn’t fit the dataset accurately leads to underfitting. Hence, the model generated is too simple to produce significant and reliable results.Experts in the field know that classification is all about finding the trade-off between overfitting and underfitting. Indeed, the model needs to derive general rules from a specific training set. This clearly leads to a major problem: if the data used to train the model are biased, the model will produce a biased result.A famous case showing the consequence of biased data is the mislabelling of two African-American young guys. Google Photos, which had recently implemented an automatic image labelling, classified the two teenagers as “gorillas” (all the references are reported at the end of the page). Google was heavily criticized, and someone starting to wonder whether a machine could be trained to be racist on purpose.The Google team immediately apologized and a spokesperson tweeted: “ Until recently, [Google Photos] was confusing white faces with dogs and seals. Machine learning is hard”.The actual reason for the misclassification is not due to racism at all, though. The cause of this error lies in the training set.Superman, criminality and racismIn order to understand what we’ve just discussed, let’s see a simple example of misclassification.Suppose we want to predict whether Clark Kent is a criminal or not. Here the dataset we have:Dataset containing 5 elementsOur training set represents 5 people, belonging to three different races: Kryptonian, Human and Robot.We are going to train a Decision Tree classifier to predict if Clark Kent, who’s a 31 Kryptonian Male guy, would be classified as Criminal or not.First, we train the model:clf = tree.DecisionTreeClassifier()X_train = data[['Sex', 'Age', 'Race']]Y_train = data[['Criminal']]clf.fit(X_train, Y_train)Then, we predict the category “Criminal” based on the trained model:# 1 -> Male# 31 -> Age# 1 -> Kryptonianpred = clf.predict([[1, 31, 1]])print('Is Clark Kent a criminal? Prediction: ',pred[0])As we can see, Clark Kent is classified as Criminal. Let’s check the importance of the features, in order to understand how variables influence the final output of the classifier.Here it is. Based on the dataset that we used to train the model, the most important feature is the variable Race.Bias in Computer VisionThis simple example shows the importance of data collection and data organization. When these two actions are performed poorly, ethical and cultural biases can be encoded in the machine learning model. As reported by a great article from Nature (link at the end), 45% of the most used image database in computer vision comes from the United States. China and India, accounting for 36% of the world population, represents just 3% of data in the ImageNet dataset. This unbalance unintentionally creates a bias and explains why computer vision algorithms label a photograph of a North Indian bride as ‘performance art’.Joy Buolamwini, researcher at MIT, addressed the lack of diversity in the data used to train computer vision algorithms a few years ago. She noticed that, while the most famous facial recognition systems at MIT classified correctly the gender of almost every white person, the accuracy dropped drastically as skin shades got darker. The lowest accuracy was related to dark-skinned females, with an error rate of 34%.How Microsoft corrupted a bot in 24 hoursBias and errors do not only happen in Image classification tasks. Natural Language Processing is the field of Artificial Intelligence that focuses on human language processing. A common methodology shared by many NLP algorithms is mapping words to geometric vectors. This technique considers documents as a collection of vectors, allowing computations between words. Bolukbasi and colleagues, in their paper “Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings”, show how a simple algorithm for analogies, trained on Google News articles, exhibits female/male gender stereotypes. As they report, the model states that ‘man’ is to ‘doctor’ as ‘woman’ is to ‘nurse’.This reminds of a similar controversy: in 2016 Microsoft deployed TayTweets, a Twitter bot trained through casual conversations on Twitter. The idea was incredibly promising, due to the large amount of textual data available every second on Twitter. Anyway, needless to say, the agent started to tweet misogynistic and racist remarks in less than 24 hours. Who would have thought?Racist bots and where to find themTL;DRAnd finally, here we are at the end of our analysis. The whole point of this article is to raise an ethical issue related to AI that is often overlooked. While scientists, engineers and data scientists need to address the unbalance in training sets, users and non-experts need to understand that Artificial Intelligence is based on Mathematics. And Math, as we all know, can be extremely complex. Neural networks, used in Image classification, are considered ‘black boxes’. The results they give are based on extremely high dimensional computations and cannot be fully controlled — even if companies are making a huge effort to understand the intermediate outputs, with amazing results (check my article about Neural Transfer Style, based on this concept).Still, we have a last question to answer, which hopefully will be discussed in the comments below. Is AI racist?Thanks for reading. For any comments or suggestions don’t hesitate to leave a comment!You can find more about me and my projects at maurocomi.com. You can also find me on Linkedin, or email me directly. I am always up for a chat, or to collaborate on new amazing projects.References:Google Photos Tags Two African-Americans As Gorillas Through Facial Recognition SoftwareWhen Brooklyn-native Jacky Alcine logged onto Photos on Sunday evening, he was shocked to find an album titled…www.forbes.comAI can be sexist and racist - it's time to make it fairWhen Google Translate converts news articles written in Spanish into English, phrases referring to women often become…www.nature.comRacist, Sexist AI Could Be A Bigger Problem Than Lost JobsJoy Buolamwini was conducting research at MIT on how computers recognized people's faces, when she started experiencing…www.forbes.comhttps://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist",https://towardsdatascience.com/is-artificial-intelligence-racist-and-other-concerns-817fa60d75e9,2018-11-12T00:56:53.970Z,,,,,,,,817fa60d75e9,['Mauro Comi'],,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiO2h0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vcXVpY2t0YWtlL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNl0gEA?oc=5,Artificial Intelligence - Bloomberg - Bloomberg,2018-11-12,Bloomberg,https://www.bloomberg.com,"Artificial intelligence, or AI, is both the stuff of Terminator-esque, end-of-humanity scenarios and an invisible but steadily increasing part of our daily lives, suggesting what news we should read, for instance, or answering our customer-service queries. Software capable of learning a single narrow task with seemingly superhuman ability is becoming commonplace. AI promises a world of more personalized products and services that are cheaper, faster and free from human error. Many companies thin","Artificial Intelligence,Software,ALPHABET INC-CL A,Military,MICROSOFT CORP,Equality,Jobs,China,Unemployment,TESLA INC,business,quicktake","Artificial intelligence, or AI, is both the stuff of Terminator-esque, end-of-humanity scenarios and an invisible but steadily increasing part of our daily lives, suggesting what news we should read, for instance, or answering our customer-service queries. Software capable of learning a single narrow task with seemingly superhuman ability is becoming commonplace. AI promises a world of more personalized products and services that are cheaper, faster and free from human error. Many companies thin","Artificial intelligence, or AI, is both the stuff of Terminator-esque, end-of-humanity scenarios and an invisible but steadily increasing part of our daily lives, suggesting what news we should read, for instance, or answering our customer-service queries. Software capable of learning a single narrow task with seemingly superhuman ability is becoming commonplace. AI promises a world of more personalized products and services that are cheaper, faster and free from human error. Many companies thin",http://schema.org,NewsMediaOrganization,https://www.bloomberg.com,"['https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iXFvVjhbNxGE/v0/1200x799.jpg', 'https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iXFvVjhbNxGE/v0/-1x-1.jpg', 'https:/assets.bwbx.io/s3/lightsaber/_next/static/media/social-default.cc6ae30e.jpg']","[{'@type': 'Person', 'name': 'Jeremy Kahn'}, {'@type': 'Person', 'name': 'Dina Bass'}]","{'@type': 'Organization', 'name': 'Bloomberg', 'url': 'https://www.bloomberg.com', 'logo': {'@type': 'ImageObject', 'url': 'https:/assets.bwbx.io/s3/lightsaber/_next/static/media/bloomberg-logo-amp.bae0aa0a.png', 'width': 262, 'height': 60}}",Artificial Intelligence,2015-07-09T13:55:15.581Z,2018-11-13T05:05:38.854Z,,Bloomberg,False,,N/A,N/A,"QuicktakeArtificial IntelligenceFacebookTwitterLinkedInEmailLinkGiftExpandPhotographer: Balint Porneczi/BloombergFacebookTwitterLinkedInEmailLinkGiftGift this articleHave a confidential tip for our reporters? Get in TouchBefore it’s here, it’s on the Bloomberg TerminalBloomberg Terminal LEARN MOREFacebookTwitterLinkedInEmailLinkGiftBy Jeremy Kahn and Dina BassJuly 9, 2015 at 9:55 AM EDTUpdated on  November 13, 2018 at 12:05 AM ESTBookmarkSaveArtificial intelligence, or AI, is both the stuff of Terminator-esque, end-of-humanity scenarios and an invisible but steadily increasing part of our daily lives, suggesting what news we should read, for instance, or answering our customer-service queries. Software capable of learning a single narrow task with seemingly superhuman ability is becoming commonplace. AI promises a world of more personalized products and services that are cheaper, faster and free from human error. Many companies think they can cut their costs substantially by deploying AI. The downside of that is the prospect of mass unemployment. And one doesn’t need to feel that AI is “summoning the demon,” in the words of Tesla Inc. Chief Executive Officer  Elon Musk, to worry about it eroding privacy and increasing inequality.AI is being used to suggest music you might want to listen to or movies you might want to watch. It’s used to spot attempts at bank fraud and cybercrime. It helps rail companies predict when trains need maintenance and doctors to read X-rays and other medical images. International Data Corp. forecasts that annual corporate spending on AI will grow to about $52 billion by 2021. Meanwhile, the McKinsey Global Institute estimates AI technologies could unlock from $9.5 trillion to $15.4 trillion in annual business value worldwide. With these kinds of numbers, it’s not surprising that many governments, including those of the U.K., France, Canada and the U.S., have come to see AI as an important economic priority. Some countries, such as ChinaBloomberg Terminal, have also made the technology an important strategic goal, taking into account its potential military and intelligence applications. Employee protests over AI’s military potential led Alphabet Inc.’s Google to  pull back from a U.S. Department of Defense contract to develop programs to analyze drone footage. Microsoft Corp. called for governments to take action  to regulate AI, particularly facial-recognition programs, which have performed particularly poorly with people with darker skin.Have a confidential tip for our reporters? Get in TouchBefore it’s here, it’s on the Bloomberg TerminalBloomberg Terminal LEARN MORE",https://www.bloomberg.com/opinion/quicktake/artificial-intelligence,2015-07-09T13:55:15.581Z,,,,,"{'@type': ['CreativeWork', 'Product'], 'name': 'Bloomberg', 'productID': 'bloomberg.com:basic'}",,,,,"[{'@type': 'CollectionPage', 'name': 'Business news', 'url': 'https://www.bloomberg.com/'}]","{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.paywall'}","{'@type': 'PostalAddress', 'addressCountry': 'USA', 'addressLocality': 'New York', 'addressRegion': 'NY', 'postalCode': '10022', 'streetAddress': '731 Lexington Avenue'}",https://www.bloomberg.com/diversity-inclusion,inquiry1@bloomberg.net,Bloomberg Finance L.P.,5493001KJTIIGC8Y1R12,(212) 318-2000,https://www.bloomberg.com/logo-bloomberg.svg,"[{'@type': 'Brand', 'name': 'Bloomberg markets', 'url': 'https://www.bloomberg.com/markets'}, {'@type': 'Brand', 'name': 'Bloomberg technology', 'url': 'https://www.bloomberg.com/technology'}, {'@type': 'Brand', 'name': 'Bloomberg pursuits', 'url': 'https://www.bloomberg.com/pursuits'}, {'@type': 'Brand', 'name': 'Bloomberg politics', 'url': 'https://www.bloomberg.com/politics'}, {'@type': 'Brand', 'name': 'Bloomberg opinion', 'url': 'https://www.bloomberg.com/opinion', 'logo': 'https://www.bloomberg.com/logo-bloomberg_opinion.svg'}, {'@type': 'Brand', 'name': 'Bloomberg businessweek', 'url': 'https://www.bloomberg.com/businessweek', 'logo': 'https://www.bloomberg.com/logo-bloomberg_businessweek.svg'}, {'@type': 'Brand', 'name': 'Bloomberg green', 'url': 'https://www.bloomberg.com/green'}, {'@type': 'Brand', 'name': 'Bloomberg equality', 'url': 'https://www.bloomberg.com/equality'}, {'@type': 'Brand', 'name': 'Bloomberg citylab', 'url': 'https://www.bloomberg.com/citylab'}, {'@type': 'Brand', 'name': 'Bloomberg crypto', 'url': 'https://www.bloomberg.com/crypto'}, {'@type': 'Brand', 'name': 'Bloomberg industries', 'url': 'https://www.bloomberg.com/industries'}, {'@type': 'Brand', 'name': 'Bloomberg economics', 'url': 'https://www.bloomberg.com/economics'}, {'@type': 'Brand', 'name': 'Bloomberg ai', 'url': 'https://www.bloomberg.com/ai'}, {'@type': 'Brand', 'name': 'Bloomberg wealth', 'url': 'https://www.bloomberg.com/wealth'}]",,,,
https://news.google.com/rss/articles/CBMiWWh0dHBzOi8vbmV3cy5taXQuZWR1LzIwMTgvbWl0LWFpLXN1bW1pdC1hZGRyZXNzZXMtdGVjaC1pbXBhY3Qtb24tam9icy1nbG9iYWwtZWNvbm9teS0xMTE10gEA?oc=5,Artificial intelligence summit addresses impact of technology on jobs and global economy - MIT News,2018-11-15,MIT News,https://news.mit.edu,Speakers at MIT&#039;s AI and the Future of Work summit discuss the impact of technology on jobs and global economy.,"MIT, MIT Sloan School of Management, MIT Department of Electrical Engineering and Computer Science (eecs), MIT Initiative on the Digital Economy, MIT Computer Science and Artificial Intelligence Laboratory MIT CSAIL, future of work, artificial intelligence, Economics, jobs, global economy",Speakers at MIT&#039;s AI and the Future of Work summit discuss the impact of technology on jobs and global economy.,N/A,,,,,,,,,,,,,,N/A,N/A,"


Speakers at the summit included Massachusetts Secretary of Labor Rosalin Acosta and former Google chairman Eric Schmidt.




Adam Conner-Simons
|
MIT CSAIL


 Publication Date:
 November 15, 2018





Press Inquiries

  Press Contact:



      
            Adam         

            Conner-Simons        

  

      Email:
     aconner@csail.mit.edu


      Phone:
              617-324-9135      
  

      
            MIT Computer Science & Artificial Intelligence Lab        

  








 Close














 Caption:
          The keynote address at the second annual AI and the Future of Work summit was delivered by Rosalin Acosta, Massachusetts Secretary of Labor and Workforce Development.      
          

 Credits:
          Photo: Rachel Gordon/CSAIL      
          









 Caption:
          MIT professor Erik Brynjolfsson (left) talks with former Google chairman Eric Schmidt at the opening fireside chat.      
          

 Credits:
          Photo: Rachel Gordon/CSAIL      
          









 Caption:
          Left to right: CSAIL's Lori Glover moderates a discussion with Walmart's Becky Schmitt and Accenture's Alison Horn.      
          

 Credits:
          Photo: Rachel Gordon/CSAIL      
          

















Previous image
Next image






















This week MIT hosted its second annual summit on “AI and the Future of Work,” bringing together representatives from industry, government and academia to discuss the opportunities and challenges of artificial intelligence (AI) and automation.
Co-hosted by MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Initiative on the Digital Economy (IDE), the event featured former Alphabet chairman Eric Schmidt and Massachusetts Secretary of Labor Rosalin Acosta, who delivered the keynote address.
A common theme throughout the event was the importance of doing more than just thinking about technological disruption and actually working to create public policy that encourages the thoughtful deployment of AI systems.
“The technologies themselves are neutral, so the question is how to organize ourselves in society in a way that addresses their potential to change the job market,” said Diana Farrell, CEO of the JPMorgan Chase Institute. “We’re kidding ourselves if we think that the market is going to, on its own, allow these technologies to infiltrate and yield the kind of outcomes from society that we want.”
The focus on public policy also extended to education. Many panelists spoke of the importance of lifelong learning, in the form of a burgeoning industry of free and low-cost online classes to pick up skills in fields like machine learning and data science that have seen major job growth.
Some speakers believed that future focus needs to happen much earlier the educational pipeline. Fred Goff, who serves as CEO of the blue-collar job-search network Jobcase, did a survey of the platform’s 90 million members about their education. Half said that their K-12 background prepared them for their job today, but less than a quarter said that they think their education will prepare them for the jobs of tomorrow.
Beyond the U.S., industry analysts spoke about the importance of considering AI in the context of the developing world, where there is often low digital literacy.
“How do we support people in remote and isolated areas so that they don’t fall further behind?” asked Tina George, an expert in global technologies for the World Bank. “We can't build Star Wars with Flintstone technology.”
There was also a growing recognition that in industry, AI could actually become something of an equalizer, especially in areas like mergers and acquisitions that rely heavily on data analysis.
""It no longer requires a multi-million dollar budget to get AI going in your company,"" said Nichole Jordan, a managing partner at Grant Thornton LLP. “It represents an opportunity to level the playing field for smaller companies.”
On the academic side, CSAIL Director Professor Daniela Rus discussed the many ways that scientists are using AI for everything from diagnosing disease to predicting food shortages. At the same time, she talked about how important it is for researchers to be thoughtful and intentional as they work on these new breakthroughs.
“AI should be able to help us all get lifted to better lives, and I think there is a lot of potential still untapped,” Rus said in video remarks. “But we can't just push technology forward and hope for the best. We have to work to ensure that the best happens.”








Share this news article on:










X











Facebook















LinkedIn




































Reddit


















Print







Related Links

AI and the Future of WorkInitiative on the Digital EconomyComputer Science and Artificial Intelligence LaboratoryDepartment of Electrical Engineering and Computer ScienceSchool of EngineeringMIT Sloan School of Management






Related Topics

Special events and guest speakers
Artificial intelligence
Computer science and technology
Computer Science and Artificial Intelligence Laboratory (CSAIL)
Technology and society
Autonomous vehicles
School of Engineering
Labor and jobs
Industry
Health care
Data
Innovation and Entrepreneurship (I&E)
MIT Sloan School of Management



Related Articles











MIT reshapes itself to shape the future













IBM and MIT to pursue joint research in artificial intelligence, establish new MIT-IBM Watson AI Lab













Meeting of the minds for machine intelligence













Cambridge Cyber Summit convenes industry, academia, and government

















Previous item
Next item
















",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiXmh0dHBzOi8vd3d3Lm5hdGlvbmFsZ2VvZ3JhcGhpYy5jb20vYW5pbWFscy9hcnRpY2xlL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWNvdW50cy13aWxkLWFuaW1hbHPSAQA?oc=5,Artificial intelligence is counting animals to help save them - National Geographic,2018-11-13,National Geographic,https://www.nationalgeographic.com,The whale sharks of Mafia Island are unusual because they don't migrate—and researchers want to know why.,N/A,"From analyzing animal photos to combing through YouTube, new software is harnessing data never before accessible to scientists.","From analyzing animal photos to combing through YouTube, new software is harnessing data never before accessible to scientists.",https://schema.org,VideoObject,,"{'url': 'https://i.natgeofe.com/n/bec7bd50-0d57-4982-aeb5-82e5f8184f89/02-machine-saving-animals-nationalgeographic_1977490.jpg', '@type': 'ImageObject'}","[{'name': 'Anne Casselman', '@type': 'Person'}]","{'logo': {'@type': 'ImageObject'}, '@type': 'Organization'}",How artificial intelligence is changing wildlife research,2018-11-13T11:00:08.000Z,,,Investigating the Mysterious Whale Sharks of Mafia Island,,,Animals,N/A,N/A,{'@type': 'WebPage'},,,,,https://i.natgeofe.com/n/cc99c3e6-72e7-4acb-8569-8b3e4cf59b72/00000159-b1e9-d46a-afdd-b5ef8d960000.jpg,,,,,,,,,,,,,,,,,2017-01-18T14:09:59.000Z,PT0H3M50S,
https://news.google.com/rss/articles/CBMiRmh0dHBzOi8vd3d3LmxhdGltZXMuY29tL2J1c2luZXNzL2xhLWZpLWhpbWktc2llZ2VsLTIwMTgxMTExLXN0b3J5Lmh0bWzSAQA?oc=5,How I Made It: Ian Siegel employs artificial intelligence to disrupt the job recruitment industry - Los Angeles Times,2018-11-11,Los Angeles Times,https://www.latimes.com,"Ian Siegel, ceo of Santa Monica-based ZipRecruiter, has taken a non-traditional route in his career and at his recruiting company.",N/A,"Ian Siegel, 45, is chief executive of Santa Monica-based ZipRecruiter, the company he co-founded in 2010 to disrupt the recruitment and hiring industry. ","Ian Siegel, 45, is chief executive of Santa Monica-based ZipRecruiter, the company he co-founded in 2010 to disrupt the recruitment and hiring industry. ",http://schema.org,NewsArticle,https://www.latimes.com/business/la-fi-himi-siegel-20181111-story.html,"[{'@context': 'http://schema.org', '@type': 'ImageObject', 'height': 836, 'url': 'https://ca-times.brightspotcdn.com/dims4/default/5a1a62f/2147483647/strip/false/crop/2047x1151+0+0/resize/1486x836!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F3b%2F95%2F526ea77747bcbc07f4f672df1f47%2Fla-1541726639-h0wlhi0wk9-snap-image', 'width': 1486}, {'@context': 'http://schema.org', '@type': 'ImageObject', 'height': 675, 'url': 'https://ca-times.brightspotcdn.com/dims4/default/efd8392/2147483647/strip/false/crop/2046x1151+0+0/resize/1200x675!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F3b%2F95%2F526ea77747bcbc07f4f672df1f47%2Fla-1541726639-h0wlhi0wk9-snap-image', 'width': 1200}]","[{'@context': 'http://schema.org', '@type': 'Person', 'affiliation': 'Los Angeles Times', 'description': 'Ronald D. White was a reporter for the Los Angeles Times from 1993 to 2024. ', 'name': 'Ronald D. White', 'url': 'https://www.latimes.com/people/ronald-d-white'}]","{'@type': 'Organization', 'name': 'Los Angeles Times', 'logo': {'@type': 'ImageObject', 'url': 'https://ca-times.brightspotcdn.com/dims4/default/954b438/2147483647/strip/false/crop/382x60+0+0/resize/382x60!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fde%2F5f%2F46c2d05b430cbc6e775301df1062%2Flogo-full-black.png', 'width': 382, 'height': 60}}",Ian Siegel employs artificial intelligence to disrupt the job recruitment industry,2018-11-11T13:00:00.000Z,2019-07-19T02:12:58.578Z,Business,Ian Siegel employs artificial intelligence to disrupt the job recruitment industry - Los Angeles Times,False,,Business,N/A,"       Ian Siegel, co-founder and chief executive of ZipRecruiter, at the company’s Santa Monica office. (Christina House / Los Angeles Times)       By Ronald D. WhiteStaff Writer    Nov. 11, 2018 5 AM PT      Share       Share via Close extra sharing options    Email     Facebook    X    LinkedIn    Threads    Reddit    WhatsApp    Copy Link URLCopied!   Print        Ian Siegel, 45, is chief executive of Santa Monica-based ZipRecruiter, the company he co-founded in 2010 to disrupt the recruitment and hiring industry. Since then, more than 1.5 million businesses and more than 430 million job seekers have used the online employment marketplace, according to the company. ZipRecruiter has nearly 1,000 employees, a quarter of whom are engineers.Business acumenStudying sociology, psychology and English at Oberlin College wasn’t the waste of time that Siegel feared it might be once he started a career in business.AdChoicesADVERTISING Advertisement   “If you think of psychology as the individual, sociology as society and social psychology as the study of small group dynamics, I was already identifying the opinion leader, the emotional leader, early on in my career. I was breaking down every group into the categories I studied while I was in college, and it actually was really helpful,” Siegel said.The inspirationThe Los Angeles native held executive-level positions at six different companies over 15 years. In hindsight, he sees the connective tissue that inspired ZipRecruiter.“I had to do my own recruiting,” Siegel said, “posting positions to different job boards, each with a different mechanism. For years I was saying, ‘This is crazy. Why isn’t there a magic button I can push and send the same job to all the different job boards and bring all the candidates onto one list, just make it easy and efficient to hire somebody?’”   The lessonSiegel was suffering massive guilt at one of those early jobs, managing a technology team on an interim basis for a West Hollywood firm called Citysearch from 1996 to 1998. He actually apologized to his team for being so unqualified, having studied only non-tech subjects in college.“They told me I was the best manager they ever had, because I listened,” Siegel said. “People didn’t need me to be an engineer. They needed me to be clear about the problem I wanted solved, and they needed the autonomy and the resources to go do it the way they wanted to do it.”Leadership style“Listen more than you talk,” Siegel said of his approach. “Describe the problem we’re trying to solve, describe what success looks like, and then keep listening, because your job is not to do all the work, your job is not to figure out the optimal solution, your job is to create space for all the smart people that you’ve hired to go work on that problem.” Advertisement    ‘Build the solution’“I pulled together three friends. Two of them were engineers, one a designer,” Siegel said. “The four of us worked at my kitchen table at night and on weekends, and we built the first version of ZipRecruiter, and literally the day we launched it, it was like a rocket ship, like right out of the gate, product market fit.”Comfortable fitSiegel had three co-founders who were essential to the start-up days, he said. Ward Poulos had worked with Siegel “at three different companies. When I was mulling on this idea, he was the first person I talked to.” Will Redd “was an engineer that we had both worked with at companies in the past.” Software wizard Joe Edmonds rounded out the group.“We were really aligned in terms of approach and personality, so it was very easy to work together,” Siegel said. All three are still with the company.Growing painsZipRecruiter found that fast growth creates its own problems, Siegel said. “It’s always difficult to scale fast,” he said. “Things always break. There’s this cliche saying that the person that’s right for a 10-person company isn’t right for a 100-person company, the person that’s right for a 100-person company isn’t right for a 1,000-person company, and it’s absolutely true.”Corporate raiders“As we grew, we literally cherry-picked the top 5% of talent that we had worked with across multiple businesses,” Siegel said, “and then we got all their No. 1 referenced friends as well. We have a development office in Israel staffed with some of the best data scientists in the world doing R&D for us. It’s high-level algorithmic work. We’re just applying it to the job category.”Embracing AI talentMany companies are stumbling as they attempt to embrace artificial intelligence and harness it to their business models. Siegel said AI has given his company a built-in advantage by more quickly extracting data from resumes and more efficiently generating likely matches.“We’re ZipRecruiter. We think we’re literally the easiest way to hire, so when we need to add all these bodies, all this expertise, we can eat our own dog food, because we have hired most of the people who work at ZipRecruiter, through ZipRecruiter, so that was probably our ace in the hole.”PersonalSiegel has been married to his wife, Rochelle, for 18 years. They have a 15-year-old daughter and an 11-year-old son. To clear his mind, Siegel prefers running to meditation, and he loves to read. “I’m fascinated by entrepreneurs telling their true truths, not the gilded PR-ready, ‘we were awesome from the beginning’ stories. I love the stories from the trenches. The ugly way that soup got made. I just find it reassuring, and it reminds me that it’s not easy for anyone.”     More to Read               Opinion: What’s behind the AI boom? Exploited humans   July 12, 2024                Granderson: So what if Gen Z applicants bring their parents to a job interview?   June 8, 2024                California advances measures targeting AI discrimination and deepfakes   May 29, 2024         ","{'@type': 'WebPage', '@id': 'https://www.latimes.com/business/la-fi-himi-siegel-20181111-story.html'}",,,"Ian Siegel, 45, is chief executive of Santa Monica-based ZipRecruiter, the company he co-founded in 2010 to disrupt the recruitment and hiring industry. Since then, more than 1.5 million businesses and more than 430 million job seekers have used the online employment marketplace, according to the company. ZipRecruiter has nearly 1,000 employees, a quarter of whom are engineers. Business acumenStudying sociology, psychology and English at Oberlin College wasn’t the waste of time that Siegel feared it might be once he started a career in business. “If you think of psychology as the individual, sociology as society and social psychology as the study of small group dynamics, I was already identifying the opinion leader, the emotional leader, early on in my career. I was breaking down every group into the categories I studied while I was in college, and it actually was really helpful,” Siegel said. The inspirationThe Los Angeles native held executive-level positions at six different companies over 15 years. In hindsight, he sees the connective tissue that inspired ZipRecruiter. “I had to do my own recruiting,” Siegel said, “posting positions to different job boards, each with a different mechanism. For years I was saying, ‘This is crazy. Why isn’t there a magic button I can push and send the same job to all the different job boards and bring all the candidates onto one list, just make it easy and efficient to hire somebody?’” The lessonSiegel was suffering massive guilt at one of those early jobs, managing a technology team on an interim basis for a West Hollywood firm called Citysearch from 1996 to 1998. He actually apologized to his team for being so unqualified, having studied only non-tech subjects in college. “They told me I was the best manager they ever had, because I listened,” Siegel said. “People didn’t need me to be an engineer. They needed me to be clear about the problem I wanted solved, and they needed the autonomy and the resources to go do it the way they wanted to do it.” Leadership style“Listen more than you talk,” Siegel said of his approach. “Describe the problem we’re trying to solve, describe what success looks like, and then keep listening, because your job is not to do all the work, your job is not to figure out the optimal solution, your job is to create space for all the smart people that you’ve hired to go work on that problem.” ‘Build the solution’“I pulled together three friends. Two of them were engineers, one a designer,” Siegel said. “The four of us worked at my kitchen table at night and on weekends, and we built the first version of ZipRecruiter, and literally the day we launched it, it was like a rocket ship, like right out of the gate, product market fit.” Comfortable fitSiegel had three co-founders who were essential to the start-up days, he said. Ward Poulos had worked with Siegel “at three different companies. When I was mulling on this idea, he was the first person I talked to.” Will Redd “was an engineer that we had both worked with at companies in the past.” Software wizard Joe Edmonds rounded out the group. “We were really aligned in terms of approach and personality, so it was very easy to work together,” Siegel said. All three are still with the company. Growing painsZipRecruiter found that fast growth creates its own problems, Siegel said. “It’s always difficult to scale fast,” he said. “Things always break. There’s this cliche saying that the person that’s right for a 10-person company isn’t right for a 100-person company, the person that’s right for a 100-person company isn’t right for a 1,000-person company, and it’s absolutely true.” Corporate raiders“As we grew, we literally cherry-picked the top 5% of talent that we had worked with across multiple businesses,” Siegel said, “and then we got all their No. 1 referenced friends as well. We have a development office in Israel staffed with some of the best data scientists in the world doing R&D for us. It’s high-level algorithmic work. We’re just applying it to the job category.” Embracing AI talentMany companies are stumbling as they attempt to embrace artificial intelligence and harness it to their business models. Siegel said AI has given his company a built-in advantage by more quickly extracting data from resumes and more efficiently generating likely matches. “We’re ZipRecruiter. We think we’re literally the easiest way to hire, so when we need to add all these bodies, all this expertise, we can eat our own dog food, because we have hired most of the people who work at ZipRecruiter, through ZipRecruiter, so that was probably our ace in the hole.” PersonalSiegel has been married to his wife, Rochelle, for 18 years. They have a 15-year-old daughter and an 11-year-old son. To clear his mind, Siegel prefers running to meditation, and he loves to read. “I’m fascinated by entrepreneurs telling their true truths, not the gilded PR-ready, ‘we were awesome from the beginning’ stories. I love the stories from the trenches. The ugly way that soup got made. I just find it reassuring, and it reminds me that it’s not easy for anyone.”",,,"{'@type': ['CreativeWork', 'Product'], 'name': 'Los Angeles Times', 'productID': 'lanews:all-access'}",,,,,,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.paywall'}",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LmNvbnN0cnVjdGlvbnNwZWNpZmllci5jb20vaG93LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWlzLWNoYW5naW5nLXRoZS1pbmR1c3RyeS1hLWNzaS1leGNsdXNpdmUv0gEA?oc=5,How artificial intelligence is changing the industry: A CSI exclusive - Construction Specifier - The Construction Specifier,2018-11-12,The Construction Specifier,https://www.constructionspecifier.com,"Machine learning, algorithms, and specialized artificial intelligence (AI) are impacting the construction industry, largely in the area of design. While jobs may not be eliminated, it will change. Certain aspects of a specifier’s work, such as activities requiring minimal complex thinking, can be done better and faster by some form of AI. Removing these job functions may also be welcomed by many professionals.",N/A,"Machine learning, algorithms, and specialized artificial intelligence (AI) are impacting the construction industry, largely in the area of design. While jobs may not be eliminated, it will change. Certain aspects of a specifier’s work, such as activities requiring minimal complex thinking, can be done better and faster by some form of AI. Removing these job functions may also be welcomed by many professionals.",N/A,https://schema.org,,,,,,,,,,,,,N/A,N/A,"

Digital craftsmanship in stone designModern technology has made its mark on stone design as well as fabrication. Today, project teams taking advantage of technology are experiencing increased freedom in control of the design concept, from development through to fabrication. 

",,,,,,,,,"[{'@type': 'Article', '@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#article', 'isPartOf': {'@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/'}, 'author': {'name': 'nithya_caleb', '@id': 'https://www.constructionspecifier.com/#/schema/person/b98ef8acdc4f61ec3c542bdb863ee469'}, 'headline': 'How artificial intelligence is changing the industry: A CSI exclusive', 'datePublished': '2018-11-12T17:31:54+00:00', 'dateModified': '2018-11-12T17:31:54+00:00', 'mainEntityOfPage': {'@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/'}, 'wordCount': 229, 'commentCount': 0, 'publisher': {'@id': 'https://www.constructionspecifier.com/#organization'}, 'image': {'@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#primaryimage'}, 'thumbnailUrl': 'https://www.constructionspecifier.com/wp-content/uploads/2018/11/featured-4.jpg', 'keywords': ['Artificial Intelligence', 'CSI', 'Industry Trends'], 'articleSection': ['Columns', 'Inside CSI'], 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'CommentAction', 'name': 'Comment', 'target': ['https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#respond']}]}, {'@type': 'WebPage', '@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/', 'url': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/', 'name': 'How artificial intelligence is changing the industry: A CSI exclusive - Construction Specifier', 'isPartOf': {'@id': 'https://www.constructionspecifier.com/#website'}, 'primaryImageOfPage': {'@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#primaryimage'}, 'image': {'@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#primaryimage'}, 'thumbnailUrl': 'https://www.constructionspecifier.com/wp-content/uploads/2018/11/featured-4.jpg', 'datePublished': '2018-11-12T17:31:54+00:00', 'dateModified': '2018-11-12T17:31:54+00:00', 'breadcrumb': {'@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#primaryimage', 'url': 'https://www.constructionspecifier.com/wp-content/uploads/2018/11/featured-4.jpg', 'contentUrl': 'https://www.constructionspecifier.com/wp-content/uploads/2018/11/featured-4.jpg', 'width': 600, 'height': 401}, {'@type': 'BreadcrumbList', '@id': 'https://www.constructionspecifier.com/how-artificial-intelligence-is-changing-the-industry-a-csi-exclusive/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.constructionspecifier.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'How artificial intelligence is changing the industry: A CSI exclusive'}]}, {'@type': 'WebSite', '@id': 'https://www.constructionspecifier.com/#website', 'url': 'https://www.constructionspecifier.com/', 'name': 'Construction Specifier', 'description': 'Solutions for the Construction Industry', 'publisher': {'@id': 'https://www.constructionspecifier.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.constructionspecifier.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.constructionspecifier.com/#organization', 'name': 'The Construction Specifier', 'url': 'https://www.constructionspecifier.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.constructionspecifier.com/#/schema/logo/image/', 'url': 'https://www.constructionspecifier.com/wp-content/uploads/2017/09/CS_Logo_BW_LR.jpg', 'contentUrl': 'https://www.constructionspecifier.com/wp-content/uploads/2017/09/CS_Logo_BW_LR.jpg', 'width': 288, 'height': 70, 'caption': 'The Construction Specifier'}, 'image': {'@id': 'https://www.constructionspecifier.com/#/schema/logo/image/'}}, {'@type': 'Person', '@id': 'https://www.constructionspecifier.com/#/schema/person/b98ef8acdc4f61ec3c542bdb863ee469', 'name': 'nithya_caleb', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.constructionspecifier.com/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/7a28b8c910d60115947aa67b1242a27b?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/7a28b8c910d60115947aa67b1242a27b?s=96&d=mm&r=g', 'caption': 'nithya_caleb'}, 'url': 'https://www.constructionspecifier.com/author/nithya_caleb/'}]",,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMi2gFodHRwczovL3d3dy5mb3JkZm91bmRhdGlvbi5vcmcvbmV3cy1hbmQtc3Rvcmllcy92aWRlb3MvaG93LWNhbi1wdWJsaWMtaW50ZXJlc3QtdGVjaC1jaGFuZ2Utb3VyLXdvcmxkLWZvci1nb29kL2pveS1idW9sYW13aW5pLWZpZ2h0aW5nLXRoZS1jb2RlZC1nYXplLWhvdy13ZS1tYWtlLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWJlbmVmaXQtYWxsLXB1YmxpYy1pbnRlcmVzdC10ZWNoL9IBAA?oc=5,Joy Buolamwini - Fighting the “coded gaze:” How we make artificial intelligence benefit all. Public Interest Tech - Ford Foundation,2018-11-13,Ford Foundation,https://www.fordfoundation.org,Artificial intelligence can be coded to be more inclusive. Joy Buolamwini shows how.,N/A,Artificial intelligence can be coded to be more inclusive. Joy Buolamwini shows how.,N/A,https://schema.org,,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,"[{'@type': 'WebPage', '@id': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/', 'url': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/', 'name': 'Joy Buolamwini - Fighting the “coded gaze:” How we make artificial intelligence benefit all. Public Interest Tech - Ford Foundation', 'isPartOf': {'@id': 'https://www.fordfoundation.org/#website'}, 'primaryImageOfPage': {'@id': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/#primaryimage'}, 'image': {'@id': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/#primaryimage'}, 'thumbnailUrl': 'https://www.fordfoundation.org/wp-content/uploads/2022/08/joy-thumbnail.jpeg', 'datePublished': '2018-11-13T05:00:00+00:00', 'dateModified': '2023-08-29T19:04:02+00:00', 'description': 'Artificial intelligence can be coded to be more inclusive. Joy Buolamwini shows how.', 'breadcrumb': {'@id': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/#primaryimage', 'url': 'https://www.fordfoundation.org/wp-content/uploads/2022/08/joy-thumbnail.jpeg', 'contentUrl': 'https://www.fordfoundation.org/wp-content/uploads/2022/08/joy-thumbnail.jpeg', 'width': 1280, 'height': 720, 'caption': 'Joy Buolamwini is a Black person wearing a bright pink blazer and matching glasses. Joy holds a faceless white mask to her face. The phrase ""Can you see me now?"" appears to the left.'}, {'@type': 'BreadcrumbList', '@id': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/joy-buolamwini-fighting-the-coded-gaze-how-we-make-artificial-intelligence-benefit-all-public-interest-tech/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.fordfoundation.org/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Videos', 'item': 'https://www.fordfoundation.org/news-and-stories/videos/'}, {'@type': 'ListItem', 'position': 3, 'name': 'How can Public Interest Tech change our world for good?', 'item': 'https://www.fordfoundation.org/news-and-stories/videos/how-can-public-interest-tech-change-our-world-for-good/'}, {'@type': 'ListItem', 'position': 4, 'name': 'Joy Buolamwini &#8211; Fighting the “coded gaze:” How we make artificial intelligence benefit all. Public Interest Tech'}]}, {'@type': 'WebSite', '@id': 'https://www.fordfoundation.org/#website', 'url': 'https://www.fordfoundation.org/', 'name': 'Ford Foundation', 'description': '', 'publisher': {'@id': 'https://www.fordfoundation.org/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.fordfoundation.org/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.fordfoundation.org/#organization', 'name': 'Ford Foundation', 'url': 'https://www.fordfoundation.org/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.fordfoundation.org/#/schema/logo/image/', 'url': 'https://www.fordfoundation.org/wp-content/uploads/2023/05/Ford-Foundation.svg', 'contentUrl': 'https://www.fordfoundation.org/wp-content/uploads/2023/05/Ford-Foundation.svg', 'caption': 'Ford Foundation'}, 'image': {'@id': 'https://www.fordfoundation.org/#/schema/logo/image/'}}]",,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMioQFodHRwczovL3d3dzIuZGVsb2l0dGUuY29tL3NnL2VuL3BhZ2VzL2ZpbmFuY2lhbC1hZHZpc29yeS9hcnRpY2xlcy90aGUtY2FzZS1mb3ItYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtaW4tY29tYmF0aW5nLW1vbmV5LWxhdW5kZXJpbmctYW5kLXRlcnJvcmlzdC1maW5hbmNpbmcuaHRtbNIBAA?oc=5,The case for artificial intelligence in combating money laundering and terrorist financing - Deloitte,2018-11-14,Deloitte,https://www2.deloitte.com,A Balancing Act In Trade Based Money Laundering Compliance,N/A,A Balancing Act In Trade Based Money Laundering Compliance,A Balancing Act In Trade Based Money Laundering Compliance,,,,,,,,,,,,,,N/A,N/A,"











The case for artificial intelligence in combating money laundering and terrorist financing has been saved 





My Deloitte 




×














The case for artificial intelligence in combating money laundering and terrorist financing has been removed 




Undo
My Deloitte 



×














An Article Titled The case for artificial intelligence in combating money laundering and terrorist financing already exists in Saved items 





My Deloitte 




×




















                                Perspectives
                            


The case for artificial intelligence in combating money laundering and terrorist financing
Machine learning technology























Save for later 






















Combating money laundering is an enormous task, and it comes with substantial costs and risks, including but not limited to regulatory, reputational and financial crime risks.
Managing these risks rest with the guardians of the financial system. Moreover, criminals continue to evolve in their laundering techniques, finding and exploiting loopholes in the system to move money. These criminal minds are also capable of using new technologies such as online banking, electronic payments, and cryptocurrencies to move illicit funds across borders at breakneck speed. This creates complex and layered transactions that are increasingly real-time, making it difficult to monitor and to detect with traditional approaches.
At the heart of criminal activity are sophisticated money launderers with the ability to move illicit funds seamlessly through the formal financial system. These money launderers are sophisticated and pose a serious threat to financial institutions across the globe, and their activities have a devastating consequence for society as well. As a result, societal ills such as terrorism, drug and human trafficking challenge social structures and order, societal governance, as well as open and fair commerce. For these reasons, the importance of continuous improvement of an organisation’s financial transaction monitoring and name screening effectiveness has never been more critical in the digital age.
Download the joint whitepaper by Deloitte and United Overseas Bank (UOB)entitled “The case for artificial intelligence in combating money laundering and terrorist financing” that discusses the promise of machine learning in compliance and its potential applications. The whitepaper also highlights a case study that depicts UOB’s journey to tap machine learning to augment and to enhance its existing systems to spot and to prevent illicit money flows.









                        The case for artificial intelligence in combating money laundering and terrorist financing
                     

                     
                        Download whitepaper here
                     
                     
                  

















Contact us





Submit RFP








",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiV2h0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9rYXJsLWZyaXN0b24tZnJlZS1lbmVyZ3ktcHJpbmNpcGxlLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlL9IBAA?oc=5,The Genius Neuroscientist Who Might Hold the Key to True AI - WIRED,2018-11-13,WIRED,https://www.wired.com,"Karl Friston’s free energy principle might be the most all-encompassing idea since the theory of natural selection. But to understand it, you need to peer inside the mind of Friston himself.","['the big story', 'science', 'psychology and neuroscience', 'ai hub', 'research', 'neural network', 'health care', 'machine learning', 'magazine-26.12', 'artificial intelligence', 'ai', 'neuroscience', 'wired classic', 'longreads', '_no-homepage', '_syndication_noshow', 'paywall subscriber only content', 'textaboveleftgridwidth', 'magazine']",Karl Friston’s free energy principle might be the most all-encompassing idea since the theory of natural selection. And Friston may be the only person who truly understands it.,Karl Friston’s free energy principle might be the most all-encompassing idea since the theory of natural selection. And Friston may be the only person who truly understands it.,https://schema.org/,BreadcrumbList,https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/,"['https://media.wired.com/photos/5be3712d72ce4c601115b2f8/16:9/w_2399,h_1349,c_limit/Karl_Friston_24349.jpg', 'https://media.wired.com/photos/5be3712d72ce4c601115b2f8/4:3/w_2396,h_1797,c_limit/Karl_Friston_24349.jpg', 'https://media.wired.com/photos/5be3712d72ce4c601115b2f8/1:1/w_1642,h_1642,c_limit/Karl_Friston_24349.jpg']","[{'@type': 'Person', 'name': 'Shaun Raviv', 'sameAs': 'https://www.wired.com/author/shaun-raviv/'}]","{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}",The Genius Neuroscientist Who Might Hold the Key to True AI,2018-11-13T06:00:00.000-05:00,2018-11-13T06:00:00.000-05:00,the big story,,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'The Big Story', 'item': 'https://www.wired.com/big-story/'}, {'@type': 'ListItem', 'position': 2, 'name': 'magazine-26.12', 'item': 'https://www.wired.com/tag/magazine-2612/'}, {'@type': 'ListItem', 'position': 3, 'name': 'The Genius Neuroscientist Who Might Hold the Key to True AI'}]",tags,N/A,"Shaun RavivThe Big StoryNov 13, 2018 6:00 AMThe Genius Neuroscientist Who Might Hold the Key to True AIKarl Friston’s free energy principle might be the most all-encompassing idea since the theory of natural selection. But to understand it, you need to peer inside the mind of Friston himself.Photograph: Kate PetersSave this storySaveSave this storySaveThe AI Database →End UserResearchSectorResearchHealth careTechnologyNeural NetworkMachine learningWhen King George III of England began to show signs of acute mania toward the end of his reign, rumors about the royal madness multiplied quickly in the public mind. One legend had it that George tried to shake hands with a tree, believing it to be the King of Prussia. Another described how he was whisked away to a house on Queen Square, in the Bloomsbury district of London, to receive treatment among his subjects. The tale goes on that George’s wife, Queen Charlotte, hired out the cellar of a local pub to stock provisions for the king’s meals while he stayed under his doctor’s care.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDThis article is exclusive to subscribers. Subscribe Now. If you're already a subscriber sign in.","{'@type': 'WebPage', '@id': 'https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/'}",,,"More than two centuries later, this story about Queen Square is still popular in London guidebooks. And whether or not it’s true, the neighborhood has evolved over the years as if to conform to it. A metal statue of Charlotte stands over the northern end of the square; the corner pub is called the Queen’s Larder; and the square’s quiet rectangular garden is now all but surrounded by people who work on brains and people whose brains need work. The National Hospital for Neurology and Neurosurgery—where a modern-day royal might well seek treatment—dominates one corner of Queen Square, and the world-renowned neuroscience research facilities of University College London round out its perimeter. During a week of perfect weather last July, dozens of neurological patients and their families passed silent time on wooden benches at the outer edges of the grass.
On a typical Monday, Karl Friston arrives on Queen Square at 12:25 pm and smokes a cigarette in the garden by the statue of Queen Charlotte. A slightly bent, solitary figure with thick gray hair, Friston is the scientific director of University College London’s storied Functional Imaging Laboratory, known to everyone who works there as the FIL. After finishing his cigarette, Friston walks to the western side of the square, enters a brick and limestone building, and heads to a seminar room on the fourth floor, where anywhere from two to two dozen people might be facing a blank white wall waiting for him. Friston likes to arrive five minutes late, so everyone else is already there.
His greeting to the group is liable to be his first substantial utterance of the day, as Friston prefers not to speak with other human beings before noon. (At home, he will have conversed with his wife and three sons via an agreed-upon series of smiles and grunts.) He also rarely meets people one-on-one. Instead, he prefers to hold open meetings like this one, where students, postdocs, and members of the public who desire Friston’s expertise—a category of person that has become almost comically broad in recent years—can seek his knowledge. “He believes that if one person has an idea or a question or project going on, the best way to learn about it is for the whole group to come together, hear the person, and then everybody gets a chance to ask questions and discuss. And so one person’s learning becomes everybody’s learning,” says David Benrimoh, a psychiatry resident at McGill University who studied under Friston for a year. “It’s very unique. As many things are with Karl.”
At the start of each Monday meeting, everyone goes around and states their questions at the outset. Friston walks in slow, deliberate circles as he listens, his glasses perched at the end of his nose, so that he is always lowering his head to see the person who is speaking. He then spends the next few hours answering the questions in turn. “A Victorian gentleman, with Victorian manners and tastes,” as one friend describes Friston, he responds to even the most confused questions with courtesy and rapid reformulation. The Q&A sessions—which I started calling “Ask Karl” meetings—are remarkable feats of endurance, memory, breadth of knowledge, and creative thinking. They often end when it is time for Friston to retreat to the minuscule metal balcony hanging off his office for another smoke.
Friston first became a heroic figure in academia for devising many of the most important tools that have made human brains legible to science. In 1990 he invented statistical parametric mapping, a computational technique that helps—as one neuroscientist put it—“squash and squish” brain images into a consistent shape so that researchers can do apples-to-apples comparisons of activity within different crania. Out of statistical parametric mapping came a corollary called voxel-­based morphometry, an imaging technique that was used in one famous study to show that the rear side of the hippocampus of London taxi drivers grew as they learned “the knowledge.”1
A study published in Science in 2011 used yet a third brain-imaging-analysis software invented by Friston—dynamic causal modeling—to determine if people with severe brain damage were minimally conscious or simply vegetative.
When Friston was inducted into the Royal Society of Fellows in 2006, the academy described his impact on studies of the brain as “revolutionary” and said that more than 90 percent of papers published in brain imaging used his methods. Two years ago, the Allen Institute for Artificial Intelligence, a research outfit led by AI pioneer Oren Etzioni, calculated that Friston is the world’s most frequently cited neuroscientist. He has an h-­index—a metric used to measure the impact of a researcher’s publications—nearly twice the size of Albert Einstein’s. Last year Clarivate Analytics, which over more than two decades has successfully predicted 46 Nobel Prize winners in the sciences, ranked Friston among the three most likely winners in the physiology or medicine category.
What’s remarkable, however, is that few of the researchers who make the pilgrimage to see Friston these days have come to talk about brain imaging at all. Over a 10-day period this summer, Friston advised an astrophysicist, several philosophers, a computer engineer working on a more personable competitor to the Amazon Echo, the head of artificial intelligence for one of the world’s largest insurance companies, a neuroscientist seeking to build better hearing aids, and a psychiatrist with a startup that applies machine learning to help treat depression. And most of them had come because they were desperate to understand something else entirely.
For the past decade or so, Friston has devoted much of his time and effort to developing an idea he calls the free energy principle. (Friston refers to his neuroimaging research as a day job, the way a jazz musician might refer to his shift at the local public library.) With this idea, Friston believes he has identified nothing less than the organizing principle of all life, and all intelligence as well. “If you are alive,” he sets out to answer, “what sorts of behaviors must you show?”
First the bad news: The free energy principle is maddeningly difficult to understand. So difficult, in fact, that entire rooms of very, very smart people have tried and failed to grasp it. A Twitter account2 with 3,000 followers exists simply to mock its opacity, and nearly every person I spoke with about it, including researchers whose work depends on it, told me they didn’t fully comprehend it.
But often those same people hastened to add that the free energy principle, at its heart, tells a simple story and solves a basic puzzle. The second law of thermodynamics tells us that the universe tends toward entropy, toward dissolution; but living things fiercely resist it. We wake up every morning nearly the same person we were the day before, with clear separations between our cells and organs, and between us and the world without. How? Friston’s free energy principle says that all life, at every scale of organization—from single cells to the human brain, with its billions of neurons—is driven by the same universal imperative, which can be reduced to a mathematical function. To be alive, he says, is to act in ways that reduce the gulf between your expectations and your sensory inputs. Or, in Fristonian terms, it is to minimize free energy.
To get a sense of the potential implications of this theory, all you have to do is look at the array of people who darken the FIL’s doorstep on Monday mornings. Some are here because they want to use the free energy principle to unify theories of the mind, provide a new foundation for biology, and explain life as we know it. Others hope the free energy principle will finally ground psychiatry in a functional understanding of the brain. And still others come because they want to use Friston’s ideas to break through the roadblocks in artificial intelligence research. But they all have one reason in common for being here, which is that the only person who truly understands Karl Friston’s free energy principle may be Karl Friston himself.
But if you ask him, this output isn’t just the fruit of an ambitious work ethic; it’s a mark of his tendency toward a kind of rigorous escapism.
Friston draws a carefully regulated boundary around his inner life, guarding against intrusions, many of which seem to consist of “worrying about other people.” He prefers being onstage, with other people at a comfortable distance, to being in private conversations. He does not have a mobile phone. He always wears navy-blue suits, which he buys two at a time at a closeout shop. He finds disruptions to his weekly routine on Queen Square “rather nerve-racking” and so tends to avoid other human beings at, say, international conferences. He does not enjoy advocating for his own ideas.
At the same time, Friston is exceptionally lucid and forthcoming about what drives him as a scholar. He finds it incredibly soothing—not unlike disappearing for a smoke—to lose himself in a difficult problem that takes weeks to resolve. And he has written eloquently about his own obsession, dating back to childhood, with finding ways to integrate, unify, and make simple the apparent noise of the world.
Friston traces his path to the free energy principle back to a hot summer day when he was 8 years old. He and his family were living in the walled English city of Chester, near Liverpool, and his mother had told him to go play in the garden. He turned over an old log and spotted several wood lice—small bugs with armadillo-shaped exoskeletons—moving about, he initially assumed, in a frantic search for shelter and darkness. After staring at them for half an hour, he deduced that they were not actually seeking the shade. “That was an illusion,” Friston says. “A fantasy that I brought to the table.”
He realized that the movement of the wood lice had no larger purpose, at least not in the sense that a human has a purpose when getting in a car to run an errand. The creatures’ movement was random; they simply moved faster in the warmth4 of the sun.
Friston calls this his first scientific insight, a moment when “all these contrived, anthropomorphized explanations of purpose and survival and the like all seemed to just peel away,” he says. “And the thing you were observing just was. In the sense that it could be no other way.”
Friston’s father was a civil engineer who worked on bridges all around England, and his family moved around with him. In just his first decade, the young Friston attended six different schools. His teachers often didn’t know what to do with him, and he drew most of his fragile self-esteem from solitary problem solving. At age 10 he designed a self-righting robot that could, in theory, traverse uneven ground while carrying a glass of water, using self-correcting feedback actuators and mercury levels. At school, a psychologist was brought in to ask him how he came up with it. “You’re very intelligent, Karl,” Friston’s mother reassured him, not for the last time. “Don’t let them tell you you’re not.” He says he didn’t believe her.
When Friston was in his mid-teens, he had another wood-lice moment. He had just come up to his bedroom from watching TV and noticed the cherry trees in bloom outside the window. He suddenly became possessed by a thought that has never let go of him since. “There must be a way of understanding everything by starting from nothing,” he thought. “If I’m only allowed to start off with one point in the entire universe, can I derive everything else I need from that?” He stayed there on his bed for hours, making his first attempt. “I failed completely, obviously,” he says.
Toward the end of secondary school, Friston and his classmates were the subjects of an early experiment in computer-­assisted advising. They were asked a series of questions, and their answers were punched into cards and run through a machine to extrapolate the perfect career choice. Friston had described how he enjoyed electronics design and being alone in nature, so the computer suggested he become a television antenna installer. That didn’t seem right, so he visited a school career counselor and said he’d like to study the brain in the context of mathematics and physics. The counselor told Friston he should become a psychiatrist, which meant, to Friston’s horror, that he had to study medicine.
Both Friston and the counselor had confused psychiatry with psychology, which is what he probably ought to have pursued as a future researcher. But it turned out to be a fortunate error, as it put Friston on a path toward studying both the mind and body,5 and toward one of the most formative experiences of his life—one that got Friston out of his own head.
After completing his medical studies, Friston moved to Oxford and spent two years as a resident trainee at a Victorian-era hospital called Littlemore. Founded under the 1845 Lunacy Act, Littlemore had originally been instituted to help transfer all “pauper lunatics” from workhouses to hospitals. By the mid-1980s, when Friston arrived, it was one of the last of the old asylums on the outskirts of England’s cities.
Friston was assigned a group of 32 chronic schizophrenic patients, the worst-off residents of Littlemore, for whom treatment mostly meant containment. For Friston, who recalls his former patients with evident nostalgia, it was an introduction to the way that connections in the brain were easily broken. “It was a beautiful place to work,” he says. “This little community of intense and florid psychopathology.”
Twice a week he led 90-minute group therapy sessions in which the patients explored their ailments together, reminiscent of the Ask Karl meetings today. The group included colorful characters who still inspire Friston’s thinking more than 30 years later. There was Hillary,6 who looked like she could play the senior cook on Downton Abbey but who, before coming to Littlemore, had decapitated her neighbor with a kitchen knife, convinced he had become an evil, human-sized crow.
There was Ernest, who had a penchant for pastel Marks & Spencer cardigans and matching plimsoll shoes, and who was “as rampant and incorrigible a pedophile as you could ever imagine,” Friston says.
And then there was Robert, an articulate young man who might have been a university student had he not suffered severe schizophrenia. Robert ruminated obsessively about, of all things, angel shit; he pondered whether the stuff was a blessing or a curse and whether it was ever visible to the eye, and he seemed perplexed that these questions had not occurred to others. To Friston, the very concept of angel shit was a miracle. It spoke to the ability of people with schizophrenia to assemble concepts that someone with a more regularly functioning brain couldn’t easily access. “It’s extremely difficult to come up with something like angel shit,” Friston says with something like admiration. “I couldn’t do it.”
After Littlemore, Friston spent much of the early 1990s using a relatively new technology—PET scans—to try to understand what was going on inside the brains of people with schizophrenia. He invented statistical parametric mapping along the way. Unusually for the time, Friston was adamant that the technique should be freely shared rather than patented and commercialized, which largely explains how it became so widespread. Friston would fly across the world—to the National Institutes of Health in Bethesda, Maryland, for example—to give it to other researchers. “It was me, literally, with a quarter of biometric tape, getting on an airplane, taking it over there, downloading it, spending a day getting it to work, teaching somebody how to use it, then going home for a rest,” Friston says. “This is how open source software worked in those days.”
Friston came to Queen Square in 1994, and for a few years his office at the FIL sat just a few doors down from the Gatsby Computational Neuroscience Unit. The Gatsby—where researchers study theories of perception and learning in both living and machine systems—was then run by its founder, the cognitive psychologist and computer scientist Geoffrey Hinton. While the FIL was establishing itself as one of the premier labs for neuroimaging, the Gatsby was becoming a training ground for neuroscientists interested in applying mathematical models to the nervous system.
Friston, like many others, became enthralled by Hinton’s “childlike enthusiasm” for the most unchildlike of statistical models, and the two men became friends.7
Over time, Hinton convinced Friston that the best way to think of the brain was as a Bayesian probability machine. The idea, which goes back to the 19th century and the work of Hermann von Helmholtz, is that brains compute and perceive in a probabilistic manner, constantly making predictions and adjusting beliefs based on what the senses contribute. According to the most popular modern Bayesian account, the brain is an “inference engine” that seeks to minimize “prediction error.”
In 2001, Hinton left London for the University of Toronto, where he became one of the most important figures in artificial intelligence, laying the groundwork8 for much of today’s research in deep learning.
Before Hinton left, however, Friston visited his friend at the Gatsby one last time. Hinton described a new technique he’d devised to allow computer programs to emulate human decisionmaking more efficiently—a process for integrating the input of many different probabilistic models, now known in machine learning as a “product of experts.”
The meeting left Friston’s head spinning. Inspired by Hinton’s ideas, and in a spirit of intellectual reciprocity, Friston sent Hinton a set of notes about an idea he had for connecting several seemingly “unrelated anatomical, physiological, and psychophysical attributes of the brain.” Friston published those notes in 2005—the first of many dozens of papers he would go on to write about the free energy principle.
It’s a white fleece throw, custom-printed with a black-and-white portrait of a stern, bearded Russian mathematician named Andrei Andreyevich Markov, who died in 1922. The blanket is a gag gift from Friston’s son, a plush, polyester inside joke about an idea that has become central to the free energy principle. Markov is the eponym of a concept called a Markov blanket, which in machine learning is essentially a shield that separates one set of variables from others in a layered, hierarchical system. The psychologist Christopher Frith—who has an h-index on par with Friston’s—once described a Markov blanket as “a cognitive version of a cell membrane, shielding states inside the blanket from states outside.”
In Friston’s mind, the universe is made up of Markov blankets inside of Markov blankets. Each of us has a Markov blanket that keeps us apart from what is not us. And within us are blankets separating organs, which contain blankets separating cells, which contain blankets separating their organelles. The blankets define how biological things exist over time and behave distinctly from one another. Without them, we’re just hot gas dissipating into the ether.
“That’s the Markov blanket you’ve read about. This is it. You can touch it,” Friston said dryly when I first saw the throw in his office. I couldn’t help myself; I did briefly reach out to feel it under my fingers. Ever since I first read about Markov blankets, I’d seen them everywhere. Markov blankets around a leaf and a tree and a mosquito. In London, I saw them around the postdocs at the FIL, around the black-clad protesters at an antifascist rally, and around the people living in boats in the canals. Invisible cloaks around everyone, and underneath each one a different living system that minimizes its own free energy.
The concept of free energy itself comes from physics, which means it’s difficult to explain precisely without wading into mathematical formulas. In a sense that’s what makes it powerful: It isn’t a merely rhetorical concept. It’s a measurable quantity that can be modeled, using much the same math that Friston has used to interpret brain images to such world-­changing effect. But if you translate the concept from math into English, here’s roughly what you get: Free energy is the difference between the states you expect to be in and the states your sensors tell you that you are in. Or, to put it another way, when you are minimizing free energy, you are minimizing surprise.
According to Friston, any biological system9 that resists a tendency to disorder and dissolution will adhere to the free energy principle—whether it’s a protozoan or a pro basketball team.
A single-celled organism has the same imperative to reduce surprise that a brain does.
The only difference is that, as self-organizing biological systems go, the human brain is inordinately complex: It soaks in information from billions of sense receptors, and it needs to organize that information efficiently into an accurate model of the world. “It’s literally a fantastic organ in the sense that it generates hypotheses or fantasies that are appropriate for trying to explain these myriad patterns, this flux of sensory information that it is in receipt of,” Friston says. In seeking to predict what the next wave of sensations is going to tell it—and the next, and the next—the brain is constantly making inferences and updating its beliefs based on what the senses relay back, and trying to minimize prediction-error signals.
So far, as you might have noticed, this sounds a lot like the Bayesian idea of the brain as an “inference engine” that Hinton told Friston about in the 1990s. And indeed, Friston regards the Bayesian model as a foundation of the free energy principle (“free energy” is even a rough synonym for “prediction error”). But the limitation of the Bayesian model, for Friston, is that it only accounts for the interaction between beliefs and perceptions; it has nothing to say about the body or action. It can’t get you out of your chair.
This isn’t enough for Friston, who uses the term “active inference” to describe the way organisms minimize surprise while moving about the world. When the brain makes a prediction that isn’t immediately borne out by what the senses relay back, Friston believes, it can minimize free energy in one of two ways: It can revise its prediction—absorb the surprise, concede the error, update its model of the world—or it can act to make the prediction true. If I infer that I am touching my nose with my left index finger, but my proprioceptors tell me my arm is hanging at my side, I can minimize my brain’s raging prediction-error signals by raising that arm up and pressing a digit to the middle of my face.
And in fact, this is how the free energy principle accounts for everything we do: perception, action, planning, problem solving. When I get into the car to run an errand, I am minimizing free energy by confirming my hypothesis—my fantasy—through action.
For Friston, folding action and movement into the equation is immensely important. Even perception itself, he says, is “enslaved by action”: To gather information, the eye darts, the diaphragm draws air into the nose, the fingers generate friction against a surface. And all of this fine motor movement exists on a continuum with bigger plans, explorations,10 and actions.
“We sample the world,” Friston writes, “to ensure our predictions become a self-fulfilling prophecy.”
So what happens when our prophecies are not self-fulfilling? What does it look like for a system to be overwhelmed by surprise? The free energy principle, it turns out, isn’t just a unified theory of action, perception, and planning; it’s also a theory of mental illness. When the brain assigns too little or too much weight to evidence pouring in from the senses, trouble occurs. Someone with schizophrenia, for example, may fail to update their model of the world to account for sensory input from the eyes. Where one person might see a friendly neighbor, Hillary might see a giant, evil crow. “If you think about psychiatric conditions, and indeed most neurological conditions, they are just broken beliefs or false inference—hallucinations and delusions,” Friston says.
Over the past few years, Friston and a few other scientists have used the free energy principle to help explain anxiety, depression, and psychosis, along with certain symptoms of autism, Parkinson’s disease, and psychopathy. In many cases, scientists already know—thanks to Friston’s neuroimaging methods—which regions of the brain tend to malfunction in different disorders and which signals tend to be disrupted. But that alone isn’t enough to go on. “It’s not sufficient to understand which synapses, which brain connections, are working improperly,” he says. “You need to have a calculus that talks about beliefs.”
So: The free energy principle offers a unifying explanation for how the mind works and a unifying explanation for how the mind malfunctions. It stands to reason, then, that it might also put us on a path toward building a mind from scratch.
This kind of pattern-matching technology—which is roughly similar to the techniques that have taught machines to recognize faces, images of cats, and speech patterns—has driven huge advances in computing over the past several years. But it requires a lot of up-front data and human supervision, and it can be brittle. Another approach to AI, called reinforcement learning, has shown incredible success at winning games: Go, chess, Atari’s Breakout. Reinforcement learning doesn’t require humans to label lots of training data; it just requires telling a neural network to seek a certain reward, often victory in a game. The neural network learns by playing the game over and over, optimizing for whatever moves might get it to the final screen, the way a dog might learn to perform certain tasks for a treat.
But reinforcement learning, too, has pretty major limitations. In the real world, most situations are not organized around a single, narrowly defined goal. (Sometimes you have to stop playing Breakout to go to the bathroom, put out a fire, or talk to your boss.) And most environments aren’t as stable and rule-bound as a game is. The conceit behind neural networks is that they are supposed to think the way we do; but reinforcement learning doesn’t really get us there.
To Friston and his enthusiasts, this failure makes complete sense. After all, according to the free energy principle, the fundamental drive of human thought isn’t to seek some arbitrary external reward. It’s to minimize prediction error. Clearly, neural networks ought to do the same. It helps that the Bayesian formulas behind the free energy principle—the ones that are so difficult to translate into English—are already written in the native language of machine learning.
Julie Pitt, head of machine-learning infrastructure at Netflix, discovered Friston and the free energy principle in 2014, and it transformed her thinking. (Pitt’s Twitter bio reads, “I infer my own actions by way of Active Inference.”) Outside of her work at Netflix, she’s been exploring applications of the principle in a side project called Order of Magnitude Labs. Pitt says that the beauty of the free energy model is that it allows an artificial agent to act in any environment, even one that’s new and unknown. Under the old reinforcement-learning model, you’d have to keep stipulating new rules and sub-rewards to get your agent to cope with a complex world. But a free energy agent always generates its own intrinsic reward: the minimization of surprise. And that reward, Pitt says, includes an imperative to go out and explore.
In late 2017, a group led by Rosalyn Moran, a neuroscientist and engineer at King’s College London, pitted two AI players against one another in a version of the 3D shooter game Doom. The goal was to compare an agent driven by active inference to one driven by reward-maximization.
The reward-based agent’s goal was to kill a monster inside the game, but the free-energy-driven agent only had to minimize surprise. The Fristonian agent started off slowly. But eventually it started to behave as if it had a model of the game, seeming to realize, for instance, that when the agent moved left the monster tended to move to the right.
After a while it became clear that, even in the toy environment of the game, the reward-­maximizing agent was “demonstrably less robust”; the free energy agent had learned its environment better. “It outperformed the reinforcement-­learning agent because it was exploring,” Moran says. In another simulation that pitted the free-energy-minimizing agent against real human players, the story was similar. The Fristonian agent started slowly, actively exploring options—epistemically foraging, Friston would say—before quickly attaining humanlike performance.
Moran told me that active inference is starting to spread into more mainstream deep-learning research, albeit slowly. Some of Friston’s students have gone on to work at DeepMind and Google Brain, and one of them founded Huawei’s Artificial Intelligence Theory lab. “It’s moving out of Queen Square,” Moran says. But it’s still not nearly as common as reinforcement learning, which even undergraduates learn. “You don’t teach undergraduates the free energy principle—yet.”
The first time I asked Friston about the connection between the free energy principle and artificial intelligence, he predicted that within five to 10 years, most machine learning would incorporate free energy minimization. The second time, his response was droll. “Think about why it’s called active inference,” he said. His straight, sparkly white teeth showed through his smile as he waited for me to follow his wordplay. “Well, it’s AI,” Friston said. “So is active inference the new AI? Yes, it’s the acronym.” Not for the first time, a Fristonian joke had passed me by.
The next morning, I asked Friston if he thought the talk went well, considering that few of those bright young minds seemed to understand him. “There is going to be a substantial proportion of the audience who—it’s just not for them,” he said. “Sometimes they get upset because they’ve heard that it’s important and they don’t understand it. They think they have to think it’s rubbish and they leave. You get used to that.”
In 2010, Peter Freed, a psychiatrist at Columbia University, gathered together 15 brain researchers to discuss one of Friston’s papers. Freed described what happened in the journal Neuropsychoanalysis: “There was a lot of mathematical knowledge in the room: three statisticians, two physicists, a physical chemist, a nuclear physicist, and a large group of neuroimagers—but apparently we didn’t have what it took. I met with a Princeton physicist, a Stanford neurophysiologist, a Cold Springs Harbor neurobiologist to discuss the paper. Again blanks, one and all: too many equations, too many assumptions, too many moving parts, too global a theory, no opportunity for questions—and so people gave up.”
But for all the people who are exasperated by Friston’s impenetrability, there are nearly as many who feel he has unlocked something huge, an idea every bit as expansive as Darwin’s theory of natural selection. When the Canadian philosopher Maxwell Ramstead first read Friston’s work in 2014, he had already been trying to find ways to connect complex living systems that exist at different scales—from cells to brains to individuals to cultures. In 2016 he met Friston, who told him that the same math that applies to cellular differentiation—the process by which generic cells become more specialized—can also be applied to cultural dynamics. “This was a life-changing conversation for me,” Ramstead says. “I almost had a nosebleed.”
“This is absolutely novel in history,” Ramstead told me as we sat on a bench in Queen Square, surrounded by patients and staff from the surrounding hospitals. Before Friston came along, “We were kind of condemned to forever wander in this multidisciplinary space without a common currency,” he continued. “The free energy principle gives you that currency.”
In 2017, Ramstead and Friston coauthored a paper, with Paul Badcock of the University of Melbourne, in which they described all life in terms of Markov blankets. Just as a cell is a Markov-blanketed system that minimizes free energy in order to exist, so are tribes and religions and species.
After the publication of Ramstead’s paper, Micah Allen, a cognitive neuroscientist then at the FIL, wrote that the free energy principle had evolved into a real-life version of Isaac Asimov’s psychohistory,11 a fictional system that reduced all of psychology, history, and physics down to a statistical science.
And it’s true that the free energy principle does seem to have expanded to the point of being, if not a theory of everything, then nearly so. (Friston told me that cancer and tumors might be instances of false inference, when cells become deluded.) As Allen asked: Does a theory that explains everything run the risk of explaining nothing?
On the last day of my trip, I visited Friston in the town of Rickmansworth, where he lives in a house filled with taxidermied animals12 that his wife prepares as a hobby.
As it happens, Rickmansworth appears on the first page of The Hitchhiker’s Guide to the Galaxy; it’s the town where “a girl sitting on her own in a small café” suddenly discovers the secret to making the world “a good and happy place.” But fate intervenes. “Before she could get to a phone to tell anyone about it, a terrible stupid catastrophe occurred, and the idea was lost forever.”
It’s unclear whether the free energy principle is the secret to making the world a good and happy place, as some of its believers almost seem to think it might be. Friston himself tended to take a more measured tone as our talks went on, suggesting only that active inference and its corollaries were quite promising. Several times he conceded that he might just be “talking rubbish.” During the last group meeting I attended at the FIL, he told those in attendance that the free energy principle is an “as if” concept—it does not require that biological things minimize free energy in order to exist; it is merely sufficient as an explanation for biotic self-organization.
Friston’s mother died a few years ago, but lately he has been thinking back to her frequent reassurances during his childhood: You’re very intelligent, Karl. “I never quite believed her,” he says. “And yet now I have found myself suddenly being seduced by her argument. Now I do believe I’m actually quite bright.” But this newfound self-esteem, he says, has also led him to examine his own egocentricity.
Friston says his work has two primary motivations. Sure, it would be nice to see the free energy principle lead to true artificial consciousness someday, he says, but that’s not one of his top priorities. Rather, his first big desire is to advance schizophrenia research, to help repair the brains of patients like the ones he knew at the old asylum. And his second main motivation, he says, is “much more selfish.” It goes back to that evening in his bedroom, as a teenager, looking at the cherry blossoms, wondering, “Can I sort it all out in the simplest way possible?”
“And that is a very self-indulgent thing. It has no altruistic clinical compassion behind it. It is just the selfish desire to try and understand things as completely and as rigorously and as simply as possible,” he says. “I often reflect on the jokes that people make about me—sometimes maliciously, sometimes very amusingly—that I can’t communicate. And I think: I didn’t write it for you. I wrote it for me.”
Friston told me he occasionally misses the last train home to Rickmansworth, lost in one of those problems that he drills into for weeks. So he’ll sleep in his office, curled on the futon under his Markov blanket, safe and securely separated from the external world.

This article appears in the December 2018 issue. Subscribe now.
Listen to this story, and other WIRED features, on the Audm app.
Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.",,"https://media.wired.com/photos/5be3712d72ce4c601115b2f8/1:1/w_1642,h_1642,c_limit/Karl_Friston_24349.jpg","{'@type': 'CreativeWork', 'name': 'WIRED'}","Karl Friston’s free energy principle might be the most all-encompassing idea since the theory of natural selection. But to understand it, you need to peer inside the mind of Friston himself.",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiqwFodHRwczovL3d3dy5tY2tpbnNleS5jb20vY2FwYWJpbGl0aWVzL21ja2luc2V5LWRpZ2l0YWwvb3VyLWluc2lnaHRzL2hvdy1ib3RzLWFsZ29yaXRobXMtYW5kLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWFyZS1yZXNoYXBpbmctdGhlLWZ1dHVyZS1vZi1jb3Jwb3JhdGUtc3VwcG9ydC1mdW5jdGlvbnPSAQA?oc=5,"How bots, algorithms, and artificial intelligence are reshaping the future of corporate support functions - McKinsey",2018-11-15,McKinsey,https://www.mckinsey.com,Industrial companies are discovering additional sources of value in applying advanced technology to general and administrative support functions. The results can be impressive for businesses that can adapt to the disruption of legacy systems.,N/A,Industrial companies are discovering additional sources of value in applying advanced technology to general and administrative support functions. The results can be impressive for businesses that can adapt to the disruption of legacy systems.,Industrial companies are discovering additional sources of value in applying advanced technology to general and administrative support functions. The results can be impressive for businesses that can adapt to the disruption of legacy systems.,https://schema.org,Article,https://www.mckinsey.com,https://www.mckinsey.com/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/how%20bots%20algorithms%20ai%20are%20reshaping/hero-bots-algorithms-and-artificial-intelligence.jpg,"[{'@type': 'Person', 'name': 'Alexander Edlich', 'url': 'https://www.mckinsey.com/our-people/alexander-edlich'}, {'@type': 'Person', 'name': 'Fanny Ip'}, {'@type': 'Person', 'name': 'Rob Whiteman'}]","{'@type': 'Organization', 'name': 'McKinsey & Company', 'logo': {'@type': 'ImageObject', 'url': 'https://www.mckinsey.com/~/media/Thumbnails/Mck_Logo'}}",,2018-11-15T00:00:00Z,2018-11-15T00:00:00Z,,,,,N/A,N/A,N/A,"{'@type': 'WebPage', '@id': 'https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/how-bots-algorithms-and-artificial-intelligence-are-reshaping-the-future-of-corporate-support-functions'}",2018-09-19T14:22:27Z,"How bots, algorithms, and artificial intelligence are reshaping the future of corporate support functions",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiR2h0dHBzOi8vd3d3Lmdlb3NwYXRpYWx3b3JsZC5uZXQvYmxvZ3MvcHV0dGluZy10cmFmZmljLXNpZ25zLW9uLXRoZS1tYXAv0gEA?oc=5,Putting traffic signs on the map: How artificial intelligence can help - Geospatial World,2018-11-14,Geospatial World,https://www.geospatialworld.net,"The GIS market is booming. It’s set to surge from $5.81 billion in 2017 to $10.03 billion by 2023, but in spite of this, many small cities across the US still just have a one-person GIS shop",N/A,"The GIS market is booming. It’s set to surge from $5.81 billion in 2017 to $10.03 billion by 2023, but in spite of this, many small cities across the US still just have a one-person GIS shop",N/A,http://schema.org,BreadcrumbList,,,,,,,,,,,"[{'@type': 'ListItem', 'position': 1, 'item': {'@type': 'WebSite', '@id': 'https://www.geospatialworld.net/', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@type': 'WebPage', '@id': 'https://www.geospatialworld.net/blogs/', 'name': 'Blogs'}}, {'@type': 'ListItem', 'position': 3, 'item': {'@type': 'WebPage', '@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/', 'name': 'Putting traffic signs on the map: How artificial intelligence can help'}}]",N/A,N/A,"

Home  Blogs  Putting traffic signs on the map: How artificial intelligence can help
Blogs

Putting traffic signs on the map: How artificial intelligence can help

By Madelen Arnesdotter -   11/14/2018      4 Minutes Read 




The GIS market is booming. It’s set to surge from $5.81 billion in 2017 to $10.03 billion by 2023, but in spite of this, many small cities across the US still just have a one-person GIS shop, tasked with everything from informing and analyzing urban planning to doing street-level inventory. Time is scarce, and if you work for a city council, money tends to be as well—yet you’re expected to keep track of street assets and their overall conditions.
Steven Hewett’s situation was no different. A GIS specialist at the City of Clovis in New Mexico, a town of 39,000, Steven needed to undertake a complete traffic sign inventory of the approximately 24 square mile city–something that had never been done before. City managers assumed this task could be completed fairly easily and within a month, but no-one knew how many signs were actually out there.
Steven in his office
Asset data collection can be painstakingly time-consuming and expensive. It usually involves either going out yourself to every location and manually recording the data you need, or hiring a contractor to do it for you. With manual traffic sign inventory costing upwards of $4-6 per sign, this would cost a city like Clovis, which Steven now knows has about 4000 traffic signs, somewhere around $16,000-24,000. To a small city council, this is not an insignificant sum of cash.
Like many cities around the world, Clovis had never done a traffic sign inventory before and had little idea how many traffic signs were even out there and what shape they were in (pun intended!). After some investigation, Steven ended up using Mapillary, an AI-powered street-level imagery platform. Instead of hiring a contractor to do surveying, Steven mounted some cameras on his car and captured 200,000 images of the streets of Clovis. Once uploaded to Mapillary, the images are processed with computer vision, a form of artificial intelligence, to automatically detect and analyze data such as traffic signs.
Steven mounting cameras on his car to go out and capture imagery of the streets of Clovis. He mapped all of Clovis in just a few months, but thinks he could have done it in just a few days if he had done it in one go.
Even though Steven was the only person collecting images, and though he juggled it alongside other day-to-day tasks, the entire process took a couple of months. Had he done it in one go, Steven says it could have been done in just a few days—a huge time saving compared to doing the work manually:
“Doing all this work manually, it wouldn’t surprise me if it had taken up to a decade to walk through each and every block to get every sign that is out there. There is no telling how many changes would come through to traffic signs in that time.”
Traffic signs automatically identified by Mapillary’s computer vision algorithms in the city of Clovis, New Mexico.
Soon after the images had been uploaded to Mapillary, Steven was able to access the automatically detected traffic sign data. The data downloaded from Mapillary is compatible with ArcGIS, allowing Steven to ensure that all traffic signs are visible, and to check whether they are in need of repair or replacement. With a full inventory, it is also possible to see where more or fewer signs may be necessary for a particular area.
The Mapillary web platform, showing the location of every identified traffic sign.
Mapillary supports a total of 1,500 traffic signs from 100 different countries. Janine Yoong, VP of Business Development at Mapillary, says traffic sign inventory represents a particular pain point for cities:
“Both small and large cities struggle with keeping tabs on their street assets, but smaller cities are often under a different kind of pressure as they tend to have fewer people working with GIS. Because it takes so long to do manual inventories, most cities just haven’t done it at all. Using computer vision to automatically identify traffic signs saves GIS officers both time and money, and opens up opportunities to focus on other areas”.
Steven working with the Mapillary traffic sign data in ArcGIS
Mapillary works with cities all over the world, ranging from Amsterdam to Stockholm and Los Angeles. As for Steven in Clovis, some of his findings included traffic signs that were so old that they’d actually been completely bleached by the sun, inevitably causing confusion on the streets.
“With this data, we’ll now look into what traffic signs need replacing and where,” Steven says.
This is a guest blog by Madelen Arnesdotter, Customer Success Manager at Mapillary
Also Read: Mapillary fixes maps with computer vision


 

 TAGSArtificial IntelligenceFuture TechnologiesgeospatialGIS & Mapping 


Facebook


Twitter


WhatsApp


Linkedin


  Madelen ArnesdotterCustomer Success Manager at Mapillary

 
",,,,,,,,,"[{'@type': 'Article', '@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/#article', 'isPartOf': {'@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/'}, 'author': {'name': 'Madelen Arnesdotter', '@id': 'https://www.geospatialworld.net/#/schema/person/60f9d8ac2683cfb1a65a7b6258057224'}, 'headline': 'Putting traffic signs on the map: How artificial intelligence can help', 'datePublished': '2018-11-14T12:22:25+00:00', 'dateModified': '2018-11-14T12:26:44+00:00', 'mainEntityOfPage': {'@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/'}, 'wordCount': 823, 'publisher': {'@id': 'https://www.geospatialworld.net/#organization'}, 'image': {'@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/#primaryimage'}, 'thumbnailUrl': 'https://geospatialmedia.s3.amazonaws.com/wp-content/uploads/2018/11/4.jpg', 'keywords': ['Artificial Intelligence', 'Future Technologies', 'geospatial', 'GIS &amp; Mapping'], 'articleSection': ['Blogs'], 'inLanguage': 'en-US', 'copyrightYear': '2018', 'copyrightHolder': {'@id': 'https://www.geospatialworld.net/#organization'}}, {'@type': 'WebPage', '@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/', 'url': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/', 'name': 'Putting traffic signs on the map: How artificial intelligence can help', 'isPartOf': {'@id': 'https://www.geospatialworld.net/#website'}, 'primaryImageOfPage': {'@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/#primaryimage'}, 'image': {'@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/#primaryimage'}, 'thumbnailUrl': 'https://geospatialmedia.s3.amazonaws.com/wp-content/uploads/2018/11/4.jpg', 'datePublished': '2018-11-14T12:22:25+00:00', 'dateModified': '2018-11-14T12:26:44+00:00', 'description': 'The GIS market is booming. It’s set to surge from $5.81 billion in 2017 to $10.03 billion by 2023, but in spite of this, many small cities across the US still just have a one-person GIS shop', 'breadcrumb': {'@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/#primaryimage', 'url': 'https://geospatialmedia.s3.amazonaws.com/wp-content/uploads/2018/11/4.jpg', 'contentUrl': 'https://geospatialmedia.s3.amazonaws.com/wp-content/uploads/2018/11/4.jpg', 'width': 1743, 'height': 1080, 'caption': 'The Mapillary web platform, showing the location of every identified traffic sign.'}, {'@type': 'BreadcrumbList', '@id': 'https://www.geospatialworld.net/blogs/putting-traffic-signs-on-the-map/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.geospatialworld.net/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Putting traffic signs on the map: How artificial intelligence can help'}]}, {'@type': 'WebSite', '@id': 'https://www.geospatialworld.net/#website', 'url': 'https://www.geospatialworld.net/', 'name': 'Geospatial World', 'description': 'Top destination for geospatial industry trends', 'publisher': {'@id': 'https://www.geospatialworld.net/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.geospatialworld.net/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.geospatialworld.net/#organization', 'name': 'Geospatial Media & Communications', 'url': 'https://www.geospatialworld.net/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geospatialworld.net/#/schema/logo/image/', 'url': 'https://geospatialmedia.s3.amazonaws.com/wp-content/uploads/2017/10/geospatial-world-logo.png', 'contentUrl': 'https://geospatialmedia.s3.amazonaws.com/wp-content/uploads/2017/10/geospatial-world-logo.png', 'width': 378, 'height': 181, 'caption': 'Geospatial Media & Communications'}, 'image': {'@id': 'https://www.geospatialworld.net/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/GeospatialMedia/', 'https://x.com/geoworldmedia', 'https://www.instagram.com/geospatialmedia/', 'https://www.linkedin.com/company/2391107', 'https://www.pinterest.com/geospatialworld/', 'https://www.youtube.com/channel/UC2UaNw8A-fQhIBBnaZPKEmA']}, {'@type': 'Person', '@id': 'https://www.geospatialworld.net/#/schema/person/60f9d8ac2683cfb1a65a7b6258057224', 'name': 'Madelen Arnesdotter', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geospatialworld.net/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/22add6bd6a7db9dd4ea8e588bdfd056c?s=96&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/22add6bd6a7db9dd4ea8e588bdfd056c?s=96&r=g', 'caption': 'Madelen Arnesdotter'}, 'description': 'Customer Success Manager at Mapillary', 'url': 'https://www.geospatialworld.net/author/madelen-arnesdotter/'}]",,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiaGh0dHBzOi8vdG93YXJkc2RhdGFzY2llbmNlLmNvbS8zLWNoYWxsZW5nZXMtb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtaW4tY29tcHV0ZXItZ3JhcGhpY3MtMjIzZTA2YmQ4NDZi0gEA?oc=5,3 Challenges of Artificial Intelligence in Computer graphics | by Mauro Comi - Towards Data Science,2018-11-15,Towards Data Science,https://towardsdatascience.com,"Computer graphics is not only stunning Visual effects. It gives us the tool to reconstruct the physical world for different kind of purposes, such as industrial or medical applications.
The Graphics…",N/A,Reducing the gap between Virtual and Real world,Reducing the gap between Virtual and Real world,http://schema.org,NewsArticle,https://towardsdatascience.com/3-challenges-of-artificial-intelligence-in-computer-graphics-223e06bd846b,['https://miro.medium.com/v2/resize:fit:1200/1*krDCIwwB_YFiNHIVYEGlkA.jpeg'],"{'@type': 'Person', 'name': 'Mauro Comi', 'url': 'https://towardsdatascience.com/@mauro_ai'}","{'@type': 'Organization', 'name': 'Towards Data Science', 'url': 'towardsdatascience.com', 'logo': {'@type': 'ImageObject', 'width': 192, 'height': 60, 'url': 'https://miro.medium.com/v2/resize:fit:384/1*cFFKn8rFH4ZndmaYeAs6iQ.png'}}",3 Challenges of Artificial Intelligence in Computer graphics,2018-11-15T20:40:58.674Z,2018-11-15T22:19:17.983Z,,3 Challenges of Artificial Intelligence in Computer graphics,,,N/A,N/A,"3 Challenges of Artificial Intelligence in Computer graphicsReducing the gap between Virtual and Real worldMauro Comi·FollowPublished inTowards Data Science·6 min read·Nov 15, 201872ListenShareComputer graphics is not only stunning Visual effects. It gives us the tool to reconstruct the physical world for different kind of purposes, such as industrial or medical applications.The Graphics Turing Test in this field was passed a long time ago, as the current state of Photography and Cinematography proves. Reality and Fantasy are virtually indistinguishable, and animators can create photorealistic worlds in a matter of days.Graphics Turing Test: The subject views and interacts with a real or computer generated scene. The test is passed if the subject can not determine reality from simulated reality better than a random guess.A considerable help in this topic comes from the very recent advances in AI. Ian Goodfellow introduced a few years ago a new type of Neural Network Architecture called Generative Adversarial Network (GAN), able to push the boundaries of what is possible in Computer graphics. The applications are endless, but as Stan Lee taught us, great powers come with great responsibilities. Indeed generating images and videos indistinguishable from reality might pose a real threat to our Society, if not properly controlled. Watch this video of a Fake Obama generated with AI to understand what I mean.Deep Fake Obama-Generative Adversarial NetworkEmergent field of artificial intelligence for Fluid simulationsSimulating the dynamics of a fluid has always been a huge mathematical challenge. State-of-the-art techniques require massive computing power, even more for real-time simulations. First, it’s important to remind that the term fluid includes liquids, gases and plasmas. Smoke, wind, water, fire are some examples of fluids.If you are familiar with Computational Fluid Dynamics (CFD), you have encountered Navier-Stokes equations multiple times on your path. These partial differential equations relate the velocity, pressure, temperature and density of a moving fluid, extending the previous simpler models introducing the effect of viscosity. We are not going to describe them since they are very complex and not suitable without a proper background in fluid mechanics, but if you’re interested in them you can check the link at the end of the article.Finding the solutions of these equations is not suitable for two reasons: they are data-dependent, so methods are truncated to fit within a computational budget, and might exhibit really slow asymptotic convergence.To solve these issues, machine learning has recentrly introduceda novel way to model fluid dynamics environments as a supervised regression problem. Neural networks and Regression forests gave good results, but they require a dataset of solutions provided by an exact solver. This is not possible during the test on simulations since the initial frames are generated by the model itself (which is not an exact solver). Results are imperfect, and the applications of this model get limited.A new exciting approach to describe the physical world involves the concept of intuitive or naïve physics. This new field of AI concerns the ability to predict outcomes of physical interactions involving macroscopic objects based on experience. Let’s see an example: babies do not know what gravity is, but through experience, they own the concept of it. The idea is to create artificial neural models able to model intuitive physics. Into this camp falls the work of Tompson et al. (link at the end), who developed a novel and tailored ConvNet architecture to replace existing Eulerian-based solvers and introduced a new training-loss which guarantees faster convergence.Nonlinear Soft-tissue dynamics: model the skinObtaining realistic human body modelling is one of the main goals in computer graphics. The surface of the body needs to deform naturally. In real-time applications, the body is controlled by skeletal movements. For this reason, the surface is linked to its skeletal pose and the animation of the skin is a function of it. How is this achieved?The most common algorithm is Linear blend skinning (LBS), used also by Unity and Unreal. It is not computationally intensive and gives good results in the majority of situations. Still, there are some drawbacks. The main issue is the excessive loss of volume for strong bends or unrealistic effects during rotation of joints.A second important strategy is data-driven body models. This model divides the parameters for pose and shape and permits a deeper control of the body. The main problem here is the absence of control for non-linear dynamics. What do I mean with non-linear dynamics? Soft tissues, like the belly of a person, do not react in a linear way to forces. It bounces, rotates, jiggles. These movements cannot be properly described by non-linear models.To solve this problem, data-driven dynamics models were introduced. These algorithms are typically used for facial movements or cloth dynamics. A very interesting application of this last category was applied recently by Casas and Otaduy (link of the paper at the end). They successfully developed a data-driven model to simulate non-linear soft-tissue dynamics trained with publicly available skeletal motion data. As every machine learning model, also this novel methodology cannot properly describe deformations far from the training set. The only way to overcome this limitation, when necessary, is to adopt physics-based body models. These models are extremely precise but significantly more complex.Cloth and Material understanding in Computer GraphicsA third great challenge is the simulation of cloth. Cloth material recovery methods need complex experimental set up to acquire physical properties. Reproducing the correct behaviour of fabric might be extremely complicated, and researchers are applying machine learning methods to recreate the dynamics of it. Understanding properties of cloth do not find its application only in Animations, but also in totally different fields. Let’s consider a virtual try-on system for clothing: capturing the dynamics of this material can open to a new broad range of services offered in e-commerce. This is especially true if we consider the advances in Virtual Reality, which would allow a more realistic virtual shopping giving us the possibility to try clothes virtually.Retrieving physical properties of a dynamical system is achieved in multiple ways at the moment. The most straightforward approach is through measurements, sampling physical quantities and estimating dependent properties. A second important strategy is to simulate, optimize and iterate the dynamical system, extracting features progressively closer to the real value.ConclusionWe have just seen three challenges of AI in Computer Graphics. In the reproduction of the physical world, these three categories are blended: think of a flag moved by the wind. In this case, all three concepts are present in a single event.There is, of course, a whole set of unsolved problems, and I am going to analyse them in a following article. If you enjoyed this article, I’d love it if you hit the clap button 👏 so others might stumble upon it. For any comments or suggestions don’t hesitate to leave a comment!To check more of my Stories:Is Artificial Intelligence Racist? (And Other Concerns)How to teach an AI to play Games: Deep Reinforcement LearningArtificial Intelligence meets Art: Neural Transfer StyleI am a Data Science major in love with Machine learning and its endless applications. You can find more about me and my projects at maurocomi.com. You can also find me on Linkedin, Twitter, or email me directly. I am always up for a chat, or to collaborate on new amazing projects.Referenceshttps://www.grc.nasa.gov/www/k-12/airplane/nseqs.htmlYang, S., Liang, J., & Lin, M. C. (2017, October). Learning-based cloth material recovery from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4383–4393).CASAS, D., & OTADUY, M. A. (2018). Learning Nonlinear Soft-Tissue Dynamics for Interactive Avatars.Tompson, J., Schlachter, K., Sprechmann, P., & Perlin, K. (2016). Accelerating eulerian fluid simulation with convolutional networks. arXiv preprint arXiv:1607.03597.",https://towardsdatascience.com/3-challenges-of-artificial-intelligence-in-computer-graphics-223e06bd846b,2018-11-15T20:40:58.674Z,,,,,,,,223e06bd846b,['Mauro Comi'],,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibmh0dHBzOi8vd3d3LnRlY2hmdW5uZWwuY29tL2hyLXRlY2gvZmFjaWFsLWV4cHJlc3Npb24tZGV0ZWN0aW9uLXVzaW5nLWFpLWhvdy1pdC1jYW4taW1wcm92ZS10aGUtaGlyaW5nLXByb2Nlc3Mv0gEA?oc=5,Facial Expression Detection Using AI: How It Can Improve the Hiring Process - TechFunnel,2018-11-13,TechFunnel,https://www.techfunnel.com,"AI is already replacing parts of the interview process, resume screening but facial expression detection could add a whole new dimension to hiring process.",N/A,"AI is already replacing parts of the interview process, resume screening but facial expression detection could add a whole new dimension to hiring process.",N/A,https://schema.org/,WebSite,https://www.techfunnel.com/,,,,,,,,TechFunnel,,,N/A,N/A,N/A,,,,,,,,,"[{'@type': 'Article', '@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/#article', 'isPartOf': {'@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/'}, 'author': {'name': 'Danni White', '@id': 'https://www.techfunnel.com/#/schema/person/55746dbe674b75c6397c423dcf47539e'}, 'headline': 'Facial Expression Detection Using AI: How It Can Improve the Hiring Process', 'datePublished': '2018-11-13T15:41:44+00:00', 'dateModified': '2018-11-13T15:41:44+00:00', 'mainEntityOfPage': {'@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/'}, 'wordCount': 882, 'publisher': {'@id': 'https://www.techfunnel.com/#organization'}, 'image': {'@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/#primaryimage'}, 'thumbnailUrl': 'https://www.techfunnel.com/wp-content/uploads/2018/11/Facial-Expression-Detection-Using-AI_-How-It-Can-Improve-the-Hiring-Process.jpg', 'keywords': ['Image Recognition'], 'articleSection': ['HR Tech'], 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/', 'url': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/', 'name': 'Facial Expression Detection Using AI: How It Can Improve the Hiring Process', 'isPartOf': {'@id': 'https://www.techfunnel.com/#website'}, 'primaryImageOfPage': {'@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/#primaryimage'}, 'image': {'@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/#primaryimage'}, 'thumbnailUrl': 'https://www.techfunnel.com/wp-content/uploads/2018/11/Facial-Expression-Detection-Using-AI_-How-It-Can-Improve-the-Hiring-Process.jpg', 'datePublished': '2018-11-13T15:41:44+00:00', 'dateModified': '2018-11-13T15:41:44+00:00', 'description': 'AI is already replacing parts of the interview process, resume screening but facial expression detection could add a whole new dimension to hiring process.', 'breadcrumb': {'@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/#primaryimage', 'url': 'https://www.techfunnel.com/wp-content/uploads/2018/11/Facial-Expression-Detection-Using-AI_-How-It-Can-Improve-the-Hiring-Process.jpg', 'contentUrl': 'https://www.techfunnel.com/wp-content/uploads/2018/11/Facial-Expression-Detection-Using-AI_-How-It-Can-Improve-the-Hiring-Process.jpg', 'width': 769, 'height': 445, 'caption': 'Facial Expression Detection Using AI: How It Can Improve the Hiring Process'}, {'@type': 'BreadcrumbList', '@id': 'https://www.techfunnel.com/hr-tech/facial-expression-detection-using-ai-how-it-can-improve-the-hiring-process/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.techfunnel.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Blog', 'item': 'https://www.techfunnel.com/blog/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Facial Expression Detection Using AI: How It Can Improve the Hiring Process'}]}, {'@type': 'WebSite', '@id': 'https://www.techfunnel.com/#website', 'url': 'https://www.techfunnel.com/', 'name': 'Techfunnel', 'description': '', 'publisher': {'@id': 'https://www.techfunnel.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.techfunnel.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.techfunnel.com/#organization', 'name': 'Techfunnel', 'url': 'https://www.techfunnel.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.techfunnel.com/#/schema/logo/image/', 'url': 'https://www.techfunnel.com/wp-content/uploads/2023/11/TF_NEWLOGOWHITE_New_logo.png', 'contentUrl': 'https://www.techfunnel.com/wp-content/uploads/2023/11/TF_NEWLOGOWHITE_New_logo.png', 'width': 256, 'height': 91, 'caption': 'Techfunnel'}, 'image': {'@id': 'https://www.techfunnel.com/#/schema/logo/image/'}}, {'@type': 'Person', '@id': 'https://www.techfunnel.com/#/schema/person/55746dbe674b75c6397c423dcf47539e', 'name': 'Danni White', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.techfunnel.com/#/schema/person/image/', 'url': 'https://www.techfunnel.com/wp-content/uploads/2024/01/Danni_image-150x150.jpg', 'contentUrl': 'https://www.techfunnel.com/wp-content/uploads/2024/01/Danni_image-150x150.jpg', 'caption': 'Danni White'}, 'description': 'Danni White is the CEO of DW Creative Consulting Agency, a digital marketing firm specializing in elevating the visibility of small-to-midsize businesses and nonprofits. She is the author of 17 books and hosts the #Hashtags and Habits Podcast, which merges digital marketing, entrepreneurship, and personal growth.', 'url': 'https://www.techfunnel.com/author/daniella/'}]",,,,,,,,,,,,,,,,"{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.techfunnel.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}"
https://news.google.com/rss/articles/CBMiSGh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9mZWktZmVpLWxpLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWh1bWFuaXR5L9IBAA?oc=5,How Fei-Fei Li Will Make Artificial Intelligence Better for Humanity - WIRED,2018-11-13,WIRED,https://www.wired.com,AI has a problem: The biases of its creators are getting hard-coded into its future. Fei-Fei Li has a plan to fix that—by rebooting the field she helped invent.,"['business', 'the big story', 'ai hub', 'content moderation', 'ethics', 'google', 'alphabet', 'research', 'images', 'machine learning', 'machine vision', 'neural network', 'magazine-26.12', 'longreads', 'artificial intelligence', 'ai', '_no-homepage', '_syndication_noshow', 'magazine']",Artificial intelligence has a problem: The biases of its creators are getting hard-coded into its future. Fei-Fei Li has a plan to fix that—by rebooting the field she helped invent.,Artificial intelligence has a problem: The biases of its creators are getting hard-coded into its future. Fei-Fei Li has a plan to fix that—by rebooting the field she helped invent.,https://schema.org/,BreadcrumbList,https://www.wired.com/story/fei-fei-li-artificial-intelligence-humanity/,"['https://media.wired.com/photos/5bd8ac3e66f3912cf4bdb747/16:9/w_2400,h_1350,c_limit/WI120118_AI_FeiFei_01.jpg', 'https://media.wired.com/photos/5bd8ac3e66f3912cf4bdb747/4:3/w_1944,h_1458,c_limit/WI120118_AI_FeiFei_01.jpg', 'https://media.wired.com/photos/5bd8ac3e66f3912cf4bdb747/1:1/w_730,h_730,c_limit/WI120118_AI_FeiFei_01.jpg']","[{'@type': 'Person', 'name': 'Jessi Hempel', 'sameAs': 'https://www.wired.com/author/jessi-hempel/'}]","{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}",How Fei-Fei Li Will Make Artificial Intelligence Better for Humanity,2018-11-13T06:00:00.000-05:00,2018-11-13T06:00:00.000-05:00,business,,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'Business', 'item': 'https://www.wired.com/business/'}, {'@type': 'ListItem', 'position': 2, 'name': 'magazine-26.12', 'item': 'https://www.wired.com/tag/magazine-2612/'}, {'@type': 'ListItem', 'position': 3, 'name': ""Fei-Fei Li's Quest to Make AI Better for Humanity""}]",tags,N/A,"Jessi HempelBusinessNov 13, 2018 6:00 AMFei-Fei Li's Quest to Make AI Better for HumanityArtificial intelligence has a problem: The biases of its creators are getting hard-coded into its future. Fei-Fei Li has a plan to fix that—by rebooting the field she helped invent.Artificial intelligence has a problem: The biases of its creators are getting hard-coded into its future. Fei-Fei Li has a plan to fix that—by rebooting the field she helped invent.Christie Hemm KlokSave this storySaveSave this storySaveThe AI Database →ApplicationContent moderationEthicsCompanyGoogleAlphabetSectorResearchSource DataImagesTechnologyMachine learningMachine visionNeural NetworkSometime around 1 am on a warm night last June, Fei-Fei Li was sitting in her pajamas in a Washington, DC, hotel room, practicing a speech she would give in a few hours. Before going to bed, Li cut a full paragraph from her notes to be sure she could reach her most important points in the short time allotted. When she woke up, the 5'3"" expert in artificial intelligence put on boots and a black and navy knit dress, a departure from her frequent uniform of a T-shirt and jeans. Then she took an Uber to the Rayburn House Office Building, just south of the US Capitol.Before entering the chambers of the US House Committee on Science, Space, and Technology, she lifted her phone to snap a photo of the oversize wooden doors. (“As a scientist, I feel special about the committee,” she said.) Then she stepped inside the cavernous room and walked to the witness table.AdChoicesADVERTISEMENTThe hearing that morning, titled “Artificial Intelligence—With Great Power Comes Great Responsibility,” included Timothy Persons, chief scientist of the Government Accountability Office, and Greg Brockman, cofounder and chief technology officer of the nonprofit ­OpenAI. But only Li, the sole woman at the table, could lay claim to a groundbreaking accomplishment in the field of AI. As the researcher who built ImageNet, a database that helps computers recognize images, she’s one of a tiny group of scientists—a group perhaps small enough to fit around a kitchen table—who are responsible for AI’s recent remarkable advances.Trending NowWIRED25: Kai-Fu Lee and Fei Fei Li On What's Next for Artificial IntelligenceThat June, Li was serving as the chief AI scientist at Google Cloud and was on leave from her position as director of the Stanford Artificial Intelligence Lab. But she was appearing in front of the committee because she was also the cofounder of a nonprofit focused on recruiting women and people of color to become builders of artificial intelligence.It was no surprise that the legislators sought her expertise that day. What was surprising was the content of her talk: the grave dangers brought on by the field she so loved.December 2018. Subscribe to WIRED.
Illustration: Axis of StrengthThe time between an invention and its impact can be short. With the help of artificial intelligence tools like ImageNet, a computer can be taught to learn a specific task and then act far faster than a person ever could. As this technology becomes more sophisticated, it’s being deputized to filter, sort, and analyze data and make decisions of global and social consequence. Though these tools have been around, in some way or another, for more than 60 years, in the past decade we’ve started using them for tasks that change the trajectory of human lives: Today artificial intelligence helps determine which treatments get used on people with illnesses, who qualifies for life insurance, how much prison time a person serves, which job applicants get interviews.Those powers, of course, can be dangerous. Amazon had to ditch AI recruiting software that learned to penalize résumés that included the word “women.” And who can forget Google’s 2015 fiasco, when its photo identification software mislabeled black people as gorillas, or Microsoft’s AI-powered social chatbot that started tweeting racial slurs. But those are problems that can be explained and therefore reversed. In the pretty near future, Li believes, we will hit a moment when it will be impossible to course-correct. That’s because the technology is being adopted so fast, and far and wide.SIGN UP TODAYGet the Backchannel newsletter for the best features and investigations on WIRED.Li was testifying in the Rayburn building that morning because she is adamant her field needs a recalibration. Prominent, powerful, and mostly male tech leaders have been warning about a future in which artificial-intelligence-driven technology becomes an existential threat to humans. But Li thinks those fears are given too much weight and attention. She is focused on a less melodramatic but more consequential question: how AI will affect the way people work and live. It’s bound to alter the human experience—and not necessarily for the better. “We have time,” Li says, “but we have to act now.” If we make fundamental changes to how AI is engineered—and who engineers it—the technology, Li argues, will be a transformative force for good. If not, we are leaving a lot of humanity out of the equation.At the hearing, Li was the last to speak. With no evidence of the nerves that drove her late-night practice, she began. “There’s nothing artificial about AI.” Her voice picked up momentum. “It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.” Around her, faces brightened. The woman who kept attendance agreed audibly, with an “mm-hmm.”JackRabbot 1, a Segway platform mobile robot, at Stanford University's AI Lab.Christie Hemm KlokFei-Fei Li grew up in Chengdu, an industrial city in southern China. She was a lonely, brainy kid, as well as an avid reader. Her family was always a bit unusual: In a culture that didn’t prize pets, her father brought her a puppy. Her mother, who had come from an intellectual family, encouraged her to read Jane Eyre. (“Emily is my favorite Brontë,” Li says. “Wuthering Heights.”) When Li was 12, her father emigrated to Parsippany, New Jersey, and she and her mother didn’t see him for several years. They joined him when she was 16. On her second day in America, Li’s father took her to a gas station and asked her to tell the mechanic to fix his car. She spoke little English, but through gestures Li figured out how to explain the problem. Within two years, Li had learned enough of the language to serve as a translator, interpreter, and advocate for her mother and father, who had learned only the most basic English. “I had to become the mouth and ears of my parents,” she says.She was also doing very well in school. Her father, who loved to scour garage sales, found her a scientific calculator, which she used in math class until a teacher, sizing up her mistaken calculations, figured out that it had a broken function key. Li credits another high school math instructor, Bob Sabella, for helping her navigate her academic life and her new American identity. Parsippany High School didn’t have an advanced calculus class, so he concocted an ad hoc version and taught Li during lunch breaks. Sabella and his wife also included her in their family, bringing her on a Disney vacation and lending her $20,000 to open a dry-cleaning business for her parents to run. In 1995, she earned a scholarship to study at Prince­ton. While there, she traveled home nearly every weekend to help run the family business.At college, Li’s interests were expansive. She majored in physics and studied computer science and engineering. In 2000, she began her doctorate at Caltech in Pasadena, working at the intersection of neuroscience and computer science.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDHer ability to see and foster connections between seemingly dissimilar fields is what led Li to think up ImageNet. Her computer-vision peers were working on models to help computers perceive and decode images, but those models were limited in scope: A researcher might write one algorithm to identify dogs and another to identify cats. Li began to wonder if the problem wasn’t the model but the data. She thought that, if a child learns to see by experiencing the visual world­—by observing countless objects and scenes in her early years—maybe a computer can learn in a similar way, by analyzing a wide variety of images and the relationships between them. The realization was a big one for Li. “It was a way to organize the whole visual concept of the world,” she says.AdvertisementBut she had trouble convincing her colleagues that it was rational to undertake the gargantuan task of tagging every possible picture of every object in one gigantic database. What’s more, Li had decided that for the idea to work, the labels would need to range from the general (“mammal”) to the highly specific (“star-nosed mole”). When Li, who had moved back to Princeton to take a job as an assistant professor in 2007, talked up her idea for ImageNet, she had a hard time getting faculty members to help out. Finally, a professor who specialized in computer architecture agreed to join her as a collaborator.Her next challenge was to get the giant thing built. That meant a lot of people would have to spend a lot of hours doing the tedious work of tagging photos. Li tried paying Princeton students $10 an hour, but progress was slow going. Then a student asked her if she’d heard of Amazon Mechanical Turk. Suddenly she could corral many workers, at a fraction of the cost. But expanding a workforce from a handful of Princeton students to tens of thousands of invisible Turkers had its own challenges. Li had to factor in the workers’ likely biases. “Online workers, their goal is to make money the easiest way, right?” she says. “If you ask them to select panda bears from 100 images, what stops them from just clicking everything?” So she embedded and tracked certain images—such as pictures of golden retrievers that had already been correctly identified as dogs—to serve as a control group. If the Turks labeled these images properly, they were working honestly.In 2009, Li’s team felt that the massive set—3.2 million images—was comprehensive enough to use, and they published a paper on it, along with the database. (It later grew to 15 million.) At first the project got little attention. But then the team had an idea: They reached out to the organizers of a computer-vision competition taking place the following year in Europe and asked them to allow competitors to use the Image­Net database to train their algorithms. This became the ImageNet Large Scale Visual Recognition Challenge.Around the same time, Li joined Stanford as an assistant professor. She was, by then, married to Silvio Savarese, a roboticist. But he had a job at the University of Michigan, and the distance was tough. “We knew Silicon Valley would be easier for us to solve our two-body problem,” Li says. (Savarese joined Stanford’s faculty in 2013.) “Also, Stanford is special because it’s one of the birthplaces of AI.”The A.I. IssueThe A.I. IssueHow to Teach Artificial Intelligence Some Common SenseClive ThompsonThe A.I. IssueThe DIY Tinkerers Harnessing the Power of AITom SimoniteThe A.I. IssueThe Genius Neuroscientist Who Might Hold the Key to True AIShaun RavivIn 2012, University of Toronto researcher Geoffrey Hinton entered the ImageNet competition, using the database to train a type of AI known as a deep neural network. It turned out to be far more accurate than anything that had come before—and he won. Li hadn’t planned to go see Hinton get his award; she was on maternity leave, and the ceremony was happening in Florence, Italy. But she recognized that history was being made. So she bought a last-minute ticket and crammed herself into a middle seat for an overnight flight. Hinton’s ImageNet-­powered neural network changed everything. By 2017, the final year of the competition, the error rate for computers identifying objects in images had been reduced to less than 3 percent, from 15 percent in 2012. Computers, at least by one measure, had become better at seeing than humans.ImageNet enabled deep learning to go big—it’s at the root of recent advances in self-driving cars, facial recognition, phone cameras that can identify objects (and tell you if they’re for sale).Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDNot long after Hinton accepted his prize, while Li was still on maternity leave, she started to think a lot about how few of her peers were women. At that moment she felt this acutely; she saw how the disparity was increasingly going to be a problem. Most scientists building AI algorithms were men, and often men of a similar background. They had a particular worldview that bled into the projects they pursued and even the dangers they envisioned. Many of AI’s creators had been boys with sci-fi dreams, thinking up scenarios from The Terminator and Blade Runner. There’s nothing wrong with worrying about such things, Li thought. But those ideas betrayed a narrow view of the possible dangers of AI.Deep learning systems are, as Li says, “bias in, bias out.” Li recognized that while the algorithms that drive artificial intelligence may appear to be neutral, the data and applications that shape the outcomes of those algorithms are not. What mattered were the people building it and why they were building it. Without a diverse group of engineers, Li pointed out that day on Capitol Hill, we could have biased algorithms making unfair loan application decisions, or training a neural network only on white faces—creating a model that would perform poorly on black ones. “I think if we wake up 20 years from now and we see the lack of diversity in our tech and leaders and practitioners, that would be my doomsday scenario,” she said.It was critical, Li came to believe, to focus the development of AI on helping the human experience. One of her projects at Stanford was a partnership with the medical school to bring AI to the ICU in an effort to cut down on problems like hospital-­acquired infections. It involved developing a camera system that could monitor a hand-washing station and alert hospital workers if they forgot to scrub properly. This type of interdisciplinary collaboration was unusual. “No one else from computer science reached out to me,” says Arnold Milstein, a professor of medicine who directs Stanford’s Clinical Excellence Research Center.That work gave Li hope for how AI could evolve. It could be built to complement people’s skills rather than simply replace them. If engineers would engage with people in other disciplines (even people in the real world!), they could make tools that expand human capacity, like automating time-­consuming tasks to allow ICU nurses to spend more time with patients, rather than building AI, say, to automate someone’s shopping experience and eliminate a cashier’s job.Considering that AI was developing at warp speed, Li figured her team needed to change the roster—as fast as possible.Fei-Fei Li in the Artificial Intelligence Lab at Stanford University.Christie Hemm KlokLi has always been drawn to math, so she recognizes that getting women and people of color into computer science requires a colossal effort. According to the National Science Foundation, in 2000, women earned 28 percent of bachelor’s degrees in computer science. In 2015 that figure was 18 percent. Even in her own lab, Li struggles to recruit underrepresented people of color and women. Though historically more diverse than your typical AI lab, it remains predominantly male, she says. “We still do not have enough women, and especially underrepresented minorities, even in the pipeline coming into the lab,” she says. “Students go to an AI conference and they see 90 percent people of the same gender. And they don’t see African Americans nearly as much as white boys.”Olga Russakovsky had almost written off the field when Li became her adviser. Russakovsky was already an accomplished computer scientist—with an undergraduate degree in math and a master’s in computer science, both from Stanford—but her dissertation work was dragging. She felt disconnected from her peers as the only woman in her lab. Things changed when Li arrived at Stanford. Li helped Russakovsky learn some skills required for successful research, “but also she helped build up my self-confidence,” says Russakovsky, who is now an assistant professor in computer science at Princeton.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDFour years ago, as Russakovsky was finishing up her PhD, she asked Li to help her create a summer camp to get girls interested in AI. Li agreed at once, and they pulled volunteers together and posted a call for high school sophomores. Within a month, they had 200 applications for 24 spots. Two years later they expanded the program, launching the nonprofit AI4All to bring underrepresented youth—including girls, people of color, and people from economically disadvantaged backgrounds—to the campuses of Stanford and UC Berkeley.AI4All is on the verge of growing out of its tiny shared office at the Kapor Center in downtown Oakland, California. It now has camps at six college campuses. (Last year there were 900 applications for 20 spots at the newly launched Carnegie Mellon camp.) One AI4All student worked on detecting eye diseases using computer vision. Another used AI to write a program ranking the urgency of 911 calls; her grandmother had died because an ambulance didn’t reach her in time. Confirmation, it would seem, that personal perspective makes a difference for the future of AI tools.The case for Toyota’s Human Support Robot at Stanford University's AI Lab.Christie Hemm KlokAfter three years running the AI Lab at Stanford, Li took a leave in 2016 to join Google as chief scientist for AI of Google Cloud, the company’s enterprise computing business. Li wanted to understand how industry worked and to see if access to customers anxious to deploy new tools would shift the scope of her own cross-­disciplinary research. Companies like Facebook, Google, and Microsoft were throwing money into AI in search of ways to harness the technology for their businesses. And companies often have more and better data than universities. For an AI researcher, data is fuel.Initially the experience was enlivening. She met with companies that had real-world uses for her science. She led the rollout of public-facing AI tools that let anyone create machine learning algorithms without writing a single line of code. She opened a new lab in China and helped to shape AI tools to improve health care. She spoke at the World Economic Forum in Davos, rubbing elbows with heads of state and pop stars.But working in a private company came with new and uncomfortable pressures. Last spring, Li was caught up in Google’s very public drubbing over its Project Maven contract with the Defense Department. The program uses AI to interpret video images that could be used to target drone strikes; according to Google, it was “low-res object identification using AI” and “saving lives was the overarching intent.” Many employees, however, objected to the use of their work in military drones. About 4,000 of them signed a petition demanding “a clear policy stating that neither Google nor its contractors will ever build warfare technology.” Several workers resigned in protest.LEARN MOREThe WIRED Guide to Artificial IntelligenceThough Li hadn’t been involved directly with the deal, the division that she worked for was charged with administering Maven. And she became a public face of the controversy when emails she wrote that looked as if they were trying to help the company avoid embarrassment were leaked to The New York Times. Publicly this seemed confusing, as she was well known in the field as someone who embodied ethics. In truth, before the public outcry she had considered the technology to be “fairly innocuous”; she hadn’t considered that it could cause an employee revolt.But Li does recognize why the issue blew up: “It wasn’t exactly what the thing is. It’s about the moment—the collective sense of urgency for our responsibility, the emerging power of AI, the dialog that Silicon Valley needs to be in. Maven just became kind of a convergence point,” she says. “Don’t be evil” was no longer a strong enough stance.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDThe controversy subsided when Google announced it wouldn’t renew the Maven contract. A group of Google scientists and executives—including Li—also wrote (public) guidelines pledging that Google would focus its AI research on technology designed for social good, would avoid implementing bias into its tools, and would avoid technology that could end up facilitating harm to people. Li had been preparing to head back to Stanford, but she felt it was critical to see the guidelines through. “I think it’s important to recognize that every organization has to have a set of principles and responsible review processes. You know how Benjamin Franklin said, when the Constitution was rolled out, it might not be perfect but it’s the best we’ve got for now,” she says. “People will still have opinions, and different sides can continue the dialog.” But when the guidelines were published, she says, it was one of her happiest days of the year: “It was so important for me personally to be involved, to contribute.”In June, I visited Li at her home, a modest split-level in a cul-de-sac on the Stanford campus. It was just after 8 in the evening, and while we talked her husband put their young son and daughter through their bedtime routines upstairs. Her parents were home for the night in the in-law unit downstairs. The dining room had been turned into a playroom, so we sat in her living room. Family photos rested on every surface, including a broken 1930s-era telephone sitting on a shelf. “Immigrant parents!” she said when I ask her about it. Her father still likes to go to yard sales.As we talked, text messages started pinging on Li’s phone. Her parents were asking her to translate a doctor’s instructions for her mother’s medication. Li can be in a meeting at the Googleplex or speaking at the World Economic Forum or sitting in the green room before a congressional hearing and her parents will text her for a quick assist. She responds without breaking her train of thought.For much of Li’s life, she has been focused on two seemingly different things at the same time. She is a scientist who has thought deeply about art. She is an American who is Chinese. She is as obsessed with robots as she is with humans.Late in July, Li called me while she was packing for a family trip and helping her daughter wash her hands. “Did you see the announcement of Shannon Vallor?” she asks. Vallor is a philosopher at Santa Clara University whose research focuses on the philosophy and ethics of emerging science and technologies, and she had just signed on to work for Google Cloud as a consulting ethicist. Li had campaigned hard for this; she’d even quoted Vallor in her testimony in Washington, saying: “There are no independent machine values. Machine values are human values.” The appointment wasn’t without precedent. Other companies have also started to put guardrails on how their AI software can be used, and who can use it. Microsoft established an internal ethics board in 2016. The company says it has turned down business with potential customers owing to ethical concerns brought forward by the board. It’s also begun placing limits on how its AI tech can be used, such as forbidding some applications in facial recognition.Related StoriesWIRED@25An AI Pioneer, and the Researcher Bringing Humanity to AIMaria Streshinsky and Jessi HempelWired25Microsoft's Nadella Says AI Can Make the World More InclusiveTom SimoniteSocial SmartsHow Artificial Intelligence Can—and Can't—Fix FacebookTom SimoniteBut to speak on behalf of ethics from inside a corporation is, to some extent, to acknowledge that, while you can guard the henhouse, you are indeed a fox. When we talked in July, Li already knew she was leaving Google. Her two-year sabbatical was coming to an end. There was plenty of speculation about her stepping down after the Project Maven debacle. But she said the reason for her return to Stanford was that she didn’t want to forfeit her academic position. She also sounded tired. After a tumultuous summer at Google, the ethics guidelines she helped write were “the light at the end of the tunnel,” she says.And she was eager to start a new project at Stanford. This fall, she and John Etchemendy, the former Stanford provost, announced the creation of an academic center that will fuse the study of AI and humanity, blending hard science, design research, and interdisciplinary studies. “As a new science, AI never had a field-wide effort to engage humanists and social scientists,” she says. Those skill sets have long been viewed as inconsequential to the field of AI, but Li is adamant that they are key to its future.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDPoliticsFar-Right Extremists Call for Violence and War After Trump ShootingBy David GilbertPoliticsElon Musk ‘Fully Endorses’ Donald Trump After Deadly Rally ShootingBy Makena Kelly, WIREDLi is fundamentally optimistic. At the hearing in June, she told the legislators, “I think deeply about the jobs that are currently dangerous and harmful for humans, from fighting fires to search and rescue to natural disaster recovery.” She believes that we should not only avoid putting people in harm’s way when possible, but that these are often the very kind of jobs where technology can be a great help.There are limits, of course, to how much a single program at a single institution—even a prominent one—can shift an entire field. But Li is adamant she has to do what she can to train researchers to think like ethicists, who are guided by principle over profit, informed by a varied array of backgrounds.On the phone, I ask Li if she imagines there could have been a way to develop AI differently, without, perhaps, the problems we’ve seen so far. “I think it’s hard to imagine,” she says. “Scientific advances and innovation come really through generations of tedious work, trial and error. It took a while for us to recognize such bias. I only woke up six years ago and realized ‘Oh my God, we’re entering a crisis.’ ”On Capitol Hill, Li said, “As a scientist, I’m humbled by how nascent the science of AI is. It is the science of only 60 years. Compared to classic sciences that are making human life better every day—physics, chemistry, biology—there’s a long, long way to go for AI to realize its potential to help people.” She added, “With proper guidance AI will make life better. But without it, the technology stands to widen the wealth divide even further, make tech even more exclusive, and reinforce biases we’ve spent generations trying to overcome.” This is the time, Li would have us believe, between an invention and its impact.Hair and Makeup by Amy Lawson for Makeup ForeverJessi Hempel wrote about Uber CEO Dara Khosrowshahi in issue 26.05. Additional reporting by Gregory Barber.This article appears in the December issue. Subscribe now.Listen to this story, and other WIRED features, on the Audm app.Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.Enter your email to get the Wired newsletterclose dialogRecommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyPlease enter abovesign upUsed consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.Recommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyYou're signed up!Used consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.close dialogMore Great WIRED StoriesThe DIY tinkerers harnessing the power of AIThe ‘pink tax’ and how women spend more on NYC transitPHOTOS: The secret tools magicians use to fool youThe Butterball Turkey Talk-Line gets new trimmingsAn aging marathoner tries to run fast after 40Hungry for even more deep dives on your next favorite topic? Sign up for the Backchannel newsletter","{'@type': 'WebPage', '@id': 'https://www.wired.com/story/fei-fei-li-artificial-intelligence-humanity/'}",,,"Before entering the chambers of the US House Committee on Science, Space, and Technology, she lifted her phone to snap a photo of the oversize wooden doors. (“As a scientist, I feel special about the committee,” she said.) Then she stepped inside the cavernous room and walked to the witness table.
The hearing that morning, titled “Artificial Intelligence—With Great Power Comes Great Responsibility,” included Timothy Persons, chief scientist of the Government Accountability Office, and Greg Brockman, cofounder and chief technology officer of the nonprofit ­OpenAI. But only Li, the sole woman at the table, could lay claim to a groundbreaking accomplishment in the field of AI. As the researcher who built ImageNet, a database that helps computers recognize images, she’s one of a tiny group of scientists—a group perhaps small enough to fit around a kitchen table—who are responsible for AI’s recent remarkable advances.
That June, Li was serving as the chief AI scientist at Google Cloud and was on leave from her position as director of the Stanford Artificial Intelligence Lab. But she was appearing in front of the committee because she was also the cofounder of a nonprofit focused on recruiting women and people of color to become builders of artificial intelligence.
It was no surprise that the legislators sought her expertise that day. What was surprising was the content of her talk: the grave dangers brought on by the field she so loved.
The time between an invention and its impact can be short. With the help of artificial intelligence tools like ImageNet, a computer can be taught to learn a specific task and then act far faster than a person ever could. As this technology becomes more sophisticated, it’s being deputized to filter, sort, and analyze data and make decisions of global and social consequence. Though these tools have been around, in some way or another, for more than 60 years, in the past decade we’ve started using them for tasks that change the trajectory of human lives: Today artificial intelligence helps determine which treatments get used on people with illnesses, who qualifies for life insurance, how much prison time a person serves, which job applicants get interviews.
Those powers, of course, can be dangerous. Amazon had to ditch AI recruiting software that learned to penalize résumés that included the word “women.” And who can forget Google’s 2015 fiasco, when its photo identification software mislabeled black people as gorillas, or Microsoft’s AI-powered social chatbot that started tweeting racial slurs. But those are problems that can be explained and therefore reversed. In the pretty near future, Li believes, we will hit a moment when it will be impossible to course-correct. That’s because the technology is being adopted so fast, and far and wide.
Li was testifying in the Rayburn building that morning because she is adamant her field needs a recalibration. Prominent, powerful, and mostly male tech leaders have been warning about a future in which artificial-intelligence-driven technology becomes an existential threat to humans. But Li thinks those fears are given too much weight and attention. She is focused on a less melodramatic but more consequential question: how AI will affect the way people work and live. It’s bound to alter the human experience—and not necessarily for the better. “We have time,” Li says, “but we have to act now.” If we make fundamental changes to how AI is engineered—and who engineers it—the technology, Li argues, will be a transformative force for good. If not, we are leaving a lot of humanity out of the equation.
At the hearing, Li was the last to speak. With no evidence of the nerves that drove her late-night practice, she began. “There’s nothing artificial about AI.” Her voice picked up momentum. “It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.” Around her, faces brightened. The woman who kept attendance agreed audibly, with an “mm-hmm.”
She was also doing very well in school. Her father, who loved to scour garage sales, found her a scientific calculator, which she used in math class until a teacher, sizing up her mistaken calculations, figured out that it had a broken function key. Li credits another high school math instructor, Bob Sabella, for helping her navigate her academic life and her new American identity. Parsippany High School didn’t have an advanced calculus class, so he concocted an ad hoc version and taught Li during lunch breaks. Sabella and his wife also included her in their family, bringing her on a Disney vacation and lending her $20,000 to open a dry-cleaning business for her parents to run. In 1995, she earned a scholarship to study at Prince­ton. While there, she traveled home nearly every weekend to help run the family business.
At college, Li’s interests were expansive. She majored in physics and studied computer science and engineering. In 2000, she began her doctorate at Caltech in Pasadena, working at the intersection of neuroscience and computer science.
Her ability to see and foster connections between seemingly dissimilar fields is what led Li to think up ImageNet. Her computer-vision peers were working on models to help computers perceive and decode images, but those models were limited in scope: A researcher might write one algorithm to identify dogs and another to identify cats. Li began to wonder if the problem wasn’t the model but the data. She thought that, if a child learns to see by experiencing the visual world­—by observing countless objects and scenes in her early years—maybe a computer can learn in a similar way, by analyzing a wide variety of images and the relationships between them. The realization was a big one for Li. “It was a way to organize the whole visual concept of the world,” she says.
But she had trouble convincing her colleagues that it was rational to undertake the gargantuan task of tagging every possible picture of every object in one gigantic database. What’s more, Li had decided that for the idea to work, the labels would need to range from the general (“mammal”) to the highly specific (“star-nosed mole”). When Li, who had moved back to Princeton to take a job as an assistant professor in 2007, talked up her idea for ImageNet, she had a hard time getting faculty members to help out. Finally, a professor who specialized in computer architecture agreed to join her as a collaborator.
Her next challenge was to get the giant thing built. That meant a lot of people would have to spend a lot of hours doing the tedious work of tagging photos. Li tried paying Princeton students $10 an hour, but progress was slow going. Then a student asked her if she’d heard of Amazon Mechanical Turk. Suddenly she could corral many workers, at a fraction of the cost. But expanding a workforce from a handful of Princeton students to tens of thousands of invisible Turkers had its own challenges. Li had to factor in the workers’ likely biases. “Online workers, their goal is to make money the easiest way, right?” she says. “If you ask them to select panda bears from 100 images, what stops them from just clicking everything?” So she embedded and tracked certain images—such as pictures of golden retrievers that had already been correctly identified as dogs—to serve as a control group. If the Turks labeled these images properly, they were working honestly.
In 2009, Li’s team felt that the massive set—3.2 million images—was comprehensive enough to use, and they published a paper on it, along with the database. (It later grew to 15 million.) At first the project got little attention. But then the team had an idea: They reached out to the organizers of a computer-vision competition taking place the following year in Europe and asked them to allow competitors to use the Image­Net database to train their algorithms. This became the ImageNet Large Scale Visual Recognition Challenge.
Around the same time, Li joined Stanford as an assistant professor. She was, by then, married to Silvio Savarese, a roboticist. But he had a job at the University of Michigan, and the distance was tough. “We knew Silicon Valley would be easier for us to solve our two-body problem,” Li says. (Savarese joined Stanford’s faculty in 2013.) “Also, Stanford is special because it’s one of the birthplaces of AI.”
In 2012, University of Toronto researcher Geoffrey Hinton entered the ImageNet competition, using the database to train a type of AI known as a deep neural network. It turned out to be far more accurate than anything that had come before—and he won. Li hadn’t planned to go see Hinton get his award; she was on maternity leave, and the ceremony was happening in Florence, Italy. But she recognized that history was being made. So she bought a last-minute ticket and crammed herself into a middle seat for an overnight flight. Hinton’s ImageNet-­powered neural network changed everything. By 2017, the final year of the competition, the error rate for computers identifying objects in images had been reduced to less than 3 percent, from 15 percent in 2012. Computers, at least by one measure, had become better at seeing than humans.
ImageNet enabled deep learning to go big—it’s at the root of recent advances in self-driving cars, facial recognition, phone cameras that can identify objects (and tell you if they’re for sale).
Not long after Hinton accepted his prize, while Li was still on maternity leave, she started to think a lot about how few of her peers were women. At that moment she felt this acutely; she saw how the disparity was increasingly going to be a problem. Most scientists building AI algorithms were men, and often men of a similar background. They had a particular worldview that bled into the projects they pursued and even the dangers they envisioned. Many of AI’s creators had been boys with sci-fi dreams, thinking up scenarios from The Terminator and Blade Runner. There’s nothing wrong with worrying about such things, Li thought. But those ideas betrayed a narrow view of the possible dangers of AI.
Deep learning systems are, as Li says, “bias in, bias out.” Li recognized that while the algorithms that drive artificial intelligence may appear to be neutral, the data and applications that shape the outcomes of those algorithms are not. What mattered were the people building it and why they were building it. Without a diverse group of engineers, Li pointed out that day on Capitol Hill, we could have biased algorithms making unfair loan application decisions, or training a neural network only on white faces—creating a model that would perform poorly on black ones. “I think if we wake up 20 years from now and we see the lack of diversity in our tech and leaders and practitioners, that would be my doomsday scenario,” she said.
It was critical, Li came to believe, to focus the development of AI on helping the human experience. One of her projects at Stanford was a partnership with the medical school to bring AI to the ICU in an effort to cut down on problems like hospital-­acquired infections. It involved developing a camera system that could monitor a hand-washing station and alert hospital workers if they forgot to scrub properly. This type of interdisciplinary collaboration was unusual. “No one else from computer science reached out to me,” says Arnold Milstein, a professor of medicine who directs Stanford’s Clinical Excellence Research Center.
That work gave Li hope for how AI could evolve. It could be built to complement people’s skills rather than simply replace them. If engineers would engage with people in other disciplines (even people in the real world!), they could make tools that expand human capacity, like automating time-­consuming tasks to allow ICU nurses to spend more time with patients, rather than building AI, say, to automate someone’s shopping experience and eliminate a cashier’s job.
Considering that AI was developing at warp speed, Li figured her team needed to change the roster—as fast as possible.
Olga Russakovsky had almost written off the field when Li became her adviser. Russakovsky was already an accomplished computer scientist—with an undergraduate degree in math and a master’s in computer science, both from Stanford—but her dissertation work was dragging. She felt disconnected from her peers as the only woman in her lab. Things changed when Li arrived at Stanford. Li helped Russakovsky learn some skills required for successful research, “but also she helped build up my self-confidence,” says Russakovsky, who is now an assistant professor in computer science at Princeton.
Four years ago, as Russakovsky was finishing up her PhD, she asked Li to help her create a summer camp to get girls interested in AI. Li agreed at once, and they pulled volunteers together and posted a call for high school sophomores. Within a month, they had 200 applications for 24 spots. Two years later they expanded the program, launching the nonprofit AI4All to bring underrepresented youth—including girls, people of color, and people from economically disadvantaged backgrounds—to the campuses of Stanford and UC Berkeley.
AI4All is on the verge of growing out of its tiny shared office at the Kapor Center in downtown Oakland, California. It now has camps at six college campuses. (Last year there were 900 applications for 20 spots at the newly launched Carnegie Mellon camp.) One AI4All student worked on detecting eye diseases using computer vision. Another used AI to write a program ranking the urgency of 911 calls; her grandmother had died because an ambulance didn’t reach her in time. Confirmation, it would seem, that personal perspective makes a difference for the future of AI tools.
Initially the experience was enlivening. She met with companies that had real-world uses for her science. She led the rollout of public-facing AI tools that let anyone create machine learning algorithms without writing a single line of code. She opened a new lab in China and helped to shape AI tools to improve health care. She spoke at the World Economic Forum in Davos, rubbing elbows with heads of state and pop stars.
But working in a private company came with new and uncomfortable pressures. Last spring, Li was caught up in Google’s very public drubbing over its Project Maven contract with the Defense Department. The program uses AI to interpret video images that could be used to target drone strikes; according to Google, it was “low-res object identification using AI” and “saving lives was the overarching intent.” Many employees, however, objected to the use of their work in military drones. About 4,000 of them signed a petition demanding “a clear policy stating that neither Google nor its contractors will ever build warfare technology.” Several workers resigned in protest.
Though Li hadn’t been involved directly with the deal, the division that she worked for was charged with administering Maven. And she became a public face of the controversy when emails she wrote that looked as if they were trying to help the company avoid embarrassment were leaked to The New York Times. Publicly this seemed confusing, as she was well known in the field as someone who embodied ethics. In truth, before the public outcry she had considered the technology to be “fairly innocuous”; she hadn’t considered that it could cause an employee revolt.
But Li does recognize why the issue blew up: “It wasn’t exactly what the thing is. It’s about the moment—the collective sense of urgency for our responsibility, the emerging power of AI, the dialog that Silicon Valley needs to be in. Maven just became kind of a convergence point,” she says. “Don’t be evil” was no longer a strong enough stance.
The controversy subsided when Google announced it wouldn’t renew the Maven contract. A group of Google scientists and executives—including Li—also wrote (public) guidelines pledging that Google would focus its AI research on technology designed for social good, would avoid implementing bias into its tools, and would avoid technology that could end up facilitating harm to people. Li had been preparing to head back to Stanford, but she felt it was critical to see the guidelines through. “I think it’s important to recognize that every organization has to have a set of principles and responsible review processes. You know how Benjamin Franklin said, when the Constitution was rolled out, it might not be perfect but it’s the best we’ve got for now,” she says. “People will still have opinions, and different sides can continue the dialog.” But when the guidelines were published, she says, it was one of her happiest days of the year: “It was so important for me personally to be involved, to contribute.”
As we talked, text messages started pinging on Li’s phone. Her parents were asking her to translate a doctor’s instructions for her mother’s medication. Li can be in a meeting at the Googleplex or speaking at the World Economic Forum or sitting in the green room before a congressional hearing and her parents will text her for a quick assist. She responds without breaking her train of thought.
For much of Li’s life, she has been focused on two seemingly different things at the same time. She is a scientist who has thought deeply about art. She is an American who is Chinese. She is as obsessed with robots as she is with humans.
Late in July, Li called me while she was packing for a family trip and helping her daughter wash her hands. “Did you see the announcement of Shannon Vallor?” she asks. Vallor is a philosopher at Santa Clara University whose research focuses on the philosophy and ethics of emerging science and technologies, and she had just signed on to work for Google Cloud as a consulting ethicist. Li had campaigned hard for this; she’d even quoted Vallor in her testimony in Washington, saying: “There are no independent machine values. Machine values are human values.” The appointment wasn’t without precedent. Other companies have also started to put guardrails on how their AI software can be used, and who can use it. Microsoft established an internal ethics board in 2016. The company says it has turned down business with potential customers owing to ethical concerns brought forward by the board. It’s also begun placing limits on how its AI tech can be used, such as forbidding some applications in facial recognition.
But to speak on behalf of ethics from inside a corporation is, to some extent, to acknowledge that, while you can guard the henhouse, you are indeed a fox. When we talked in July, Li already knew she was leaving Google. Her two-year sabbatical was coming to an end. There was plenty of speculation about her stepping down after the Project Maven debacle. But she said the reason for her return to Stanford was that she didn’t want to forfeit her academic position. She also sounded tired. After a tumultuous summer at Google, the ethics guidelines she helped write were “the light at the end of the tunnel,” she says.
And she was eager to start a new project at Stanford. This fall, she and John Etchemendy, the former Stanford provost, announced the creation of an academic center that will fuse the study of AI and humanity, blending hard science, design research, and interdisciplinary studies. “As a new science, AI never had a field-wide effort to engage humanists and social scientists,” she says. Those skill sets have long been viewed as inconsequential to the field of AI, but Li is adamant that they are key to its future.
Li is fundamentally optimistic. At the hearing in June, she told the legislators, “I think deeply about the jobs that are currently dangerous and harmful for humans, from fighting fires to search and rescue to natural disaster recovery.” She believes that we should not only avoid putting people in harm’s way when possible, but that these are often the very kind of jobs where technology can be a great help.
There are limits, of course, to how much a single program at a single institution—even a prominent one—can shift an entire field. But Li is adamant she has to do what she can to train researchers to think like ethicists, who are guided by principle over profit, informed by a varied array of backgrounds.
On the phone, I ask Li if she imagines there could have been a way to develop AI differently, without, perhaps, the problems we’ve seen so far. “I think it’s hard to imagine,” she says. “Scientific advances and innovation come really through generations of tedious work, trial and error. It took a while for us to recognize such bias. I only woke up six years ago and realized ‘Oh my God, we’re entering a crisis.’ ”
On Capitol Hill, Li said, “As a scientist, I’m humbled by how nascent the science of AI is. It is the science of only 60 years. Compared to classic sciences that are making human life better every day—physics, chemistry, biology—there’s a long, long way to go for AI to realize its potential to help people.” She added, “With proper guidance AI will make life better. But without it, the technology stands to widen the wealth divide even further, make tech even more exclusive, and reinforce biases we’ve spent generations trying to overcome.” This is the time, Li would have us believe, between an invention and its impact.
Hair and Makeup by Amy Lawson for Makeup Forever

Jessi Hempel wrote about Uber CEO Dara Khosrowshahi in issue 26.05. Additional reporting by Gregory Barber.
This article appears in the December issue. Subscribe now.
Listen to this story, and other WIRED features, on the Audm app.
Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.

More Great WIRED Stories

The DIY tinkerers harnessing the power of AI
The ‘pink tax’ and how women spend more on NYC transit
PHOTOS: The secret tools magicians use to fool you
The Butterball Turkey Talk-Line gets new trimmings
An aging marathoner tries to run fast after 40
Hungry for even more deep dives on your next favorite topic? Sign up for the Backchannel newsletter",,"https://media.wired.com/photos/5bd8ac3e66f3912cf4bdb747/1:1/w_730,h_730,c_limit/WI120118_AI_FeiFei_01.jpg","{'@type': 'CreativeWork', 'name': 'WIRED'}",AI has a problem: The biases of its creators are getting hard-coded into its future. Fei-Fei Li has a plan to fix that—by rebooting the field she helped invent.,,,,,,,,,,,,,,,,,
