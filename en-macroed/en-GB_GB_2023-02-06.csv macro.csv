URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,article:section,article:summary,article text,@context,@type,url,image,author,publisher,headline,datePublished,dateModified,articleSection,name,isAccessibleForFree,itemListElement,articleBody,isBasedOn,thumbnailUrl,isPartOf,alternativeHeadline,mainEntityOfPage,itemListOrder,dateCreated,wordCount,creator,address,@graph,speakable,alternateName,video,hasPart,inLanguage,breadcrumb
https://news.google.com/rss/articles/CBMiVWh0dHBzOi8vd3d3LnRoZWd1YXJkaWFuLmNvbS90ZWNobm9sb2d5LzIwMjMvZmViLzA4L2FpLWNoYXRncHQtam9icy1lY29ub215LWluZXF1YWxpdHnSAVVodHRwczovL2FtcC50aGVndWFyZGlhbi5jb20vdGVjaG5vbG9neS8yMDIzL2ZlYi8wOC9haS1jaGF0Z3B0LWpvYnMtZWNvbm9teS1pbmVxdWFsaXR5?oc=5,US experts warn AI likely to kill off jobs – and widen wealth inequality - The Guardian,2023-02-08,The Guardian,https://www.theguardian.com,Economists wary of firm predictions but say advances could create new raft of billionaires while other workers are laid off,N/A,Economists wary of firm predictions but say advances could create new raft of billionaires while other workers are laid off,N/A,Technology,N/A," New technologies like AI often produce jobs that no one could predict – before the invention of computers, who would have foreseen the job of computer programmer? Photograph: Jaap Arriens/NurPhoto/REX/ShutterstockView image in fullscreenNew technologies like AI often produce jobs that no one could predict – before the invention of computers, who would have foreseen the job of computer programmer? Photograph: Jaap Arriens/NurPhoto/REX/ShutterstockArtificial intelligence (AI) This article is more than 1 year oldUS experts warn AI likely to kill off jobs – and widen wealth inequalityThis article is more than 1 year oldEconomists wary of firm predictions but say advances could create new raft of billionaires while other workers are laid offSteven GreenhouseWed 8 Feb 2023 02.00 ESTLast modified on Wed 8 Feb 2023 02.01 ESTShareChatGPT is just the latest technology to fuel worries that it will wipe out the jobs of millions of workers, whether advertising copywriters, Wall Street traders, salespeople, writers of basic computer code or journalists.But while many workforce experts say the fears that ChatGPT and other artificial intelligence (AI) technologies will cause unemployment to skyrocket are overblown, they point to another fear about AI: that it will widen the US’s already huge income and wealth inequality by creating a new wave of billionaire tech barons at the same time that it pushes many workers out of better paid jobs.Like many revolutionary technologies before it, AI is likely to eliminate jobs. But, as has been the case in the past, experts argue, AI will likely offset much of that by spurring the creation of new jobs in addition to enhancing many existing jobs. The big question is: what sort of jobs?I’m a copywriter. I’m pretty sure artificial intelligence is going to take my jobRead more“AI will wipe out a lot of current jobs, as has happened with all past technologies,” said Lawrence Katz, a labor economist at Harvard. “But I have no reason to think that AI and robots won’t continue changing the mix of jobs. The question is: will the change in the mix of jobs exacerbate existing inequalities? Will AI raise productivity so much that even as it displaces a lot of jobs, it creates new ones and raises living standards?”Anu Madgavkar, who leads labor market research at the McKinsey Global Institute, estimates that one in four workers in the US are going to see more AI and technology adopted in their jobs. She said 50-60% of companies say they are pursuing AI-related projects. “So one way or the other people are going to have to learn to work with AI,” Madgavkar said.One way or the other people are going to have to learn to work with AIAnu MadgavkarWhile past rounds of automation affected factory jobs most, Madgavkar said that AI will hit white-collar jobs most. “It’s increasingly going into office-based work and customer service and sales,” she said. “They are the job categories that will have the highest rate of automation adoption and the biggest displacement. These workers will have to work with it or move into different skills.”In other words, many office workers could face downward mobility.Workforce experts are asking many questions about AI: will it displace many call-center workers or merely make those workers more productive by quickly delivering needed information to them as they speak to customers? Will AI displace radiologists because of its ability to read cancer scans or will it help radiologists by enabling them to focus on more complicated, nuanced issues in interpreting scans? Will AI be able to replace some journalists by writing stories about baseball games or Wall Street’s daily ups and downs?Some workforce experts say AI and other new technologies will hurt middle-level, white-collar jobs more than lower-paying, physically intensive jobs. McKinsey’s Madgavkar said it will be hard for AI or robots to do the jobs of janitors. In food service, she said, new technologies might be able to take customer orders, but “we’re not going to see many little robots that bring the food to a particular table”.At law firms, AI might eliminate some attorneys’ jobs by being able to prepare first drafts of business contracts. But AI might also enable paralegals to oversee preparation of draft contracts, and that increased responsibility could mean higher pay for paralegals.“If you make workers more productive, workers are then supposed to make more money,” said William Spriggs, an economics professor at Howard University and chief economist at the AFL-CIO, the nation’s main labor federation. “Companies don’t want to have a discussion about sharing the benefits of these technologies. They’d rather have a discussion to scare the bejesus out of you about these new technologies. They want you to concede that you’re just grateful to have a job and that you’ll pay us peanuts.”Spriggs noted that when a wave of automation swept through the auto industry from the 1950s through 1970s, “the UAW said to Ford and GM, we’re a lot more productive and you’re a lot more profitable. As a result, the workers got a lot more money.”David Autor, an economics professor at MIT, is wary of making predictions about ChatGPT and AI. “There’s just enormous uncertainty,” he said.The concern is: will AI reduce the value of a lot of skill sets and make labor more commodified?David Autor,But he’s not concerned with the US running out of jobs. “If anything, we don’t have enough people for jobs right now,” he said. “I’m concerned about the change of composition in jobs.” He voiced concern that AI, by eliminating some middle-class jobs and de-skilling some jobs, will move many workers into lower-paying jobs like food service. “The concern is: will AI reduce the value of a lot of skill sets and make labor more commodified?”New technologies like AI often produce jobs that no one could predict – before the invention of computers, who would have foreseen the job of computer programmer? Workforce experts say AI will create more jobs for engineers and certain types of managers, and that any AI-caused decrease in jobs could be offset by increases in the number of healthcare jobs as the overall population ages. AI might call for a beefed-up system of retraining to, for instance, prepare a laid-off salesperson for a hospital job.Juliet Schor, an economist at Boston College, said it would be far better if employers, instead of laying off people because of AI, would trim employees’ work time, perhaps to three or four days a week, instead of five. “Work-time reduction is really the far better way to respond to labor-displacing technological change,” Schor said. She voiced fears that AI could produce a large pool of jobless Americans, and even with some system of universal basic income, “that would create inequality between the people who have work and the people who don’t. That’s a big problem.”Julie Shah, an MIT professor who leads the Interactive Robotics Group at MIT’s Computer Science and Artificial Intelligence Laboratory, said she works with employers to get them to use AI and robots to “augment and enhance workers, rather than replace them”. She said some employers want to use robots to have a lights-out factory without any human workers, while other companies want robots to work alongside humans to make them more efficient — and to have human workers on hand to propose future ideas for innovation.Shah pointed to a study of large French corporations that introduced robots; those firms increased overall employment even as their domestic competitors reduced their workforce. She also cited a study of Canadian companies that began using robots and ultimately reduced the number of middle managers, while increasing the number of production workers. In the US, she noted, some companies adopted robots and offered higher wages, while having fewer jobs overall.“These technologies are not leading to one future, but to many possible futures,” Shah said.Harvard’s Katz is also worried about AI’s effects on income inequality. “It’s likely to continue to reduce labor’s share of income as many tasks get automated,” he said.Katz said a big issue is who will share in the gains if AI yields major productivity growth, and how those gains will be shared. “How much will need to come through redistribution policies?” he asked. “If it’s really good and massively increases productivity, even if workers get a smaller share of the pie, they could end up with higher incomes.”But these gains are unlikely to trickle down to workers in the US given current circumstances. “Having a stronger say for workers and their representatives in this process is an important element to adjusting to these changes. That’s happened in countries that have stronger unions and works councils. That’s an area where we in the US have fallen behind,” Katz said.Explore more on these topicsArtificial intelligence (AI)ChatGPTUS income inequalitynewsShareReuse this contentMost viewed‘A wise investment’: Trump’s $6,000 gift to Kamala Harris comes back to bite himLiveUS elections: Kamala Harris to make first speech since Biden endorsement as more top Democrats back VP – live updatesFrance Unbowed MP sparks outrage by saying Israeli Olympians not welcome The post-Biden era may be uncertain for the Democrats, but for Trump it will be utterly dismayingSimon TisdallWhat is the Kamala Harris coconut tree meme and why is everyone sharing it?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvYmVybmFyZG1hcnIvMjAyMy8wMi8wNy93aWxsLWNoYXRncHQtcHV0LWRhdGEtYW5hbHlzdHMtb3V0LW9mLXdvcmsv0gEA?oc=5,Will ChatGPT Put Data Analysts Out Of Work? - Forbes,2023-02-07,Forbes,https://www.forbes.com,"AI, particularly ChatGPT, has raised job security concerns among data analysts. Here we look at the potential impact and discuss that despite limitations like frequent mistakes and limited data upload capabilities, ChatGPT has the potential to automate data gathering and analysis tasks in the future","ChatGPT,LLM,NLP,LLMs,OpenAI,AI,Artificial Intelligence","AI, particularly ChatGPT, has raised job security concerns among data analysts. Here we look at the potential impact and discuss that despite limitations like frequent mistakes and limited data upload capabilities, ChatGPT has the potential to automate data gathering and analysis tasks in the future","AI, particularly ChatGPT, has raised job security concerns among data analysts. Here we look at the potential impact and discuss that despite limitations like frequent mistakes and limited data upload capabilities, ChatGPT has the potential to automate data gathering and analysis tasks in the future",Enterprise Tech,N/A,"More From ForbesJul 22, 2024,01:49am EDTBusiness Leaders Still Aren’t Prepared For The AI RevolutionJul 19, 2024,02:59pm EDTNext Steps For Tech & Security Leaders After CrowdStrike Global OutageJul 19, 2024,09:00am EDTThe Transformative Power Of Generative AI And Knowledge ManagementJul 19, 2024,02:31am EDTOptical Media From Cerabyte Promises Fast Access Archival StorageJul 19, 2024,01:59am EDTHow Walmart Business Is Revolutionizing B2B Shopping With AI: A Deep Dive Into The Future Of RetailJul 18, 2024,06:57pm EDTCatalyzing Change: How Juicyhub Is Fostering Local Innovation In BrazilJul 17, 2024,11:06am EDTAI Adoption Is Driving Real Top - And Bottom-Line Impact For EnterprisesEdit StoryForbesInnovationEnterprise TechWill ChatGPT Put Data Analysts Out Of Work?Bernard MarrContributorOpinions expressed by Forbes Contributors are their own.FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got it3Feb 7, 2023,01:40am ESTShare to FacebookShare to TwitterShare to LinkedinIf your work involves analyzing and reporting on data, then it’s understandable that you might feel a bit concerned by the rapid advances being made by artificial intelligence (AI). In particular, the viral ChatGPT app has captured the imagination of the general public in recent months, acting as a powerful demonstration of what AI is already capable of. For some, it may also seem like a warning about what might be in store for the future.
Will ChatGPT Put Data Analysts Out of Work? Adobe Stock

Undoubtedly, one of the strengths of AI is its ability to make sense of large amounts of data – searching out patterns and putting it into reports, documents, and formats that humans can easily understand. This is the day-to-day “bread and butter” of data analysts as well as many other knowledge economy professionals whose work involves working with data and analytics.

It’s true that artificial intelligence – a term that generally, in business and industry, refers to machine learning – has been used for years in these fields. What ChatGPT and similar tools built on large language models (LLM) and natural language processing (NLP) bring to the table is that it can be easily and effectively used by anybody. If a CEO can simply say to a computer, “what do I need to do to improve customer satisfaction?” or “how can I make more sales?” do they need to worry about hiring, training, and maintaining an expensive analytics team to answer those questions?

PROMOTED
Well, fortunately, the answer probably, is yes. In fact, as AI becomes more accessible and mainstream, that team may well become even more critical to the business than it already is. What is beyond doubt, though, is that their jobs will substantially change. So, here’s my rundown of how this technology may affect the field of data and analytics as it becomes mainstream in the near future.


Firstly, what are ChatGPT, LLMs, and NLP?
MORE FROMFORBES ADVISORBest High-Yield Savings Accounts Of 2024ByKevin PayneContributorBest 5% Interest Savings Accounts of 2024ByCassidy HortonContributor
ChatGPT is a publicly-available conversational (or chatbot) interface powered by a LLM called GPT-3, developed by the research institute OpenAI. The LLM (Large Language Model) is part of a field of machine learning known as natural language processing, which essentially means that it enables us to talk to machines, and for them to reply to us in “natural” (i.e., human) languages. In short, this means that we can ask it a question in English, or in fact, one of almost 100 languages. It can also read, understand and generate computer code in a number of popular programming languages, including Python, Javascript, and C++. We've gotten used to interacting with NLP technology for some time now thanks largely to AI assistants like Alexa and Siri, but the LLM powering GPT-3 and ChatGPT is orders of magnitude larger, enabling it to understand far more complex inputs and provide far more sophisticated outputs.









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



The GPT-3 LLM appears to be able to use language in a very sophisticated way because it was trained on a huge dataset of information, said to consist of over 175 billion parameters. This includes an open repository of web data called Common Crawl and several online book archives. By processing all of this data, it is able to learn how words are connected to each other and predict what is likely to be the most suitable response to any prompt (a question or other input) that it’s given. It’s sometimes called “generative AI” because it creates new outputs that haven’t been seen before.


1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGE

What are the limitations of ChatGPT?
Before we get too excited about what it can do, it’s worth pointing out that despite the hype, there are some fairly significant limits on what the technology can do today. Firstly, it frequently makes mistakes – sometimes very basic ones – which could easily leave anyone relying on it in a professional capacity looking somewhat silly if they aren’t careful.
For example, when I was working on this article, an obvious thing to do was ask ChatGPT what parts of a data analyst's job it’s capable of automating. One of the first answers it gave was, “ChatGPT can generate graphs, charts, and other visualizations."" This is clearly wrong, as it’s only capable of generating text.
Where data analytics is concerned, ChatGPT is also limited by the fact that we can’t upload data to it beyond any information that can be input as text. We can’t, for example, upload an Excel sheet of sales figures and ask it for insights. Of course, there’s no telling what future versions will be able to do. With that in mind, let’s look at how it can be used and speculate a little about what may be possible with LLMs and NLP in the near future.

How can ChatGPT, LLMs, and NLP be used in data and analytics?
Here are some of the key ways ChatGPT, LLMs, and NLP can be used in data and analytics:
· Create code and applications that can analyze data or automate processes such as data gathering, data formatting, or data cleansing.
· Define data structures – for example, what fields should be included in records in a database or what row and column headings are needed for a spreadsheet.
· Tell us how charts, graphs, diagrams, or infographics should be constructed and what information needs to be included.
· Suggest what information to include in reports in order that different audiences – executives, departmental heads, managers, and so on – will be able to take action based on them.
· Create training material to teach workers how to apply analytics to their own data.
· Identify data sources that are likely to contain the insights we need for a particular task – for example, ""Where can I find data on financial fraud in India?”
· Create dummy or synthetic data for a variety of purposes, such as training other machine learning models or testing algorithms.
· Provide advice on compliance, regulation, and practical steps that can be taken to ensure data operations are legal, unbiased, and ethical.
· Identify analytical processes and suggest best practices that are most likely to give the desired results.

Is ChatGPT a threat to jobs in data and analytics?
As we’ve seen, ChatGPT can easily automate some of the tasks that are traditionally carried out in analytical jobs – such as business, data, and financial analyst roles. Future iterations of the technology are likely to become even more effective and efficient at doing so.
But that doesn’t mean that anyone who works in an analytical role is going to be out of a job right away. This is primarily because today’s most sophisticated LLMs and NLP tools still lack abilities like critical thinking, strategic planning, and complex problem-solving. Most experts agree that it isn’t likely that machine learning-based tools will be able to carry out these functions at the same level as humans any time soon.
It's likely that businesses and other organizations will still have a need for humans who are experts in this field for some time to come.
Having said that, analytics roles that only require repetitive work are likely to become largely automated in the near future, and it’s probably inevitable that some jobs will be lost due to this.
At the same time, new jobs will be created. These are likely to revolve around the ability to deploy tools like ChatGPT while at the same time practicing human decision-making, problem-solving, leadership, strategy, leadership, and team-building.

I work in data and analytics; how can I make sure I don’t become redundant?
There are two very important rules to follow here. Firstly, whatever you do, do not stick your head in the sand and pretend this isn’t happening and that AI isn't about to dramatically change the way you work.
Secondly, learn to use this technology as a tool. Understand what its abilities are to augment your own skills by using tools like ChatGPT or whatever comes next to automate routine and repetitive tasks. In this piece, I’ve listed a number of tasks that this can be applied to right away – work through them and make sure you understand how each one can be done. Then, learn how to take advantage of the time and efficiency gains that this creates in order to develop your skillset and focus on areas where you can really make a difference.
Ignoring the arrival of AI in your profession is only likely to result in being left behind, as colleagues and competitors who are willing to move with the times reap the rewards. Right now, all we’re seeing is the tip of the iceberg. As the technology evolves, more and more aspects of all of our day-to-day work will become automated. Staying ahead of this curve, teaching yourself to use new tools as they become available, and maintaining awareness of areas where the human touch is still necessary, is the key to thriving in the age of AI.

To stay on top of the latest on new and emerging business and tech trends, make sure to subscribe to my newsletter, follow me on Twitter, LinkedIn, and YouTube, and check out my books ‘Future Skills: The 20 Skills And Competencies Everyone Needs To Succeed In A Digital World’ and ‘Business Trends in Practice, which won the 2022 Business Book of the Year award.
Follow me on Twitter or LinkedIn. Check out my website or some of my other work here. Bernard MarrFollowingFollowBernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is... Read MoreEditorial StandardsPrintReprints & PermissionsJoin The ConversationComments 3One Community. Many Voices. Create a free account to share your thoughts. Read our community guidelines  here.Forbes Community GuidelinesOur community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space.In order to do so, please follow the posting rules in our site's Terms of Service.  We've summarized some of those key rules below. Simply put, keep it civil.Your post will be rejected if we notice that it seems to contain:False or intentionally out-of-context or misleading informationSpamInsults, profanity, incoherent, obscene or inflammatory language or threats of any kindAttacks on the identity of other commenters or the article's authorContent that otherwise violates our site's terms.User accounts will be blocked if we notice or believe that users are engaged in:Continuous attempts to re-post comments that have been previously moderated/rejectedRacist, sexist, homophobic or other discriminatory commentsAttempts or tactics that put the site security at riskActions that otherwise violate our site's terms.So, how can you be a power user?Stay on topic and share your insightsFeel free to be clear and thoughtful to get your point across‘Like’ or ‘Dislike’ to show your point of view.Protect your community.Use the report tool to alert us when someone breaks the rules.Thanks for reading our community guidelines.  Please read the full list of posting rules found in our site's Terms of Service.",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/bernardmarr/2023/02/07/will-chatgpt-put-data-analysts-out-of-work/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/specials-images/imageserve/63e1f2076c234631fd4c9761/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Bernard Marr', 'url': 'https://www.forbes.com/sites/bernardmarr/', 'description': 'Bernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society. He has written over 20 best-selling and award-winning books and advises and coaches many of the world’s best-known organisations. He has a combined following of 4 million people across his social media channels and newsletters and was ranked by LinkedIn as one of the top 5 business influencers in the world. Follow Bernard on LinkedIn, X (Twitter) or YouTube. Join his newsletter, check out his website and books.', 'sameAs': ['https://www.linkedin.com/in/bernardmarr/', 'https://www.twitter.com/BernardMarr', 'https://bernardmarr.com/']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",Will ChatGPT Put Data Analysts Out Of Work?,2023-02-07T01:40:16-05:00,2024-02-20T12:12:07-05:00,Enterprise Tech,Will ChatGPT Put Data Analysts Out Of Work?,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Enterprise Tech', 'item': 'https://www.forbes.com/enterprise-tech/'}]",,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiSGh0dHBzOi8vbmV3cy5taXQuZWR1LzIwMjMvbGFyZ2UtbGFuZ3VhZ2UtbW9kZWxzLWluLWNvbnRleHQtbGVhcm5pbmctMDIwN9IBAA?oc=5,Solving a machine-learning mystery | MIT News | Massachusetts Institute of Technology - MIT News,2023-02-07,MIT News,https://news.mit.edu,"MIT researchers have explained how large language models like GPT-3 are able to learn new tasks without updating their parameters, despite not being trained to perform those tasks. They found that these large language models write smaller linear models inside their hidden layers, which the large models can train to complete a new task using simple learning algorithms.","Ekin Akyürek, Jacob Andreas, large language models, GPT-3, In-context learning, Transformer model, LLM","MIT researchers have explained how large language models like GPT-3 are able to learn new tasks without updating their parameters, despite not being trained to perform those tasks. They found that these large language models write smaller linear models inside their hidden layers, which the large models can train to complete a new task using simple learning algorithms.",N/A,N/A,N/A,"


A new study shows how large language models like GPT-3 can learn a new task from just a few examples, without the need for any new training data.




Adam Zewe
|
MIT News Office


 Publication Date:
 February 7, 2023





Press Inquiries

  Press Contact:



      
            Abby        

            Abazorius        

  

      Email:
     abbya@mit.edu


      Phone:
              617-253-2709      
  

      
            MIT News Office        

  


 Media Download



 


          ↓ Download Image



 Caption:
        
              MIT researchers found that massive neural network models that are similar to large language models are capable of containing smaller linear models inside their hidden layers, which the large models could train to complete a new task using simple learning algorithms.      
 Credits:
        
              Image: Jose-Luis Olivares, MIT      





*Terms of Use:

    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a 
    Creative Commons Attribution Non-Commercial No Derivatives license.
    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided 
    below, credit the images to ""MIT."" 
  







 Close














 Caption:
          MIT researchers found that massive neural network models that are similar to large language models are capable of containing smaller linear models inside their hidden layers, which the large models could train to complete a new task using simple learning algorithms.      
          

 Credits:
          Image: Jose-Luis Olivares, MIT      
          

















Previous image
Next image






















Large language models like OpenAI’s GPT-3 are massive neural networks that can generate human-like text, from poetry to programming code. Trained using troves of internet data, these machine-learning models take a small bit of input text and then predict the text that is likely to come next.

But that’s not all these models can do. Researchers are exploring a curious phenomenon known as in-context learning, in which a large language model learns to accomplish a task after seeing only a few examples — despite the fact that it wasn’t trained for that task. For instance, someone could feed the model several example sentences and their sentiments (positive or negative), then prompt it with a new sentence, and the model can give the correct sentiment.

Typically, a machine-learning model like GPT-3 would need to be retrained with new data for this new task. During this training process, the model updates its parameters as it processes new information to learn the task. But with in-context learning, the model’s parameters aren’t updated, so it seems like the model learns a new task without learning anything at all.

Scientists from MIT, Google Research, and Stanford University are striving to unravel this mystery. They studied models that are very similar to large language models to see how they can learn without updating parameters.

The researchers’ theoretical results show that these massive neural network models are capable of containing smaller, simpler linear models buried inside them. The large model could then implement a simple learning algorithm to train this smaller, linear model to complete a new task, using only information already contained within the larger model. Its parameters remain fixed.

An important step toward understanding the mechanisms behind in-context learning, this research opens the door to more exploration around the learning algorithms these large models can implement, says Ekin Akyürek, a computer science graduate student and lead author of a paper exploring this phenomenon. With a better understanding of in-context learning, researchers could enable models to complete new tasks without the need for costly retraining.

""Usually, if you want to fine-tune these models, you need to collect domain-specific data and do some complex engineering. But now we can just feed it an input, five examples, and it accomplishes what we want. So, in-context learning is an unreasonably efficient learning phenomenon that needs to be understood,"" Akyürek says.

Joining Akyürek on the paper are Dale Schuurmans, a research scientist at Google Brain and professor of computing science at the University of Alberta; as well as senior authors Jacob Andreas, the X Consortium Assistant Professor in the MIT Department of Electrical Engineering and Computer Science and a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL); Tengyu Ma, an assistant professor of computer science and statistics at Stanford; and Danny Zhou, principal scientist and research director at Google Brain. The research will be presented at the International Conference on Learning Representations.

A model within a model

In the machine-learning research community, many scientists have come to believe that large language models can perform in-context learning because of how they are trained, Akyürek says.

For instance, GPT-3 has hundreds of billions of parameters and was trained by reading huge swaths of text on the internet, from Wikipedia articles to Reddit posts. So, when someone shows the model examples of a new task, it has likely already seen something very similar because its training dataset included text from billions of websites. It repeats patterns it has seen during training, rather than learning to perform new tasks.

Akyürek hypothesized that in-context learners aren’t just matching previously seen patterns, but instead are actually learning to perform new tasks. He and others had experimented by giving these models prompts using synthetic data, which they could not have seen anywhere before, and found that the models could still learn from just a few examples. Akyürek and his colleagues thought that perhaps these neural network models have smaller machine-learning models inside them that the models can train to complete a new task.

“That could explain almost all of the learning phenomena that we have seen with these large models,” he says.

To test this hypothesis, the researchers used a neural network model called a transformer, which has the same architecture as GPT-3, but had been specifically trained for in-context learning.

By exploring this transformer’s architecture, they theoretically proved that it can write a linear model within its hidden states. A neural network is composed of many layers of interconnected nodes that process data. The hidden states are the layers between the input and output layers.

Their mathematical evaluations show that this linear model is written somewhere in the earliest layers of the transformer. The transformer can then update the linear model by implementing simple learning algorithms.

In essence, the model simulates and trains a smaller version of itself.

Probing hidden layers

The researchers explored this hypothesis using probing experiments, where they looked in the transformer’s hidden layers to try and recover a certain quantity.

“In this case, we tried to recover the actual solution to the linear model, and we could show that the parameter is written in the hidden states. This means the linear model is in there somewhere,” he says.

Building off this theoretical work, the researchers may be able to enable a transformer to perform in-context learning by adding just two layers to the neural network. There are still many technical details to work out before that would be possible, Akyürek cautions, but it could help engineers create models that can complete new tasks without the need for retraining with new data.

“The paper sheds light on one of the most remarkable properties of modern large language models — their ability to learn from data given in their inputs, without explicit training. Using the simplified case of linear regression, the authors show theoretically how models can implement standard learning algorithms while reading their input, and empirically which learning algorithms best match their observed behavior,” says Mike Lewis, a research scientist at Facebook AI Research who was not involved with this work. “These results are a stepping stone to understanding how models can learn more complex tasks, and will help researchers design better training methods for language models to further improve their performance.”

Moving forward, Akyürek plans to continue exploring in-context learning with functions that are more complex than the linear models they studied in this work. They could also apply these experiments to large language models to see whether their behaviors are also described by simple learning algorithms. In addition, he wants to dig deeper into the types of pretraining data that can enable in-context learning.

“With this work, people can now visualize how these models can learn from exemplars. So, my hope is that it changes some people’s views about in-context learning,” Akyürek says. “These models are not as dumb as people think. They don’t just memorize these tasks. They can learn new tasks, and we have shown how that can be done.”








Share this news article on:










X











Facebook















LinkedIn




































Reddit


















Print









Paper






Paper: “What Learning Algorithm Is In-Context Learning? Investigations with Linear Models”








Press Mentions


MotherboardMotherboard reporter Tatyana Woodall writes that a new study co-authored by MIT researchers finds that AI models that can learn to perform new tasks from just a few examples create smaller models inside themselves to achieve these new tasks. “Learning is entangled with [existing] knowledge,” graduate student Ekin Akyürek explains. “We show that it is possible for these models to learn from examples on the fly without any parameter update we apply to the model.”











Full story via Motherboard →















Previous item
Next item



















Related Links

Ekin AkyürekJacob AndreasComputer Science and Artificial Intelligence LaboratoryDepartment of Electrical Engineering and Computer ScienceSchool of EngineeringMIT Schwarzman College of Computing






Related Topics

Research
Computer science and technology
Artificial intelligence
Machine learning
Algorithms
Computer Science and Artificial Intelligence Laboratory (CSAIL)
Electrical Engineering & Computer Science (eecs)
School of Engineering
MIT Schwarzman College of Computing



Related Articles











Large language models help decipher clinical notes 













Demystifying machine-learning systems













AI that can learn the patterns of human language













What words can convey

















Previous item
Next item
















",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiOmh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9ndWlkZS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS_SAQA?oc=5,What Defines Artificial Intelligence? The Complete WIRED Guide - WIRED,2023-02-08,WIRED,https://www.wired.com,"Supersmart algorithms won't take all the jobs, But they are learning faster than ever, doing everything from medical diagnostics to serving up ads.","['business', 'artificial intelligence', 'wired guide', 'textaboveleftsmall', 'web']","Supersmart algorithms won't take all the jobs, But they are learning faster than ever, doing everything from medical diagnostics to serving up ads.","Supersmart algorithms won't take all the jobs, But they are learning faster than ever, doing everything from medical diagnostics to serving up ads.",tags,N/A,"Tom SimoniteBusinessFeb 8, 2023 7:00 AMThe WIRED Guide to Artificial IntelligenceSupersmart algorithms won’t take all the jobs, but they are learning faster than ever, doing everything from medical diagnostics to serving up ads.Play/Pause ButtonPauseIllustrations by RadioSave this storySaveSave this storySaveArtificial intelligence is here. It’s overhyped, poorly understood, and flawed but already core to our lives—and it’s only going to extend its reach.AI powers driverless car research, spots otherwise invisible signs of disease on medical images, finds an answer when you ask Alexa a question, and lets you unlock your phone with your face to talk to friends as an animated poop on the iPhone X using Apple’s Animoji. Those are just a few ways AI already touches our lives, and there’s plenty of work still to be done. But don’t worry, superintelligent algorithms aren’t about to take all the jobs or wipe out humanity.AdChoicesADVERTISEMENTThe current boom in all things AI was catalyzed by breakthroughs in an area known as machine learning. It involves “training” computers to perform tasks based on examples, rather than relying on programming by a human. A technique called deep learning has made this approach much more powerful. Just ask Lee Sedol, holder of 18 international titles at the complex game of Go. He got creamed by software called AlphaGo in 2016.Featured VideoRE:WIRED 2021: Timnit Gebru on AI and the Discipline of HopeThere’s evidence that AI can make us happier and healthier. But there’s also reason for caution. Incidents in which algorithms picked up or amplified societal biases around race or gender show that an AI-enhanced future won’t automatically be a better one.The Beginnings of Artificial IntelligenceArtificial intelligence as we know it began as a vacation project. Dartmouth professor John McCarthy coined the term in the summer of 1956, when he invited a small group to spend a few weeks musing on how to make machines do things like use language.Moments that Shaped AI1956The Dartmouth Summer Research Project on Artificial Intelligence coins the name of a new field concerned with making software smart like humans.1965Joseph Weizenbaum at MIT creates Eliza, the first chatbot, which poses as a psychotherapist.1975Meta-Dendral, a program developed at Stanford to interpret chemical analyses, makes the first discoveries by a computer to be published in a refereed journal.1987A Mercedes van fitted with two cameras and a bunch of computers drives itself 20 kilometers along a German highway at more than 55 mph, in an academic project led by engineer Ernst Dickmanns.1997IBM’s computer Deep Blue defeats chess world champion Garry Kasparov.2004The Pentagon stages the Darpa Grand Challenge, a race for robot cars in the Mojave Desert that catalyzes the autonomous-car industry.2012Researchers in a niche field called deep learning spur new corporate interest in AI by showing their ideas can make speech and image recognition much more accurate.2016AlphaGo, created by Google unit DeepMind, defeats a world champion player of the board game Go.He had high hopes of a breakthrough in the drive toward human-level machines. “We think that a significant advance can be made,” he wrote with his co-organizers, “if a carefully selected group of scientists work on it together for a summer.”Those hopes were not met, and McCarthy later conceded that he had been overly optimistic. But the workshop helped researchers dreaming of intelligent machines coalesce into a recognized academic field.Early work often focused on solving fairly abstract problems in math and logic. But it wasn’t long before AI started to show promising results on more human tasks. In the late 1950s, Arthur Samuel created programs that learned to play checkers. In 1962, one scored a win over a master at the game. In 1967, a program called Dendral showed it could replicate the way chemists interpreted mass-spectrometry data on the makeup of chemical samples.AdvertisementAs the field of AI developed, so did different strategies for making smarter machines. Some researchers tried to distill human knowledge into code or come up with rules for specific tasks, like understanding language. Others were inspired by the importance of learning to understand human and animal intelligence. They built systems that could get better at a task over time, perhaps by simulating evolution or by learning from example data. The field hit milestone after milestone as computers mastered tasks that could previously only be completed by people.Deep learning, the rocket fuel of the current AI boom, is a revival of one of the oldest ideas in AI. The technique involves passing data through webs of math loosely inspired by the working of brain cells that are known as artificial neural networks. As a network processes training data, connections between the parts of the network adjust, building up an ability to interpret future data.Artificial neural networks became an established idea in AI not long after the Dartmouth workshop. The room-filling Perceptron Mark 1 from 1958, for example, learned to distinguish different geometric shapes and got written up in The New York Times as the “Embryo of Computer Designed to Read and Grow Wiser.” But neural networks tumbled from favor after an influential 1969 book coauthored by MIT’s Marvin Minsky suggested they couldn’t be very powerful.Not everyone was convinced by the skeptics, however, and some researchers kept the technique alive over the decades. They were vindicated in 2012, when a series of experiments showed that neural networks fueled with large piles of data could give machines new powers of perception. Churning through so much data was difficult using traditional computer chips, but a shift to graphics cards precipitated an explosion in processing power.Most PopularSecurityHow One Bad CrowdStrike Update Crashed the World’s ComputersBy Lily Hay NewmanSecurityDon’t Fall for CrowdStrike Outage ScamsBy Lily Hay NewmanCultureThe 19 Best Movies on Amazon Prime Right NowBy Matt KamenCultureThe 49 Best Shows on Netflix Right NowBy Matt KamenIn one notable result, researchers at the University of Toronto trounced rivals in an annual competition where software is tasked with categorizing images. In another, researchers from IBM, Microsoft, and Google teamed up to publish results showing deep learning could also deliver a significant jump in the accuracy of speech recognition. Tech companies began frantically hiring all the deep-learning experts they could find. It's important to note however that the AI field has had several booms and busts (aka, “AI winters”) in the past, and a sea change remains a possibility again today.The State of AI TodayImprovements to AI hardware, growth in training courses in machine learning, and open source machine-learning projects have accelerated the spread of AI to other industries, from national security to business support and medicine.  Alphabet-owned DeepMind has turned its AI loose on a variety of problems: the movement of soccer players, the restoration of ancient texts, and even ways to control nuclear fusion. In 2020, DeepMind said that its AlphaFold AI could predict the structure of proteins, a long-standing problem that had hampered research. This was widely seen as one of the first times a real scientific question has been answered with AI. AlphaFold was subsequently used to study Covid-19 and is now helping scientists study neglected diseases.Meanwhile, consumers can expect to be pitched more gadgets and services with AI-powered features. Google and Amazon, in particular, are betting that improvements in machine learning will make their virtual assistants and smart speakers more powerful. Amazon, for example, has devices with cameras to look at their owners and the world around them.Much progress has been made in the past two decades, but there’s plenty to work on. Despite the flurry of recent progress in AI and wild prognostications about its near future, there are still many things that machines can’t do, such as understanding the nuances of language, commonsense reasoning, and learning new skills from just one or two examples.AI software will need to master tasks like these if it is to get close to the multifaceted, adaptable, and creative intelligence of humans, an idea known as artificial general intelligence that may never be possible. One deep-learning pioneer, Google’s Geoff Hinton, argues that making progress on that grand challenge will require rethinking some of the foundations of the field.Generative AI and Its ControversiesThere’s a particular type of AI making headlines—in some cases, actually writing them too. Generative AI is a catch-all term for AI that can cobble together bits and pieces from the digital world to make something new—well, new-ish—such as art, illustrations, images, complete and functional code, and tranches of text that pass not only the Turing test, but MBA exams.Tools such as OpenAI’s Chat-GPT text generator and Stable Diffusion’s text-to-image maker manage this by sucking up unbelievable amounts of data, analyzing the patterns using neural networks, and regurgitating it in sensible ways. The natural language system behind Chat-GPT has churned through the entire internet, as well as an untold number of books, letting it answer questions, write content from prompts, and—in the case of CNET—write explanatory articles for websites to match search terms. (To be clear, this article was not written by Chat-GPT, though including text generated by the natural language system is quickly becoming an AI-writing cliche.)Most PopularSecurityHow One Bad CrowdStrike Update Crashed the World’s ComputersBy Lily Hay NewmanSecurityDon’t Fall for CrowdStrike Outage ScamsBy Lily Hay NewmanCultureThe 19 Best Movies on Amazon Prime Right NowBy Matt KamenCultureThe 49 Best Shows on Netflix Right NowBy Matt KamenWhile investors are drooling, writers, visual artists, and other creators are naturally worried: Chatbots are (or at least appear to be) cheap, and humans require a livable income. Why pay an illustrator for an image when you can prompt Dall-E to make something for free?Content makers aren’t the only ones concerned. Google is quietly ramping up its AI efforts in response to OpenAI’s accomplishments, and the search giant should be worried about what happens to people’s search habits when chatbots can answer questions for us. So long Googling, hello Chat-GPTing?Challenges loom on the horizon, however. AI models need more and more data to improve, but OpenAI has already used the easy sources; finding new piles of written text to use won’t be easy or free. Legal challenges also loom: OpenAI is training its system on text and images that may be under copyright, perhaps even created by the very same people whose jobs are at risk from this technology. And as more online content is created using AI, it creates a feedback loop in which the online data-training models won’t be created by humans, but by machines.Data aside, there’s a fundamental problem with such language models: They spit out text that reads well enough but is not necessarily accurate. As smart as these models are, they don’t know what they’re saying or have any concept of truth—that’s easily forgotten amid the mad rush to make use of such tools for new businesses or to create content. Words aren’t just supposed to sound good, they’re meant to convey meaning too.The Challenges (and Future) of Artificial IntelligenceThere are as many critics of AI as there are cheerleaders—which is good news, given the hype surrounding this set of technologies. Criticism of AI touches on issues as disparate as sustainability, ethics, bias, disinformation, and even copyright, with some arguing the technology is not as capable as most believe and others predicting it’ll be the end of humanity as we know it. It’s a lot to consider.Your AI Decoder RingArtificial intelligenceThe development of computers capable of tasks that typically require human intelligence.Machine learningUsing example data or experience to refine how computers make predictions or perform a task.Deep learningA machine learning technique in which data is filtered through self-adjusting networks of math loosely inspired by neurons in the brain.Supervised learningShowing software labeled example data, such as photographs, to teach a computer what to do.Unsupervised learningLearning without annotated examples, just from experience of data or the world—trivial for humans but not generally practical for machines. Yet.Reinforcement learningSoftware that experiments with different actions to figure out how to maximize a virtual reward, such as scoring points in a game.Artificial general intelligenceAs yet nonexistent software that displays a humanlike ability to adapt to different environments and tasks, and transfer knowledge between them.To start, deep learning inherently requires huge swathes of data, and though innovations in chips mean we can do that faster and more efficiently than ever, there’s no question that AI research churns through energy. A startup estimated that in teaching one system to solve a Rubik’s Cube using a robotic hand OpenAI consumed 2.8 gigawatt-hours of electricity—as much as three nuclear plants could output in an hour. Other estimates suggest training an AI model emits as much carbon dioxide as five American cars being manufactured and driven for their average lifespan.There are techniques to reduce the impact: Researchers are developing more efficient training techniques, models can be chopped up so only necessary sections are run, and data centers and labs are shifting to cleaner energy. AI also has a role to play in improving efficiencies in other industries and otherwise helping address the climate crisis. But boosting the accuracy of AI generally means having more complicated models sift through more data—OpenAI’s GPT2 model reportedly had 1.5 billion weights to assess data, while GPT3 had 175 billion—suggesting AI’s sustainability could get worse before it improves.Vacuuming up all the data needed to build these models creates additional challenges, beyond the shrinking availability of fresh data mentioned above. Bias remains a core problem: Data sets reflect the world around us, and that means models absorb our racism, sexism, and other cultural assumptions. This causes a host of serious problems: AI trained to spot skin cancer performs better on white skin; software designed to predict recidivism inherently rates Black people as more likely to reoffend; and flawed AI facial recognition software has already incorrectly identified Black men, leading to their arrests. And sometimes the AI simply doesn’t work: One violent crime prediction tool for police was wildly inaccurate because of an apparent coding error.Most PopularSecurityHow One Bad CrowdStrike Update Crashed the World’s ComputersBy Lily Hay NewmanSecurityDon’t Fall for CrowdStrike Outage ScamsBy Lily Hay NewmanCultureThe 19 Best Movies on Amazon Prime Right NowBy Matt KamenCultureThe 49 Best Shows on Netflix Right NowBy Matt KamenAgain, mitigations are possible. More inclusive data sets could help tackle bias at the source, while forcing tech companies to explain algorithmic decision-making could add a layer of accountability. Diversifying the industry beyond white men wouldn’t hurt, either. But the most serious challenges may require regulating—and perhaps banning—the use of AI decision-making in situations with the most risk of serious damage to people.Those are a few examples of unwanted outcomes. But people are also already using AI for nefarious ends, such as to create deepfakes and spread disinformation. While AI-edited or AI-generated videos and images have intriguing use cases—such as filling in for voice actors after they leave a show or pass away—generative AI has also been used to make deepfake porn, adding famous faces to adult actors, or used to defame everyday individuals. And AI has been used to flood the web with disinformation, though fact-checkers have turned to the technology to fight back.As AI systems grow more powerful, they will rightly invite more scrutiny. Government use of software in areas such as criminal justice is often flawed or secretive, and corporations like Meta have begun confronting the downsides of their own life-shaping algorithms. More powerful AI has the potential to create worse problems, for example by perpetuating historical biases and stereotypes against women or Black people. Civil-society groups and even the tech industry itself are now exploring rules and guidelines on the safety and ethics of AI.But the hype around generative models suggests we still haven’t learned our lesson when it comes to AI. We need to calm down; understand how it works and when it doesn’t; and then roll out this tool in a careful, considered manner, mitigating concerns as they’re raised. AI has real potential to better—and even extend—our lives, but to truly reap the benefits of machines getting smarter, we’ll need to get smarter about machines.Learn MoreAI Shouldn’t Compete With Workers—It Should Supercharge ThemThe economy could get a boost if machine-learning engineers switched from copying human abilities to augmenting them.How to Stop Robots From Becoming RacistAlgorithms can amplify patterns of discrimination. Robotics researchers are calling for new ways to prevent mechanical bodies acting out those biases.Generative AI Won’t Revolutionize Game Development Just YetHypesters say artificial intelligence will one day automate all the hard work of video game creation. But it’s not that simple.It’s Time to Teach AI How to Be ForgetfulBy emulating the human ability to forget some of the data, psychological AIs will transform algorithmic accuracy.The Race to Build a ChatGPT-Powered Search EngineA search bot you converse with could make finding answers easier—if it doesn’t tell fibs. Microsoft, Google, Baidu, and others are working on it.AI Reveals the Most Human Parts of WritingWhen do writers want help finding inspiration? And when do they want full control? Computers could expose the true future of the medium.ChatGPT Isn’t the Only Way to Use AI in EducationAI can be a tool to create meaningful connections and learning experiences for children—and may help foster more equitable outcomes.Where the AI Art Boom Came From—and Where It’s GoingIn the past year, algorithms got a lot better at generating illustrations, art, and photo-realistic scenes. Next up? Video.Plus! AI’s hallucination problem and more WIRED artificial intelligence coverage.This guide was last updated on February 8, 2023.Enjoyed this deep dive? Check out more WIRED Guides.Enter your email to get the Wired newsletterclose dialogRecommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyPlease enter abovesign upUsed consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.Recommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyYou're signed up!Used consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.close dialog",https://schema.org/,BreadcrumbList,https://www.wired.com/story/guide-artificial-intelligence/,"['https://media.wired.com/photos/5a72ad9dd8520d1deb145f61/16:9/w_975,h_548,c_limit/Guides_AI.jpg', 'https://media.wired.com/photos/5a72ad9dd8520d1deb145f61/4:3/w_980,h_735,c_limit/Guides_AI.jpg', 'https://media.wired.com/photos/5a72ad9dd8520d1deb145f61/1:1/w_735,h_735,c_limit/Guides_AI.jpg']","[{'@type': 'Person', 'name': 'Tom Simonite', 'sameAs': 'https://www.wired.com/author/tom-simonite/'}]","{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}",What Defines Artificial Intelligence? The Complete WIRED Guide,2018-02-01T09:22:24.441-05:00,2023-02-08T07:00:00.000-05:00,business,,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'Business', 'item': 'https://www.wired.com/business/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Wired Guide', 'item': 'https://www.wired.com/tag/wired-guide/'}, {'@type': 'ListItem', 'position': 3, 'name': 'The WIRED Guide to Artificial Intelligence'}]","AI powers driverless car research, spots otherwise invisible signs of disease on medical images, finds an answer when you ask Alexa a question, and lets you unlock your phone with your face to talk to friends as an animated poop on the iPhone X using Apple’s Animoji. Those are just a few ways AI already touches our lives, and there’s plenty of work still to be done. But don’t worry, superintelligent algorithms aren’t about to take all the jobs or wipe out humanity.
The current boom in all things AI was catalyzed by breakthroughs in an area known as machine learning. It involves “training” computers to perform tasks based on examples, rather than relying on programming by a human. A technique called deep learning has made this approach much more powerful. Just ask Lee Sedol, holder of 18 international titles at the complex game of Go. He got creamed by software called AlphaGo in 2016.
There’s evidence that AI can make us happier and healthier. But there’s also reason for caution. Incidents in which algorithms picked up or amplified societal biases around race or gender show that an AI-enhanced future won’t automatically be a better one.
The Beginnings of Artificial Intelligence
Artificial intelligence as we know it began as a vacation project. Dartmouth professor John McCarthy coined the term in the summer of 1956, when he invited a small group to spend a few weeks musing on how to make machines do things like use language.
He had high hopes of a breakthrough in the drive toward human-level machines. “We think that a significant advance can be made,” he wrote with his co-organizers, “if a carefully selected group of scientists work on it together for a summer.”
Those hopes were not met, and McCarthy later conceded that he had been overly optimistic. But the workshop helped researchers dreaming of intelligent machines coalesce into a recognized academic field.
Early work often focused on solving fairly abstract problems in math and logic. But it wasn’t long before AI started to show promising results on more human tasks. In the late 1950s, Arthur Samuel created programs that learned to play checkers. In 1962, one scored a win over a master at the game. In 1967, a program called Dendral showed it could replicate the way chemists interpreted mass-spectrometry data on the makeup of chemical samples.
As the field of AI developed, so did different strategies for making smarter machines. Some researchers tried to distill human knowledge into code or come up with rules for specific tasks, like understanding language. Others were inspired by the importance of learning to understand human and animal intelligence. They built systems that could get better at a task over time, perhaps by simulating evolution or by learning from example data. The field hit milestone after milestone as computers mastered tasks that could previously only be completed by people.
Deep learning, the rocket fuel of the current AI boom, is a revival of one of the oldest ideas in AI. The technique involves passing data through webs of math loosely inspired by the working of brain cells that are known as artificial neural networks. As a network processes training data, connections between the parts of the network adjust, building up an ability to interpret future data.
Artificial neural networks became an established idea in AI not long after the Dartmouth workshop. The room-filling Perceptron Mark 1 from 1958, for example, learned to distinguish different geometric shapes and got written up in The New York Times as the “Embryo of Computer Designed to Read and Grow Wiser.” But neural networks tumbled from favor after an influential 1969 book coauthored by MIT’s Marvin Minsky suggested they couldn’t be very powerful.
Not everyone was convinced by the skeptics, however, and some researchers kept the technique alive over the decades. They were vindicated in 2012, when a series of experiments showed that neural networks fueled with large piles of data could give machines new powers of perception. Churning through so much data was difficult using traditional computer chips, but a shift to graphics cards precipitated an explosion in processing power.
In one notable result, researchers at the University of Toronto trounced rivals in an annual competition where software is tasked with categorizing images. In another, researchers from IBM, Microsoft, and Google teamed up to publish results showing deep learning could also deliver a significant jump in the accuracy of speech recognition. Tech companies began frantically hiring all the deep-learning experts they could find. It's important to note however that the AI field has had several booms and busts (aka, “AI winters”) in the past, and a sea change remains a possibility again today.
The State of AI Today
Improvements to AI hardware, growth in training courses in machine learning, and open source machine-learning projects have accelerated the spread of AI to other industries, from national security to business support and medicine.
Alphabet-owned DeepMind has turned its AI loose on a variety of problems: the movement of soccer players, the restoration of ancient texts, and even ways to control nuclear fusion. In 2020, DeepMind said that its AlphaFold AI could predict the structure of proteins, a long-standing problem that had hampered research. This was widely seen as one of the first times a real scientific question has been answered with AI. AlphaFold was subsequently used to study Covid-19 and is now helping scientists study neglected diseases.
Meanwhile, consumers can expect to be pitched more gadgets and services with AI-powered features. Google and Amazon, in particular, are betting that improvements in machine learning will make their virtual assistants and smart speakers more powerful. Amazon, for example, has devices with cameras to look at their owners and the world around them.
Much progress has been made in the past two decades, but there’s plenty to work on. Despite the flurry of recent progress in AI and wild prognostications about its near future, there are still many things that machines can’t do, such as understanding the nuances of language, commonsense reasoning, and learning new skills from just one or two examples.
AI software will need to master tasks like these if it is to get close to the multifaceted, adaptable, and creative intelligence of humans, an idea known as artificial general intelligence that may never be possible. One deep-learning pioneer, Google’s Geoff Hinton, argues that making progress on that grand challenge will require rethinking some of the foundations of the field.
Generative AI and Its Controversies
There’s a particular type of AI making headlines—in some cases, actually writing them too. Generative AI is a catch-all term for AI that can cobble together bits and pieces from the digital world to make something new—well, new-ish—such as art, illustrations, images, complete and functional code, and tranches of text that pass not only the Turing test, but MBA exams.
Tools such as OpenAI’s Chat-GPT text generator and Stable Diffusion’s text-to-image maker manage this by sucking up unbelievable amounts of data, analyzing the patterns using neural networks, and regurgitating it in sensible ways. The natural language system behind Chat-GPT has churned through the entire internet, as well as an untold number of books, letting it answer questions, write content from prompts, and—in the case of CNET—write explanatory articles for websites to match search terms. (To be clear, this article was not written by Chat-GPT, though including text generated by the natural language system is quickly becoming an AI-writing cliche.)
While investors are drooling, writers, visual artists, and other creators are naturally worried: Chatbots are (or at least appear to be) cheap, and humans require a livable income. Why pay an illustrator for an image when you can prompt Dall-E to make something for free?
Content makers aren’t the only ones concerned. Google is quietly ramping up its AI efforts in response to OpenAI’s accomplishments, and the search giant should be worried about what happens to people’s search habits when chatbots can answer questions for us. So long Googling, hello Chat-GPTing?
Challenges loom on the horizon, however. AI models need more and more data to improve, but OpenAI has already used the easy sources; finding new piles of written text to use won’t be easy or free. Legal challenges also loom: OpenAI is training its system on text and images that may be under copyright, perhaps even created by the very same people whose jobs are at risk from this technology. And as more online content is created using AI, it creates a feedback loop in which the online data-training models won’t be created by humans, but by machines.
Data aside, there’s a fundamental problem with such language models: They spit out text that reads well enough but is not necessarily accurate. As smart as these models are, they don’t know what they’re saying or have any concept of truth—that’s easily forgotten amid the mad rush to make use of such tools for new businesses or to create content. Words aren’t just supposed to sound good, they’re meant to convey meaning too.
The Challenges (and Future) of Artificial Intelligence
There are as many critics of AI as there are cheerleaders—which is good news, given the hype surrounding this set of technologies. Criticism of AI touches on issues as disparate as sustainability, ethics, bias, disinformation, and even copyright, with some arguing the technology is not as capable as most believe and others predicting it’ll be the end of humanity as we know it. It’s a lot to consider.
To start, deep learning inherently requires huge swathes of data, and though innovations in chips mean we can do that faster and more efficiently than ever, there’s no question that AI research churns through energy. A startup estimated that in teaching one system to solve a Rubik’s Cube using a robotic hand OpenAI consumed 2.8 gigawatt-hours of electricity—as much as three nuclear plants could output in an hour. Other estimates suggest training an AI model emits as much carbon dioxide as five American cars being manufactured and driven for their average lifespan.
There are techniques to reduce the impact: Researchers are developing more efficient training techniques, models can be chopped up so only necessary sections are run, and data centers and labs are shifting to cleaner energy. AI also has a role to play in improving efficiencies in other industries and otherwise helping address the climate crisis. But boosting the accuracy of AI generally means having more complicated models sift through more data—OpenAI’s GPT2 model reportedly had 1.5 billion weights to assess data, while GPT3 had 175 billion—suggesting AI’s sustainability could get worse before it improves.
Vacuuming up all the data needed to build these models creates additional challenges, beyond the shrinking availability of fresh data mentioned above. Bias remains a core problem: Data sets reflect the world around us, and that means models absorb our racism, sexism, and other cultural assumptions. This causes a host of serious problems: AI trained to spot skin cancer performs better on white skin; software designed to predict recidivism inherently rates Black people as more likely to reoffend; and flawed AI facial recognition software has already incorrectly identified Black men, leading to their arrests. And sometimes the AI simply doesn’t work: One violent crime prediction tool for police was wildly inaccurate because of an apparent coding error.
Again, mitigations are possible. More inclusive data sets could help tackle bias at the source, while forcing tech companies to explain algorithmic decision-making could add a layer of accountability. Diversifying the industry beyond white men wouldn’t hurt, either. But the most serious challenges may require regulating—and perhaps banning—the use of AI decision-making in situations with the most risk of serious damage to people.
Those are a few examples of unwanted outcomes. But people are also already using AI for nefarious ends, such as to create deepfakes and spread disinformation. While AI-edited or AI-generated videos and images have intriguing use cases—such as filling in for voice actors after they leave a show or pass away—generative AI has also been used to make deepfake porn, adding famous faces to adult actors, or used to defame everyday individuals. And AI has been used to flood the web with disinformation, though fact-checkers have turned to the technology to fight back.
As AI systems grow more powerful, they will rightly invite more scrutiny. Government use of software in areas such as criminal justice is often flawed or secretive, and corporations like Meta have begun confronting the downsides of their own life-shaping algorithms. More powerful AI has the potential to create worse problems, for example by perpetuating historical biases and stereotypes against women or Black people. Civil-society groups and even the tech industry itself are now exploring rules and guidelines on the safety and ethics of AI.
But the hype around generative models suggests we still haven’t learned our lesson when it comes to AI. We need to calm down; understand how it works and when it doesn’t; and then roll out this tool in a careful, considered manner, mitigating concerns as they’re raised. AI has real potential to better—and even extend—our lives, but to truly reap the benefits of machines getting smarter, we’ll need to get smarter about machines.
Learn More

AI Shouldn’t Compete With Workers—It Should Supercharge Them
The economy could get a boost if machine-learning engineers switched from copying human abilities to augmenting them.
How to Stop Robots From Becoming Racist
Algorithms can amplify patterns of discrimination. Robotics researchers are calling for new ways to prevent mechanical bodies acting out those biases.
Generative AI Won’t Revolutionize Game Development Just Yet
Hypesters say artificial intelligence will one day automate all the hard work of video game creation. But it’s not that simple.
It’s Time to Teach AI How to Be Forgetful
By emulating the human ability to forget some of the data, psychological AIs will transform algorithmic accuracy.
The Race to Build a ChatGPT-Powered Search Engine
A search bot you converse with could make finding answers easier—if it doesn’t tell fibs. Microsoft, Google, Baidu, and others are working on it.
AI Reveals the Most Human Parts of Writing
When do writers want help finding inspiration? And when do they want full control? Computers could expose the true future of the medium.
ChatGPT Isn’t the Only Way to Use AI in Education
AI can be a tool to create meaningful connections and learning experiences for children—and may help foster more equitable outcomes.
Where the AI Art Boom Came From—and Where It’s Going
In the past year, algorithms got a lot better at generating illustrations, art, and photo-realistic scenes. Next up? Video.
Plus! AI’s hallucination problem and more WIRED artificial intelligence coverage.


This guide was last updated on February 8, 2023.
Enjoyed this deep dive? Check out more WIRED Guides.",,"https://media.wired.com/photos/5a72ad9dd8520d1deb145f61/1:1/w_735,h_735,c_limit/Guides_AI.jpg","{'@type': 'CreativeWork', 'name': 'WIRED'}","Supersmart algorithms won't take all the jobs, But they are learning faster than ever, doing everything from medical diagnostics to serving up ads.","{'@type': 'WebPage', '@id': 'https://www.wired.com/story/guide-artificial-intelligence/'}",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiQmh0dHBzOi8vd3d3LmF4aW9zLmNvbS8yMDIzLzAyLzA2L2NoYXRncHQtdGVjaC1naWFudHMtZ2VuZXJhdGl2ZS1hadIBAA?oc=5,Tech's giants rush to put generative AI to work - Axios,2023-02-06,Axios,https://www.axios.com,The ChatGPT craze has industry leaders scrambling to show they're on top of the trend.,"Microsoft,Axios License,OpenAI,Meta (company),Visuals,Artificial intelligence,Illustrations,Amazon,Google,Machine learning,Apple",The ChatGPT craze has industry leaders scrambling to show they're on top of the trend.,The ChatGPT craze has industry leaders scrambling to show they're on top of the trend.,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiMmh0dHBzOi8vd3d3Lm5hdG8uaW50L2Nwcy9lbi9uYXRvaHEvbmV3c18yMTE0OTguaHRt0gEA?oc=5,NATO starts work on Artificial Intelligence certification standard - NATO HQ,2023-02-07,NATO HQ,https://www.nato.int,"NATO’s Data and Artificial Intelligence Review Board (DARB) met on Tuesday (7 February 2023) to start the development of a user-friendly and responsible Artificial Intelligence (AI) certification standard to help industries and institutions across the Alliance make sure that new AI and data projects are in line with international law, as well as NATO’s norms and values.","News,Emerging-disruptive-technologies","NATO’s Data and Artificial Intelligence Review Board (DARB) met on Tuesday (7 February 2023) to start the development of a user-friendly and responsible Artificial Intelligence (AI) certification standard to help industries and institutions across the Alliance make sure that new AI and data projects are in line with international law, as well as NATO’s norms and values.",N/A,N/A,N/A,"



NATO starts work on Artificial Intelligence certification standard 


            07 Feb. 2023 - 
          
 | 
Last updated: 07 Feb. 2023 19:55





English

French


Russian


Ukrainian







 

 NATO’s Data and Artificial Intelligence Review Board (DARB) met on Tuesday (7 February 2023) to start the development of a user-friendly and responsible Artificial Intelligence (AI) certification standard to help industries and institutions across the Alliance make sure that new AI and data projects are in line with international law, as well as NATO’s norms and values. 






The standard, which also applies to data exploitation and will include quality controls, is due to be completed by the end of 2023. Its aim is to translate NATO’s Principles of Responsible Use, approved in October 2021 as part of NATO’s first ever AI strategy, into concrete checks and balances, notably in terms of governability, traceability and reliability. This will help to build trust among the innovation community, operational end users, and the general public.
The Board is composed of nominated representatives from Allied countries, invitees Finland and Sweden, as well as NATO experts. Representatives include lawyers, engineers, military personnel and experts in ethics. It will also serve as a unique platform to exchange best practices, guide innovators and operational end-users throughout the development phase. Its work will contribute positively to NATO’s digital transformation. At present, NATO is piloting AI in areas as diverse as cyber defence, climate change and imagery analysis.
















      Official texts
    





Summary of NATO’s Autonomy Implementation Plan

13 Oct. 2022





NATO’s Data and Artificial Intelligence Review Board

13 Oct. 2022










      Topics
    





Emerging and disruptive technologies 











        News
    





NATO releases first-ever strategy for Artificial Intelligence

22 Oct. 2021





NATO Allies take further steps towards responsible use of AI, data, autonomy and digital transformation

13 Oct. 2022















",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMikwFodHRwczovL3d3dy53aG8uaW50L2F6ZXJiYWlqYW4vbmV3cy9pdGVtLzA2LTAyLTIwMjMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtaW4tbWVudGFsLWhlYWx0aC1yZXNlYXJjaC0tbmV3LXdoby1zdHVkeS1vbi1hcHBsaWNhdGlvbnMtYW5kLWNoYWxsZW5nZXPSAQA?oc=5,Artificial intelligence in mental health research: new WHO study on applications and challenges - World Health Organization (WHO),2023-02-06,World Health Organization (WHO),https://www.who.int,,N/A,"Using artificial intelligence (AI) in mental health services and research has potential, but a new study finds significant shortcomings that may indicate overly accelerated promotion of new AI models that have yet to be evaluated as viable in the
        real world. How AI can support mental health services  In 2021, over 150 million people in the WHO European Region were living with a mental health condition. Over the last few years, the COVID-19 pandemic has made matters worse. People have been less able to access services, and increases in stress,
        adverse economic conditions, conflict and violence have shown how vulnerable mental health can be. In parallel, AI has been giving rise to a revolution in medicine and health care. AI is seen as a novel tool in the planning of mental health services, as well as in identifying and monitoring mental health problems in individuals and populations.
        AI-driven tools can use digitized health-care data – available in a range of formats including electronic health records, medical images and hand-written clinical notes – to automate tasks, support clinicians and deepen understanding
        of the causes of complex disorders.  WHO/Europe’s “Regional digital health action plan for the WHO European Region 2023–2030”, launched in September 2022, also recognizes the need for innovation in predictive analytics for better health through big data and AI.  “Given the increasing use of AI in health care, it is relevant to assess the current status of the application of AI for mental health research to inform about trends, gaps, opportunities and challenges,” says Dr David Novillo-Ortiz, Regional
        Adviser on Data and Digital Health at WHO/Europe, and co-author of the study. Challenges  “Methodological and quality flaws in the use of artificial intelligence in mental health research: a systematic review”, authored by experts from the Polytechnic University of Valencia, Spain, and WHO/Europe, looked at the use of AI for
        mental health disorder studies between 2016 and 2021.   “We found that AI application use in mental health research is unbalanced and is mostly used to study depressive disorders, schizophrenia and other psychotic disorders. This indicates a significant gap in our understanding of how they can be
        used to study other mental health conditions,” adds Dr Ledia Lazeri, Regional Advisor for Mental Health at WHO/Europe. Because of the possibilities AI offers, policy-makers may gain insight into more efficient strategies to promote health and the current state of mental disorders. However, AI often involves complex use of statistics, mathematical approaches and high-dimensional
        data that could lead to bias, inaccurate interpretation of results and over-optimism of AI performance if it is not adequately handled. The study found significant flaws in how the AI applications process statistics, infrequent data validation
        and little evaluation of the risk of bias.  In addition, several other areas cause concern, such as the lack of transparent reporting on AI models, which undermines their replicability. The study found that data and models mostly remain private, and there is little collaboration between researchers.  “The lack of transparency and methodological flaws are concerning, as they delay AI’s safe, practical implementation. Also, data engineering for AI models seems to be overlooked or misunderstood, and data is often not adequately managed.
        These significant shortcomings may indicate overly accelerated promotion of new AI models without pausing to assess their real-world viability,” explains Dr Novillo-Ortiz. “Artificial intelligence stands as a cornerstone of the upcoming digital revolution. In this study, we had a glimpse of what is to come in the next few years and will drive health-care systems to adapt their structures and procedures to advance
        in the provision of mental health services,” adds Antonio Martinez-Millana, Assistant Professor at the Polytechnic University of Valencia, and co-author of the study. Select study results were presented at an event organized by WHO/Europe on 7 December 2022. Entitled “Big data analytics and AI in mental health,” the event brought together experts from across the European Region to discuss how to realistically
        use AI models in planning mental health services, as well as safety and success factors, such as involving people with mental health conditions in the development process. ",N/A,N/A,N/A,"
Using artificial intelligence (AI) in mental health services and research has potential, but a new study finds significant shortcomings that may indicate overly accelerated promotion of new AI models that have yet to be evaluated as viable in the
        real world. How AI can support mental health services  In 2021, over 150 million people in the WHO European Region were living with a mental health condition. Over the last few years, the COVID-19 pandemic has made matters worse. People have been less able to access services, and increases in stress,
        adverse economic conditions, conflict and violence have shown how vulnerable mental health can be. In parallel, AI has been giving rise to a revolution in medicine and health care. AI is seen as a novel tool in the planning of mental health services, as well as in identifying and monitoring mental health problems in individuals and populations.
        AI-driven tools can use digitized health-care data – available in a range of formats including electronic health records, medical images and hand-written clinical notes – to automate tasks, support clinicians and deepen understanding
        of the causes of complex disorders.  WHO/Europe’s “Regional digital health action plan for the WHO European Region 2023–2030”, launched in September 2022, also recognizes the need for innovation in predictive analytics for better health through big data and AI.  “Given the increasing use of AI in health care, it is relevant to assess the current status of the application of AI for mental health research to inform about trends, gaps, opportunities and challenges,” says Dr David Novillo-Ortiz, Regional
        Adviser on Data and Digital Health at WHO/Europe, and co-author of the study. Challenges  “Methodological and quality flaws in the use of artificial intelligence in mental health research: a systematic review”, authored by experts from the Polytechnic University of Valencia, Spain, and WHO/Europe, looked at the use of AI for
        mental health disorder studies between 2016 and 2021.   “We found that AI application use in mental health research is unbalanced and is mostly used to study depressive disorders, schizophrenia and other psychotic disorders. This indicates a significant gap in our understanding of how they can be
        used to study other mental health conditions,” adds Dr Ledia Lazeri, Regional Advisor for Mental Health at WHO/Europe. Because of the possibilities AI offers, policy-makers may gain insight into more efficient strategies to promote health and the current state of mental disorders. However, AI often involves complex use of statistics, mathematical approaches and high-dimensional
        data that could lead to bias, inaccurate interpretation of results and over-optimism of AI performance if it is not adequately handled. The study found significant flaws in how the AI applications process statistics, infrequent data validation
        and little evaluation of the risk of bias.  In addition, several other areas cause concern, such as the lack of transparent reporting on AI models, which undermines their replicability. The study found that data and models mostly remain private, and there is little collaboration between researchers.  “The lack of transparency and methodological flaws are concerning, as they delay AI’s safe, practical implementation. Also, data engineering for AI models seems to be overlooked or misunderstood, and data is often not adequately managed.
        These significant shortcomings may indicate overly accelerated promotion of new AI models without pausing to assess their real-world viability,” explains Dr Novillo-Ortiz. “Artificial intelligence stands as a cornerstone of the upcoming digital revolution. In this study, we had a glimpse of what is to come in the next few years and will drive health-care systems to adapt their structures and procedures to advance
        in the provision of mental health services,” adds Antonio Martinez-Millana, Assistant Professor at the Polytechnic University of Valencia, and co-author of the study. Select study results were presented at an event organized by WHO/Europe on 7 December 2022. Entitled “Big data analytics and AI in mental health,” the event brought together experts from across the European Region to discuss how to realistically
        use AI models in planning mental health services, as well as safety and success factors, such as involving people with mental health conditions in the development process. 
",http://schema.org,WebPage,,,,,,,,,item,,"[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://www.who.int/azerbaijan', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://www.who.int/azerbaijan/news', 'name': 'News'}}, {'@type': 'ListItem', 'position': 3, 'item': {'@id': 'https://www.who.int/azerbaijan/news/item', 'name': 'item'}}, {'@type': 'ListItem', 'position': 4, 'item': {'@id': '', 'name': 'Artificial intelligence in mental health research: new WHO study on applications and challenges'}}]",,,,,,,ItemListOrderAscending,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifGh0dHBzOi8vd3d3LnRlY2hub2xvZ3lyZXZpZXcuY29tLzIwMjMvMDIvMDcvMTA2NzkyOC93aHktZGV0ZWN0aW5nLWFpLWdlbmVyYXRlZC10ZXh0LWlzLXNvLWRpZmZpY3VsdC1hbmQtd2hhdC10by1kby1hYm91dC1pdC_SAYABaHR0cHM6Ly93d3cudGVjaG5vbG9neXJldmlldy5jb20vMjAyMy8wMi8wNy8xMDY3OTI4L3doeS1kZXRlY3RpbmctYWktZ2VuZXJhdGVkLXRleHQtaXMtc28tZGlmZmljdWx0LWFuZC13aGF0LXRvLWRvLWFib3V0LWl0L2FtcC8?oc=5,Why detecting AI-generated text is so difficult (and what to do about it) - MIT Technology Review,2023-02-07,MIT Technology Review,https://www.technologyreview.com,Plus: AI models generate copyrighted images and photos of real people.,,Plus: AI models generate copyrighted images and photos of real people.,Plus: AI models generate copyrighted images and photos of real people.,N/A,N/A,N/A,http://schema.org,Organization,https://www.technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/,"{'@type': 'ImageObject', 'url': 'https://wp.technologyreview.com/wp-content/uploads/2023/02/detect-AI.jpeg?resize=854,569', 'height': 569, 'width': 854}","{'@type': 'Person', 'name': 'Melissa Heikkilä'}","{'@type': 'Organization', 'name': 'MIT Technology Review', 'logo': {'@type': 'ImageObject', 'url': 'https://wp.technologyreview.com/wp-content/themes/mittr/client/src/images/logo.png', 'width': 203, 'height': 100}}",Why detecting AI-generated text is so difficult (and what to do about it),2023-02-07T04:41:10-05:00,2023-02-07T04:41:10-05:00,Artificial intelligence,MIT Technology Review,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Artificial intelligence', 'item': {'@type': 'Thing', '@id': 'https://www.technologyreview.com/topic/artificial-intelligence/'}}, {'@type': 'ListItem', 'position': 2, 'name': 'Why detecting AI-generated text is so difficult (and what to do about it)', 'item': {'@type': 'Thing', '@id': 'https://www.technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/'}}]",,,"https://wp.technologyreview.com/wp-content/uploads/2023/02/detect-AI.jpeg?resize=854,569",,,"{'@type': 'WebPage', '@id': 'https://www.technologyreview.com/2023/02/07/1067928/why-detecting-ai-generated-text-is-so-difficult-and-what-to-do-about-it/'}",,2023-02-07T04:41:10-05:00,1197.0,"{'@type': 'Person', 'name': 'Melissa Heikkilä'}","{'@type': 'PostalAddress', 'addressLocality': 'Cambridge, MA, USA', 'postalCode': '02142', 'streetAddress': '1 Main Street'}",,,,,,,
https://news.google.com/rss/articles/CBMiUmh0dHBzOi8vd3d3LmJ1LmVkdS9hcnRpY2xlcy8yMDIzL2Nhbi13ZS10cnVzdC1jaGF0Z3B0LWFuZC1hcnRpZmljaWFsLWludGVsbGlnZW5jZS_SAQA?oc=5,Can We Trust ChatGPT and Artificial Intelligence to Do Humans' Work? - Boston University,2023-02-08,Boston University,https://www.bu.edu,"ChatGPT can generate essays and articles, hold conversations, and write speeches—but should we trust it?",N/A,"ChatGPT can generate essays and articles, hold conversations, and write speeches—but should we trust it?","ChatGPT can generate essays and articles, hold conversations, and write speeches—but should we trust it?",N/A,N/A,"
Can We Trust ChatGPT and Artificial Intelligence to Do Humans’ Work?OpenAI’s new AI chatbot is making (and writing) headlines, but research by BU behavioral scientist Chiara Longoni suggests we’re still skeptical of machine learning 




An AI-generated image created using the prompt: “artificial intelligence in the future.” Image created by Shutterstock’s AI image generator


Artificial Intelligence

																	Can We Trust ChatGPT and Artificial Intelligence to Do Humans’ Work?															
OpenAI’s new AI chatbot is making (and writing) headlines, but research by BU behavioral scientist Chiara Longoni suggests we’re still skeptical of machine learning






February 8, 2023

0



Andrew Thurston



Twitter
Facebook




This article was not written by ChatGPT. The artificial intelligence–powered chatbot—which can generate essays and articles with a simple prompt, have natural-sounding conversations, debug computer code, write songs, and even draft Congressional floor speeches—has quickly become a phenomenon. Developed by the Microsoft-backed OpenAI, the computer program reportedly hit 100 million users in January alone and has been called an AI breakthrough. Its apparent prowess—in one study, it fooled respected scientists into believing its fake research paper abstracts—has left professional writers feeling nervous and spooked Google into urgently ramping up its own AI efforts.
But is all the hype overblown? Behavioral scientist Chiara Longoni, a Boston University Questrom School of Business assistant professor of marketing, says we may all be intrigued by ChatGPT—and AI in general—but we’re a long way from embracing it or trusting it over a human.
In recent studies, Longoni has looked at whether people believe AI-generated news stories or trust a computer program’s medical diagnoses. She’s found we’re mostly pretty skeptical of AI. A machine can write a fact-packed news article, but we’ll second-guess its veracity; a program can give a more accurate medical analysis than a human, but we’re still more likely to go with our doctor’s advice.
Chiara Longoni has found we’ll cut people some slack when they mess up, but we’re much less forgiving of artificial intelligence. Photo by Eric Levin
Longoni’s latest paper, published in the Journal of Marketing Research, examines trust in AI programs used by the government. With researchers from Babson College and the University of Virginia, she studied how consumers react when AI gets something wrong—in calculating benefits or tracking social security fraud, for example—versus what happens if a person messes up. When an AI program made a mistake, Longoni discovered, people distrusted all AI, something the researchers called “algorithmic transference.” But we’re less likely to mistrust all people when one person drops the ball—meaning, we cut people slack if they screw up, but don’t give AI the same leeway.
So, if we’re not ready to trust AI, what to make of the rapid growth of ChatGPT? Could it mark a turning point in our attitude toward algorithms? The Brink asked Longoni, a Dean’s Research Scholar, about the latest AI leap, our resistance to intelligent computer programs, and whether she’s worried about her students submitting an influx of ChatGPT-penned essays.
Q&Awith Chiara Longoni
The Brink: As a journalist, should I be worried ChatGPT is coming for my job—or are readers still more likely to trust my articles over those by a computer?
Longoni: We have a paper—one of the authors is my Questrom colleague Andrey Fradkin—on generative AI for text. We looked at how people respond when generative AI is applied to news production. We took real news headlines and manipulated whether these news stories were tagged as written by a human reporter or by an AI algorithm. We were interested in the extent to which people perceived this news as accurate or not. Because we used real stories, we knew whether the news was objectively fake or true. The finding was really interesting, especially now as I’m sensing people tend to perhaps be overly reliant on what ChatGPT actually writes, because we found that, irrespective of the actual veracity of the news, people discounted news that was tagged as written by AI—they were thinking that the news was less accurate. People tend to believe generative AI less compared to the same content written by a human reporter. It was an effect that was robust—it manifested no matter what the political affiliation of the person was, their background, income, socioeconomic status, age.I don’t have any research that has specifically looked at ChatGPT, but my sense is—and it’s completely my speculation—many areas of our society are fearing ChatGPT’s potential for algorithmic amplification of inaccurate or biased information. Excessive reliance on these generative AIs might propagate bias if people are not aware that it can have limitations.
The Brink: ChatGPT has reportedly outpaced TikTok’s initial growth. What’s made people so open to using it and do you think this might be a tipping point in our comfort with AI?
Longoni: The fact that people want to use it doesn’t mean that people trust it. It’s something that’s very cool; it’s a type of AI that is fairly revolutionary and accessible to people—they can just log in and see. There’s a lot of curiosity underlying that. Number two, I was talking to one of my students recently and they mentioned they tried to use it, and they got locked out. So, there’s that scarcity, which makes it even more sought after—the fact that you want to see how this thing works, and try it yourself, but you can’t access it. Scarcity is something that works really well in terms of driving and generating interest for technology.I don’t know if this is going to have implications in terms of baseline trust for these technologies at large. It’s going to be really interesting to see how this evolves, because it’s possible that the accessibility of this technology will create a spillover positive effect on trust of other technologies, which may be good and bad. Does it make people more vulnerable to misinformation or does it actually make people more familiar with the technology and with the limitations of that technology? In general, my view is that all of these technologies are tools, which means that the question should be how to use them optimally, being aware of their potential and their limitations and weaknesses. So, it’s possible that these would actually prompt people to learn how to leverage these technologies. When we were writing our paper, we thought at length about the repercussions for ethicists and policymakers. There are a lot of news outlets that have their own automated software for generating news, and typically these are not disclosed to people. There’re already people talking about how there need to be guidelines to show if an article has been generated with the assistive role of AI in any form.
The Brink: Your research seems to get at the psychology of when and why people trust AI—or don’t trust it.
Longoni: What is the reason for trust or distrust? In my work, I have looked at reasons for trust or distrust in more applied settings like in the context of AI for healthcare rather than text or image generation. In these cases, the focal task is a prediction—like the forecasting of a person having a heart attack, a recommendation for having coronary bypass surgery, or making a skin cancer risk assessment—that could be done by an algorithm or by a physician.
The Brink: So, you’re looking to see if we trust a diagnosis made by a computer?
Longoni: Yes, exactly. We tested across a number of decisions, spanning preventive services, treatment recommendations, and systematically varied who—or what—was going to make that decision, whether it was an algorithm or physician. The general finding was that people are resistant to medical AI. They are more reluctant to pursue healthcare if it’s provided by AI compared to the same healthcare provided by a human. This, too, is an effect that was robust—it carried over, no matter the age of the person or the type of medical decision.
The Brink: Where does this resistance to AI come from?
Longoni: There are two reasons that we investigated. One is what we call uniqueness neglect. It essentially captures the fact that people believe that their health, status, and health condition is unique to them, and people also believe that AI treats everybody in the same way—you become a number, a statistic to AI. The fear of having our own unique circumstances neglected, or not accounted for properly, is what explains why people want to have a doctor—a doctor is going to get me because they’re a human being.Another reason we identified is both an illusion and a reality. These algorithms are often black boxes. In fact, the more sophisticated the neural network that’s used, the harder it is to make that algorithm transparent and to open the black box and make it explainable. It’s the reason ethicists and policymakers want to talk about the importance of explaining algorithms. But we also have this illusory understanding of the way humans make decisions. We think that we can look at the human machinery in the way in which a physician makes a medical decision, and that’s transparent to us as fellow human beings. That is an illusion. Just like we don’t know how an algorithm makes a medical diagnosis, we truly don’t know how a doctor makes a medical diagnosis. Because we have this illusory understanding of how transparent human decision-making is, we kind of punish algorithms for their black box-ness.
The Brink: Is that AI skepticism a common finding across your studies, whether in news, medicine, or elsewhere? Are there places where we’re ready to accept AI?
Longoni: There is a general distrust for AI, no matter what the application, more often than there is a tendency to embrace AI. That said, there are circumstances where people do embrace the help of AI. One condition that we specifically saw where people actually prefer the AI recommendation over a person is when they want to optimize the practicalities, the utilitarian value of the recommendation, such as a recommendation for a practical apartment or a recipe high in micronutrients. Another circumstance is when the task is presented to people as heavily quantitative and objective, like estimating the weight of a person or forecasting the weather. It makes sense that a recommendation by a person is assumed to be more subjective and qualitative, and maybe based on their own personal opinion rather than based on objective criteria. So, when quantitative and objective parameters are more salient, people tend to prefer the recommendation of AI.Another type of circumstance where people are not necessarily more favorable to AI, but rather indifferent to AI—equally likely to choose AI or stick with a person—is when AI serves in an assistant role to a person who retains the veto or remains the ultimate decision-maker. So, people are fine with AI doing some things as long as the final call is made by a person.
The Brink: Do you feel like the conversation around AI is stuck between extremes—AI is wonderful, AI is terrible—and actually the truth is somewhere in the middle?
Longoni: There tends to be a view of AI as the ultimate utopia or dystopia. My personal opinion: I’m an AI optimist overall, without necessarily believing that AI is a cure-all. It’s a tool, a technology. There certainly needs to be oversight and regulation, but I think there’s a lot of potential to do a lot of good. And also, it’s inevitable, I don’t think it can be stopped.For instance, and this speaks to the main body of my work, which is on medical AI, AI can democratize healthcare. It can really do a lot of good things—like the use of an app for a skin cancer risk assessment. People that can’t afford to go to a dermatologist or people that could simply not have access to that type of care, now can be alerted to that primary red flag—go check this mole. So, AI can have the potential to make prescriptively good outcomes available to people that otherwise could not have access to it. And so the question that I’m interested in is, if AI is a good thing, how can we understand how people view it and how can we foster uptake?
The Brink: On a different note, as an educator, how concerned are you about ChatGPT and the prospect of students submitting AI-generated essays?
Longoni: When I first read about it, I was like, okay, do I allow it in my class or not? I think it varies by class and learning goals. There are some classes in which I think AI could be allowed—I’m seeing some colleagues who teach programming or technical classes allow it—and some classes in which it could be banned or perhaps regulated, such as a creative writing class. Eventually, this help from technology will become more and more normal in our classes and education. I feel like there are maybe some cases in which it could be an equalizer, too, on condition of being disclosed. I’m thinking about services that maybe are really expensive, like copy editing services. I’ve heard from students that they’ve used it to generate an outline they would then edit. Maybe one day it will become a source, and be cited like other sources.I think what’s really important is to teach ourselves and teach the potential users what the tools can and cannot do, how to correctly scrutinize the output, and how to write the prompts.

This interview has been edited—by a human—for clarity and brevity.


			Explore Related Topics:		



							Apps						



							Business						



							Business & Social Sciences						



							Communications						



							Computers						



							Data Sciences						



							Healthcare						



							Healthcare Technologies						



							Innovation						



							Questrom						



							Research						






		Share this story		

0
Comments
Add





Share
Can We Trust ChatGPT and Artificial Intelligence to Do Humans’ Work?
ShareTwitterFacebookRedditLinkedInEmailPrint 




Copy URL:

Copy







",http://schema.org,NewsArticle,,"['/files/2023/02/Can-We-Trust-ChatGPT-feat.jpg', '/files/2023/02/Can-We-Trust-ChatGPT-feat.jpg', '/files/2023/02/Can-We-Trust-ChatGPT-feat.jpg']","{'@type': 'Person', 'name': 'Andrew Thurston'}","{'@type': 'Organization', 'name': 'The Brink', 'logo': {'@type': 'ImageObject', 'url': '/files/2021/06/brink_60px_vh.png'}}",Can We Trust ChatGPT and Artificial Intelligence to Do Humans’ Work?,2023-02-08T12:47:00-04:00,2023-02-15T10:21:32-04:00,,,,,,,,,,"{'@type': 'WebPage', '@id': 'https://www.bu.edu/articles/2023/can-we-trust-chatgpt-and-artificial-intelligence/'}",,,,,,"[{'@type': 'Organization', '@id': 'https://www.bu.edu/#organization', 'name': 'Boston University', 'url': 'https://www.bu.edu/', 'sameAs': ['https://www.facebook.com/BostonUniversity/', 'https://www.instagram.com/bostonu/', 'https://www.linkedin.com/school/boston-university/', 'https://www.youtube.com/user/bu', 'https://www.pinterest.com/bostonuniv/', 'https://en.wikipedia.org/wiki/Boston_University', 'https://twitter.com/BU_Tweets']}, {'@type': 'WebSite', '@id': 'https://www.bu.edu/#website', 'url': 'https://www.bu.edu/', 'name': 'Boston University', 'publisher': {'@id': 'https://www.bu.edu/#organization'}, 'potentialAction': {'@type': 'SearchAction', 'target': 'https://www.bu.edu/?s={search_term_string}', 'query-input': 'required name=search_term_string'}}, {'@type': 'WebPage', '@id': 'https://www.bu.edu/articles/2023/can-we-trust-chatgpt-and-artificial-intelligence/#webpage', 'url': 'https://www.bu.edu/articles/2023/can-we-trust-chatgpt-and-artificial-intelligence/', 'inLanguage': 'en-US', 'name': 'Can We Trust ChatGPT and Artificial Intelligence to Do Humans’ Work? | The Brink | Boston University', 'isPartOf': {'@id': 'https://www.bu.edu/#website'}, 'image': {'@type': 'ImageObject', '@id': 'https://www.bu.edu/articles/2023/can-we-trust-chatgpt-and-artificial-intelligence/#primaryimage', 'url': '/files/2023/02/Can-We-Trust-ChatGPT-feat.jpg', 'width': 600, 'height': 401, 'caption': 'Image: AI-generated retro illustration of a purple-themed robot placed against a black background.'}, 'primaryImageOfPage': {'@id': 'https://www.bu.edu/articles/2023/can-we-trust-chatgpt-and-artificial-intelligence/#primaryimage'}, 'datePublished': '2023-02-08T17:47:00-04:00', 'dateModified': '2023-02-15T15:21:32-04:00', 'description': 'ChatGPT can generate essays and articles, hold conversations, and write speeches—but should we trust it?'}]","{'@type': 'SpeakableSpecification', 'xpath': ['/html/body/div[2]/div[2]/div/main/div/div/article/header/h1', ""/html/head/meta[@name='description']/@content""]}",,,,,
https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vd3d3Lm5ld3lvcmtlci5jb20vY3VsdHVyZS9pbmZpbml0ZS1zY3JvbGwvaXMtYWktYXJ0LXN0ZWFsaW5nLWZyb20tYXJ0aXN0c9IBAA?oc=5,Is A.I. Art Stealing from Artists? - The New Yorker,2023-02-10,The New Yorker,https://www.newyorker.com,"Kyle Chayka writes about a class-action lawsuit by visual artists against the artificial-intelligence image generators Midjourney, Stable Diffusion, and DreamUp.","['infinite scroll', 'artificial intelligence (a.i.)', 'internet', 'technology', 'artists', 'lawsuits', 'splitscreenimageleftfullbleed', 'inverted', 'web']","According to the lawyer behind a new class-action suit, every image that a generative tool produces “is an infringing, derivative work.”","According to the lawyer behind a new class-action suit, every image that a generative tool produces “is an infringing, derivative work.”",tags,N/A,"Infinite ScrollIs A.I. Art Stealing from Artists?According to the lawyer behind a new class-action suit, every image that a generative tool produces “is an infringing, derivative work.”By Kyle ChaykaFebruary 10, 2023FacebookXEmailPrintSave StoryIllustration by Nicholas Konrad / The New Yorker; ; Source photographs from GettySave this storySave this storySave this storySave this storyLast year, a Tennessee-based artist named Kelly McKernan noticed that their name was being used with increasing frequency in A.I.-driven image generation. McKernan makes paintings that often feature nymphlike female figures in an acid-colored style that blends Art Nouveau and science fiction. A list published in August, by a Web site called Metaverse Post, suggested “Kelly McKernan” as a term to feed an A.I. generator in order to create “Lord of the Rings”-style art. Hundreds of other artists were similarly listed according to what their works evoked: anime, modernism, “Star Wars.” On the Discord chat that runs an A.I. generator called Midjourney, McKernan discovered that users had included their name more than twelve thousand times in public prompts. The resulting images—of owls, cyborgs, gothic funeral scenes, and alien motorcycles—were distinctly reminiscent of McKernan’s works. “It just got weird at that point. It was starting to look pretty accurate, a little infringe-y,” they told me. “I can see my hand in this stuff, see how my work was analyzed and mixed up with some others’ to produce these images.”Last month, McKernan joined a class-action lawsuit with two other artists, Sarah Andersen and Karla Ortiz, filed by the attorneys Matthew Butterick and Joseph Saveri, against Midjourney and two other A.I. imagery generators, Stable Diffusion and DreamUp. (Other tools, such as DALL-E, run on the same principles.) All three models make use of LAION-5B, a nonprofit, publicly available database that indexes more than five billion images from across the Internet, including the work of many artists. The alleged wrongdoing comes down to what Butterick summarized to me as “the three ‘C’s”: The artists had not consented to have their copyrighted artwork included in the LAION database; they were not compensated for their involvement, even as companies including Midjourney charged for the use of their tools; and their influence was not credited when A.I. images were produced using their work. When producing an image, these generators “present something to you as if it’s copyright free,” Butterick told me, adding that every image a generative tool produces “is an infringing, derivative work.”AdChoicesCopyright claims based on questions of style are often tricky. In visual art, courts have sometimes ruled in favor of the copier rather than the copied. When the artist Richard Prince incorporated photographs by Patrick Cariou into his work, for instance, a 2013 court case found that some of the borrowing was legal under transformative use—Prince had changed the source material enough to escape any claim of infringement. In music, recent judgments tend to be more conservative. Robin Thicke and Pharrell Williams lost a 2013 case against the Marvin Gaye estate, which alleged that their song “Blurred Lines” was too close to Gaye’s “Got to Give It Up.” The intellectual-property lawyer Kate Downing wrote, in a recent essay on Butterick and Saveri’s suit published on her personal Web site, that the A.I. image generators might be closer to the former than the latter: ​​“It may well be argued that the ‘use’ of any one image from the training data is . . . not substantial enough to call the output a derivative work of any one image.” “Mathematically speaking, the work comes from everything,” Downing told me.But Butterick and Saveri allege that what A.I. generators do falls short of transformative use. There is no transcending of the source material, just a mechanized “blending together,” Butterick said. “We’re not litigating image by image, we’re litigating the whole technique behind the system.” The litigators are not alone. Last week, Getty Images filed a lawsuit against Stable Diffusion alleging that the generator’s use of Getty stock photography amounts to “brazen infringement . . . on a staggering scale.” Whatever their legal strengths, such claims possess a certain moral weight. A.I. generators could not operate without the labor of humans like McKernan who unwittingly provide source material. As the technology critic and philosopher Jaron Lanier wrote in his 2013 book “Who Owns the Future?,” “Digital information is really just people in disguise.” (A spokesperson from Stability AI, the studio that developed Stable Diffusion, said in a statement that “the allegations in this suit represent a misunderstanding of how generative A.I. technology works and the law surrounding copyright,” but provided no further detail. Neither DeviantArt, which owns DreamUp, nor Midjourney responded to requests for comment.)Visual artists began reaching out to Butterick after he and Saveri filed a lawsuit, last November, in the related but distinct realm of software copyrights. The target of the earlier suit was Copilot, an A.I.-driven coding assistant developed by GitHub and OpenAI. Copilot is trained on code that is publicly available online. Coders who post their projects on open-source platforms retain the copyright to their work—under certain licenses, anyone who uses the code must credit its creator. Copilot did not. Like the artists whose work feeds Midjourney, human coders suddenly found their specialized labor reproduced infinitely, quickly, and cheaply without attribution. Butterick and Saveri’s legal complaint (against OpenAI, GitHub, and Microsoft, which acquired GitHub in 2018) argued that Copilot’s actions amount to “software piracy on an unprecedented scale.” In January, the defendants filed to have the case dismissed. “We will file oppositions to these motions,” Butterick said.Butterick told me that, given the proliferation of A.I., “everybody who creates for a living should be in code red.” Writers had their turn to be spooked in January, when BuzzFeed announced that it would use OpenAI’s new large language model, ChatGPT, to augment its creation of quizzes. McKernan, who draws income from print sales as well as commissioned illustrations, told me they suspect that the amount of work available in their field is already declining as A.I. tools become more accessible online. “There are publishers that are using A.I. instead of hiring cover artists,” McKernan told me. “I can pay my rent with just one cover, and we’re seeing that already disappearing.” They added, “We’re just the canaries in the coal mine.”In some sense, you could say that artists are losing their monopoly on being artists. With generative A.I., any user can become an author of sorts. In late January, Mayk.it, a Los Angeles-based music-making app, released Drayk.it, a Web site that allowed users to create A.I.-generated Drake songs based on a given prompt. The results could not be mistaken for actual Drake tracks; they tend toward the lo-fi and the absurd. But they possess a certain fundamental Drakeness: lounge beats, depressive lyrics, monotone delivery. The company’s head of product, Neer Sharma, told me that users had created hundreds of thousands of A.I. Drake songs, a new track every three seconds. The site drew upon software resources such as Tacotron and Uberduck, which generate voices and offer specific voice models, including one trained on the œuvre of Drake. The Web site includes a disclaimer that the songs it generates are parodies, which are protected under fair use, and Sharma told me that the company didn’t receive any complaints from Drake’s camp. But the site has already shut down. The project was designed “just to test out the tech,” Sharma said. “We didn’t expect it to get this big.” The team is now preparing more “A.I. music drops.”Video From The New YorkerSurfing on Kelly Slater’s Machine-Made WaveAs Sharma sees it, the increasing accessibility of A.I. means that “everything just becomes remixable.” The artists who might thrive in this scenario are those who have the most replicable or exportable “vibe and aesthetic,” he said, among them Drake, the rare pop star who has embraced his status as an open-ended meme. Fans could already dress like Drake or act like him; now they can make his music, too, and the line between fan and creator will blur further as the generative technology improves. Sharma said that the company has heard from executives at music labels who are interested in exploring the creation of A.I. voice models for their artists. He predicted that musicians who resist being “democratized”—giving creative agency to their fans—will be left behind. “The people who could win before just by being there will not necessarily win tomorrow,” Sharma said.A startup called Authentic Artists is seeking to bypass human artists altogether by creating musician characters based on A.I.-generated styles of music. Its label, WarpSound, features “virtual artists” like GLiTCH, a computer-rendered figure derived from a Bored Ape Yacht Club N.F.T., who plays genres such as “chillwave” and “glitch hop” in endlessly streaming feeds of auto-generated music. Authentic Artists’ founder Chris McGarry told me that the character is meant to give a face to the A.I. machine. “We wanted to answer the question, what is the source of the music? A semiconductor or cloud-based server or ones and zeros didn’t seem to be a terribly interesting answer to that question,” he said.Listening to Authentic Artists’ music, however, is a bit like trying to enjoy the wavering buzz of highway traffic. If you’re not paying attention, it can serve as a passable background soundtrack, but the moment you tune in closely any sense of coherent sound gives way to an uncanny randomness. It called to my mind a much-memed comment that the Studio Ghibli director Hayao Miyazaki made after being shown a particularly grotesque A.I.-generated animation in 2016: “I strongly feel that this is an insult to life itself.” I wouldn’t go quite so far, but Authentic Artists’ project does strike me as an insult to human-made music. They can manufacture sound, but they can’t manufacture the feeling or creative intention that even the most amateur musicians put into a recording.Kelly McKernan sometimes snoops on conversations about generative A.I. on Reddit or Discord chats, in part to see how users perceive the role of original artists in the A.I. image-making process. McKernan said that they often see people criticizing artists who are against A.I.: “They have this belief that career artists, people who have dedicated their whole lives to their work, are gatekeeping, keeping them from making the art they want to make. They think we’re élitist and keeping our secrets.” Defenders of A.I. art-making could point out that artists have always taken from and riffed on each other’s work, from the ancient Romans making copies of even older Greek sculptures to Roy Lichtenstein reproducing comic-book frames as highbrow Pop art. Maybe A.I. imagery is just a new wave of appropriation art? (It lacks any conceptual intention, however.) Downing, the intellectual-property lawyer, argued in her piece that the prompts that users input into A.I. generators may amount to independent acts of invention. “There is no Stable Diffusion without users pouring their own creative energy into its prompts,” she wrote.McKernan told me about Beep Boop Art, a Facebook group with forty-seven thousand followers that posts A.I.-generated art and runs an online storefront selling prints and merchandise. The images tend toward the fantastical: a wizard hat or a lunar landscape in a Lisa Frank-ish style, or a tree house growing above the ocean. It may not be a direct riff on McKernan’s work, but it does reflect a banal over-all sameness across generated art. McKernan described typical A.I. style as having “this general sugary, candy look,” adding, “It looks pretty, but it tastes terrible. It has no depth, but it serves the purpose that they want.” The new generation of tools offers the instant gratification of a single image, shorn of the messy association with a single, living artist. One question is who gets to profit from such works. Another is more existential. “It kind of boils down to: what is art?” McKernan said. “Is art the process, is art the human component, is art the conversation? All of that is out of the picture once you’re just generating it.” ♦",https://schema.org/,BreadcrumbList,https://www.newyorker.com/culture/infinite-scroll/is-ai-art-stealing-from-artists,"['https://media.newyorker.com/photos/63e3f7ee37967d9f2e5f5883/16:9/w_1487,h_836,c_limit/chayka_ai_copyright.jpg', 'https://media.newyorker.com/photos/63e3f7ee37967d9f2e5f5883/4:3/w_1499,h_1124,c_limit/chayka_ai_copyright.jpg', 'https://media.newyorker.com/photos/63e3f7ee37967d9f2e5f5883/1:1/w_1500,h_1500,c_limit/chayka_ai_copyright.jpg']","[{'@type': 'Person', 'name': 'Kyle Chayka', 'sameAs': 'https://www.newyorker.com/contributors/kyle-chayka'}]","{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'The New Yorker', 'logo': {'@type': 'ImageObject', 'url': 'https://www.newyorker.com/verso/static/the-new-yorker/assets/social-image-hub.jpg', 'width': '500px', 'height': '117px'}, 'url': 'https://www.newyorker.com'}",Is A.I. Art Stealing from Artists?,2023-02-10T06:00:00.000-05:00,2023-02-10T06:00:00.000-05:00,infinite scroll,,True,"[{'@type': 'ListItem', 'position': 1, 'name': 'Culture', 'item': 'https://www.newyorker.com/culture'}, {'@type': 'ListItem', 'position': 2, 'name': 'Artificial Intelligence (A.I.)', 'item': 'https://www.newyorker.com/tag/artificial-intelligence-ai'}, {'@type': 'ListItem', 'position': 3, 'name': 'Is A.I. Art Stealing from Artists?'}]","Last month, McKernan joined a class-action lawsuit with two other artists, Sarah Andersen and Karla Ortiz, filed by the attorneys Matthew Butterick and Joseph Saveri, against Midjourney and two other A.I. imagery generators, Stable Diffusion and DreamUp. (Other tools, such as DALL-E, run on the same principles.) All three models make use of LAION-5B, a nonprofit, publicly available database that indexes more than five billion images from across the Internet, including the work of many artists. The alleged wrongdoing comes down to what Butterick summarized to me as “the three ‘C’s”: The artists had not consented to have their copyrighted artwork included in the LAION database; they were not compensated for their involvement, even as companies including Midjourney charged for the use of their tools; and their influence was not credited when A.I. images were produced using their work. When producing an image, these generators “present something to you as if it’s copyright free,” Butterick told me, adding that every image a generative tool produces “is an infringing, derivative work.”
Copyright claims based on questions of style are often tricky. In visual art, courts have sometimes ruled in favor of the copier rather than the copied. When the artist Richard Prince incorporated photographs by Patrick Cariou into his work, for instance, a 2013 court case found that some of the borrowing was legal under transformative use—Prince had changed the source material enough to escape any claim of infringement. In music, recent judgments tend to be more conservative. Robin Thicke and Pharrell Williams lost a 2013 case against the Marvin Gaye estate, which alleged that their song “Blurred Lines” was too close to Gaye’s “Got to Give It Up.” The intellectual-property lawyer Kate Downing wrote, in a recent essay on Butterick and Saveri’s suit published on her personal Web site, that the A.I. image generators might be closer to the former than the latter: ​​“It may well be argued that the ‘use’ of any one image from the training data is . . . not substantial enough to call the output a derivative work of any one image.” “Mathematically speaking, the work comes from everything,” Downing told me.
But Butterick and Saveri allege that what A.I. generators do falls short of transformative use. There is no transcending of the source material, just a mechanized “blending together,” Butterick said. “We’re not litigating image by image, we’re litigating the whole technique behind the system.” The litigators are not alone. Last week, Getty Images filed a lawsuit against Stable Diffusion alleging that the generator’s use of Getty stock photography amounts to “brazen infringement . . . on a staggering scale.” Whatever their legal strengths, such claims possess a certain moral weight. A.I. generators could not operate without the labor of humans like McKernan who unwittingly provide source material. As the technology critic and philosopher Jaron Lanier wrote in his 2013 book “Who Owns the Future?,” “Digital information is really just people in disguise.” (A spokesperson from Stability AI, the studio that developed Stable Diffusion, said in a statement that “the allegations in this suit represent a misunderstanding of how generative A.I. technology works and the law surrounding copyright,” but provided no further detail. Neither DeviantArt, which owns DreamUp, nor Midjourney responded to requests for comment.)
Visual artists began reaching out to Butterick after he and Saveri filed a lawsuit, last November, in the related but distinct realm of software copyrights. The target of the earlier suit was Copilot, an A.I.-driven coding assistant developed by GitHub and OpenAI. Copilot is trained on code that is publicly available online. Coders who post their projects on open-source platforms retain the copyright to their work—under certain licenses, anyone who uses the code must credit its creator. Copilot did not. Like the artists whose work feeds Midjourney, human coders suddenly found their specialized labor reproduced infinitely, quickly, and cheaply without attribution. Butterick and Saveri’s legal complaint (against OpenAI, GitHub, and Microsoft, which acquired GitHub in 2018) argued that Copilot’s actions amount to “software piracy on an unprecedented scale.” In January, the defendants filed to have the case dismissed. “We will file oppositions to these motions,” Butterick said.
Butterick told me that, given the proliferation of A.I., “everybody who creates for a living should be in code red.” Writers had their turn to be spooked in January, when BuzzFeed announced that it would use OpenAI’s new large language model, ChatGPT, to augment its creation of quizzes. McKernan, who draws income from print sales as well as commissioned illustrations, told me they suspect that the amount of work available in their field is already declining as A.I. tools become more accessible online. “There are publishers that are using A.I. instead of hiring cover artists,” McKernan told me. “I can pay my rent with just one cover, and we’re seeing that already disappearing.” They added, “We’re just the canaries in the coal mine.”
As Sharma sees it, the increasing accessibility of A.I. means that “everything just becomes remixable.” The artists who might thrive in this scenario are those who have the most replicable or exportable “vibe and aesthetic,” he said, among them Drake, the rare pop star who has embraced his status as an open-ended meme. Fans could already dress like Drake or act like him; now they can make his music, too, and the line between fan and creator will blur further as the generative technology improves. Sharma said that the company has heard from executives at music labels who are interested in exploring the creation of A.I. voice models for their artists. He predicted that musicians who resist being “democratized”—giving creative agency to their fans—will be left behind. “The people who could win before just by being there will not necessarily win tomorrow,” Sharma said.
A startup called Authentic Artists is seeking to bypass human artists altogether by creating musician characters based on A.I.-generated styles of music. Its label, WarpSound, features “virtual artists” like GLiTCH, a computer-rendered figure derived from a Bored Ape Yacht Club N.F.T., who plays genres such as “chillwave” and “glitch hop” in endlessly streaming feeds of auto-generated music. Authentic Artists’ founder Chris McGarry told me that the character is meant to give a face to the A.I. machine. “We wanted to answer the question, what is the source of the music? A semiconductor or cloud-based server or ones and zeros didn’t seem to be a terribly interesting answer to that question,” he said.
Listening to Authentic Artists’ music, however, is a bit like trying to enjoy the wavering buzz of highway traffic. If you’re not paying attention, it can serve as a passable background soundtrack, but the moment you tune in closely any sense of coherent sound gives way to an uncanny randomness. It called to my mind a much-memed comment that the Studio Ghibli director Hayao Miyazaki made after being shown a particularly grotesque A.I.-generated animation in 2016: “I strongly feel that this is an insult to life itself.” I wouldn’t go quite so far, but Authentic Artists’ project does strike me as an insult to human-made music. They can manufacture sound, but they can’t manufacture the feeling or creative intention that even the most amateur musicians put into a recording.
Kelly McKernan sometimes snoops on conversations about generative A.I. on Reddit or Discord chats, in part to see how users perceive the role of original artists in the A.I. image-making process. McKernan said that they often see people criticizing artists who are against A.I.: “They have this belief that career artists, people who have dedicated their whole lives to their work, are gatekeeping, keeping them from making the art they want to make. They think we’re élitist and keeping our secrets.” Defenders of A.I. art-making could point out that artists have always taken from and riffed on each other’s work, from the ancient Romans making copies of even older Greek sculptures to Roy Lichtenstein reproducing comic-book frames as highbrow Pop art. Maybe A.I. imagery is just a new wave of appropriation art? (It lacks any conceptual intention, however.) Downing, the intellectual-property lawyer, argued in her piece that the prompts that users input into A.I. generators may amount to independent acts of invention. “There is no Stable Diffusion without users pouring their own creative energy into its prompts,” she wrote.
McKernan told me about Beep Boop Art, a Facebook group with forty-seven thousand followers that posts A.I.-generated art and runs an online storefront selling prints and merchandise. The images tend toward the fantastical: a wizard hat or a lunar landscape in a Lisa Frank-ish style, or a tree house growing above the ocean. It may not be a direct riff on McKernan’s work, but it does reflect a banal over-all sameness across generated art. McKernan described typical A.I. style as having “this general sugary, candy look,” adding, “It looks pretty, but it tastes terrible. It has no depth, but it serves the purpose that they want.” The new generation of tools offers the instant gratification of a single image, shorn of the messy association with a single, living artist. One question is who gets to profit from such works. Another is more existential. “It kind of boils down to: what is art?” McKernan said. “Is art the process, is art the human component, is art the conversation? All of that is out of the picture once you’re just generating it.” ♦",https://www.newyorker.com/culture/infinite-scroll/is-ai-art-stealing-from-artists,"https://media.newyorker.com/photos/63e3f7ee37967d9f2e5f5883/3:2/w_1500,h_1000,c_limit/chayka_ai_copyright.jpg","{'@type': 'CreativeWork', 'name': 'The New Yorker'}","Kyle Chayka writes about a class-action lawsuit by visual artists against the artificial-intelligence image generators Midjourney, Stable Diffusion, and DreamUp.","{'@type': 'WebPage', '@id': 'https://www.newyorker.com/culture/infinite-scroll/is-ai-art-stealing-from-artists'}",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vd3d3LmdzZS5oYXJ2YXJkLmVkdS9pZGVhcy9lZGNhc3QvMjMvMDIvZWR1Y2F0aW5nLXdvcmxkLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNl0gEA?oc=5,AI in Education - Harvard Graduate School of Education,2023-02-09,Harvard Graduate School of Education,https://www.gse.harvard.edu,Chris Dede discusses how education can evolve to work with — rather than fight against — artificial intelligence,N/A,N/A,N/A,N/A,N/A,"




												EdCast
					

Educating in a World of Artificial Intelligence
  Chris Dede discusses how education can evolve to work with — rather than fight against — artificial intelligence




Posted
 February 9, 2023



By
 Jill Anderson







Learning Design and Instruction


Teachers and Teaching


Technology and Media






















Senior Researcher Chris Dede isn't overly worried about growing concerns over generative artificial intelligence, like ChatGPT, in education. As a longtime researcher on emerging technologies, he's seen many decades where new technologies promised to upend the field. Instead, Dede says artificial intelligence requires educators to get smarter about how they teach in order to truly take advantage of what AI has to offer.“The trick about AI is that to get it, we need to change what we're educating people for because if you educate people for what AI does well, you're just preparing them to lose to AI. But if you educate them for what AI can't do, then you've got IA [Intelligence Augmentation],” he says. Dede, the associate director of research for the National AI Institute for Adult Learning and Online Education, says AI raises the bar and it has the power to significantly impact learning in powerful ways.In this episode of the Harvard EdCast, Dede talks about how the field of education needs to evolve and get smarter, in order to work with — not against — artificial intelligence. ADDITIONAL RESOURCESDede's keynote on Intelligence Augmentation, delivered at an AI and Education conferenceBrief on Intelligence Augmentation, co-authored by Dede for HGSE’s Next Level Lab TRANSCRIPTJill Anderson: I'm Jill Anderson. This is the Harvard EdCast. Chris Dede thinks we need to get smarter about using artificial intelligence and education. He has spent decades exploring emerging learning technologies as a Harvard researcher. The recent explosion of generative AI, like ChatGPT, has been met with mixed reactions in education. Some public school districts have banned it. Some colleges and universities have tweaked their teaching and learning already. Generative AI raises que






Chris Dede: I've actually been working with AI for more than half a century. Way back when when I was a graduate student, I read the first article on AI in education, which was published in 1970. And the author confidently predicted that we wouldn't need teachers within five or six years because AI was going to do everything. And of course, we still see predictions like that today. But having lived through nine hype cycles for AI, I'm both impressed by how much it's advanced, but I'm also wary about elaborate claims for it. And there is a lot of excitement now about generative AI is the term that people are using, which includes programs like ChatGPT. It includes things like Dolly that are capable of creating images. It includes really AI on its own doing performances that we previously would have thought were something that people would have to do. But it's interesting to compare ChatGPT to a search engine. And people don't remember this, but there was a time when-- before search engines when people really struggled to find resources, and there was enormous excitement when search engines came out. And search engines are, in fact, AI. They are based on AI at the back end, coming up with lists of things that hopefully match what you typed in. In fact, the problem with the search engine becomes not trying to find anything, but trying to filter everything to decide what's really useful. So you can think of ChatGPT as the next step beyond a search engine where instead of getting a list of things and then you decide which might be useful and you examine them, you get an answer that says, this is what I think you want. And that is really more the AI taking charge than it is the AI saying, I can help you. Here's some things that you might look at and decide about. That makes me wary because AI is not at a stage where it really understands what it's saying. And so it will make up things when it doesn't know them, kind of a not very good student seeing if they can fake out the teacher. And it will provide answers that are not customized to somebody's culture or to somebody's reading level or to somebody's other characteristics. So it's really quite limited. I know that Harvard has sent some wording out that I've now put into my syllabi about students being welcome to use whatever tools they want. But when they present something as their work, it has to be something that they wrote themselves. It can't be something that somebody else wrote, which is classic plagiarism. It can't be something that Chat AI wrote that they're presenting as their work and so on. I think that what Chat AI does is it raises the bar for human performance. I know a lot about what people are going through now in terms of job interviews because my older daughter is an HR manager, and my younger daughter just graduated. And she's having a lot of job interviews. And in contrast to earlier times, now, job interviews typically involve a performance. If you're going to be hired for a marketing position, they'll say bring in a marketing plan when we do our face-to-face interview on this, and we'll evaluate it. Or in her case, in mechanical engineering, they say when you come in, there's this system that you're going to have a chance to debug, and we'll see how well you do it. Those employers are going to type the same thing into Chat AI. And if someone comes in with something that isn't any better than Chat AI, they're not going to get hired because why hire somebody that can't outcompete a free resource? Jill Anderson: Oh interesting. Chris Dede: So it raises the bar for human performance in an interesting way. Jill Anderson: Your research looks at something called intelligence augmentation. I want to know what that means and how that's different from artificial intelligence. Chris Dede: Intelligence augmentation is really about the opposite of this sort of negative example I was describing where now you've got to outthink Chat AI if you want to get a job. It says, when is the whole more than the sum of the parts? When do a person and AI working together do things that neither one could do as well on their own? And often, people think, well, yeah, I can see a computer programmer, there might be intelligence augmentation because I know that machines can start to do programming. What they don't realize is that it applies to a wide range of jobs, including mine, as a college professor. So I am the associate director for research in a national AI institute funded by the National Science Foundation on adult learning and online education. And one of the things the Institute is building is AI assistants for college faculty. So there's question answering assistants to help with student questions, and there's tutoring assistants and library assistants and laboratory assistants. There's even a social assistant that can help students in a large class meet other students who might be good learning partners. So now, as a professor, I'm potentially surrounded by all these assistants who are doing parts of my job, and I can be deskilled by that, which is a bad future. You sort of end up working for the assistant where they say, well, here's a question I can't answer. So you have to do it. Or you can upskill because the assistant is taking over routine parts of the job. And in turn, you can focus much more deeply on personalization to individual students, on bringing in cultural dimensions and equity dimensions that AI does not understand and cannot possibly help with. The trick about AI is that to get it, we need to change what we're educating people for because if you educate people for what AI does well, you're just preparing them to lose to AI. But if you educate them for what AI can't do, then you've got IA. Jill Anderson: So that's the goal here. We have to change the way that we're educating young people, even older people at this point. I mean, everybody needs to change the way that they're learning about these things and interacting with them. Chris Dede: They do. And we're hampered by our system of assessment because the assessments that we use, including Harvard with the GRE and the SAT and so on, those are what AI does well. AI can score really well on psychometric tests. So we're using the wrong measure, if you will. We need to use performance assessments to measure what people can do to get into places like Harvard or higher education in general because that's emphasizing the skills that are going to be really useful for them. Jill Anderson: You mentioned at the start artificial intelligence isn't really something brand new. This has been around for decades, but we're so slow to adapt and prepare and alter the way that we do things that once it reaches kind of the masses, we're already behind. Chris Dede: Well, we are. And the other part of it is that we keep putting old wine in new bottles. I mean, this is — if I had to write a headline for the entire history of educational technology, it would be old wine in new bottles. But we don't understand what the new bottle really means. So let me give you an example of something that I think generative AI could make a big difference, be very powerful, but I'm not seeing it discussed in all the hype about generative AI. And that is evidence-based modeling for local decisions. So let's take climate change. One of the problems with climate change is that let's say that you're in Des Moines, Iowa, and you read about all this flooding in California. And you say to yourself, well, I'm not next to an ocean. I don't live in California. And I don't see why I should be that worried about this stuff. Now, no one has done a study, I assume, of flooding in Des Moines, Iowa, in 2050 based on mid-level projections about climate change. But with generative AI, we can estimate that now. Generative AI can reach out across topographic databases, meteorological databases, and other related databases to come up with here's the parts of Des Moines that are going to go underwater in 2050 and here's how often this is going to happen if these models are correct. That really changes the dialogue about climate change because now you're talking about wait a minute. You mean that park I take my kids to is going to have a foot of water in it? So I think that kind of evidence-based modeling is not something that people are doing with generative AI right now, but it's perfectly feasible. And that's the new wine that we can put in the new bottle. Jill Anderson: That's really a great way to use that. I mean, and you could even use that in your classroom. Something that you said a long, long time ago was that — and this is paraphrasing — the idea that we often implement new technology, and we make this mistake of focusing on students first rather than teachers. Chris Dede: In December, I gave a keynote at a conference called Empowering Learners for the Age of AU that has been held the last few years. And one of the things I talked about was the shift from teaching to learning. Both are important, but teaching is ultimately sort of pouring knowledge into the minds of learners. And learning is much more open ended, and it's essential for the future because every time you need to learn something new, you can't afford to go back and have another master's degree. You need to be able to do self-directed learning. And where AI can be helpful with this is that AI can be like an intellectual partner, even when you don't have a teacher that can help you learn in different ways. One of the things that I've been working on with a professor at the Harvard Business School is AI systems that can help you learn negotiation. Now, the AI can't be the person you're negotiating with. AI is not good at playing human beings — not yet and not for quite a long time, I think. But what AI can do is to create a situation where a human being can play three people at once. So here you are. You're learning how to negotiate a raise. You go into a virtual conference room. There's three virtual people who are three bosses. There's one simulation specialist behind all three, and you negotiate with them. And then at the end, the system gives you some advice on what you did well and not so well. And if you have a human mentor, that person gives you advice as well. Ronda Bandy, who was a professor in HGSE until she moved to Hunter College, she and I have published five articles on the work we did for the HGSE's Reach Every Reader Project on using this kind of digital puppeteering to help teachers practice equitable discussion leading. So again, here's something that people aren't talking about where AI on the front end can create rich evocative situations, and AI and machine learning on the back end can find really interesting patterns for improvement. Jill Anderson: You know, Chris, how hard is it to get there for educators? Chris Dede: I think, in part, that's what these national AI institutes are about. Our institute, which is really adult learning with a workplace focus, is looking at that part of the spectrum. There's another institute whose focus is middle school and high school and developing AI partners for students where the student and the partner are learning together in a different kind of IA. There's a third Institute that's looking at narrative and storytelling as a powerful form of education and how can AI help with narrative and storytelling. You can imagine sitting down. Mom and dad aren't around. You've got a storybook like Goldilocks and the Three Bears, and you've got something like Alexa that can listen to what you're reading and respond. And so you begin, and you say, Goldilocks went out of her house one day and went into the woods and got lost. And Alexa says, why do you think Goldilocks went into the woods? Was she a naughty girl? No. Or was she an adventurous girl, or was she deeply concerned about climate change and wanting to study ecosystems? I mean, I'm being playful about this, but I think the point is that AI doesn't understand any of the questions that it's asking but it can ask the questions, and then the child can start to think deeper than just regurgitating the story. So there's all sorts of possibilities here that we just have to think of as new wine instead of asking how can AI automate our order thinking about teaching and learning. Jill Anderson: I've been hearing a lot of concern about writing in particular-- writing papers where young people are actually expressing their own ideas, concerns about plagiarism and cheating, which I would say the latter have long existed as challenges in education, aren't really a new one. Does AI really change this? And how might a higher ed or any educator really look at this differently? Chris Dede: So I think where AI changes this is it helps us understand the kind of writing that we should be teaching versus the kind of writing that we are teaching. So I remember preparing my children for the SAT, and it used to have something called the essay section. And you had to write this very formal essay that was a certain number of paragraphs, and the topic sentences each had to do this and so on. Nobody in the world writes those kinds of essays in the real world. They're just like an academic exercise. And of course, AI now can do that beautifully. But any reporter will tell you that they could never use Chat AI to write their stories because stories is what they write. They write narratives. If you just put in a description, you'll be fired from your reportorial job because no one is interested in descriptions. They want a story. So giving students a description and teaching them to turn it into a story or teaching them to turn it into something else that has a human and creative dimension for it, how would you write this for a seventh-grader that doesn't have much experience with the world? How would you write this for somebody in Russia building on the foundation of what AI gives you and taking it in ways that only people can? That's where writing should be going. And of course, good writing teachers will tell you, well, that's nothing new. I've been teaching my students how to write descriptive essays. The people who are most qualified to talk about the limits of AI are the ones who teach what the AI is supposedly doing. Jill Anderson: So do you have any helpful tips for educators regardless of what level they're working at on where to kind of begin embracing this technology? Chris Dede: What AI can do well is what's called reckoning, which is calculative prediction. And I've given some examples of that with flooding in Des Moines and other kinds of things. And what people do is practical wisdom, if you will, and it involves culture and ethics and what it's like to be embodied and to have the biological things that are part of human nature and so on. So when I look at what I'm teaching, I have to ask myself, how much of what I'm teaching is reckoning? So I'm preparing people to lose to AI. And how much of what I'm teaching is practical wisdom? So for example, we spend a lot of time in vocational technical education and standard academic education teaching people to factor. How do you factor these complex polynomials? There is no workplace anywhere in the world, even in the most primitive possible conditions, where anybody makes a living by factoring. It's an app. It's an app on a phone. Should you know a little bit about factoring so it's not magic? Sure. Should you become fluent in factoring? Absolutely not. It's on the wrong side of the equation. So I think just teachers and curriculum developers and assessors and stakeholders in the outcomes of education need to ask themselves, what is being taught now, and which parts of it are shifting over? And how do we include enough about those parts that AI isn't magic? But how do we change the balance of our focus to be more on the practical wisdom side? Jill Anderson: So final thoughts here — don't be scared but figure out how to use this to your advantage? Chris Dede: Yeah, don't be scared. AI is not smart. It really isn't. People would be appalled if they knew how little AI understands what it's telling you, especially given how much people seem to be relying on it. But it is capable of taking over parts of what you do that are routine and predictable and, in turn, freeing up the creative and the innovative and the human parts that are really the rewarding part of both work the life. EdCast: Chris Dede is a senior research fellow at the Harvard Graduate School of Education. He is also a co-principal investigator of the National Artificial Intelligence Institute in adult learning and online education. I'm Jill Anderson. This is the Harvard EdCast produced by the Harvard Graduate School of Education. Thanks for listening. [MUSIC PLAYING] 








Subscribe to the Harvard EdCast.

















EdCast


An education podcast that keeps the focus simple: what makes a difference for learners, educators, parents, and communities



				Explore All Articles

				










Related Articles



















  Education Now


Sal Khan on Innovations in the Classroom














  How new innovations in remote learning can be harnessed to improve instruction in classrooms and make up for learning missed during the pandemic




















  Usable Knowledge


Embracing Artificial Intelligence in the Classroom














 Generative AI tools can reflect our failure of imagination and that is when the real learning starts




















  News


Learning in Digital Worlds














  A doctoral student studies the benefits of immersive technology in the classroom.






",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMicWh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvdG9tYXNwcmVtdXppYy8yMDIzLzAyLzA3L2hvdy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1jYW4tYm9vc3QtZGl2ZXJzaXR5LS1pbmNsdXNpb24v0gEA?oc=5,How Artificial Intelligence Can Boost Diversity & Inclusion - Forbes,2023-02-08,Forbes,https://www.forbes.com,Why AI can be a powerful tool for de-biasing organizations (and humans).,"Artificial Intelligence,HR,DEIB,UBER,Uber,AI,chatGPT,bias,diversity,inclusion",Why AI can be a powerful tool for de-biasing organizations (and humans).,Why AI can be a powerful tool for de-biasing organizations (and humans).,Leadership Strategy,N/A,"More From ForbesJul 21, 2024,03:56pm EDTPresident Biden: Putting Ego SecondJul 21, 2024,01:38pm EDTExit, Stage Left: How To Plan For Team Exits From Family OfficesJul 21, 2024,09:36am EDTEmployment Grew More Under Biden And Immigrants Did Not Get Most JobsJul 21, 2024,08:00am EDTEmpathy Meets Technology: Creating A Connected Human ExperienceJul 20, 2024,09:10pm EDTJoe Biden Joins These 3 Notable Presidents In Not Seeking Re-ElectionJul 20, 2024,05:15pm EDTBiden’s Decision To ‘Step Aside’ Has Parallels In The Corporate WorldJul 20, 2024,12:05pm EDTThe 5/5/5 Method: How The World’s Best Teams Get Smarter In 15-Minute SprintsEdit StoryForbesLeadershipLeadership StrategyHow Artificial Intelligence Can Boost Diversity & InclusionTomas Chamorro-PremuzicContributorOpinions expressed by Forbes Contributors are their own.I write about the psychology of leadership, tech and entrepreneurship.FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got it0Feb 7, 2023,05:57am ESTUpdated Feb 8, 2023, 09:58am ESTShare to FacebookShare to TwitterShare to LinkedinWhy AI could be a powerful tool for de-biasing organizations (and humans)
Can AI de-bias the workplace?getty

After decades of research and progress in the area of artificial intelligence (AI), we appeared to have reached a point in which AI is no longer confined to utopian or dystopian conversations about the future, but a present reality, impacting all industries, businesses, and aspects of life. Just like any past technological innovations, the widespread impact of AI has elicited much concern, resistance, and backlash, including alarmist accusations of algorithms as vessels for “coded bias”, “weapons of math destruction”, and “sexist and racist robots”.

But, could AI be an improbable weapon for improving Diversity, Equity, Inclusion, and Belonging (DEIB) initiatives? It is a question that matters more than HR practitioners tend to think, not least in light of the lackluster impact of typical DEIB interventions. Alas, HR seems far more fearful of AI than aware of its utility; DEIB is no exception.

PROMOTED
As I illustrate in my latest book, I, Human: AI, Automation, and the Quest to Reclaim What Makes us Unique, there's no question that AI will add value in two specific areas of DEIB. The first is to diagnose things better, telling us what truly goes on in a culture, revealing some of the hidden dynamics underlying many of the critical interactions between people at work, including the silent forces that determine why some people are more likely to get promoted than others, particularly when their performance isn’t the answer. For example, research shows that even in the absence of gender differences in everyday, granular work behaviors, men are significantly more likely to get promoted into management and leadership roles. If this cannot be attributed to more effective work behaviors or real performance differences, then the answer is bias.

The second is being able to actually measure inclusion, in particular whether someone’s demographic status or identity can predict their actual status at work. Diversity is easy to quantify, at least once organizations pick their target categories and goals (e.g., get more women, minorities, older workers, neurodiverse individuals, etc). But inclusion, which is about how people are really treated, is much harder to assess, let alone track.
MORE FROMFORBES ADVISORBest Travel Insurance CompaniesByAmy DaniseEditorBest Covid-19 Travel Insurance PlansByAmy DaniseEditor
Think of AI as a data mining tool that is the equivalent of an X ray for human interactions, and can tell us what goes on when people interact with each other, and how people are treated when they are part of vulnerable or underrepresented groups, especially compared to those who benefit from privilege. This is important because it allows us to go beyond perceptions and decode whether there are biases in behavior, which is really what we should be tackling.









CxO
US


CEO: C-suite news, analysis, and advice for top decision makers right to your inbox.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the CEO newsletter!


                More Newsletters
            


You’re all set! Enjoy the CEO newsletter!

                More Newsletters
            



Despite the popular appeal of “unconscious bias” interventions, it is time to accept that there is very little scientific support for the idea that toxic behavior and discrimination are the produced of unconscious or implicit attitudes, or that making people aware of their biases is a valid approach for creating fair or equitable work environments. In fact, it is not people’s thoughts we ought to monitor, for humans are biased by design, by their actions. For thousands of years we have been able to adhere to polite etiquette and act kindly towards our colleagues and neighbors, while we complain or bitch about them in private. This is not a bad measure of civilization: since we are not prewired to embrace or celebrate those who think or look differently from us, let us at least learn to work and live with them in harmony, which will require tolerance and rational compassion, especially when we are unable to naturally empathize with people (precisely because they appear too different from us).


1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGE
Importantly, it is perfectly feasible to think of a world in which human biases and meritocracy may co-exist, and this scenario would represent substantial progress. In fact, since human biases are a given (unless we eliminate humans), the goal should be to leverage data and evidence to promote fairer evidence-based practices. Simply put, if you want to increase meritocracy you need to align people's career success with their actual performance, defined as the value they add to a team or organization. Although the past 200-years have seen an unprecedented transition from more nepotistic to more meritocratic hiring, as embodied by the present intellectual capital age, there is still much room for improvement. Indeed, if you walk into any business and ask a random group of employees whether in their company those who are most successful (senior, better paid, etc) truly contribute the biggest value, they will probably laugh at you.
Politics and nepotism are still alive and kicking, and dramatically corrode efforts to create fair and effective organizations, which is why there is far less progress on DEIB than there should be. Here's where AI will help: revealing (as well as exposing) the actual contribution people make to their teams and organizations, beyond perceptions and popular opinion, purifying our measurement of performance, and managing people based on their true output, which, incidentally, would put to the bed the tedious discussion about hybrid work and working from anywhere.
Consider how Uber uses AI to manage its army of drivers (around 3.5 million). Uber does not rely on human managers to decide whether a driver is better than others, which would surely unleash that manager's preferences, biases, and subjective views, which are unreliable indicators of employees’ performance. Instead, its algorithms measure the driver's number of trips, revenues, profits, accident claims, and passenger ratings. Granted, some drivers may be rated unfairly (too high or too low) because of factors unrelated to their actual performance, such as their gender, race, or social class, but in the grand scheme of things, the level of noise and bias will be marginal compared to the typical performance rating given to an employee by their boss.
To be sure, it is unlikely that AI (or any other invention) will ever fully eliminating bias, because humans are biased by design. In fact, we would probably not want to fully eradicate bias completely even if we could, because it would make us very boring, homogeneous, and robot like. For instance, much of the positive influence people have on each other when they work together is based on subjective or biased attitudes: ""I work well with X because we have so much in common"", or ""hiring leader X will energize people because she stands for their values and beliefs"".
However, if we are genuinely interested in creating more open and diverse societies, it is clearly useful to keep our biases in check. This starts by accepting that when we are free to follow our instincts or intuition, we are rarely as open minded as we like to think. Left to their own devices, managers would mostly hire people like them and promote them based on how similar their opinions are, which is the right recipe for creating a cult rather than a healthy culture. Likewise, without the tools and data to reveal how different people are treated at work, particularly when they are different, leaders will continue to perpetuate their self-serving delusion of having created an inclusive culture, an experienced shared only by those who continue to enjoy the nepotistic privileges of belonging to the in-group.
Follow me on Twitter. Check out my website. Tomas Chamorro-PremuzicFollowingFollowDr. Tomas Chamorro-Premuzic is the Chief Innovation Officer at ManpowerGroup, a professor of business psychology at University College London and... Read MoreEditorial StandardsPrintReprints & Permissions
1/1BitGo Cofounder And CEO Explains The Sudden Surge Of Bitcoin Worth





Skip Ad
 
Continue watchingBitGo Cofounder And CEO Explains The Sudden Surge Of Bitcoin Worthafter the adVisit Advertiser websiteGO TO PAGEJoin The ConversationComments 0One Community. Many Voices. Create a free account to share your thoughts. Read our community guidelines  here.Forbes Community GuidelinesOur community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space.In order to do so, please follow the posting rules in our site's Terms of Service.  We've summarized some of those key rules below. Simply put, keep it civil.Your post will be rejected if we notice that it seems to contain:False or intentionally out-of-context or misleading informationSpamInsults, profanity, incoherent, obscene or inflammatory language or threats of any kindAttacks on the identity of other commenters or the article's authorContent that otherwise violates our site's terms.User accounts will be blocked if we notice or believe that users are engaged in:Continuous attempts to re-post comments that have been previously moderated/rejectedRacist, sexist, homophobic or other discriminatory commentsAttempts or tactics that put the site security at riskActions that otherwise violate our site's terms.So, how can you be a power user?Stay on topic and share your insightsFeel free to be clear and thoughtful to get your point across‘Like’ or ‘Dislike’ to show your point of view.Protect your community.Use the report tool to alert us when someone breaks the rules.Thanks for reading our community guidelines.  Please read the full list of posting rules found in our site's Terms of Service.",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/tomaspremuzic/2023/02/07/how-artificial-intelligence-can-boost-diversity--inclusion/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/specials-images/imageserve/63d1d8d14ed5f44202acf6f2/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Tomas Chamorro-Premuzic', 'url': 'https://www.forbes.com/sites/tomaspremuzic/', 'description': 'Dr. Tomas Chamorro-Premuzic is the Chief Innovation Officer at ManpowerGroup, a professor of business psychology at University College London and Columbia University, co-founder of deepersignals.com, and an associate at Harvard’s Entrepreneurial Finance Lab. He is the author of Why Do So Many Incompetent Men Become Leaders? (and How to Fix It), upon which his TEDx talk was based. Hist latest book is I, Human: AI, Automation, and the Quest to Reclaim What Makes Us Unique. Find him on Twitter: @drtcp or at www.drtomas.com', 'sameAs': ['https://www.twitter.com/@drtcp', 'http://www.drtomas.com']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",How Artificial Intelligence Can Boost Diversity & Inclusion,2023-02-07T05:57:05-05:00,2023-02-08T09:58:41-05:00,Leadership Strategy,How Artificial Intelligence Can Boost Diversity & Inclusion,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Leadership', 'item': 'https://www.forbes.com/leadership/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Leadership Strategy', 'item': 'https://www.forbes.com/leadership-strategy/'}]",,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVWh0dHBzOi8vd3d3LmZhc3Rjb21wYW55LmNvbS85MDg0ODIyOC93aHktZ2VuZXJhdGl2ZS1haS1zY2FyZXMtYXJ0aXN0cy1idXQtbm90LXdyaXRlcnPSAQA?oc=5,Why generative AI scares artists but not writers - Fast Company,2023-02-10,Fast Company,https://www.fastcompany.com,N/A,N/A,N/A,N/A,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vd3d3LnZpY2UuY29tL2VuL2FydGljbGUvNWQzN3phL3ZvaWNlLWFjdG9ycy1zaWduLWF3YXktcmlnaHRzLXRvLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNl0gEA?oc=5,'Disrespectful to the Craft:' Actors Say They're Being Asked to Sign Away Their Voice to AI - VICE,2023-02-07,VICE,https://www.vice.com,"Motherboard spoke to multiple voice actors and advocacy organizations, some of which said contracts including language around synthetic voices are now very prevalent.",N/A,"Motherboard spoke to multiple voice actors and advocacy organizations, some of which said contracts including language around synthetic voices are now very prevalent.","Motherboard spoke to multiple voice actors and advocacy organizations, some of which said contracts including language around synthetic voices are now very prevalent.",N/A,N/A,N/A,https://schema.org,,,,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'BreadcrumbList', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.vice.com/en'}, {'@type': 'ListItem', 'position': 2, 'name': 'Tech', 'item': 'https://www.vice.com/en/section/tech'}]}, {'@context': 'https://schema.org', '@type': 'NewsArticle', 'mainEntityOfPage': {'@type': 'WebPage', '@id': 'https://www.vice.com/en/article/5d37za/voice-actors-sign-away-rights-to-artificial-intelligence'}, 'headline': '‘Disrespectful to the Craft:’ Actors Say They’re Being Asked to Sign Away Their Voice to AI', 'image': ['https://video-images.vice.com/articles/63e15800c65fbdd0189a1571/lede/1675713760711-audio.jpeg?crop=1xw:0.9375xh;center,center&resize=1200:*'], 'datePublished': '2023-02-07T14:00:00.000Z', 'dateModified': '2023-02-07T14:00:00.000Z', 'author': {'@type': 'Person', 'name': 'Joseph Cox'}, 'publisher': {'@type': 'Organization', 'name': 'VICE', 'logo': {'@type': 'ImageObject', 'url': 'https://vice-web-statics-cdn.vice.com/images/vice-og.png'}}}, {'@context': 'https://schema.org', '@type': 'ItemList', 'name': '‘Disrespectful to the Craft:’ Actors Say They’re Being Asked to Sign Away Their Voice to AI', 'itemListElement': []}, []]",,,,,,
https://news.google.com/rss/articles/CBMicmh0dHBzOi8vd3d3LmJ1c2luZXNzd2lyZS5jb20vbmV3cy9ob21lLzIwMjMwMjA3MDA1NjQxL2VuL1NvdW5kSG91bmQtVW52ZWlscy1EeW5hbWljLUludGVyYWN0aW9uLVdpdGgtR2VuZXJhdGl2ZS1BSdIBAA?oc=5,SoundHound Unveils Dynamic Interaction With Generative AI - Business Wire,2023-02-07,Business Wire,https://www.businesswire.com,"SoundHound AI, Inc. (Nasdaq: SOUN) (“SoundHound”), a global leader in voice artificial intelligence, today unveiled Dynamic Interaction™ With Generati",N/A,"SoundHound AI, Inc. (Nasdaq: SOUN) (“SoundHound”), a global leader in voice artificial intelligence, today unveiled Dynamic Interaction™ With Generati","SoundHound AI, Inc. (Nasdaq: SOUN) (“SoundHound”), a global leader in voice artificial intelligence, today unveiled Dynamic Interaction™ With Generati",N/A,N/A,"




SoundHound Unveils Dynamic Interaction With Generative AI






Voice AI leader’s breakthrough technology provides a seamless interface combining its proprietary AI and Generative AI platforms, like ChatGPT














Download







SoundHound's newly released demo showcases how Dynamic Interaction with Generative AI would work in-vehicle for the automotive industry. (Graphic: Business Wire)












SoundHound's newly released demo showcases how Dynamic Interaction with Generative AI would work in-vehicle for the automotive industry. (Graphic: Business Wire)





Image








Full Size








Small








Preview








Thumbnail


















Image








Full Size








Small








Preview








Thumbnail














February 07, 2023 09:00 AM Eastern Standard Time



SANTA CLARA, Calif.--(BUSINESS WIRE)--SoundHound AI, Inc. (Nasdaq: SOUN) (“SoundHound”), a global leader in voice artificial intelligence, today unveiled Dynamic Interaction™ With Generative AI, further extending the capabilities of the company’s groundbreaking real-time, multimodal interface by granting users hands-free access to a vast wealth of ideas, information, and sophisticated search functionality.

“Dynamic Interaction is capable of creating incredible user experiences, and allowing consumers to interact with computers in the most natural, intuitive way”Post this

Launched in November 2022, SoundHound Dynamic Interaction raises the bar for human-computer interaction by offering a full-duplex consumer interface which provides real-time, continuous audiovisual feedback in response to both verbal and touch commands – with no wake words, awkward pauses, or turn-taking. Engineered for a seamless exchange, the system ignores off-topic speech, makes proactive suggestions to the user, and intelligently decides when to use audio or visual output.

Now SoundHound Dynamic Interaction will also offer effortless, voice-enabled access to Generative AI, meaning users can unlock relevant information and ideas – just by speaking naturally. Generative AI, a technique popularized by recent advancements such as Dall-E 2 and ChatGPT, has marked a new breakthrough in the field of artificial intelligence and machine learning.

SoundHound Dynamic Interaction with Generative AI can be adapted for a variety of products and services, with the company’s newly released demo showcasing how it would work in-vehicle for the automotive industry. SoundHound has publicly announced wins with 17 automotive brands since launching its conversational AI platform in 2016.

In the example, a driver uses the interface to source ideas and arrange plans in the local area, making an ordinary journey productive, educational, and inspirational.

Driver: “Can you help me plan a fun day of hiking in San Francisco for tomorrow?”
Dynamic Interaction with Generative AI: “Start at Lands End trailhead. This trail offers amazing views of the Pacific Ocean and the Golden Gate Bridge. Then go to the Sutro Baths ruins, an old public bathhouse which has been converted to a park with trails and scenic views. Then finish your hike at the famous Seal Rocks, where you can enjoy the beautiful sunset over the ocean.”

While Generative AI can answer questions about static content and provide creative answers such as writing a poem, most AI assistants also need to be able to retrieve real-time information and generate customized actions and responses designed by business owners, as opposed to relying entirely on third party models with unpredictable results. SoundHound is addressing these requirements using its Collective AI architecture of hundreds of domains and its Deep Meaning Understanding technology.

“Dynamic Interaction is capable of creating incredible user experiences, and allowing consumers to interact with computers in the most natural, intuitive way,” says Keyvan Mohajer, CEO and Co-Founder of SoundHound. “By introducing Generative AI, we’re providing a gateway to a new world of ideas, information, and capabilities – from anywhere, completely hands-free.”

The demo video shows a range of features, including:


Dynamic Interaction: removes turn taking, adding multimodal aspects, and avoids breaking the conversation when out of context words are spoken.


Deep Meaning Understanding: allows the user to ask complex questions and be more precise about what they want versus keyword-based queries.


Collective AI: an architecture of interconnected domains that is bigger than the sum of its parts and always learning to do more things.


Personalization: users don’t need to start every interaction from scratch, and the interface can make useful, proactive suggestions.


Generative AI: an integration that opens a voice-enabled portal to boundless information.


In November, SoundHound introduced Dynamic Interaction for food ordering – ushering in a new chapter for drive-thru, kiosk, and tablet-based voice AI automation. With hundreds of thousands of views, the demo video has reinforced the role of voice AI innovation for customer service.

To learn more about Dynamic Interaction and this announcement, visit SoundHound.com.

About SoundHound

SoundHound (Nasdaq: SOUN), a leading innovator of conversational intelligence, offers an independent voice AI platform that enables businesses across industries to deliver best-in-class conversational experiences to their customers. Built on proprietary Speech-to-Meaning® and Deep Meaning Understanding® technologies, SoundHound’s advanced voice AI platform provides exceptional speed and accuracy and enables humans to interact with products and services like they interact with each other—by speaking naturally. SoundHound is trusted by companies around the globe, including Hyundai, Mercedes-Benz, Pandora, Qualcomm, Netflix, Snap, Square, Toast, LG, VIZIO, KIA, and Stellantis. www.soundhound.com



Contacts

Fiona McEvoy
(415) 610-6590
PR@SoundHound.com




",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiI2h0dHBzOi8vdm9pLmlkL2VuL3RlY2hub2xvZ3kvMjUzMDIz0gEcaHR0cHM6Ly92b2kuaWQvZW4vYW1wLzI1MzAyMw?oc=5,Know 5 Types Of Artificial Intelligence That Make Human Work Easier - VOI English,2023-02-10,VOI English,https://voi.id,Several types of artificial intelligence (AI) have begun to be integrated into the workplace. Implementation has made it easier for human work... type AI based...,Artificial Intelligence Type,Several types of artificial intelligence (AI) have begun to be integrated into the workplace. Implementation has made it easier for human work... type AI based...,Several types of artificial intelligence (AI) have begun to be integrated into the workplace. Implementation has made it easier for human work... type AI based...,N/A,N/A,"



Share:






















YOGYAKARTA  Artificial intelligence or artificial intelligence may be the most complex and astonishing creation of man. Although the technology is still under development, the potential for the future is inconceivable.ADVERTISEMENTSeveral types of artificial intelligence (AI) have begun to be integrated into the workplace. Its implementation has assisted the HR department in automating operations and increasing efficiency.VOI this time will discuss the types of artificial intelligence that have been created, so that you have an idea of existing AI capabilities.ADVERTISEMENTArtificial Intelligence is a system or machine that mimics human intelligence and can analyze various data.AI has transformed into the most advanced technology in the world today, and every industry adopts it. Every AI app we see today is just the tip of the iceberg in terms of AI.ADVERTISEMENTQuoted from Forbes, Friday, February 10, 2023, AI classification is based on its ability to facilitate human work. In this case, artificial intelligence is divided into five types, including:Reactive Machine is the oldest AI technology with very limited capabilities. This artificial intelligence will identify and respond automatically to various situations.A popular example of a reactive AI engine is IBM's Deep Blue, a machine that beat chess grandmaster Garry Kasparov in 1997.The application of the Reactive Machine allows bot players to determine what steps to take and be able to learn the tactics used by their opponents.Artificial intelligence limited memory is a technology that is able to improve decision-making capabilities by using data or knowledge from events that have been previously studied. This type of intelligence is able to store previous data within a certain period of time.this is different from Reactive Machine. Limited memory is able to learn from previous events by analyzing the actions or data given to this machine. Examples are virtual assistants, chatbots, self-driving cars, and others.Almost all existing AI applications, from virtual chatbots and assistants to self-driving vehicles, are all driven by AI with limited memory.Theory of mind AI is the next level of the AI system currently being developed by researchers.Theory of mind AI can connect with human behavior and emotions. This means that this machine can understand how humans, animals, and other machines feel.After successfully understanding these feelings, this machine is required to be able to take advantage of the information to make decisions that reflect itself.Self-aware is an AI that has evolved to be so similar to the human brain that it has developed self-awareness.In AI's self-aware type, the machine already has human-level awareness. In addition to understanding the emotional state of humans, it has also started to have its own emotions and understands itself in this world.This type of artificial intelligence represents all existing AIs, even including the most complicated and capable AIs ever made to date.Artificial narrow intelligence (ANI) refers to an AI system that can only perform certain tasks independently using human-like capabilities.These machines can't do anything other than what has been programmed. That is, these AIs have a very limited or narrow range of competence.According to the classification system mentioned above, the system is in accordance with all AI reactive machine and limited memory. Even the most complex AI that uses machine learning and deep learning to teach itself is under ANI.This is information about the type of artificial intelligence that facilitates human work. May it be useful!

The English, Chinese, Japanese, Arabic, and French versions are automatically generated by the AI. So there may still be inaccuracies in translating, please always see Indonesian as our main language.
                                    (system supported by DigitalSiber.id)



                                                Tag:
                                                                                                    teknologi
artificial intelligence
aplikasi
kecerdasan buatan





Most Popular Tags
#donald trump
#joe biden
#Palestine
#election 2024
#giias 2024

Popular











| TECHNOLOGY





TECNOlon 30 Premier 5G: Advanced Phones At Affordable Prices





 22 Juli 2024,  03:01 















| TECHNOLOGY





OnePlus Open 2 Coming With Giant Battery, Ready To Rock The World Folding Phones!





 22 Juli 2024,  08:05 















| TECHNOLOGY





Microsoft's Great Failure: What Can Be Learned To Prevent Next Events?





 22 Juli 2024,  02:05 















| TECHNOLOGY





Platform X Creates Deactivate Link Feature In Comments





 22 Juli 2024,  13:05 















| TECHNOLOGY





Google Messages Will Become A New Default App On Galaxy Phones





 22 Juli 2024,  06:05 







",http://schema.org,BreadcrumbList,https://voi.id/en/technology/253023,https://imgsrv2.voi.id/vDQRQc3MMnrE5CHVBNLY_0oHEo51SzHfTGbBKgces6g/auto/970/544/sm/1/bG9jYWw6Ly8vcHVibGlzaGVycy8yNTMwMjMvMjAyMzAyMTAxMjM2LW1haW4uY3JvcHBlZF8xNjc2MDA5Mjg1LmpwZw.jpg,"{'url': 'https://voi.id/artikel/tim-redaksi/45/editor', '@type': 'Person', 'name': 'Tim VOI'}","{'@type': 'Organization', 'name': 'VOI.ID', 'logo': {'@type': 'ImageObject', 'url': 'https://imgsrv2.voi.id/Sf0nDvIw_CCgQ92Tj0p__LRHSQYD7luLIHtEhMNvyLI/fill/72/37/sm/1/bG9jYWw6Ly8vaW1hZ2VzL2xvZ28vbG9nby12b2kucG5n.png', 'width': 118, 'height': 42}}",Know 5 Types Of Artificial Intelligence That Make Human Work Easier,2023-02-10WIB18:30:00+07:00,2023-02-10WIB18:30:00+07:00,,,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Aktual', 'item': 'https://voi.id/kanal/15/aktual'}, {'@type': 'ListItem', 'position': 2, 'name': 'Teknologi', 'item': 'https://voi.id/kanal/17/aktual/teknologi'}]",,,https://imgsrv2.voi.id/fDqHqlMk8tTLdatoDM_2-f3iaz46EfcCiOYybFe4Q1s/auto/336/188/sm/1/bG9jYWw6Ly8vcHVibGlzaGVycy8yNTMwMjMvMjAyMzAyMTAxMjM2LW1haW4uY3JvcHBlZF8xNjc2MDA5Mjg1LmpwZw.jpg,,,"{'@type': 'WebPage', '@id': 'https://voi.id/en/technology/253023'}",,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vaW50ZXJlc3RpbmdlbmdpbmVlcmluZy5jb20vaW5ub3ZhdGlvbi9haS1yZXZvbHV0aW9uLWFpa28tYWlkZW4tYWktaW50ZXJuc9IBAA?oc=5,"AI revolution meets marketing returns: Aiko, Aiden world's first AI interns - Interesting Engineering",2023-02-07,Interesting Engineering,https://interestingengineering.com,The goal of Codeword's purported experiment in human-AI collaboration is to gain more insight into the application of AI in the creative sector.,AI,The goal of Codeword's purported experiment in human-AI collaboration is to gain more insight into the application of AI in the creative sector.,The goal of Codeword's purported experiment in human-AI collaboration is to gain more insight into the application of AI in the creative sector.,N/A,N/A,"ShareInnovationAI revolution meets marketing returns: Aiko, Aiden world’s first AI internsThe goal of Codeword’s purported experiment in human-AI collaboration is to gain more insight into the application of AI in the creative sector.
Published: Feb 07, 2023 07:05 AM ESTBaba Tamima year ago0ShareCodeword's AI-interns: Aiko (left) and Aiden (right).Interesting Engineering/Codeword Humans “will be challenged even more to perform better if artificial intelligence (AI) is successful.” 
This is what an aerospace industry intern told Interesting Engineering (IE) while reacting to the recent news of AI interns being hired by a remote marketing firm in the United States. 
“The human interns who are not performing well will face a serious threat to their future,” said Can Unen, a student trainee at an aviation engineering company in Turkey.  Coating wine bottle with wax and honey extraction, the best of IE this week
1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGE
As AI continues on the path of being the new technological revolution, many companies are having a go at it, but some are taking it to the next crazy level.
Earlier last month, PR and digital marketing firm Codeword took a unique step toward the future of the workforce by adding two new interns to their team with a twist.
Unlike other employees, Aiko and Aiden are not real people but are creations of artificial intelligence that “work” as interns for Codeword. The two AI interns are the first of their kind in the agency industry.
The goal of Codeword’s purported experiment in human-AI collaboration is to gain more insight into the application of AI in the creative sector.
“We decided to start experimenting with AI tools, to see what AI can do better than creative professionals, and what creative professionals can do better than AI,” Kyle Monson, co-founder of Codeword, told IE.
Reasons behind the AI ‘Experiment’
The age of marketing automation has seen strategists and analysts use AI tools in their daily workflow, and Codeword is taking this a step further.
The company works at the nexus of marketing, technology, and media and focuses on the use of new resources. 
Aiden and Aiko will integrate into Codeword’s tech stack, which already consists of platforms driven by AI in order to concentrate more on creative output, claims the company.
“The next question is whether AI can augment creative work streams — the kind of designing, writing, and ideation that creative companies do,” said Monson, referring to the company’s “experiment.” 
Will it obstruct the work of humans, replace humans entirely, or excel in a particular creative area, leaving the rest to human workers? The company “will be exploring all those questions,” added Monson. 

Human-AI collaboration.jpg

David Gyung/iStock 



‘Sandwich’ workflow: Human-AI collaboration 
The workflow paradigm known as the “sandwich” serves as the foundation for Codeword’s concept of AI-human integration.
Three steps make up this model: A human first has an inspired thought and prompts the AI. The human then selects an option from a selection that the AI has generated, modifies it, and adds any finishing touches they see fit. 
The model incorporates AI in step 2 and is comparable to the conventional agency creative process. 
The objective is to determine what Codeword can do with AI that they couldn’t do without it and to investigate whether AI can speed up, reduce the cost, and improve their core creative job.	
“I think there is still a need for human touch at the end, like in the sandwich workflow,” says Unen. 
“It’s inspirational and experimentally important from a scientific point of view as well, as the outputs from this trial will likely guide AI researchers.”
Meanwhile, the AI interns have performed exceptionally well on projects that call for the analysis of substantial amounts of data or inputs, similar to the study of tone in written material, claims Monson.
When ghostwriting, the AI can replicate a human tone. When making mood boards for brands, it can draw from a wide range of visual sources, he further added.
However, Aiden and Aiko don’t get the work “100% right or 100% done” and still need human supervision.
“They get us 70% of the way there, and then the human team can provide the final 30% — cleaning up the work, making sure it’s accurate,” stated Monson.
Role of AI interns
Aiko will be part of the Codeword design team, working under Senior Art Director Emilio Ramos and handling design duties like image editing, concept sketch creation, and icon design. 
Aiden will become a member of the editorial staff and report to Senior Editor Terrence Doyle. He will conduct voice-and-tone analyses, analyze news stories, and perhaps even write internal material.
They will have monthly performance reviews, as do all interns, and if their internship is a success, they will have the option to move up to a full-time position, according to Codeword.  
Finding the interns’ skills and abilities and developing them throughout the internship will be one of the team’s main challenges, according to the company. 
It is also interesting to learn about the creation of these interns, who are, in essence, structures that sit on top of AI tools used by the company.
“We asked ChatGPT, ‘What are common names that begin with AI?’ and Aiden and Aiko were the first two,” recalled Monson.
“Then we asked ‘What does a young person named Aiko look like?’ and that’s how we got their faces.”
There’s been a lot of chatter about whether it was appropriate to give them names and faces, but it definitely makes the project more engaging and interesting, according to the marketing company. 
By using the tools to create “their own likeness and name,” Codeword has tried to minimize potential bias, claimed the firm’s co-founder. 
AI and the future of the creative sector 
The agency, its team, and its community will learn more about human-AI cooperation thanks to Codeword’s innovative AI project. 
Aiko and Aiden will work as full-fledged members of the Codeword team, with the aim of identifying fresh approaches to incorporating AI into the creative process. 

Embracing AI in the workforce.jpg

Thinkhubstudio/iStock 



The trial, says the firm, is a step closer to a time when AI and human creativity will coexist.
However, when IE asked about the “criteria for choosing the two AI interns,” Monson gave a funny reply; Codeword “didn’t give them a job interview.” So, the AI interns began with a big advantage over human applicants.
“So far, it’s been all positives,” he said, speaking about the “experiment.” 
“The team is energized and finding interesting ways to use them. Aiko designed our all-hands deck last week and did a beautiful job,” he continued. 
Meanwhile, skeptical Unen is hopeful that AI will open new doors for creative people rather than close doors to human workers, as some fear. 
“People who have [a] better experience with AI can get major roles during the transformation, which is a big opportunity for them,” he said. 
“I think ones with a creative mind and necessary skills will benefit from AI as they can interact and gain different perspectives from it.”
Publicity stunt or embracing the reality 
Codeword’s “goal,” according to the co-founder’s blog, is to be as open and honest about our procedures and lessons learned so that the firm and its clients can benefit directly from the program.
When questioned about the intentions of this endeavor, Monson replied: “Sure, let’s say it’s [a] 20% marketing stunt. We’re a marketing agency! But the other 80% is the important part.”  
“The issues we’re tackling and the questions we’re trying to answer with this experiment are very real.” 
It has been argued that, for decades, Japanese roboticists have been at the forefront of realizing the dream of building companion robots for humans powered by AI.
The argument continues that, contrary to some Western institutions, which focused more on algorithms in the abstract, these Japanese institutions believed that AI innovation should be developed alongside—or rather, within—a real artificial body.
Although we are still determining if we are ready to fully embrace AI without fear, along with the potential job losses, Monson thinks AI will make headlines for years to come.
“AI-human relationship will be the biggest media and marketing story over the next decade,” he said.  
Meanwhile, the Codeword blog asserts that Aiden and Aiko will not replace the human workforce at the firm, and their “intern wages” will be donated to Grace Hopper Celebration, the largest conference for women technologists in the world.RECOMMENDED ARTICLES

Nevertheless, the firm has not made the bots ready for media yet. 
“We don’t make our interns available to the media until we hire them full-time.” Monson smiled.       
0COMMENTNEWSLETTERThe Blueprint DailyStay up-to-date on engineering, tech, space, and science news with The Blueprint.Sign Up  By clicking sign up, you confirm that you accept this site's Terms of Use and Privacy PolicyABOUT THE EDITORBaba Tamim Baba Tamim, an award-winning journalist, has traversed the world of social justice to explore the dynamic frontiers of technology. From early dispatches for outlets like USA Today and Al Jazeera to in-depth analyses of AI and hypersonic advancements for Interesting Engineering, his storytelling prowess has evolved alongside the ever-changing landscape of innovation. Tamim's expertise lies in uncovering the human impact of groundbreaking tech, highlighting both its potential and its complexities.Newsinnovation











Sponsored By

AXBB





Automate accounts payable




Take your spending to a new level of clarity and control with American Express One AP®.






Learn More





POPULAR ARTICLES1militaryDespite fraud charges, Boeing secures deal with USAF to supply E-7 aircraftBojan Stojkovskia day ago02innovationNew fabric stiffens on demand, suitable for drone limbs, medical devicesPrabhat Ranjan Mishraa day ago03militaryUS Navy’s SM-6 missile guided by Army’s new, advanced radar hits targetBojan Stojkovskia day ago04healthNew drug extends lifespan by 25%, fights aging, could prevent cancerGairika Mitraa day ago0RELATED ARTICLEScultureBiden shocks campaign staff by announcing presidential race exit via XscienceBizarre male mosquitos with female traits may help stop dengue, Zika virusinnovationGraphene-based brain chip offers 200x more power for Parkinson’s patientstransportationDelta delays drag on: Microsoft glitch grounds 3,500+ flights, 1000s strandedJOBSSee All

Find Your Place In The World BY Amply







Accenture Federal Services

DevOps Engineer
Chantilly
$76,000 - $144,000 a year
See Job







Navy Federal Credit Union

ETS Engineer IV / Principal - Cloud Infrastructure Engineer
Vienna
$112,200 - $199,300 a year
See Job







G-Research

Senior DevSecOps Engineer
Dallas
See Job







Capgemini

Mechanical & Physical Engineer - Design Responsible Engineer for HEV Driveline
Auburn Hills
See Job



Search More Roles


FEATURED STORIES



 
 








",https://schema.org,,,,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'NewsArticle', '@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns#article', 'isPartOf': {'@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns'}, 'author': {'name': 'Baba Tamim', '@id': 'https://interestingengineering.com/#/schema/person/57b6da06102666c3b691e70160354fb1'}, 'headline': 'AI revolution meets marketing returns: Aiko, Aiden world&#8217;s first AI interns', 'datePublished': '2023-02-07T12:05:11+00:00', 'dateModified': '2023-02-07T12:05:11+00:00', 'mainEntityOfPage': {'@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns'}, 'wordCount': 1394, 'commentCount': 0, 'publisher': {'@id': 'https://interestingengineering.com/#organization'}, 'image': {'@type': 'ImageObject', 'url': False, 'caption': False}, 'thumbnailUrl': 'https://images.interestingengineering.com/2023/02/07/image/jpeg/O8CejrgvN8Pf7JSN2cgDWKQcw2hAV7cwUosnLO7Y.jpg', 'keywords': ['AI'], 'articleSection': ['Innovation'], 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'CommentAction', 'name': 'Comment', 'target': ['https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns#respond']}], 'copyrightYear': '2023', 'copyrightHolder': {'@id': 'https://interestingengineering.com/#organization'}}, {'@type': 'WebPage', '@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns', 'url': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns', 'name': ""AI revolution meets marketing returns: Aiko, Aiden world's first AI interns"", 'isPartOf': {'@id': 'https://interestingengineering.com/#website'}, 'primaryImageOfPage': {'@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns#primaryimage'}, 'image': {'@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns#primaryimage'}, 'thumbnailUrl': 'https://images.interestingengineering.com/2023/02/07/image/jpeg/O8CejrgvN8Pf7JSN2cgDWKQcw2hAV7cwUosnLO7Y.jpg', 'datePublished': '2023-02-07T12:05:11+00:00', 'dateModified': '2023-02-07T12:05:11+00:00', 'description': ""The goal of Codeword's purported experiment in human-AI collaboration is to gain more insight into the application of AI in the creative sector."", 'breadcrumb': {'@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns#primaryimage', 'url': 'https://images.interestingengineering.com/2023/02/07/image/jpeg/O8CejrgvN8Pf7JSN2cgDWKQcw2hAV7cwUosnLO7Y.jpg', 'contentUrl': 'https://images.interestingengineering.com/2023/02/07/image/jpeg/O8CejrgvN8Pf7JSN2cgDWKQcw2hAV7cwUosnLO7Y.jpg'}, {'@type': 'BreadcrumbList', '@id': 'https://interestingengineering.com/innovation/ai-revolution-aiko-aiden-ai-interns#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'News', 'item': 'https://interestingengineering.com/news'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://interestingengineering.com/innovation'}, {'@type': 'ListItem', 'position': 3, 'name': 'AI revolution meets marketing returns: Aiko, Aiden world&#8217;s first AI interns'}]}, {'@type': 'WebSite', '@id': 'https://interestingengineering.com/#website', 'url': 'https://interestingengineering.com/', 'name': 'Interesting Engineering', 'description': '', 'publisher': {'@id': 'https://interestingengineering.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://interestingengineering.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://interestingengineering.com/#organization', 'name': 'Interesting Engineering', 'url': 'https://interestingengineering.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://interestingengineering.com/#/schema/logo/image/', 'url': 'https://cms.interestingengineering.com/wp-content/uploads/2024/02/Interesting_Engineering-1.jpg', 'contentUrl': 'https://cms.interestingengineering.com/wp-content/uploads/2024/02/Interesting_Engineering-1.jpg', 'width': 590, 'height': 198, 'caption': 'Interesting Engineering'}, 'image': {'@id': 'https://interestingengineering.com/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/interestingengineering/', 'https://x.com/IntEngineering', 'https://www.linkedin.com/company/interestingengineering/'], 'email': 'info@interestingengineering.com', 'numberOfEmployees': {'@type': 'QuantitativeValue', 'minValue': '201', 'maxValue': '500'}, 'publishingPrinciples': 'https://interestingengineering.com/home-page'}]",,,,,,
https://news.google.com/rss/articles/CBMiW2h0dHBzOi8vd3d3LmNiYy5jYS9uZXdzL2J1c2luZXNzL2NoYXRncHQtaW50ZWxsaWdlbmNlLW93bmVyc2hpcC1jb2x1bW4tZG9uLXBpdHRpcy0xLjY3MzkwMjXSASBodHRwczovL3d3dy5jYmMuY2EvYW1wLzEuNjczOTAyNQ?oc=5,ChatGPT may reset the world of work as businesses rush to own artificial intelligence - CBC News,2023-02-09,CBC News,https://www.cbc.ca,"Businesses have profited by owning the factories and machinery, but as artificial intelligence systems like ChatGPT strut their stuff, companies are rushing to own a sort of brain power too. Where does that leave the rest of us?",N/A,"Businesses have profited by owning the factories and machinery, but as artificial intelligence systems like ChatGPT strut their stuff, companies are rushing to own a sort of brain power too. Where does that leave the rest of us?","Businesses have profited by owning the factories and machinery, but as artificial intelligence systems like ChatGPT strut their stuff, companies are rushing to own a sort of brain power too. Where does that leave the rest of us?",N/A,N/A,N/A,http://schema.org/,WebPage,,"[{'datePublished': '2023-02-06T22:39Z', '@type': 'ImageObject', 'name': 'JAPAN-ECONOMY/CAPEX', 'description': ""Humanoid robots work side by side with employees in the assembly line at a factory of Glory Ltd., a manufacturer of automatic change dispensers, in Kazo, north of Tokyo, Japan, July 1, 2015. Japanese firms are ramping up spending on robotics and automation, responding at last to premier Shinzo Abe's efforts to stimulate the economy and end two decades of stagnation and deflation. Picture taken July 1, 2015. REUTERS/Issei Kato"", 'dateModified': '2023-02-06T22:37Z', '@context': 'http://schema.org/', 'url': 'https://i.cbc.ca/1.6739060.1675723053!/fileImage/httpImage/japan-economy-capex.JPG'}, {'datePublished': '2023-02-08T14:55Z', '@type': 'ImageObject', 'name': 'BAIDU-AI/', 'description': 'FILE PHOTO: Men interact with a Baidu AI robot near the company logo at its headquarters in Beijing, China April 23, 2021. REUTERS/Florence Lo/File Photo', 'dateModified': '2023-02-08T14:51Z', '@context': 'http://schema.org/', 'url': 'https://i.cbc.ca/1.6741048.1675867909!/fileImage/httpImage/baidu-ai.JPG'}, {'datePublished': '2023-02-08T18:39Z', '@type': 'ImageObject', 'name': 'GLOBAL-METALS/', 'description': 'Children play on a steel jungle gym at a playground in Taipei, Taiwan March 25, 2018. REUTERS/Tyrone Siu', 'dateModified': '2023-02-08T18:34Z', '@context': 'http://schema.org/', 'url': 'https://i.cbc.ca/1.6741382.1675881298!/fileImage/httpImage/global-metals.JPG'}]","[{'image': {'datePublished': '2016-10-13T20:07Z', '@type': 'ImageObject', 'name': 'Don Pittis', 'description': ""Don Pittis was a forest firefighter and a ranger in Canada's High Arctic islands. After moving into journalism, he was principal business reporter for Radio Television Hong Kong before the handover to China. He has produced and reported for the CBC in Saskatchewan and Toronto and the BBC in London. He is currently senior producer at CBC's business unit."", 'dateModified': '2016-10-13T20:07Z', '@context': 'http://schema.org/', 'url': 'https://i.cbc.ca/1.3803860.1476389263!/fileImage/httpImage/don-pittis.jpg'}, 'contactPoint': {'@type': 'ContactPoint', '@context': 'http://schema.org/', 'url': 'https://www.cbc.ca/news/author/don-pittis-1.1870052', 'email': ''}, '@type': 'Person', 'jobTitle': 'Business columnist', 'name': 'Don Pittis', 'description': ""Based in Toronto, Don Pittis is a business columnist and senior producer for CBC News. Previously, he was a forest firefighter, and a ranger in Canada's High Arctic islands. After moving into journalism, he was principal business reporter for Radio Television Hong Kong before the handover to China. He has produced and reported for the CBC in Saskatchewan and Toronto and the BBC in London."", '@context': 'http://schema.org/'}]","{'foundingDate': '1936-11-02T05:00Z', 'ethicsPolicy': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'verificationFactCheckingPolicy': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', '@type': 'NewsMediaOrganization', '@context': 'http://schema.org/', 'ownershipFundingInfo': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'actionableFeedbackPolicy': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'missionCoveragePrioritiesPolicy': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'diversityPolicy': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'masthead': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'diversityStaffingReport': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'unnamedSourcesPolicy': 'https://www.cbc.ca/news/about-cbc-news-1.1294364', 'correctionsPolicy': 'https://www.cbc.ca/news/about-cbc-news-1.1294364'}",ChatGPT may reset the world of work as businesses rush to own artificial intelligence,2023-02-09T09:00Z,2023-02-09T11:34Z,Business,ChatGPT may reset the world of work as businesses rush to own artificial intelligence,,,,,https://i.cbc.ca/1.6739060.1675723053!/fileImage/httpImage/japan-economy-capex.JPG,,,,,2023-02-09T09:00Z,,,,,"{'@type': 'SpeakableSpecification', 'cssSelector': ['.detailHeadline', '.detailSummary']}",What happens to ordinary humans when businesses own a replacement for their brain power?,"[{'datePublished': '2022-12-10T02:46Z', 'alternativeHeadline': 'ChatGPT is artificial intelligence chatbot software capable of writing poems, college-level essays and even computer code. Experts say the software highlights how far AI has come in just a few years, while still spotlighting concerns around accuracy.', 'identifier': '2141634627521', 'contentUrl': 'https://www.cbc.ca/player/play/2141634627521', 'uploadDate': '2022-12-10T02:46Z', '@type': 'VideoObject', 'name': 'ChatGPT software highlights advances, limitations of modern artificial intelligence', 'description': 'ChatGPT is artificial intelligence chatbot software capable of writing poems, college-level essays and even computer code. Experts say the software highlights how far AI has come in just a few years, while still spotlighting concerns around accuracy.', 'dateModified': '2022-12-10T15:58Z', '@context': 'http://schema.org/', 'thumbnailUrl': 'https://thumbnails.cbc.ca/maven_legacy/thumbnails/566/962/MPX_PATEL_CHATBOT.jpg'}]",,,
https://news.google.com/rss/articles/CBMiU2h0dHBzOi8vd3d3LnNmY2hyb25pY2xlLmNvbS9zZi9hcnRpY2xlL2FpLWpvYnMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtMTc3NjQ0MTIucGhw0gEA?oc=5,Can AI companies revive downtown SF? Here's what economists say - San Francisco Chronicle,2023-02-07,San Francisco Chronicle,https://www.sfchronicle.com,"There is hope that AI companies based in San Francisco could boost the city’s economy and revitalize the downtown area, but some economists aren’t so sure.",N/A,"There is hope that AI companies based in San Francisco could boost the city’s economy and revitalize the downtown area, but some economists aren’t so sure.",Job posting data shows the San Francisco metropolitan statistical area steadily losing its...,N/A,N/A,"The San Francisco metropolitan statistical area steadily is losing its share of artificial intelligence job postings, data shows.Carlos Avila Gonzalez, Staff Photographer / The ChronicleThe buzz in recent weeks over leaps in artificial intelligence, or AI, technology has caused venture capitalists and marquee companies like Microsoft to pour billions of dollars into the trending industry. And with many of the most recognizable AI companies based in San Francisco, there are hopes that the rising industry could shift the city’s fortunes and not just revitalize its tech sector, but help fill empty downtown streets, offices and shops with the next wave of tech workers whose spending could filter through the rest of the city’s economy.But some economists aren’t so sure.On one hand, overall hiring in San Francisco’s tech sector in recent months has been flat. That is a mostly positive sign, according to city data and San Francisco Chief Economist Ted Egan, especially since ongoing layoffs have battered companies based in the city in recent weeks and months.AdvertisementArticle continues below this adBut job posting data aggregated from thousands of hiring sites nationally by jobs site ZipRecruiter shows the San Francisco metropolitan statistical area steadily losing its share of job postings in the burgeoning AI field. It’s not entirely clear where those jobs might be located from the data. Some postings could be located in other cities, or a remote role might not have a location tied to it at all.ZipRecruiter’s analysis shows that the San Francisco area’s average share of active U.S. AI online job postings peaked during the past three years at 7.35% in May 2020. The most recent numbers show that number at just under 3% in January, having mostly dropped month to month since the high point.Embed 5022278Z-1675466948160 San Francisco's share of U.S. AI job postings on ZipRecruiter, 2020-23That’s a worrying sign for the city and the region, said ZipRecruiter Chief Economist Julia Pollak.AdvertisementArticle continues below this ad“San Francisco has kind of gotten the rawest deal of all,” Pollak said. Whereas the city once had the heaviest concentration of tech jobs nationally, “geography has become so much less important than before the pandemic.” The huge shift to full or partial remote work has allowed companies, even if they are based here, to hire people anywhere, she added.“It doesn’t seem likely that San Francisco is going to manage to claw back those jobs,” she said.A city analysis from April last year showed that 33% of local office workers would probably do their jobs remotely, up from an estimate of 15% in January 2022.More For YouAre tech layoffs a sign of looming Bay Area recession? Here’s what experts sayEconomists and experts said while the layoffs are disruptive, they don’t tell the whole...Here’s how many S.F. employees were laid off by SalesforceThe company said in state filings that the layoffs would commence effective March 24And that could have profound effects for the city, which has continued to lose residents, and the ongoing recovery of its economy.“Once you have a critical mass of tech workers leaving San Francisco and a critical mass of people hired elsewhere, you can’t go back,” Pollak said. “With people leaving the area, there are more properties and offices vacant,” which has led to less foot traffic and economic activity in places like downtown, she said.San Francisco saw a nearly 46% office occupancy rate during the last full week of January according to office card-swipe company Kastle Systems, slowly creeping up from where it has hovered, around 40%, for months.San Francisco city data shows that remote work has led to continually rising office vacancies, however, which were above 25% in the fourth quarter of last year.But Egan, the city’s chief economist, was quick to differentiate job postings from actual hires.“Attempts to hire are not necessarily net new growth,” he said, referring to the postings aggregated by ZipRecruiter, which looked for keywords in job listings to pick out AI and machine-learning roles.He noted that companies in San Francisco and elsewhere may also be adding AI keywords to their job postings to make them appear more attractive, because the industry is seeing so much interest right now.“The important context is that job listings can be a leading indicator for the labor market,” Egan said. “Like all leading indicators, they come with an element of inaccuracy.”The city, in its monthly economic analysis, does not differentiate AI jobs from tech more broadly, Egan said.That is despite seemingly daily stories of tech companies large and small cutting hundreds or thousands of jobs.San Francisco’s most recent monthly report shows that the unemployment rate dropped to 2% in January. The report noted that while the information sector, which includes many tech roles, lost jobs on a net basis, growth in professional and technical services, which also includes some tech as well as health care and retail trade, more than made up for the loss.“The real question is, given all the layoffs, why isn’t tech employment dropping faster?” Egan said.Some reasons could be that not all layoffs at companies based in the city affect workers based there. There are also frequently lag times between layoff announcements and actual separations, Egan said. It’s also likely that people are being laid off but finding other jobs.Federal government statistics released last week showed that jobs in professional and business services jumped nationally last month by 82,000, half of that number in the professional, scientific and technical services.Egan said it was difficult to say, if indeed the city’s share of AI jobs was trending down, why that might be, but he cast doubt on San Francisco-based companies offering enough remote positions to meaningfully drive down the number of jobs in that industry in the San Francisco area.Whatever the data says, some AI industry luminaries remain bullish on the city, despite its shortcomings.In a recent tweet, Sam Altman, CEO of OpenAI, whose ChatGPT product has impressed users with its fluent responses if not its accuracy, called San Francisco the “center of the AI revolution.”“S.F. remains super relevant for the next decade,” Altman said.Chase DiFeliciantonio is a San Francisco Chronicle staff writer. Email: chase.difeliciantonio@sfchronicle.com Twitter: @ChaseDiFelice",https://schema.org,WebPage,https://www.sfchronicle.com/sf/article/ai-jobs-artificial-intelligence-17764412.php,"{'@type': 'imageObject', 'width': 2048, 'height': 1365, 'url': 'https://s.hdnux.com/photos/01/31/32/13/23438063/3/rawImage.jpg', 'thumbnailUrl': 'https://s.hdnux.com/photos/01/31/32/13/23438063/3/square_small.jpg', 'caption': 'The San Francisco metropolitan statistical area steadily is losing its share of artificial intelligence job postings, data shows.'}","[{'@type': 'Person', 'name': 'Chase DiFeliciantonio', 'jobTitle': 'Reporter', 'url': 'https://www.sfchronicle.com/author/chase-difeliciantonio/'}]","{'@type': 'NewsMediaOrganization', 'name': 'San Francisco Chronicle', 'url': 'https://www.sfchronicle.com/', 'address': {'@type': 'PostalAddress', 'addressLocality': 'San Francisco', 'addressRegion': 'CA'}, 'logo': {'@type': 'ImageObject', 'url': 'https://www.sfchronicle.com/img/ldjson_logo.png', 'width': '600', 'height': '60'}, 'sameAs': ['https://www.facebook.com/SFChronicle/', 'https://twitter.com/sfchronicle', 'https://www.instagram.com/sfchronicle/', 'https://www.linkedin.com/company/sfchronicle']}",Can AI companies revive downtown SF? Here's what economists say,2023-02-06T12:00:00Z,2023-02-08T03:30:54Z,Bay Area/San Francisco,,False,,"The buzz in recent weeks over leaps in artificial intelligence, or AI, technology has caused venture capitalists and marquee companies like Microsoft to pour billions of dollars into the trending industry. And with many of the most recognizable AI companies based in San Francisco, there are hopes that the rising industry could shift the city’s fortunes and not just revitalize its tech sector, but help fill empty downtown streets, offices and shops with the next wave of tech workers whose spending could filter through the rest of the city’s economy. But some economists aren’t so sure. On one hand, overall hiring in San Francisco’s tech sector in recent months has been flat. That is a mostly positive sign, according to city data and San Francisco Chief Economist Ted Egan, especially since ongoing layoffs have battered companies based in the city in recent weeks and months. But job posting data aggregated from thousands of hiring sites nationally by jobs site ZipRecruiter shows the San Francisco metropolitan statistical area steadily losing its share of job postings in the burgeoning AI field. It’s not entirely clear where those jobs might be located from the data. Some postings could be located in other cities, or a remote role might not have a location tied to it at all. ZipRecruiter’s analysis shows that the San Francisco area’s average share of active U.S. AI online job postings peaked during the past three years at 7.35% in May 2020. The most recent numbers show that number at just under 3% in January, having mostly dropped month to month since the high point. Embed 5022278Z-1675466948160 San Francisco's share of U.S. AI job postings on ZipRecruiter, 2020-23 That’s a worrying sign for the city and the region, said ZipRecruiter Chief Economist Julia Pollak. “San Francisco has kind of gotten the rawest deal of all,” Pollak said. Whereas the city once had the heaviest concentration of tech jobs nationally, “geography has become so much less important than before the pandemic.” The huge shift to full or partial remote work has allowed companies, even if they are based here, to hire people anywhere, she added. “It doesn’t seem likely that San Francisco is going to manage to claw back those jobs,” she said. A city analysis from April last year showed that 33% of local office workers would probably do their jobs remotely, up from an estimate of 15% in January 2022. And that could have profound effects for the city, which has continued to lose residents, and the ongoing recovery of its economy. “Once you have a critical mass of tech workers leaving San Francisco and a critical mass of people hired elsewhere, you can’t go back,” Pollak said. “With people leaving the area, there are more properties and offices vacant,” which has led to less foot traffic and economic activity in places like downtown, she said. San Francisco saw a nearly 46% office occupancy rate during the last full week of January according to office card-swipe company Kastle Systems, slowly creeping up from where it has hovered, around 40%, for months. San Francisco city data shows that remote work has led to continually rising office vacancies, however, which were above 25% in the fourth quarter of last year. But Egan, the city’s chief economist, was quick to differentiate job postings from actual hires. “Attempts to hire are not necessarily net new growth,” he said, referring to the postings aggregated by ZipRecruiter, which looked for keywords in job listings to pick out AI and machine-learning roles. He noted that companies in San Francisco and elsewhere may also be adding AI keywords to their job postings to make them appear more attractive, because the industry is seeing so much interest right now. “The important context is that job listings can be a leading indicator for the labor market,” Egan said. “Like all leading indicators, they come with an element of inaccuracy.” The city, in its monthly economic analysis, does not differentiate AI jobs from tech more broadly, Egan said. That is despite seemingly daily stories of tech companies large and small cutting hundreds or thousands of jobs. San Francisco’s most recent monthly report shows that the unemployment rate dropped to 2% in January. The report noted that while the information sector, which includes many tech roles, lost jobs on a net basis, growth in professional and technical services, which also includes some tech as well as health care and retail trade, more than made up for the loss. “The real question is, given all the layoffs, why isn’t tech employment dropping faster?” Egan said. Some reasons could be that not all layoffs at companies based in the city affect workers based there. There are also frequently lag times between layoff announcements and actual separations, Egan said. It’s also likely that people are being laid off but finding other jobs. Federal government statistics released last week showed that jobs in professional and business services jumped nationally last month by 82,000, half of that number in the professional, scientific and technical services. Egan said it was difficult to say, if indeed the city’s share of AI jobs was trending down, why that might be, but he cast doubt on San Francisco-based companies offering enough remote positions to meaningfully drive down the number of jobs in that industry in the San Francisco area. Whatever the data says, some AI industry luminaries remain bullish on the city, despite its shortcomings. In a recent tweet, Sam Altman, CEO of OpenAI, whose ChatGPT product has impressed users with its fluent responses if not its accuracy, called San Francisco the “center of the AI revolution.” “S.F. remains super relevant for the next decade,” Altman said. Chase DiFeliciantonio is a San Francisco Chronicle staff writer. Email: chase.difeliciantonio@sfchronicle.com Twitter: @ChaseDiFelice",,,,,https://www.sfchronicle.com/sf/article/ai-jobs-artificial-intelligence-17764412.php,,,,,,,,,,"{'@type': 'WebPageElement', 'isAccessibleForFree': 'False', 'cssSelector': '.article'}",en,"{'@type': 'BreadcrumbList', 'itemListElement': [{'@type': 'ListItem', 'position': '1', 'item': {'@type': 'WebPage', '@id': 'https://www.sfchronicle.com', 'name': 'Home'}}, {'@type': 'ListItem', 'position': '2', 'item': {'@type': 'WebPage', '@id': 'https://www.sfchronicle.com/bayarea/', 'name': 'Bay Area'}}, {'@type': 'ListItem', 'position': '3', 'item': {'@type': 'WebPage', '@id': 'https://www.sfchronicle.com/sf/', 'name': 'San Francisco'}}]}"
https://news.google.com/rss/articles/CBMiZmh0dHBzOi8vdG9kYXkudGFtdS5lZHUvMjAyMy8wMi8wOS93aWxsLWFpLXB1dC1sYXd5ZXJzLW91dC1vZi13b3JrLWEtdGV4YXMtYW0tZXhwZXJ0LXNheXMtaXRzLXVubGlrZWx5L9IBAA?oc=5,Will AI Put Lawyers Out Of Work? A Texas A&M Expert Says It's Unlikely - Texas A&M University Today,2023-02-09,Texas A&M University Today,https://today.tamu.edu,News from Texas A&M University,N/A,"Artificial intelligence could make legal services more streamlined and accessible — but many aspects of the profession still require a distinctly human touch, says A&M Law Professor Milan Markovic.",N/A,N/A,N/A,N/A,https://schema.org,,,,,,,,,,,,,,,,,,,,,,,,"[{'@type': 'WebPage', '@id': 'https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/', 'url': 'https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/', 'name': 'Will AI Put Lawyers Out Of Work? A Texas A&M Expert Says It’s Unlikely - Texas A&amp;M Today', 'isPartOf': {'@id': 'https://today.tamu.edu/#website'}, 'primaryImageOfPage': {'@id': 'https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/#primaryimage'}, 'image': {'@id': 'https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/#primaryimage'}, 'thumbnailUrl': 'https://today.tamu.edu/wp-content/uploads/2023/02/GettyImages-1158987219.jpg', 'datePublished': '2023-02-09T17:05:49+00:00', 'dateModified': '2023-02-09T17:05:49+00:00', 'author': {'@id': 'https://today.tamu.edu/#/schema/person/c1f22f40e6466596b3240ceb6877ca1a'}, 'description': 'Artificial intelligence could make legal services more streamlined and accessible — but many aspects of the profession still require a distinctly human touch, says A&M Law Professor Milan Markovic.', 'breadcrumb': {'@id': 'https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/#primaryimage', 'url': 'https://today.tamu.edu/wp-content/uploads/2023/02/GettyImages-1158987219.jpg', 'contentUrl': 'https://today.tamu.edu/wp-content/uploads/2023/02/GettyImages-1158987219.jpg', 'width': 2121, 'height': 1414, 'caption': ""While a company's plan to use artificial intelligence to advise clients in traffic court was recently abandoned, there is still growing speculation about potential roles for AI in the legal field.""}, {'@type': 'BreadcrumbList', '@id': 'https://today.tamu.edu/2023/02/09/will-ai-put-lawyers-out-of-work-a-texas-am-expert-says-its-unlikely/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://today.tamu.edu/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Archives', 'item': 'https://today.tamu.edu/archives/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Will AI Put Lawyers Out Of Work? A Texas A&#038;M Expert Says It’s Unlikely'}]}, {'@type': 'WebSite', '@id': 'https://today.tamu.edu/#website', 'url': 'https://today.tamu.edu/', 'name': 'Texas A&amp;M Today', 'description': 'News from Texas A&amp;M University', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://today.tamu.edu/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Person', '@id': 'https://today.tamu.edu/#/schema/person/c1f22f40e6466596b3240ceb6877ca1a', 'name': 'luke.henkhaus', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://today.tamu.edu/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/d491632404ae4eacc1afa53cf88bd317?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/d491632404ae4eacc1afa53cf88bd317?s=96&d=mm&r=g', 'caption': 'luke.henkhaus'}}]",,,,,,
https://news.google.com/rss/articles/CBMiSWh0dHBzOi8vbmV3cy51bWFuaXRvYmEuY2EvYWNhZGVtaWMtaW50ZWdyaXR5LWFuZC1hcnRpZmljaWFsLWludGVsbGlnZW5jZS_SAQA?oc=5,UM Today | Students | Academic Integrity and Artificial Intelligence - UM Today,2023-02-10,UM Today,https://news.umanitoba.ca,"You've probably used AI tools without even knowing it – Grammarly, Google Translate, Quillbot, Jasper, auto-complete and grammar checker. But now, there are more sophisticated AI tools available, li...",N/A,"You've probably used AI tools without even knowing it – Grammarly, Google Translate, Quillbot, Jasper, auto-complete and grammar checker. But now, there are more sophisticated AI tools available, li ","You've probably used AI tools without even knowing it – Grammarly, Google Translate, Quillbot, Jasper, auto-complete and grammar checker. But now, there are more sophisticated AI tools available, li...",N/A,N/A,"

 

Academic Integrity and Artificial Intelligence
What students need to know



February 10, 2023 — 
You’ve probably used AI tools without even knowing it – Grammarly, Google Translate, Quillbot, Jasper, auto-complete and grammar checker. But now, there are more sophisticated AI tools available, like ChatGPT and DALL • E, that are able to create content in ways never seen before. ChatGPT has attracted a lot of attention recently, including commentary from celebrities and well-known authorities. Actor Ryan Reynolds used Chat GPT to write a script, musician Nick Cave wrote a viral blog post on why AI could never replace artists, and one of the largest scientific, academic journals not accepting ChatGPT as authors in any publications.
Francois Jordaan, a UM Academic Integrity Specialist, says,  “Students are asking if and how they can use these new AI tools to compete coursework. For example, students want to know if they can use these tools for inspiration, to get started or as a reference guide? Or, can they submit parts of AI-generated work if the context allows? If so, how do they cite it? Does using AI allow them to still work with integrity?” Jordaan explains that, “As with other forms of collaboration, the lines between appropriate and inappropriate can get blurred. The answers aren’t always black or white and context is everything.”
Here are some guiding principles for maintaining academic integrity in relation to artificial intelligence..

AI does not replace your own thinking, analysis, or judgment. Using AI inappropriately doesn’t demonstrate your understanding of a topic and it doesn’t help you learn the skills that are expected of you both within the classroom and in the workplace.
Whether or not AI use is appropriate depends on your instructor. Unless you are asked to use AI on a course assignment, assume you are not allowed to use it.
AI can be detected. There are tools that can detect AI produced content. Your instructors will assess you work based on their expertise, the learning objectives of the course and comparisons to your previous work.
If you have used AI without your instructor’s permission, you are at risk of facing an allegation of academic misconduct. A finding of academic misconduct has academic and personal consequences, including lowering your GPA, delaying your graduation and notations on your transcript. If in doubt whether AI is permitted – remember IF YOU NEED TO KNOW, ASK.
AI-generated content can be redundant, inaccurate and repetitive at best, and offensive or biased at worst. Remember that whatever you submit for a course assignment is your full and ultimate responsibility.
If you are permitted to use AI, you must cite it.

Staff from the Academic Learning Centre will be available to help you understand Artificial Intelligence and how it applies to Academic Integrity on the main floor of UMSU University Centre on February 15 and 16 from 10 a.m. to 2 p.m. There will be games and prizes as well as Valentines candy, so come check it out.
Academic Learning Centre staff

Asper School of Business, Desautels Faculty of Music, Faculty and Staff, Faculty of Agricultural and Food Sciences, faculty of architecture, Faculty of Arts, Faculty of Graduate Studies, Faculty of Law, graduate students, Kinesiology and Recreation Management, Price Faculty of Engineering, Rady Faculty of Health Sciences, Research and International, Staff, Things to Know








 







",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMie2h0dHBzOi8vN25ld3MuY29tLmF1L3ZpZGVvL3RlY2hub2xvZ3kvcmlzZS1vZi10aGUtcm9ib3RzLWhvdy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1jb3VsZC1hZmZlY3QteW91ci1qb2ItYmMtNjMyMDAwNDUxODExMtIBAA?oc=5,Rise of the robots: how artificial intelligence could affect your job - 7NEWS,2023-02-08,7NEWS,https://7news.com.au,Rise of the robots: how artificial intelligence could affect your job,N/A,Rise of the robots: how artificial intelligence could affect your job,Rise of the robots: how artificial intelligence could affect your job,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
