URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@graph,article:section,article:summary,article text,@type,headline,mainEntityOfPage,image,datePublished,dateModified,author,publisher,url,logo,sameAs,mainEntity,itemListElement,inLanguage,alternativeHeadline,hasPart,copyrightHolder,sourceOrganization,copyrightYear,isAccessibleForFree,isPartOf,name,@id,diversityPolicy,ethicsPolicy,masthead,foundingDate,thumbnailUrl,legalName,telephone,address,potentialAction
https://news.google.com/rss/articles/CBMie2h0dHBzOi8vd3d3LmJyb29raW5ncy5lZHUvYXJ0aWNsZXMvd2hhdC1qb2JzLWFyZS1hZmZlY3RlZC1ieS1haS1iZXR0ZXItcGFpZC1iZXR0ZXItZWR1Y2F0ZWQtd29ya2Vycy1mYWNlLXRoZS1tb3N0LWV4cG9zdXJlL9IBAA?oc=5,"What jobs are affected by AI? Better-paid, better-educated workers face the most exposure | Brookings - Brookings Institution",2019-11-20,Brookings Institution,https://www.brookings.edu,This analysis demonstrates a new way to identify the kinds of tasks and occupations likely to be affected by AI’s machine learning capabilities.,N/A,This analysis demonstrates a new way to identify the kinds of tasks and occupations likely to be affected by AI’s machine learning capabilities.,N/A,https://schema.org,"[{'@type': 'WebPage', '@id': 'https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/', 'url': 'https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/', 'name': 'What jobs are affected by AI? Better-paid, better-educated workers face the most exposure | Brookings', 'isPartOf': {'@id': 'https://www.brookings.edu/#website'}, 'primaryImageOfPage': {'@id': 'https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/#primaryimage'}, 'image': {'@id': 'https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/#primaryimage'}, 'thumbnailUrl': 'https://www.brookings.edu/wp-content/uploads/2019/11/banner.jpg?quality=75', 'datePublished': '2019-11-14T20:45:59+00:00', 'dateModified': '2022-03-09T04:38:13+00:00', 'description': 'This analysis demonstrates a new way to identify the kinds of tasks and occupations likely to be affected by AI’s machine learning capabilities.', 'breadcrumb': {'@id': 'https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/#primaryimage', 'url': 'https://www.brookings.edu/wp-content/uploads/2019/11/banner.jpg?quality=75', 'contentUrl': 'https://www.brookings.edu/wp-content/uploads/2019/11/banner.jpg?quality=75', 'width': 7360, 'height': 2367, 'caption': 'Banner image'}, {'@type': 'BreadcrumbList', '@id': 'https://www.brookings.edu/articles/what-jobs-are-affected-by-ai-better-paid-better-educated-workers-face-the-most-exposure/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.brookings.edu/'}, {'@type': 'ListItem', 'position': 2, 'name': 'What jobs are affected by AI? Better-paid, better-educated workers face the most exposure'}]}, {'@type': 'WebSite', '@id': 'https://www.brookings.edu/#website', 'url': 'https://www.brookings.edu/', 'name': 'Brookings', 'description': '', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.brookings.edu/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}]",N/A,N/A,"

 Back to Janesville 









                        Back to Janesville 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWmh0dHBzOi8vZm9ydHVuZS5jb20vMjAxOS8xMS8xOS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS13aWxsLW9ibGl0ZXJhdGUtdGhlc2Utam9icy1ieS0yMDMwL9IBAA?oc=5,Types of Jobs Artificial Intelligence Will Eliminate By 2030 - Fortune,2019-11-19,Fortune,https://fortune.com,"Forrester projects that artificial intelligence will severely impact jobs like cubicle workers, location-based workers, and loan processors.","artificial intelligence, jobs eliminated by artificial intelligence, ai jobs, what is ai doing to jobs, does ai destroy jobs, the future of artificial intelligence, what is ai, what is artificial intelligence, jobs safe from artificial intelligence","""All gone.""","""All gone.""",,,N/A,N/A,"Newsletters - Data SheetThe security company that Alphabet may buy for $23 billion launched at the perfect timeBYDavid MeyerJuly 16, 2024",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMicmh0dHBzOi8vd3d3LnN0YXJ0cmVrLmNvbS9uZXdzL2hvdy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1pcy1nZXR0aW5nLXVzLWNsb3Nlci10by1zdGFyLXRyZWtzLXVuaXZlcnNhbC10cmFuc2xhdG9yc9IBAA?oc=5,How Artificial Intelligence Is Getting Us Closer to Star Trek's Universal Translators - Star Trek,2019-11-20,Star Trek,https://www.startrek.com,"Current automatic translation can’t compete with the 24th century’s, but it’s made astronomical progress in the past decade.",N/A,"Current automatic translation can’t compete with the 24th century’s, but it’s made astronomical progress in the past decade.",N/A,https://schema.org,,N/A,N/A,N/A,Organization,How Artificial Intelligence Is Getting Us Closer to Star Trek’s Universal Translators,"{'@type': 'WebPage', '@id': 'https://www.StarTrek.com/news/how-artificial-intelligence-is-getting-us-closer-to-star-treks-universal-translators'}","['https://images.prismic.io/star-trek-untold/MGRlZjhhYzAtYTBlNC00MTkwLWEyYWUtYjQxOWJhYzA5OGFm_universal_translator_001.png?auto=compress,format&rect=0,0,2000,1080&w=2000&h=1080']",2023-07-25T00:09:31+0000,2023-08-08T20:43:04+0000,"[{'@type': 'Person', 'name': 'Lorelei Laird'}]","{'@type': 'Organization', 'name': 'Star Trek', 'logo': {'@type': 'ImageObject', 'url': 'https://www.startrek.com/delta-serp.png'}}",https://www.startrek.com,https://www.startrek.com/delta-serp.png,"['https://twitter.com/StarTrek', 'https://www.instagram.com/startrek/', 'https://www.facebook.com/StarTrek/', 'https://www.startrek.com/', 'https://www.youtube.com/startrek', 'https://en.wikipedia.org/wiki/Star_Trek']",,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiMWh0dHBzOi8vd3d3Lm5hdHVyZS5jb20vYXJ0aWNsZXMvczQxNzQ2LTAxOS0wMTg5LTfSAQA?oc=5,Human–machine partnership with artificial intelligence for chest radiograph diagnosis | npj Digital Medicine - Nature.com,2019-11-18,Nature.com,https://www.nature.com,"Human-in-the-loop (HITL) AI may enable an ideal symbiosis of human experts and AI models, harnessing the advantages of both while at the same time overcoming their respective limitations. The purpose of this study was to investigate a novel collective intelligence technology designed to amplify the diagnostic accuracy of networked human groups by forming real-time systems modeled on biological swarms. Using small groups of radiologists, the swarm-based technology was applied to the diagnosis of pneumonia on chest radiographs and compared against human experts alone, as well as two state-of-the-art deep learning AI models. Our work demonstrates that both the swarm-based technology and deep-learning technology achieved superior diagnostic accuracy than the human experts alone. Our work further demonstrates that when used in combination, the swarm-based technology and deep-learning technology outperformed either method alone. The superior diagnostic accuracy of the combined HITL AI solution compared to radiologists and AI alone has broad implications for the surging clinical AI deployment and implementation strategies in future practice.",N/A,N/A,npj Digital Medicine - Human–machine partnership with artificial intelligence for chest radiograph diagnosis,https://schema.org,,N/A,N/A,"




Download PDF








Article

Open access

Published: 18 November 2019

Human–machine partnership with artificial intelligence for chest radiograph diagnosis
Bhavik N. Patel 
            ORCID: orcid.org/0000-0001-5157-99031, Louis Rosenberg2, Gregg Willcox2, David Baltaxe2, Mimi Lyons2, Jeremy Irvin3, Pranav Rajpurkar3, Timothy Amrhein 
            ORCID: orcid.org/0000-0002-9354-94864, Rajan Gupta4, Safwan Halabi 
            ORCID: orcid.org/0000-0003-1317-984X1, Curtis Langlotz 
            ORCID: orcid.org/0000-0002-8972-80511, Edward Lo1, Joseph Mammarappallil4, A. J. Mariano1, Geoffrey Riley1, Jayne Seekins1, Luyao Shen1, Evan Zucker1 & …Matthew P. Lungren1 Show authors

npj Digital Medicine
volume 2, Article number: 111 (2019)
            Cite this article




19k Accesses


93 Citations


201 Altmetric


Metrics details










An Author Correction to this article was published on 10 December 2019







This article has been updated



AbstractHuman-in-the-loop (HITL) AI may enable an ideal symbiosis of human experts and AI models, harnessing the advantages of both while at the same time overcoming their respective limitations. The purpose of this study was to investigate a novel collective intelligence technology designed to amplify the diagnostic accuracy of networked human groups by forming real-time systems modeled on biological swarms. Using small groups of radiologists, the swarm-based technology was applied to the diagnosis of pneumonia on chest radiographs and compared against human experts alone, as well as two state-of-the-art deep learning AI models. Our work demonstrates that both the swarm-based technology and deep-learning technology achieved superior diagnostic accuracy than the human experts alone. Our work further demonstrates that when used in combination, the swarm-based technology and deep-learning technology outperformed either method alone. The superior diagnostic accuracy of the combined HITL AI solution compared to radiologists and AI alone has broad implications for the surging clinical AI deployment and implementation strategies in future practice.



Similar content being viewed by others






Collaborative strategies for deploying artificial intelligence to complement physician diagnoses of acute respiratory distress syndrome
                                        


Article
Open access
08 April 2023









Deep learning for distinguishing normal versus abnormal chest radiographs and generalization to two unseen diseases tuberculosis and COVID-19
                                        


Article
Open access
01 September 2021









A multistage framework for respiratory disease detection and assessing severity in chest X-ray images
                                        


Article
Open access
29 May 2024








IntroductionRecent notable applications of deep learning in medicine include automated detection of diabetic retinopathy, classification of skin cancers, and detection of metastatic lymphadenopathy in patients with breast cancer, all of which demonstrated expert level diagnostic accuracy.1,2,3 Recently, a deep-learning model was found to match or outperform human expert radiologists in diagnosing 10 or more pathologies on chest radiographs.4,5 The success of AI in diagnostic imaging has fueled a growing debate6,7,8,9 regarding the future role of radiologists in an era, where deep-learning models are capable of performing important diagnostic tasks autonomously and speculation surrounds whether the comprehensive diagnostic interpretive skillsets of radiologist can be replicated in algorithms. However, AI is also plagued with several disadvantages including biases due to limited training data, lack of cross-population generalizability, and inability of deep-learning models to contextualize.8,10,11,12Human-in-the-loop (HITL) AI may offer advantages where both radiologists and machine-learning algorithms fall short.13,14 This paradigm allows leveraging all the advantages of AI models (i.e. rapid automated detection) but having a human at various checkpoints to fill gaps where algorithms are not confident in their probabilities or where they may fall short due to underlying biases. For example, a machine-learning algorithm could analyze a large dataset and provide output for the presence of disease in a short period of time, some with high confidence (i.e. high probability of the presence or absence of the disease relative to the probabilistic threshold for the detection of that disease) and others with low. The lower confidence outputs could then be validated by a human to create a combined better decision on the input; this approach could harness the best of human intelligence and artificial intelligence to create a collective super intelligence. Recent work has shown superior task performance of a combined human and AI augmented model compared to either human15 or machine alone.15,16 To date, however, no studies have harvested the full collective intelligence of a group of radiologists and then examined the performance of an augmented model.In this study, we employ a novel collective intelligence platform called Swarm17,18,19 designed to amplify the accuracy of networked human groups by enabling the groups to work together in real-time systems modeled after biological swarms. In contrast to traditional crowds, swarm intelligence refers to stigmergic collaborative behavior of large groups of independent agents that form a closed-loop system, resulting in an collective super intelligence whose capacity exceeds that of any individual agent.17,19 The most studied form of swarm intelligence in nature is the honeybee swarm, which has been shown to make decisions through a process that is surprisingly similar to neurological brains.20,21,22 Both employ large populations of simple excitable units (i.e., bees and neurons) that work in unison to integrate noisy and incomplete information, weigh competing alternatives and converge on unified decisions in real-time synchrony. In both brains and swarms, outcomes are arrived at through a real-time competition among sub-populations of excitable units. When one sub-population exceeds threshold support, the corresponding alternative is chosen. In honeybees, this enables the colonies to converge on optimal decisions to highly complex problems, such as selecting an optimal home location from among a large set of alternatives.21,22,23When using the platform with groups of radiologists, the swarm-based technology was applied to the diagnosis of pneumonia on chest radiographs. Diagnostic accuracy of the swarm-based technology was compared against the human experts alone and two state-of-the-art deep-learning AI models that have demonstrated expert level performance in automated pneumonia and multiple diagnosis detection, respectively.5,6 In addition, a novel combination of the swarm-based technology and the deep-learning AI models was compared against each of the methods in isolation.ResultsExperiment designComplete details regarding the chest radiograph dataset, two deep-learning model architectures, and swarm-based collective intelligence platform is discussed in the online methods section. In brief, a total of 13 expert radiologists split over two sessions (7 radiologists in group A; 6 radiologists in group B) provided their estimate of the probability of the presence or absence of pneumonia on 50 chest radiographs, first alone then collectively using the real-time swarm platform. Two state-of-the art deep-learning models, CheXNet and CheXMax, were also used to evaluate the chest radiographs and the performance between individual human experts, real-time swarms, and AI were compared. Finally, a novel combination of the real-time swarm platform and the deep-learning models was compared against each method in isolation.The aggregate performance of individual human experts was calculated in two ways: first, the average probability of all radiologists in each group was calculated and used as the crowd-based mean performance. Second, a crowd-based majority diagnosis was calculated using a vote—if more radiologists diagnosed pneumonia than no pneumonia using a 50% probability cutoff, the crowd-based majority diagnosis was pneumonia.The probabilistic diagnoses from CheXNet and CheXMax were turned into binary classifications of pneumonia using a discrimination threshold—any probabilistic diagnosis above the threshold is assigned a prediction of “Pneumonia” and any diagnosis below the threshold is assigned a prediction of “No Pneumonia”. The thresholds for these algorithms were set to maximize the performance of the algorithms on their respective training data sets. The discrimination threshold for CheXNet is 50%, and the discrimination threshold for CheXMax is 4.008%.Diagnostic performance resultsResults of the diagnostic performance of individuals, real-time swarms, and AI models are summarized in Tables 1 and 2. The individual radiologists outperformed CheXNet in diagnosing pneumonia in this study (AUC of 0.698 vs. 0.545, p < 0.01). CheXMax on the other hand, outperformed the individual radiologists (AUC of 0.938 vs. 0.698, p < 0.01). The swarm platform also outperformed the individual radiologists. For both swarm sessions, swarm interpolation achieved higher diagnostic accuracy than individual human performance, crowd-based performance, and CheXNet (Fig. 1). For group A, swarm achieved a statistically higher AUC of 0.840 [0.691, 0.937] compared to 0.763 [0.709, 0.817] (p < 0.05) average AUC of all radiologists in group A and to 0.685 [0.520, 0.854] (p < 0.01) AUC for CheXNet. For group B, the swarm had a statistically higher diagnostic accuracy than individual radiologists and CheXNet for all performance metrics (e.g. AUC of 0.889 vs. 0.810 and 0.685, respectively). When results from both swarm sessions were combined, swarm-based diagnoses resulted in statistically higher (p < 0.01) accuracies compared to radiologists and CheXNet (e.g. AUC of 0.868 vs. 0.785 and 0.685, respectively) (Fig. 1). There was no difference between CheXMax and the combined swarms in terms of accuracy or F1 score (accuracy of 82% vs. 84%, p = 0.423; F1 of 0.788 vs. 0.800, p = 0.34). CheXMax outperformed the combined swarm in AUC (0.938 vs. 0.868, p < 0.01), but the swarm outperformed CheXMax in terms of Brier score and mean absolute error (Brier scores of 0.287 vs. 0.134, p < 0.01; MAE of 0.357 vs. 0.233, p < 0.01).Table 1 Diagnostic performance parameters for individual particpants, swarm sessions, and AI models.Full size tableTable 2 Sensitvity and specificity for individual particpants, swarm sessions, and AI models.Full size tableFig. 1Bootstrapped average AUC curves. AUC curves show that the swarms (blue bars) outperform group A (left image), group B (middle image), and combined group (right image). Radiologists (orange bars) performances in diagnosing pneumonia. Swarm also outperforms CheXNet (green bars).Full size imageThe sensitivity and specificity of each of the diagnostic methods is compared in Table 2. In terms of the sensitivity of each diagnostic method, the swarm outperforms CheXNet in all groups (a combined group sensitivity of 0.700 [0.578, 0.814] versus CheXNet’s 0.450 [0.326, 0.579], p < 0.01), while CheXMax and the combined model outperform the swarm in all groups, with sensitivities of 0.900 [0.773, 1.00] (p < 0.05) and 0.875 [0.783, 0.956] (p < 0.05), respectively. In terms of the specificity of each diagnostic method, the swarm outperforms CheXMax, with a specificity of 0.933 [0.855, 0.968] compared to CheXMax’s specificity of 0.767 [0.672, 0.857] (p < 0.01). The Augmented HITL model obtains a specificity of 0.933 [0.877, 0.983], the same as the swarm. Interestingly, the human diagnostic methods all show a lower sensitivity than specificity, while CheXMax shows a higher sensitivity than specificity. The swarm shows the highest specificity of any of the diagnostic methods, while CheXMax shows the highest sensitivity of any of the diagnostic methods.Due to the differences in sensitivity and specificity between the swarm and CheXMax, it is clear that these two diagnostic methods have different strengths and advantages when diagnosing pneumonia: CheXMax has a higher sensitivity, so this machine-learning model is more precise at detecting pneumonia if it does exist, whereas the swarm has a higher specificity, so this human-driven model is more precise at detecting when pneumonia does not exist in an image. An augmentation model was created to determine whether these two strengths could be combined into a single model to achieve higher accuracy as compared to either model alone. CheXMax was used first to diagnose the probability of pneumonia across all 50 cases. Cases in which CheXMax yielded a low-confidence diagnosis were then passed on to swarm. Low-confidence was defined as a CheXMax probability between 2.5% and 5.5%, resulting in 20 cases passed on to swarm for final prediction of pneumonia. This selection was chosen as a fair low-confidence band around the average CheXMax prediction (p = 4.008%) and yielded 20 cases on which to swarm: 13 negative predictions and 7 positive predictions. Each swarm’s probabilistic diagnoses on these 20 cases were then used as the augmented model’s final diagnoses, in place of the ML system’s low-confidence diagnoses. All other cases remained diagnosed exclusively by the ML system.This augmented combined system achieved a statistically higher diagnostic performance than either CheXNet or CheXMax and combined swarm alone using two of the performance metrics (e.g. accuracy 92% vs. 82% and 84%; F1 score of 0.89 vs. 0.80 and 0.78, respectively; p < 0.01) (Table 1). The augmented model system achieves the lowest diagnostic error rate, in terms of number of diagnoses incorrect, compared to any of the other examined diagnostic methods (error rates of 9% augmented model; 16% combined swarm; 18% CheXMax; 24% individuals). Moreover, the augmented system achieves near-best performance in both sensitivity and specificity, with a sensitivity of 0.875 [0.783, 0.956] and a specificity of 0.933 [0.877, 0.983]. It appears that this augmented model is therefore able to combine the best aspects of both the machine-learning system (CheXMax), which has high sensitivity, and the swarm, which has high specificity.To better visualize how the HITL augmentation process changed the probabilistic diagnoses of CheXMax and why these changes resulted in a more accurate system, a scatterplot of probabilistic diagnoses is shown in Fig. 2. Over all 100 of the evaluated cases, the swarm and CheXMax disagreed on a total of 24, of which the vast majority (21, or 87.5%) were cases where the swarm gave a negative diagnosis, but CheXMax gave a positive diagnosis. Over all 24 cases where the Swarm and CheXMax disagreed, the swarm was correct on 12 diagnoses, while CheXMax was correct on the other 12 diagnoses.Fig. 2Scatterplot of swarm vs. CheXMax probabilistic diagnoses, with cases colored by ground truth. The scatterplots show that CheXMax and human swarms assign very different probabilities to each case (left image). The gray “Augmented Cases” range shows cases that were sent from CheXMax to the Swarm for augmentation. CheXMax has a high incidence of True Positives (blue-colored cases below the horizontal CheXMax Threshold line), but when the CheXMax gives a weak positive diagnosis (between 0.04008 and 0.055 on the y-axis), it is often incorrect (11 out of 15 cases correct, or an accuracy of 73%). Using a human swarm to re-classify these weak positive cases results in correctly labeling 14 out of 15 of the cases—an accuracy improvement of 20%. The cases on which the two diagnostic methods disagreed are more clearly visualized in the scatterplot of diagnostic disagreement (right image).Full size imageThe gray band across each image represents the range of cases that CheXMax diagnosed with low confidence (probabilistic diagnosis between 0.025 and 0.055), and subsequently sent to the swarm for a second opinion. These 20 cases were each evaluated by both Group A and Group B, for a total of 40 diagnoses generated by the swarms in the augmented system. Of these 40 diagnoses, 29 agreed with CheXMax’s original evaluation: three cases where both diagnostic methods gave a positive reading, with 100% accuracy, and 26 cases where both methods gave a negative reading, with 84.6% accuracy. Only 11 diagnoses disagreed with CheXMax’s original evaluation, all of which were low-confidence positive diagnoses by CheXMax, and high confidence negative diagnoses by swarm (less than a 20% probability of pneumonia). Of these 11 cases, the swarm correctly changed CheXMax’s original diagnosis 10 out of 11 times (91%). Figure 3 provides examples of correct diagnosis by CheXMax over swarm, and vice-versa, as well as an augmented case with correct diagnosis changed by swarm from CheXMax.Fig. 3Case examples. Each of the three rows a–c represent three different patients. Grayscale image is on the left with the corresponding class activation map to its right. The top row example a includes a patient with pneumonia in the left lung, correctly predicted by CheXMax but incorrectly by swarm. The middle row b is an example of a patient with metastatic disease but without pneumonia, correctly predicted by swarm and incorrectly by CheXMax. The bottom row c is an example of an augmented case, where CheXMax provided a low confidence positive prediction (p = 0.41) but was correctly predicted as negative by swarm.Full size imageBecause the boost in performance of the augmented model depends on the selection of cases (i.e. how “low confidence” is defined) sent to the swarm, a sensitivity analysis was performed to determine the impact of the quantity of cases selected to be sent to the swarm on the accuracy of the augmented model. In this sensitivity analysis, cases are selected based on their distance to the discrimination threshold (4.008%) of CheXMax, starting with the lowest-confidence cases (those closest to the discrimination threshold). An equal number of positive and negative cases are selected to be diagnosed with Swarm at each cutoff, where the number of cases selected ranges from 0 to 50 (0–100% of the data). The number of cases correctly diagnosed by the augmented system is calculated for each cutoff.A bootstrapping analysis with 1000 bootstraps is used to find the 90% confidence interval of accuracy for each cutoff by randomly re-sampling a full set of cases from the observed population 1000 times and calculating the observed accuracy for each resampled set of cases. The average accuracy increases of this augmented system relative to CheXMax and 90% confidence interval of this accuracy increase are shown in Figs. 4 and 5. This sensitivity analysis suggests that the success of the augmented model is only slightly sensitive to the choice of threshold at which a case is deemed “low-confidence”. Regardless of the choice of threshold, the average accuracy of the augmented system is greater than or equal to that of the ML system (Fig. 6). When the proportion of cases sent to the swarm is between 6% and 32%, however, the augmented system diagnoses more cases correctly than the ML system alone (p < 0.05), indicating that the augmented model outperforms CheXMax across a wide range of “low confidence” thresholds.Fig. 4Sensitivity analysis of augmented model accuracy. The shape of the average accuracy line shows a consistent increase in the accuracy of the augmented model when the 0–14% lowest-confidence cases are sent to the swarm, from 82% correct of CheXMax (sending 0% of cases) to 90% correct when sending the 14% of lowest-confidence positive and negative cases to the swarm. The model performs similarly when 16–32% of cases are sent to the swarm, achieving between 88% and 92% accuracy across this sensitivity range. If more than 32% of cases are sent to the swarm, the accuracy of the system decreases, until the limit of sending all diagnoses to the swarm is reached (100% of cases swarmed), where the accuracy returns to the swarm score of 84%.Full size imageFig. 5Sensitivity analysis of accuracy increase relative to CheXMax. Sensitivity analysis shows a band between 6% and 34%, where the 90% confidence interval is only ever >0%. This indicates that when sending between 6% and 34% of the lowest-confidence cases to the swarm using this method, there is high confidence that the augmented model would diagnose the cases more accurately than the CheXMax alone. If the range is limited between 14% and 28%, the average improvement in accuracy is 7.75% correct.Full size imageFig. 6Bootstrapped average specificity and sensitivity of aggregate diagnostic methods. The bootstrapped specificity histograms show that the swarms in the combined group (blue bars) outperform CheXMax (green bars) in terms of specificity (left image), but CheXMax outperforms the swarms in terms of sensitivity (right image). The HITL Combined model combines the best of both the CheXMax and swarm diagnostic methods, by attaining swarm-level specificity and CheXMax-level sensitivity.Full size imageDiscussionOur study shows that, using a test set of 50 chest radiographs with strong ground truth using clinical outcomes, highest diagnostic performance can be achieved with HITL AI when radiologists and AI technologies work together. We combined a novel real-time interactive platform that utilizes the biological concept of swarm intelligence with a deep-learning model and found the maximum diagnostic performance that neither alone was able to achieve.Using a swarm platform, we found that the diagnostic performance was higher than individual radiologist performance in diagnosing pneumonia. Moreover, when results of both swarm sessions were combined, swarm-based diagnoses outperformed crowd-based majority vote. This has important implications as many studies involving deep-learning models often use either individual expert, consensus, or majority vote to provide ground truth labels for validation and test sets when stronger metrics, such as pathology results, are unavailable or not applicable.4,15,24,25 Results from our study shows that swarm-based diagnoses outperforms crowd-based diagnoses, and thus may represent a novel means for generating image labels that provide more accurate ground truth than conventional consensus labeling methods for training datasets for deep-learning algorithms. Moreover, some centers may not readily have access to experts, and labeling images through swarm sessions may allow such centers to achieve expert level labels.It has been well-established that crowds of people can outperform individuals and achieve estimates close to the true value that would otherwise not be possible with individual estimates, a concept known as “wisdom of crowd effect”.26,27,28,29,30 In fact, the use of this effect specifically for medical decision making has also been described.31,32,33,34 However, this effect is a statistical aggregation of individual estimates. Traditionally, its power is shown through votes, conducting polls, or collecting surveys such that the input from each individual member is captured in isolation (or near isolation) and then combined with the data collected from other members to then pass through a post-hoc statistical processing. Thus, the dynamics of a real-time collaboration and negotiation are void within these types of “crowds.” Studies have also shown that wisdom of crowd effect could be undermined by social influence, and that the crowd-based decision can be biased resulting in tending away from higher accuracy compared to the individual.28,35,36 This is, however, dependent on how the communication network for information exchange is structured.37Modeled after complex decision processes used by swarming honeybees, the real-time algorithms that connect users of the Swarm platform enable human groups to work together to integrate noisy and incomplete information, weigh competing alternatives, and converge on unified decisions in real-time synchrony. In this way, the swarm-based technology utilized in this study enabled networked groups of radiologists to outperform individual radiologists, groups of radiologists taking a traditional vote, and the CheXNet deep-learning system when diagnosing pneumonia on chest radiographs.Similar to the human experiment in which we aimed to harness the maximum diagnostic potential, we retrained CheXNet,38 which was originally trained on a publicly available NIH dataset,39 on a recently released large dataset of chest radiographs with radiologist level validation and test sets.5 This newly trained deep-learning algorithm, CheXMax, outperformed the average radiologist for the detection of pneumonia (e.g. 82% vs. 76% accuracy, respectively). Moreover, this newly trained algorithm outperformed the swarm-based method when using the AUC metric (the standard measure of diagnostic classification accuracy) and underperformed the swarm-based method when using mean absolute error and Brier score (the standard measures of probabilistic accuracy) (Table 1). Since the AUC metric measures the success of ordering the cases from least to most likely to contain pneumonia, while the Brier score and mean absolute error scores measure the probabilistic accuracy of the diagnoses—e.g. whether a diagnosis of a 10% chance of pneumonia actually contained pneumonia only 10% of the time—this result suggests that the CheXMax is better at ordering the cases from least to most likely to contain pneumonia, while the swarm is better at assessing the probabilistic likelihood of pneumonia in a specific case.As deep-learning models continue to improve though larger and higher quality dataset for training, as we found with retraining CheXNet,5 an unanswered question remains as to what the exact scenario of implementation within clinical workflow will be. Advantages of deep-learning algorithms include rapidity in diagnosis proving to be useful as a triage tool. Disadvantages include biases introduced by training dataset and inability to contextualize to clinical context.8,11,12,40 Thus, many have advocated that a clinical workflow model in which healthcare workers leverage AI might yield the greatest benefit to patients.40,41 To that extent, few studies have shown superior performance of human augmented by AI compared to either human or machine alone.15,16 In our study, we showed the ability of a deep-learning algorithm in CheXMax to provide rapid confident diagnoses for pneumonia for over a half of the cases in the test set. Low-confidence cases were then passed on to the human through swarm to yield the final diagnosis and the combined HITL model resulted in higher diagnostic accuracy than either radiologists or AI models alone. The clinical significance of this could imply that, in a landscape of increasing clinical volumes, complexity of cases, and medical record documentation, physicians could leverage deep learning to improve operational efficiency; deep-learning algorithms could provide automated rapid diagnosis for high confident cases as a triage tool so that physicians could spend less time on high confidence cases evaluated by an AI model and more time on relatively complex cases. In such HITL scenarios, active learning could be provided to AI algorithms through feedback from radiologists in the form of additional training data, which the model did not initially provide a confident diagnosis. Further studies are needed to determine the optimal workflow and implementation of deep-learning algorithms in the healthcare setting.Several limitations of our study merit consideration, including those that are attributed to a retrospective design; despite having a strong clinical reference standard, we utilized a very small test set of 50 cases which, though achieving the best available clinical ground truth, nonetheless, it is possible that patients who were included may have had other pathologies that were clinically treated as pneumonia in routine clinical care, which may have confounded the diagnosis had the follow-up period been prospectively designed or more invasive testing been performed (i.e. sputum cultures, bronchoscopy, direct sampling), which may have altered the results of the study. This size was chosen as to practically perform swarm sessions with synchronous groups of radiologists in a timely manner, and judging by the statistical analysis in this pilot it is possible that larger sample sizes would have observed a similar trend. While we studied the improved diagnostic accuracy of combining the capabilities of CheXMax with swarm, we did not perform a simulation experiment where CheXMax results are given real-time during the swarm sessions. The cutoff for the model decisions were selected by maximizing Youden’s J on the validation set and this low probability leads to a good tradeoff between sensitivity and specificity for this task. Finally, one could argue that such a platform as swarm may not be practically necessary for decisions and tasks as relatively simple as diagnosing pneumonia on chest radiographs.In conclusion, we demonstrated increased diagnostic performance of radiologists in diagnosing pneumonia on chest radiographs through swarm intelligence using a novel platform, superior performance of a deep-learning algorithm trained on higher quality data compared to individual radiologists and achieved the maximum diagnostic accuracy using an HITL AI approach through an augmented model combining both radiologists (through swarm) and AI (using a deep-learning model). Although we focused on one common medical-imaging task, future work could assess the feasibility and application for other various medical diagnostic tasks and decision making.MethodsThis Health Insurance Portability and Accountability Act-compliant study was approved by the Institutional Review Board of Stanford University, and a waiver of informed consent was obtained.Chest radiograph pneumonia datasetWe retrospectively searched our electronic medical record database and picture and archiving communications system (PACS) for patients who underwent chest radiographs (anterioposterior or posterioanterior) in the emergency room or in the outpatient clinic setting over a 2-year period between 2015 and 2017. The search yielded an initial target population of 7826 unique patients with 11,127 chest radiographs. Patients were eligible for inclusion in the study if they presented with clinical signs and symptoms concerning for pneumonia, such as fever, cough, shortness of breath, elevated white blood cell count, crackles on physical examination, etc.42,43 Subjects were excluded from the study if: (a) the clinical reference standard was inadequate (see below) or (b) an inadequate examination due to a suboptimal technique or incomplete imaging data available. The first consecutive 50 unique patients who met the aforementioned eligibility were included in the final population. A test set size cutoff of 50 chest radiographs was used in order to practically perform the human reader evaluation in a timely fashion and so as not to introduce reader fatigue that might occur with larger datasets. The final retrospective cohort was comprised of 27 males and 23 females (mean age ± standard deviation, 62.1 ± 21.0 years; range, 19–100 years) with a test set of 50 frontal chest radiographs.Clinical reference standardOnly those patients and their frontal chest radiographs were included, if they are presented with aforementioned signs and symptoms and clinical concern for pneumonia. An image was labeled negative if all of the following criteria were met: (a) chest radiograph was interpreted as negative for pneumonia by a board-certified diagnostic radiologist at the time of examination; (b) a follow-up chest computed tomography (CT) within 1 day after the index chest radiograph confirmed lack of pneumonia on imaging; (c) the patient was not administered antibiotics. An image was labeled positive for pneumonia if all of the following criteria were met: (a) chest radiograph was interpreted as positive for pneumonia by a board-certified diagnostic radiologist at the time of examination; (b) patient was treated with antibiotics; (c) a follow-up chest CT or chest radiograph within 7 days after treatment showed interval improvement or resolution of pneumonia; (d) patient showed clinical signs of improvement after treatment on follow-up visit. Using this reference standard, the test set contained a class balance of 30 negative and 20 positive exams for pneumonia.Deep-learning models and architecturesTwo previously developed and described state-of-the-art convolutional neural networks for chest radiographs were used.4,5 First, a 121-layer dense convolutional neural network (DenseNet), CheXNet, was used on the 50 test cases. This model was trained using the publicly available dataset released by Wang et al.39 CheXNet was previously tested on 14 different chest radiograph pathologies, including pneumonia, and outperformed a group of board-certified diagnostic radiologists5 as well as previous models39,44 using the same dataset. Though large datasets, such as the one released by Wang et al. 39 have allowed progress in deep-learning automation of chest radiographs, those efforts can only achieve a certain advancement before reaching a plateau. This is due to the fact that large well-labeled datasets with strong clinical reference standards are needed. Publicly available large datasets are often limited as the labels are derived from automatic labelers that extract information from existing radiology reports.39 Additionally, these labelers cannot account for uncertainty that may be conveyed in free text radiology reports. Thus, the advantages of access to available large datasets can come at a cost of weak labels. A recent large dataset of chest radiographs was released that addresses these limitations with labels that account for uncertainty and has strong reference standards with radiologist labeled validation and test sets.5 Using the this recently released database, we retrained CheXNet model (the newly trained model referred to as CheXMax), hypothesizing that the improved training dataset would boost the diagnostic potential of this deep-learning algorithm for chest radiographs. The test set of 50 chest radiographs were evaluated with CheXMax and probabilities of pneumonia for each exam were derived.RadiologistsA total of 13 board certified diagnostic radiologists (average years of experience: 7.8 years; range 1–23 years) across two major busy tertiary care centers (Stanford University and Duke University) participated in this study. The 13 radiologists were arbitrarily divided into two groups (group A—7 radiologists, average(range) of experience: 6.6 (1–11); group B—6 radiologists, average(range) of experience: 9.2 (1–23)) based on their availability. Each group participated in a 2-h session (see the “Swarm sessions” section) to evaluate a test set of 50 chest radiographs, first individually and then as a swarm.Swarm platform and model architectureIn order to assess both individual diagnostic performance and maximal collective human diagnostic performance, we employed a novel real-time collaborative software platform called Swarm that has been assessed in a variety of prior studies and has been shown to amplify the combined intelligence of networked human groups.23,45,46,47 While traditional systems that harness the intelligence of groups collect data from participants in isolation, usually through an online survey, and then combine the input statistically to determine the group response, the Swarm platform enables participants to work together in real-time, converging on a group decision as a unified system that employs biological principle of Swarm Intelligence. This is achieved using a unique system architecture that includes a central processing engine that runs swarming algorithms on a cloud-based server (Fig. 7a). The processing engine is connected over the internet to a set of remote workstations used by the human participants (Fig. 7b). Each workstation runs a client application that provides a unique graphical interface for capturing real-time behavioral input from participants and for providing real-time feedback generated by the processing engine.Fig. 7Swarm platform. A system diagram (left image) of the Swarm platform shows the connection of networked human users. A Swarm engine algorithm received continuous input from the humans as they are making their decision and provides real-time collaborative feedback back to the humans to create a dynamic feedback loop. Swarm Platform positioned next to a second screen for viewing radiograph (middle image). A snapshot (right image) of the real-time swarm of six radiologists (group B) shows small magnets controlled by radiologists pulling on the circular puck in the process of collectively converging towards a probability of pneumonia. To view a video of the above question being answered in the Swarm platform, visit the following link: https://unanimous.ai/wp-content/uploads/2019/05/Radiology-Swarm.gif.Full size imageThe processing engine employs algorithms modeled on the decision-making process of honeybee swarms. The underlying algorithms enable networked groups to work together in parallel to (a) integrate noisy and incomplete information, (b) weigh competing alternatives, and (c) converge in synchrony on optimized decision, all while allowing participants to react to the collective impact they are having on the changing system in real-time, thereby closing a feedback loop around the whole group.21 To use this platform, distributed groups of participants (in this case radiologists) log on to a central server from their own individual workstations and are simultaneously asked a series of questions to be answered together as a swarm. In this study, each question in the series involved assessing the probability of a patient having pneumonia based upon a displayed chest radiograph.To answer each question, the participants collaboratively move a graphical pointer represented as a glass puck (Fig. 7c). An answer is reached when the group moves the puck from the center of the screen to a target associated with one of the available answer options. In this study, the displayed question was “What is the probability this patient has pneumonia?” and the answer options were five percentage ranges that the participants could choose among. The ranges were (0–5%), (5–25%), (25–65%), (65–85%), and (85–100%).To influence the motion of the puck, each participant controls a graphical magnet using their mouse or touchscreen. The magnet enables each participant to express their intent upon the collaborative system by pulling the graphical puck in the direction they believe it should go. It is important to note that these user inputs are not discrete votes, but continuous streams of vectors provided simultaneously by the full set of participants, enabling the group to collectively pull on the system in opposing and/or supporting directions until they converge, moving the puck to one solution they can best agree upon. It is also important to note that the impact that each user has on the motion of the puck is determined by the swarm algorithms at every time step. The algorithms evaluate the relative conviction that each participant has at each moment based on their behaviors over time (i.e. how their magnets move as compared to each of the other participants). In this way, the software enables real-time control system such that (i) the participants provide behavioral input at every time-step, (ii) the swarming algorithms determine how the graphical pointer should move based on the behavioral input, (iii) the participants to react to the updated motion of the pointer, updating their behaviors in real-time, and (iv) the swarming algorithms react to the updated behaviors, thereby creating a real-time, closed-loop feedback system. This process repeats in real-time until the participants converge on a final answer by positioning the pointer upon one of the five targets.Using this method, the distributed group of users quickly converge on solutions, each answer being generated in under 60 s. After the solutions is reached, the behavioral data is fed into an interpolation algorithm which computes a refined probability as to the likelihood that patient associated with the displayed radiograph is positive for pneumonia. This interpolation is performed because the group of participants were provided a simple set of five options to choose from, each representing a wide range of probabilities. By interpolating the behavioral data captured while the group guided the puck to the target, a refined probability value can be computed with a high degree of precision.Swarm sessionsTwo groups (A and B) of radiologists participated in two separate swarm sessions, split randomly based on the availability of each radiologist to participate on a given date. Each session diagnosed 50 cases. For each case, participants were first asked to view a DICOM image of a frontal chest radiograph using their own independent workstation with a DICOM viewer of their preference. Individual assessments of the probability of pneumonia within this image were made through an online questionnaire using the swarm platform. These individual assessments were not revealed to other participants. Individuals were not given a time limit for the completion of the online questionnaire, and never took more than 1 min to review each image and complete the questionnaire. Subsequently, the group worked together as a real-time swarm, converging on a probabilistic diagnosis as to the likelihood that the patient has pneumonia using the aforementioned magnets to move the puck. The radiologists had no direct communication during the swarm and were anonymous to one another. The diagnosis was arrived at through a two-step process in which the swarm first converged on a coarse range of probabilities and then converged on a refined value within the chosen range. The full process of deliberation for each case, as moderated by the real-time swarm artificial intelligence algorithm, generally took between 15 and 60 s. No swarm failed to reach an answer within 60 s. Each swarm session took 2 h to complete the entire test set.Statistical analysisProbabilities produced by CheXNet and CheXMax were converted to binary prediction using a discrimination threshold (p = 50.0% for CheXNet and p = 4.006% for CheXMax). Similarly, for the human assessments of chest radiographs prior to the swarm-based decision, probabilities of pneumonia were converted to binary prediction using a 50% threshold—any diagnoses >50% probability were labeled as “pneumonia predicted”. This was performed for individual radiologist diagnoses, the average of all radiologist diagnoses for a single image, as well as by a crowd-based majority vote. For the swarm session, results of the two separate sessions were analyzed separately as well as together. The final probability selected by the swarm was further refined through using underlying data generated during the convergence process. This was done using a weighted averaging process referred to as squared impulse interpolation or swarm interpolation. This process, as outlined in equations below, calculates a weighted average of the probabilities in the swarm using the squared net “pull” towards each answer as weights. The pull is represented as the force (F) imparted by members of the swarm and the weight for each answer wi is calculated as the squared impulse towards that answer (Eq. (1)). The weighted average over the answer choice values vi is then computed (Eq. (2)). The answer choice values vi are taken as the midpoint of each bin. For example, the bin “0–5%” has a midpoint vi of 2.5%.$$w_i = \frac{{F(i)^2}}{{\mathop {\sum}\nolimits_{a \in {\mathrm {Answers}}} F(a)^2}}$$
                    (1)
                $${\mathrm {Refined}}\,{\mathrm {probabilistic}}\,{\mathrm {diagnosis}}\,{\sum} {w_iv_i}$$
                    (2)
                This process can be visualized by plotting the net vector force of each radiologist over the course of the swarm, as shown in Fig. 8.Fig. 8Support density visualization. In this support density visualization corresponding to the swarm in Fig. 1, the puck’s trajectory is shown as a white dotted line, and the distribution of force over the hex is plotted as a Gaussian kernel density heatmap. Notice that this swarm was split between the “5–25%” and “0–5%” bins, and more force was directed towards the 5–25%. This aggregate behavior is reflected in the swarm’s interpolated diagnosis of 11.1%.Full size imageFinal diagnostic performance was compared between radiologists (average performance of individual radiologists, averaging individual diagnoses on an image within a group to calculate the group’s average probabilistic diagnosis, and taking a vote of individual radiologist diagnoses to label the image in a binary manner), the AI models, and diagnosis by swarm. Five different diagnostic performance metrics were used to make the comparisons: (a) percent correct, (b) mean absolute error, (c) Brier score, (d) AUC, and (e) F1 score.Reporting summaryFurther information on research design is available in the Nature Research Reporting Summary linked to this article.


Data availability
Data are available on request due to privacy or other restrictions. The data that support the findings of this study are available on request from the corresponding author (B.N.P.). The data are not publicly available due to privacy information embedded directly within the data.
Code availability
Restrictions apply regarding the release of source code used in this study.
Change history10 December 2019An amendment to this paper has been published and can be accessed via a link at the top of the paper.ReferencesDe Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat. Med. 24, 1342–1350 (2018).Article 
    
                    Google Scholar 
                Ehteshami Bejnordi, B. et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. JAMA 318, 2199–2210 (2017).Article 
    
                    Google Scholar 
                Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA 316, 2402–2410 (2016).Article 
    
                    Google Scholar 
                Rajpurkar, P. et al. Deep learning for chest radiograph diagnosis: a retrospective comparison of the CheXNeXt algorithm to practicing radiologists. PLoS Med. 15, e1002686 (2018).Article 
    
                    Google Scholar 
                Irvin, J. et al. CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison. In: Proc. AAAI Conference on Artificial Intelligence, North America (2019).Recht, M. & Bryan, R. N. Artificial intelligence: threat or boon to radiologists? J. Am. Coll. Radiol. 14, 1476–1480 (2017).Article 
    
                    Google Scholar 
                Schier, R. Artificial intelligence and the practice of radiology: an alternative view. J. Am. Coll. Radiol. 15, 1004–1007 (2018).Article 
    
                    Google Scholar 
                Obermeyer, Z. & Emanuel, E. J. Predicting the future—big data, machine learning, and clinical medicine. N. Engl. J. Med. 375, 1216–1219 (2016).Article 
    
                    Google Scholar 
                Kressel, H. Y. Setting sail: 2017. Radiology 282, 4–6 (2017).Article 
    
                    Google Scholar 
                Chartrand, G. et al. Deep learning: a primer for radiologists. Radiographics 37, 2113–2131 (2017).Article 
    
                    Google Scholar 
                Gianfrancesco, M. A., Tamang, S., Yazdany, J. & Schmajuk, G. Potential biases in machine learning algorithms using electronic health record data. JAMA Intern. Med. 178, 1544–1547 (2018).Article 
    
                    Google Scholar 
                Gehrmann, S. et al. Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives. PLoS ONE 13, e0192360 (2018).Article 
    
                    Google Scholar 
                Verghese, A., Shah, N. H. & Harrington, R. A. What this computer needs is a physician: humanism and artificial intelligence. JAMA 319, 19–20 (2018).Article 
    
                    Google Scholar 
                Liew, C. The future of radiology augmented with artificial intelligence: a strategy for success. Eur. J. Radiol. 102, 152–156 (2018).Article 
    
                    Google Scholar 
                Bien, N. et al. Deep-learning-assisted diagnosis for knee magnetic resonance imaging: development and retrospective validation of MRNet. PLoS Med. 15, e1002699 (2018).Article 
    
                    Google Scholar 
                Lakhani, P. & Sundaram, B. Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks. Radiology 284, 574–582 (2017).Article 
    
                    Google Scholar 
                Beni, G. From Swarm Intelligence to Swarm Robotics. 1–9 (Springer, Berlin, Heidelberg, 2005).Wang, J. & Beni, G. Pattern generation in cellular robotic systems. In: Proc. IEEE International Symposium on Intelligent Control, 63–69 (IEEE, 1988).Rosenberg, L. Artificial swarm intelligence, a human-in-the-loop approach to A.I. In: Proc. 13th AAAI Conference on Artificial Intelligence, 4381–4382 (AAAI Press, Phoenix, AZ, 2016).Marshall, J. A. et al. On optimal decision-making in brains and social insect colonies. J. R. Soc. Interface 6, 1065–1074 (2009).Article 
    
                    Google Scholar 
                Seeley, T. D. et al. Stop signals provide cross inhibition in collective decision-making by honeybee swarms. Science 335, 108–111 (2012).Article 
    CAS 
    
                    Google Scholar 
                Seeley, T. D. & Buhrman, S. C. Nest-site selection in honey bees: how well do swarms implement the “best-of-N” decision rule? Behav. Ecol. Sociobiol. 49, 416–427 (2001).Article 
    
                    Google Scholar 
                Rosenberg, L. Artificial Swarm Intelligence, a Human-in-the-Loop Approach to A.I. Thirtieth AAAI Conference on Artificial Intelligence. (2016).Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115–118 (2017).Article 
    CAS 
    
                    Google Scholar 
                Titano, J. J. et al. Automated deep-neural-network surveillance of cranial images for acute neurologic events. Nat. Med. 24, 1337–1341 (2018).Article 
    CAS 
    
                    Google Scholar 
                Galton, F. Vox populi (The wisdom of crowds). Nature 75, 450–451 (1907).Article 
    
                    Google Scholar 
                Lorge, I., Fox, D., Davitz, J. & Brenner, M. A survey of studies contrasting the quality of group performance and individual performance, 1920–1957. Psychol. Bull. 55, 337–372 (1958).Article 
    CAS 
    
                    Google Scholar 
                Lorenz, J., Rauhut, H., Schweitzer, F. & Helbing, D. How social influence can undermine the wisdom of crowd effect. Proc. Natl Acad. Sci. USA 108, 9020–9025 (2011).Article 
    CAS 
    
                    Google Scholar 
                Miner, T. The wisdom of crowds: why the many are smarter than the few, and how collective wisdom shapes business, economies, societies, and nations. J. Exp. Educ. 27, 351 (2005).
                    Google Scholar 
                Rauhut, H. & Lorenz, J. The wisdom of crowds in one mind: how individuals can simulate the knowledge of diverse societies to reach better decisions. J. Math. Psychol. 55, 191–197 (2011).Article 
    
                    Google Scholar 
                Sonabend, A. M. et al. Defining glioblastoma resectability through the wisdom of the crowd: a proof-of-principle study. Neurosurgery 80, 590–601 (2017).PubMed 
    PubMed Central 
    
                    Google Scholar 
                King, A. J., Gehl, R. W., Grossman, D. & Jensen, J. D. Skin self-examinations and visual identification of atypical nevi: comparing individual and crowdsourcing approaches. Cancer Epidemiol. 37, 979–984 (2013).Article 
    
                    Google Scholar 
                McKenna, M. T. et al. Strategies for improved interpretation of computer-aided detections for CT colonography utilizing distributed human intelligence. Med. Image Anal. 16, 1280–1292 (2012).Article 
    
                    Google Scholar 
                Lee, Y. J., Arida, J. A. & Donovan, H. S. The application of crowdsourcing approaches to cancer research: a systematic review. Cancer Med. 6, 2595–2605 (2017).Article 
    
                    Google Scholar 
                Moussaid, M., Kammer, J. E., Analytis, P. P. & Neth, H. Social influence and the collective dynamics of opinion formation. PLoS ONE 8, e78433 (2013).Article 
    CAS 
    
                    Google Scholar 
                Baddeley, M. Herding, social influence and economic decision-making: socio-psychological and neuroscientific analyses. Philos. Trans. R. Soc. Lond. B 365, 281–290 (2010).Article 
    
                    Google Scholar 
                Becker, J., Brackbill, D. & Centola, D. Network dynamics of social influence in the wisdom of crowds. Proc. Natl Acad. Sci. USA 114, E5070–E5076 (2017).Article 
    CAS 
    
                    Google Scholar 
                Rajpurkar, P. et al. CheXNet: radiologist-level pneumonia detection on chest X-rays with deep learning. Preprint at arXiv:1711.05225 (2017).Wang, X. et al. ChestX-ray8: Hospital-scale Chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. Proceedings of the IEEE conference on computer vision and pattern recognition. Preprint at arXiv:1705.02315 (2017).Hosny, A., Parmar, C., Quackenbush, J., Schwartz, L. H. & Aerts, H. Artificial intelligence in radiology. Nat. Rev. Cancer 18, 500–510 (2018).Article 
    CAS 
    
                    Google Scholar 
                Jha, S. & Topol, E. J. Adapting to artificial intelligence: radiologists and pathologists as information specialists. JAMA 316, 2353–2354 (2016).Article 
    
                    Google Scholar 
                Prina, E., Ranzani, O. T. & Torres, A. Community-acquired pneumonia. Lancet 386, 1097–1108 (2015).Article 
    
                    Google Scholar 
                Metlay, J. P., Kapoor, W. N. & Fine, M. J. Does this patient have community-acquired pneumonia? Diagnosing pneumonia by history and physical examination. JAMA 278, 1440–1445 (1997).Article 
    CAS 
    
                    Google Scholar 
                Yao, L. et al. Learning to diagnose from scratch by exploiting dependencies among labels. Preprint at arXiv:1710.10501 (2017).Rosenberg, L. & Pescetelli, N. Amplifying Prediction Accuracy Using Swarm A.I. In: Intelligent Systems Conference (IntelliSys). (IEEE, 2017).Rosenberg, L., Baltaxe, D. & Pescetelli, N. Crowds vs. swarms, a comparison of intelligence. In: Proc. 2016 Swarm/Human Blended Intelligence Workshop (SHBI), 1–4 (2016).Rosenberg, L. et al. Artificial swarm intelligence employed to amplify diagnostic accuracy in radiology. In: Proc. 2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON) 1186–1191 (2018).Download referencesAuthor informationAuthors and AffiliationsDepartment of Radiology, Stanford University School of Medicine, 300 Pasteur Dr., H1307, Stanford, CA, 94305, USABhavik N. Patel, Safwan Halabi, Curtis Langlotz, Edward Lo, A. J. Mariano, Geoffrey Riley, Jayne Seekins, Luyao Shen, Evan Zucker & Matthew P. LungrenUnanimous AI, 2443 Fillmore Street #116, San Francisco, CA, 94115-1814, USALouis Rosenberg, Gregg Willcox, David Baltaxe & Mimi LyonsDepartment of Computer Science, Stanford University School of Medicine, 353 Serra Mall (Gates Building), Stanford, CA, 94305, USAJeremy Irvin & Pranav RajpurkarDepartment of Radiology, Duke University Medical Center, Box 3808 Erwin Rd, Durham, NC, 27710, USATimothy Amrhein, Rajan Gupta & Joseph MammarappallilAuthorsBhavik N. PatelView author publicationsYou can also search for this author in
                        PubMed Google ScholarLouis RosenbergView author publicationsYou can also search for this author in
                        PubMed Google ScholarGregg WillcoxView author publicationsYou can also search for this author in
                        PubMed Google ScholarDavid BaltaxeView author publicationsYou can also search for this author in
                        PubMed Google ScholarMimi LyonsView author publicationsYou can also search for this author in
                        PubMed Google ScholarJeremy IrvinView author publicationsYou can also search for this author in
                        PubMed Google ScholarPranav RajpurkarView author publicationsYou can also search for this author in
                        PubMed Google ScholarTimothy AmrheinView author publicationsYou can also search for this author in
                        PubMed Google ScholarRajan GuptaView author publicationsYou can also search for this author in
                        PubMed Google ScholarSafwan HalabiView author publicationsYou can also search for this author in
                        PubMed Google ScholarCurtis LanglotzView author publicationsYou can also search for this author in
                        PubMed Google ScholarEdward LoView author publicationsYou can also search for this author in
                        PubMed Google ScholarJoseph MammarappallilView author publicationsYou can also search for this author in
                        PubMed Google ScholarA. J. MarianoView author publicationsYou can also search for this author in
                        PubMed Google ScholarGeoffrey RileyView author publicationsYou can also search for this author in
                        PubMed Google ScholarJayne SeekinsView author publicationsYou can also search for this author in
                        PubMed Google ScholarLuyao ShenView author publicationsYou can also search for this author in
                        PubMed Google ScholarEvan ZuckerView author publicationsYou can also search for this author in
                        PubMed Google ScholarMatthew P. LungrenView author publicationsYou can also search for this author in
                        PubMed Google ScholarContributionsAll authors contributed extensively to the work presented in this paper. B.N.P. and M.L. designed the experiments and wrote the manuscript while all other authors commented on the manuscript. L.R., G.W., D.B. and M.L. provided technical support. B.N.P., J.I., P.R., T.A., R.G., S.H., C.L., E.L., J.M., A.J.M., G.R., J.S., L.S., E.Z. and M.L. carried out experiments.Corresponding authorCorrespondence to
                Bhavik N. Patel.Ethics declarations
Competing interests
The authors had control of the data and the information submitted for publication. Four authors (L.R., D.B., G.W. and M.L.) are employees of Unanimous AI, who developed the swarm platform used in this study. All other authors are not employees of or consultants for Unanimous AI and had control of the study methodology, data analysis, and results. There was no industry support specifically for this study. This study was supported in part by NSF through Award ID 1840937.
Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary informationReporting Summary ChecklistRights and permissions
Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.
Reprints and permissionsAbout this articleCite this articlePatel, B.N., Rosenberg, L., Willcox, G. et al. Human–machine partnership with artificial intelligence for chest radiograph diagnosis.
                    npj Digit. Med. 2, 111 (2019). https://doi.org/10.1038/s41746-019-0189-7Download citationReceived: 25 February 2019Accepted: 23 October 2019Published: 18 November 2019DOI: https://doi.org/10.1038/s41746-019-0189-7Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        
Subjects

Computer scienceRadiography





This article is cited by





                                        Deep Learning for Chest X-ray Diagnosis: Competition Between Radiologists with or Without Artificial Intelligence Assistance
                                    


Lili GuoChangsheng ZhouGuangming Lu

Journal of Imaging Informatics in Medicine (2024)




                                        Human–AI collaboration enables more empathic conversations in text-based peer-to-peer mental health support
                                    


Ashish SharmaInna W. LinTim Althoff

Nature Machine Intelligence (2023)




                                        Evaluating efficiency and accuracy of deep-learning-based approaches on study selection for psychiatry systematic reviews
                                    


Aaron J. GorelikMark G. GorelikManpreet K. Singh

Nature Mental Health (2023)




                                        DEEP MOVEMENT: Deep learning of movie files for management of endovascular thrombectomy
                                    


Brendan KellyMesha MartinezEdward H. Lee

European Radiology (2023)




                                        “I’m afraid I can’t let you do that, Doctor”: meaningful disagreements with AI in medical contexts
                                    


Hendrik KemptJan-Christoph HeilingerSaskia K. Nagel

AI & SOCIETY (2023)






",WebPage,,,,,,,,,,,"{'headline': 'Human–machine partnership with artificial intelligence for chest radiograph diagnosis', 'description': 'Human-in-the-loop (HITL) AI may enable an ideal symbiosis of human experts and AI models, harnessing the advantages of both while at the same time overcoming their respective limitations. The purpose of this study was to investigate a novel collective intelligence technology designed to amplify the diagnostic accuracy of networked human groups by forming real-time systems modeled on biological swarms. Using small groups of radiologists, the swarm-based technology was applied to the diagnosis of pneumonia on chest radiographs and compared against human experts alone, as well as two state-of-the-art deep learning AI models. Our work demonstrates that both the swarm-based technology and deep-learning technology achieved superior diagnostic accuracy than the human experts alone. Our work further demonstrates that when used in combination, the swarm-based technology and deep-learning technology outperformed either method alone. The superior diagnostic accuracy of the combined HITL AI solution compared to radiologists and AI alone has broad implications for the surging clinical AI deployment and implementation strategies in future practice.', 'datePublished': '2019-11-18T00:00:00Z', 'dateModified': '2019-12-10T00:00:00Z', 'pageStart': '1', 'pageEnd': '10', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'sameAs': 'https://doi.org/10.1038/s41746-019-0189-7', 'keywords': ['Computer science', 'Radiography', 'Medicine/Public Health', 'general', 'Biomedicine', 'Biotechnology'], 'image': ['https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig1_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig2_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig3_HTML.jpg', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig4_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig5_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig6_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig7_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41746-019-0189-7/MediaObjects/41746_2019_189_Fig8_HTML.png'], 'isPartOf': {'name': 'npj Digital Medicine', 'issn': ['2398-6352'], 'volumeNumber': '2', '@type': ['Periodical', 'PublicationVolume']}, 'publisher': {'name': 'Nature Publishing Group UK', 'logo': {'url': 'https://www.springernature.com/app-sn/public/images/logo-springernature.png', '@type': 'ImageObject'}, '@type': 'Organization'}, 'author': [{'name': 'Bhavik N. Patel', 'url': 'http://orcid.org/0000-0001-5157-9903', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], 'email': 'bhavikp@stanford.edu', '@type': 'Person'}, {'name': 'Louis Rosenberg', 'affiliation': [{'name': 'Unanimous AI', 'address': {'name': 'Unanimous AI, San Francisco, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Gregg Willcox', 'affiliation': [{'name': 'Unanimous AI', 'address': {'name': 'Unanimous AI, San Francisco, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'David Baltaxe', 'affiliation': [{'name': 'Unanimous AI', 'address': {'name': 'Unanimous AI, San Francisco, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Mimi Lyons', 'affiliation': [{'name': 'Unanimous AI', 'address': {'name': 'Unanimous AI, San Francisco, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Jeremy Irvin', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Computer Science, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Pranav Rajpurkar', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Computer Science, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Timothy Amrhein', 'url': 'http://orcid.org/0000-0002-9354-9486', 'affiliation': [{'name': 'Duke University Medical Center', 'address': {'name': 'Department of Radiology, Duke University Medical Center, Durham, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Rajan Gupta', 'affiliation': [{'name': 'Duke University Medical Center', 'address': {'name': 'Department of Radiology, Duke University Medical Center, Durham, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Safwan Halabi', 'url': 'http://orcid.org/0000-0003-1317-984X', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Curtis Langlotz', 'url': 'http://orcid.org/0000-0002-8972-8051', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Edward Lo', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Joseph Mammarappallil', 'affiliation': [{'name': 'Duke University Medical Center', 'address': {'name': 'Department of Radiology, Duke University Medical Center, Durham, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'A. J. Mariano', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Geoffrey Riley', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Jayne Seekins', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Luyao Shen', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Evan Zucker', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Matthew P. Lungren', 'affiliation': [{'name': 'Stanford University School of Medicine', 'address': {'name': 'Department of Radiology, Stanford University School of Medicine, Stanford, USA', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}], 'isAccessibleForFree': True, '@type': 'ScholarlyArticle'}",,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiOGh0dHBzOi8vd3d3LnViZXIuY29tL2VuLUFVL2Jsb2cvYWktbWVldC10aGUtdGVhbS16aGFsZWgv0gEA?oc=5,Artificial Intelligence - Meet the Team - Uber,2019-11-20,Uber,https://www.uber.com,Uber’s AI team work on some amazing features within the app! Zhaleh’s team have been working on the hands-free voice assistant for drivers.,N/A,Uber’s AI team work on some amazing features within the app! Zhaleh’s team have been working on the hands-free voice assistant for drivers.,Uber’s AI team work on some amazing features within the app! Zhaleh’s team have been working on the hands-free voice assistant for drivers.,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiTGh0dHBzOi8vYmR0ZWNodGFsa3MuY29tLzIwMTkvMTEvMTgvd2hhdC1pcy1zeW1ib2xpYy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS_SAVBodHRwczovL2JkdGVjaHRhbGtzLmNvbS8yMDE5LzExLzE4L3doYXQtaXMtc3ltYm9saWMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UvYW1wLw?oc=5,What is symbolic artificial intelligence? - TechTalks,2019-11-18,TechTalks,https://bdtechtalks.com,"Everything you need to know about symbolic artificial intelligence, the branch of AI that dominated for five decades.",N/A,"Everything you need to know about symbolic artificial intelligence, the branch of AI that dominated for five decades.","Everything you need to know about symbolic artificial intelligence, the branch of AI that dominated for five decades.",http://schema.org,"[{'@type': 'BlogPosting', '@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#blogposting', 'name': 'What is symbolic artificial intelligence? - TechTalks', 'headline': 'What is symbolic artificial intelligence?', 'author': {'@id': 'https://bdtechtalks.com/author/bendee983/#author'}, 'publisher': {'@id': 'https://bdtechtalks.com/#organization'}, 'image': {'@type': 'ImageObject', 'url': 'https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2019/11/human-brain-gears.jpg?fit=4300%2C2580&ssl=1', 'width': 4300, 'height': 2580, 'caption': 'Image credit: Depositphotos'}, 'datePublished': '2019-11-18T14:00:25+00:00', 'dateModified': '2019-11-17T17:29:21+00:00', 'inLanguage': 'en-US', 'commentCount': 3, 'mainEntityOfPage': {'@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#webpage'}, 'isPartOf': {'@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#webpage'}, 'articleSection': 'What is..., Artificial intelligence (AI), Demystifying AI, symbolic artificial intelligence, bendee983'}, {'@type': 'BreadcrumbList', '@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#breadcrumblist', 'itemListElement': [{'@type': 'ListItem', '@id': 'https://bdtechtalks.com/#listItem', 'position': 1, 'name': 'Home', 'item': 'https://bdtechtalks.com/', 'nextItem': 'https://bdtechtalks.com/2019/#listItem'}, {'@type': 'ListItem', '@id': 'https://bdtechtalks.com/2019/#listItem', 'position': 2, 'name': '2019', 'item': 'https://bdtechtalks.com/2019/', 'nextItem': 'https://bdtechtalks.com/2019/11/#listItem', 'previousItem': 'https://bdtechtalks.com/#listItem'}, {'@type': 'ListItem', '@id': 'https://bdtechtalks.com/2019/11/#listItem', 'position': 3, 'name': 'November', 'item': 'https://bdtechtalks.com/2019/11/', 'nextItem': 'https://bdtechtalks.com/2019/11/18/#listItem', 'previousItem': 'https://bdtechtalks.com/2019/#listItem'}, {'@type': 'ListItem', '@id': 'https://bdtechtalks.com/2019/11/18/#listItem', 'position': 4, 'name': '18', 'item': 'https://bdtechtalks.com/2019/11/18/', 'nextItem': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#listItem', 'previousItem': 'https://bdtechtalks.com/2019/11/#listItem'}, {'@type': 'ListItem', '@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#listItem', 'position': 5, 'name': 'What is symbolic artificial intelligence?', 'previousItem': 'https://bdtechtalks.com/2019/11/18/#listItem'}]}, {'@type': 'Organization', '@id': 'https://bdtechtalks.com/#organization', 'name': 'TechTalks', 'description': 'Technology solving problems... and creating new ones', 'url': 'https://bdtechtalks.com/'}, {'@type': 'Person', '@id': 'https://bdtechtalks.com/author/bendee983/#author', 'url': 'https://bdtechtalks.com/author/bendee983/', 'name': 'Ben Dickson', 'image': {'@type': 'ImageObject', '@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#authorImage', 'url': 'https://secure.gravatar.com/avatar/5184782561a26df20cb56c8eb87eef27?s=96&d=identicon&r=g', 'width': 96, 'height': 96, 'caption': 'Ben Dickson'}}, {'@type': 'WebPage', '@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#webpage', 'url': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/', 'name': 'What is symbolic artificial intelligence? - TechTalks', 'description': 'Everything you need to know about symbolic artificial intelligence, the branch of AI that dominated for five decades.', 'inLanguage': 'en-US', 'isPartOf': {'@id': 'https://bdtechtalks.com/#website'}, 'breadcrumb': {'@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#breadcrumblist'}, 'author': {'@id': 'https://bdtechtalks.com/author/bendee983/#author'}, 'creator': {'@id': 'https://bdtechtalks.com/author/bendee983/#author'}, 'image': {'@type': 'ImageObject', 'url': 'https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2019/11/human-brain-gears.jpg?fit=4300%2C2580&ssl=1', '@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#mainImage', 'width': 4300, 'height': 2580, 'caption': 'Image credit: Depositphotos'}, 'primaryImageOfPage': {'@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/#mainImage'}, 'datePublished': '2019-11-18T14:00:25+00:00', 'dateModified': '2019-11-17T17:29:21+00:00'}, {'@type': 'WebSite', '@id': 'https://bdtechtalks.com/#website', 'url': 'https://bdtechtalks.com/', 'name': 'TechTalks', 'description': 'Technology solving problems... and creating new ones', 'inLanguage': 'en-US', 'publisher': {'@id': 'https://bdtechtalks.com/#organization'}}]",N/A,N/A,"

What is...

What is symbolic artificial intelligence?

By Ben Dickson -   November 18, 2019 




Facebook


Twitter


ReddIt


Linkedin




Image credit: Depositphotos
This article is part of Demystifying AI, a series of posts that (try to) disambiguate the jargon and myths surrounding AI.
Today, artificial intelligence is mostly about artificial neural networks and deep learning. But this is not how it always was. In fact, for most of its six-decade history, the field was dominated by symbolic artificial intelligence, also known as “classical AI,” “rule-based AI,” and “good old-fashioned AI.”
Symbolic AI involves the explicit embedding of human knowledge and behavior rules into computer programs. The practice showed a lot of promise in the early decades of AI research. But in recent years, as neural networks, also known as connectionist AI, gained traction, symbolic AI has fallen by the wayside.
The role of symbols in artificial intelligence
Symbols are things we use to represent other things. Symbols play a vital role in the human thought and reasoning process. If I tell you that I saw a cat up in a tree, your mind will quickly conjure an image.
We use symbols all the time to define things (cat, car, airplane, etc.) and people (teacher, police, salesperson). Symbols can represent abstract concepts (bank transaction) or things that don’t physically exist (web page, blog post, etc.). They can also describe actions (running) or states (inactive). Symbols can be organized into hierarchies (a car is made of doors, windows, tires, seats, etc.). They can also be used to describe other symbols (a cat with fluffy ears, a red carpet, etc.).
Being able to communicate in symbols is one of the main things that make us intelligent. Therefore, symbols have also played a crucial role in the creation of artificial intelligence.
The early pioneers of AI believed that “every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.” Therefore, symbolic AI took center stage and became the focus of research projects. Scientists developed tools to define and manipulate symbols.
Many of the concepts and tools you find in computer science are the results of these efforts. Symbolic AI programs are based on creating explicit structures and behavior rules.
An example of symbolic AI tools is object-oriented programming. OOP languages allow you to define classes, specify their properties, and organize them in hierarchies. You can create instances of these classes (called objects) and manipulate their properties. Class instances can also perform actions, also known as functions, methods, or procedures. Each method executes a series of rule-based instructions that might read and change the properties of the current and other objects.
Using OOP, you can create extensive and complex symbolic AI programs that perform various tasks.
The benefits and limits of symbolic AI
Symbolic artificial intelligence showed early progress at the dawn of AI and computing. You can easily visualize the logic of rule-based programs, communicate them, and troubleshoot them.
Flowcharts can depict the logic of symbolic AI programs very clearly
Symbolic artificial intelligence is very convenient for settings where the rules are very clear cut,  and you can easily obtain input and transform it into symbols. In fact, rule-based systems still account for most computer programs today, including those used to create deep learning applications.
But symbolic AI starts to break when you must deal with the messiness of the world. For instance, consider computer vision, the science of enabling computers to make sense of the content of images and video. Say you have a picture of your cat and want to create a program that can detect images that contain your cat. You create a rule-based program that takes new images as inputs, compares the pixels to the original cat image, and responds by saying whether your cat is in those images.
This will only work as you provide an exact copy of the original image to your program. A slightly different picture of your cat will yield a negative answer. For instance, if you take a picture of your cat from a somewhat different angle, the program will fail.
One solution is to take pictures of your cat from different angles and create new rules for your application to compare each input against all those images. Even if you take a million pictures of your cat, you still won’t account for every possible case. A change in the lighting conditions or the background of the image will change the pixel value and cause the program to fail. You’ll need millions of other pictures and rules for those.
And what if you wanted to create a program that could detect any cat? How many rules would you need to create for that?
The cat example might sound silly, but these are the kinds of problems that symbolic AI programs have always struggled with. You can’t define rules for the messy data that exists in the real world. For instance, how can you define the rules for a self-driving car to detect all the different pedestrians it might face?
Also, some tasks can’t be translated to direct rules, including speech recognition and natural language processing.
There have been several efforts to create complicated symbolic AI systems that encompass the multitudes of rules of certain domains. Called expert systems, these symbolic AI models use hardcoded knowledge and rules to tackle complicated tasks such as medical diagnosis. But they require a huge amount of effort by domain experts and software engineers and only work in very narrow use cases. As soon as you generalize the problem, there will be an explosion of new rules to add (remember the cat detection problem?), which will require more human labor. As some AI scientists point out, symbolic AI systems don’t scale.
Neural networks vs symbolic AI

Neural networks are almost as old as symbolic AI, but they were largely dismissed because they were inefficient and required compute resources that weren’t available at the time. In the past decade, thanks to the large availability of data and processing power, deep learning has gained popularity and has pushed past symbolic AI systems.
The advantage of neural networks is that they can deal with messy and unstructured data. Take the cat detector example. Instead of manually laboring through the rules of detecting cat pixels, you can train a deep learning algorithm on many pictures of cats. The neural network then develops a statistical model for cat images. When you provide it with a new image, it will return the probability that it contains a cat.
Deep learning and neural networks excel at exactly the tasks that symbolic AI struggles with. They have created a revolution in computer vision applications such as facial recognition and cancer detection. Deep learning has also driven advances in language-related tasks.
Deep neural networks are also very suitable for reinforcement learning, AI models that develop their behavior through numerous trial and error. This is the kind of AI that masters complicated games such as Go, StarCraft, and Dota.
But the benefits of deep learning and neural networks are not without tradeoffs. Deep learning has several deep challenges and disadvantages in comparison to symbolic AI. Notably, deep learning algorithms are opaque, and figuring out how they work perplexes even their creators. And it’s very hard to communicate and troubleshoot their inner-workings.
Neural networks are also very data-hungry. And unlike symbolic AI, neural networks have no notion of symbols and hierarchical representation of knowledge. This limitation makes it very hard to apply neural networks to tasks that require logic and reasoning, such as science and high-school math.
The current state of symbolic AI
Some believe that symbolic AI is dead. But this assumption couldn’t be farther from the truth. In fact, rule-based AI systems are still very important in today’s applications. Many leading scientists believe that symbolic reasoning will continue to remain a very important component of artificial intelligence.
There are now several efforts to combine neural networks and symbolic AI. One such project is the Neuro-Symbolic Concept Learner (NSCL), a hybrid AI system developed by the MIT-IBM Watson AI Lab. NSCL uses both rule-based programs and neural networks to solve visual question-answering problems. As opposed to pure neural network–based models, the hybrid AI can learn new tasks with less data and is explainable. And unlike symbolic-only models, NSCL doesn’t struggle to analyze the content of images.
Maybe in the future, we’ll invent AI technologies that can both reason and learn. But for the moment, symbolic AI is the leading method to deal with problems that require logical thinking and knowledge representation.
Like this:Like Loading... 


TAGSArtificial intelligence (AI)Demystifying AIsymbolic artificial intelligence 


Facebook


Twitter


ReddIt


Linkedin


 Previous articleHow does Google Stadia compare to hardware gaming?Next articleSocial artificial intelligence: intuitive or intrusive? Ben DicksonBen is a software engineer and the founder of TechTalks. He writes about technology, business and politics.




  
",BreadcrumbList,,,,,,,,,,,,"[{'@type': 'ListItem', 'position': 1, 'item': {'@type': 'WebSite', '@id': 'https://bdtechtalks.com/', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@type': 'WebPage', '@id': 'https://bdtechtalks.com/category/what-is/', 'name': 'What is...'}}, {'@type': 'ListItem', 'position': 3, 'item': {'@type': 'WebPage', '@id': 'https://bdtechtalks.com/2019/11/18/what-is-symbolic-artificial-intelligence/', 'name': 'What is symbolic artificial intelligence?'}}]",,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVGh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMTkvMTEvMTkvdGVjaG5vbG9neS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1kYXduLXNvbmcuaHRtbNIBAA?oc=5,Building a World Where Data Privacy Exists Online (Published 2019) - The New York Times,2019-11-19,The New York Times,https://www.nytimes.com,"Dawn Song, an expert in computer security and trustworthy artificial intelligence, is working on making that vision a reality.",N/A,"Dawn Song, an expert in computer security and trustworthy artificial intelligence, is working on making that vision a reality.","Dawn Song, an expert in computer security and trustworthy artificial intelligence, is working on making that vision a reality.",https://schema.org,,Technology,N/A,"Artificial IntelligenceMicrosoft’s Risk-TakerFine Print ChangesQuiz: Fake or Real Images?Apple Enters A.I. FrayMeta’s A.I. ScrapingAdvertisementSKIP ADVERTISEMENTSupported bySKIP ADVERTISEMENTBuilding a World Where Data Privacy Exists OnlineDawn Song, an expert in computer security and trustworthy artificial intelligence, is working on making that vision a reality.Share full articleRead in appDawn Song, a professor at the University of California, Berkeley, is an expert in computer security and trustworthy artificial intelligence.Credit...Jason Henry for The New York TimesBy Craig S. SmithPublished Nov. 10, 2019Updated Nov. 19, 2019This article is part of our Women and Leadership special section, which focuses on approaches taken by women, minorities or other disadvantaged groups challenging traditional ways of thinking.Data is valuable — something that companies like Facebook, Google and Amazon realized far earlier than most consumers did. But computer scientists have been working on alternative models, even as the public has grown weary of having their data used and abused.Dawn Song, a professor at the University of California, Berkeley, and one of the world’s foremost experts in computer security and trustworthy artificial intelligence, envisions a new paradigm in which people control their data and are compensated for its use by corporations. While there have been many proposals for such a system, Professor Song is one actually building the platform to make it a reality.“As we talk about data as the new oil, it’s particularly important to develop technologies that can utilize data in a privacy-preserving way,” Professor Song said recently from her San Francisco office with an expansive view of the bay.AdvertisementSKIP ADVERTISEMENTIt is an unlikely trajectory for Professor Song, who grew up in Dalian, China, a seaport in the northeastern province of Liaoning. She is the daughter of two local civil servants with no background in computers. And while she was an exceptional student in high school, she dreamed of being a National Geographic-style nature photographer. One of her teachers, a mentor, gently dissuaded her.Her mother wanted her to study business and filled out an application on her behalf for a well-known business school. Then, shortly before the national college entrance exams, her mentor intervened again, convincing her mother that a brighter future lay ahead for her daughter in science. Professor Song applied instead to Tsinghua University, China’s top science university, to study physics. She went on to study physics at Cornell University but transferred to Carnegie Mellon University, where she received an M.S. in computer science before settling at Berkeley to finally finish her Ph.D. in computer science. By then, she was focused on computer security.Professor Song drew attention while still a graduate student at Berkeley with pioneering work that showed a machine-learning algorithm can infer what someone is typing from the timing of their keystrokes picked up by eavesdropping on a network. Since then, she has been at the forefront of trustworthy A.I., including improving the resilience of machine-learning models themselves, the recursive blocks of computer code that learn to recognize patterns in the data they consume.ImageProfessor Song with examples of stop signs that, with a few stickers attached, were able to fool computer-vision systems.Credit...Jason Henry for The New York TimesMachine-learning models, as amazing as they are at identifying everything from tumors in X-ray images to words in slurred speech, remain disturbingly easy to fool. Professor Song and her students were the first ones to demonstrate that computer-vision systems could be fooled into identifying a stop sign as a 40-miles-per-hour speed limit sign simply by applying a few innocuous stickers to the sign. Examples of these altered traffic signs have been on exhibit at London’s Science Museum.AdvertisementSKIP ADVERTISEMENT“Her work on the stop sign was among the first to craft adversarial examples in the physical domain rather than just manipulating image pixels on a computer,” said Battista Biggio, an assistant professor at Italy’s University of Cagliari and one of the first people to study the vulnerabilities of such systems.Professor Song, who has taught at Berkeley for a dozen years, has been working to develop techniques and systems that not only can provide security to computer systems, but also privacy. She envisions a world of secure networks where individuals control their personal data and even derive income from it. She compares the world today to a time in human history when people did not have a clear notion of property rights. Once those rights were institutionalized and protected, she notes, it helped revolutionize economies.She recently started a company, Oasis Labs, that is building a platform that can give people the ability to control their data and audit how it is used. She believes that once data is viewed as property, it can propel the global economy in ways unseen before. “New business models can be built on this,” she said.Data, of course, is not like a physical object. If a person gives a friend an apple, then someone else cannot have that apple. But data is different, with a property that scientists call nonrivalry. People can give (or sell) as many copies as they want.AdvertisementSKIP ADVERTISEMENTMost people give away their data, signing it over to companies by clicking “accept,” not even bothering to read the fine print. Either people online accept the terms and participate in the digital world or they unplug — something that is not really an option for anyone operating in the global economy. Fortunes were built on that data, enriching a handful of entrepreneurs.“Our data has never been more at risk, and our need for new kinds of robust privacy solutions has never been greater,” said Guy Zyskind, co-founder and chief executive of Enigma, another company building a decentralized private computation protocol.When people go online, data is collected and stored on centralized servers that are vulnerable to attack. But Professor Song and her colleagues believe that by marrying specialized computer chips and blockchain technology, they can build a system that provides greater scalability and privacy protection.ImageProfessor Song with Bennet Yee, left, and Rebekah Kim, right, both software engineers at Oasis Labs in San Francisco.Credit...Jason Henry for The New York TimesSome computer chips — those in most cellphones, for example — already incorporate a secure zone, called a trusted execution environment, that protects software from most kinds of attack. Professor Song’s group is working on enhancing the security of those zones by building an open-source secure enclave, Keystone. Within the secure enclave, bits of computer code, called smart contracts, allow data owners to control who has access to their data and how it is used.AdvertisementSKIP ADVERTISEMENT“You can actually have the integrity that the blockchain ledger provides and also you can have privacy or confidentiality for the smart contract execution that’s provided by the secure enclave,” said Professor Song, who speaks rapidly as if rushing to keep pace with her thoughts. “No central server ever sees the data.”Oasis Labs has been building a platform to support enterprises and developers. They have begun a pilot with Nebula Genomics, a direct-to-consumer gene-sequencing company, that offers whole genome sequencing reports on ancestry, wellness, and genetic traits with weekly updates. Using Oasis Labs’ privacy-preserving tools, Nebula customers will retain full control and ownership over their genomic data, while enabling Nebula to run specific analysis on the data without exposing the underlying information.Another application, called  Kara, a collaboration with Dr. Robert Chang at the Stanford University School of Medicine, gives eye patients the option to share retina scans and other medical data with researchers who use the data to train machine-learning models to recognize disease. Part of the Kara project is studying what kind of incentives patients will find meaningful in return for contributing their data for medical research.“Her approach is unique from other data aggregators,” Dr. Chang said. “This project is really asking the important question — who really owns the data?”AdvertisementSKIP ADVERTISEMENTSomeday, Professor Song believes, people will have an individual revenue stream from their data. It may not be significant on a monthly or even annual basis, but the fees that accumulate over the course of a lifetime from companies using personal data could contribute to retirement savings, for example. Or revenue from groups of people could be used to fund particular causes. The unlocking of data, meanwhile, could lead to improved services for consumers.“Today, companies are taking users’ data and essentially using it as a product; they monetize it,” Professor Song said. “The world can be very different if this is turned around and users maintain control of the data and get revenue from it.”Craig S. Smith is a former correspondent for The Times and now hosts the podcast Eye on A.I.A version of this article appears in print on Nov. 18, 2019 in The New York Times International Edition. Order Reprints | Today’s Paper | SubscribeShare full articleRead in appAdvertisementSKIP ADVERTISEMENTTell us about yourself. Take the survey.",NewsMediaOrganization,Building a World Where Data Privacy Exists Online,https://www.nytimes.com/2019/11/19/technology/artificial-intelligence-dawn-song.html,"[{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2019/11/20/multimedia/20sp-women-song-1/20sp-women-song-1-videoSixteenByNineJumbo1600.jpg', 'height': 900, 'width': 1600, 'contentUrl': 'https://static01.nyt.com/images/2019/11/20/multimedia/20sp-women-song-1/20sp-women-song-1-videoSixteenByNineJumbo1600.jpg', 'caption': 'Dawn Song, a professor at the University of California, Berkeley, is an expert in computer security and trustworthy artificial intelligence.', 'creditText': 'Jason Henry for The New York Times'}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2019/11/20/multimedia/20sp-women-song-1/merlin_163691466_e9950b00-e8da-4d87-a704-514a4c31dfd1-superJumbo.jpg', 'height': 1638, 'width': 2048, 'contentUrl': 'https://static01.nyt.com/images/2019/11/20/multimedia/20sp-women-song-1/merlin_163691466_e9950b00-e8da-4d87-a704-514a4c31dfd1-superJumbo.jpg', 'caption': 'Dawn Song, a professor at the University of California, Berkeley, is an expert in computer security and trustworthy artificial intelligence.', 'creditText': 'Jason Henry for The New York Times'}]",2019-11-10T13:00:14.000Z,2019-11-19T14:50:07.000Z,"[{'@context': 'https://schema.org', '@type': 'Person', 'url': 'https://www.nytimes.com/by/craig-s-smith', 'name': 'Craig S. Smith'}]","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",https://www.nytimes.com/,"{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'height': 291, 'width': 291, 'contentUrl': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'creditText': 'The New York Times'}",https://en.wikipedia.org/wiki/The_New_York_Times,,,en-US,Turning the tables,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.meteredContent'}","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",2024.0,False,"{'@type': ['CreativeWork', 'Product'], 'name': 'The New York Times', 'productID': 'nytimes.com:basic'}",The New York Times,https://www.nytimes.com/#publisher,https://www.nytco.com/company/diversity-and-inclusion/,https://www.nytco.com/company/standards-ethics/,https://www.nytimes.com/interactive/2023/01/28/admin/the-new-york-times-masthead.html,1851-09-18,,,,,
https://news.google.com/rss/articles/CBMigwFodHRwczovL3d3dy5hdW50bWlubmllLmNvbS9pbWFnaW5nLWluZm9ybWF0aWNzL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2FydGljbGUvMTU2MjQ2NzAvbXVsdGlwbGUtYWktYWxnb3JpdGhtcy13b3JrLWJldHRlci10b2dldGhlctIBAA?oc=5,Multiple AI algorithms work better together - AuntMinnie,2019-11-20,AuntMinnie,https://www.auntminnie.com,"Employing multiple artificial intelligence (AI) algorithms to attack a task together results in better performance compared with using a single AI algorithm on its own, according to a study published online November 20 in Radiology: Artificial Intelligence.",N/A,"Employing multiple artificial intelligence (AI) algorithms to attack a task together results in better performance compared with using a single AI algorithm on its own, according to a study published online November 20 in Radiology: Artificial Intelligence.","Employing multiple artificial intelligence (AI) algorithms to attack a task together results in better performance compared with using a single AI algorithm on its own, according to a study published online November 20 in Radiology: Artificial Intelligence.",https://schema.org,,N/A,N/A,"Imaging InformaticsArtificial IntelligenceMultiple AI algorithms work better togetherBrian CaseyNov 20, 2019Facebook IconLinkedIn IconTwitter X iconPinterest Icon
Employing multiple artificial intelligence (AI) algorithms to attack a task together results in better performance compared with using a single AI algorithm on its own, according to a study published online November 20 in Radiology: Artificial Intelligence.
Called ensemble learning, the technique is designed to combine the best aspects of multiple machine learning algorithms into a single model to perform a task, such as identifying suspicious lung lesions or estimating bone age based on skeletal x-rays. Ensemble learning works best when each of the models performs well in their own right and correlation between the predictions of each model is relatively low.
To test ensemble learning, a research team led by medical student Ian Pan of the Warren Alpert Medical School at Brown University in Providence, RI, started with 48 algorithms that were submitted as part of the RSNA's pediatric bone age challenge competition at RSNA 2017.
They then used combinations of up to 10 algorithms at a time that were trained on a dataset of over 12,000 pediatric hand x-rays in which radiologists had determined true bone age. Then, the ensemble combinations were used to analyze 200 x-rays, with the combined model producing a mean absolute deviation (MAD) of bone age, with higher deviation indicating less accuracy.
The group found that a single algorithm had a mean absolution deviation of 4.55 months, while the best-performing ensemble -- consisting of four algorithms -- had a MAD of 3.79 months.
Pan et al noted that the findings could be important as artificial intelligence begins to transition from research environments to clinical settings. Clinicians might benefit from using different AI algorithms on the same task, much like having multiple radiologists interpret an image.
The researchers also believe that competitions like the 2017 bone age challenge are important because they provide a standardized use case, a common training set, and an objective assessment method that can be applied equally to all models.
""Machine learning competitions within radiology should be encouraged to spur development of heterogeneous models whose predictions can be combined to achieve optimal performance,"" Pan noted.Facebook IconLinkedIn IconTwitter X iconPinterest Icon Stay in the Know Delivered right to your inbox, Aunt Minnie's newsletters. Subscribe to get exclusive access! Email Address *I have read and agree to the privacy policy & terms of service and wish to opt-in. Sign Up  Comments  Post a Comment  You must be signed in to leave a comment. To sign in or create an account, enter your email address and we'll send you a one-click sign-in link.  Email Address * Continue  This article hasn’t received any comments yet. Want to start the conversation? View All CommentsLatest in Artificial IntelligenceBreast Cancer Canada launches research funding fellowshipJuly 15, 2024AI in radiology: The kids are alrightJuly 15, 2024Experts outline how best to deploy AI in radiologyJuly 11, 2024AI improves evaluations of knee osteoarthritis on x-raysJuly 9, 2024Related StoriesDigital X-RayThai researchers bolster forensic radiology studies with AIDigital X-RayAI aids assessment of skeletal age on radiographsArtificial IntelligencePractical Considerations of AI: Part 6 -- Ready, fire, aimDigital X-RayDeep-learning algorithm bolsters lung cancer detection",NewsArticle,Multiple AI algorithms work better together,"{'@type': 'WebPage', '@id': 'https://www.auntminnie.com/imaging-informatics/artificial-intelligence/article/15624670/multiple-ai-algorithms-work-better-together'}",['https://img.auntminnie.com/files/base/smg/all/image/2019/11/am.2017_03_30_10_29_47_941_computer_data_400.png?auto=format%2Ccompress&fit=max&q=70&w=1200'],2019-11-21T00:00:00.000Z,2019-11-20T17:31:00.000Z,"{'@type': 'Person', 'name': 'Brian Casey'}",,https://www.auntminnie.com/imaging-informatics/artificial-intelligence/article/15624670/multiple-ai-algorithms-work-better-together,,,,,,,,,,,,,Multiple AI algorithms work better together,,,,,,https://img.auntminnie.com/files/base/smg/all/image/2019/11/am.2017_03_30_10_29_47_941_computer_data_400.png?auto=format%2Ccompress&fit=max&q=70&w=1200,,,,
https://news.google.com/rss/articles/CBMicmh0dHBzOi8vd3d3LmdlZWt3aXJlLmNvbS8yMDE5L3N0dWR5LXN1Z2dlc3RzLWFpcy1kaXNydXB0aXZlLWVmZmVjdC1qb2JzLXdpbGwtaGl0LWhpZ2hlci1lY29ub21pYy1lZHVjYXRpb24tc2NhbGVzL9IBAA?oc=5,Study suggests AI's disruptive effect on jobs will hit higher on economic and education scales - GeekWire,2019-11-20,GeekWire,https://www.geekwire.com,"When experts talk about the disruptive effects of artificial intelligence, they tend to focus on low-paid laborers — but a newly published study suggests",N/A,"When experts talk about the disruptive effects of artificial intelligence, they tend to focus on low-paid laborers — but a newly published study suggests",N/A,https://schema.org,"[{'@type': 'NewsArticle', '@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#article', 'isPartOf': {'@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/'}, 'author': [{'@id': 'https://www.geekwire.com/#/schema/person/9641ab4b8b61dd1b5a6d0ebc6236d5ab'}], 'headline': 'Study suggests AI&#8217;s disruptive effect on jobs will hit higher on economic and education scales', 'datePublished': '2019-11-20T22:56:12+00:00', 'dateModified': '2019-11-20T23:32:45+00:00', 'mainEntityOfPage': {'@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/'}, 'wordCount': 844, 'commentCount': 0, 'publisher': {'@id': 'https://www.geekwire.com/#organization'}, 'image': {'@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#primaryimage'}, 'thumbnailUrl': 'https://cdn.geekwire.com/wp-content/uploads/2019/11/191120-jobs3.jpg', 'keywords': ['AI', 'artificial intelligence', 'Employment'], 'articleSection': ['Bot or Not', 'Science'], 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'CommentAction', 'name': 'Comment', 'target': ['https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#respond']}]}, {'@type': 'WebPage', '@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/', 'url': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/', 'name': ""Study suggests AI's disruptive effect on jobs will hit higher on economic and education scales &#8211; GeekWire"", 'isPartOf': {'@id': 'https://www.geekwire.com/#website'}, 'primaryImageOfPage': {'@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#primaryimage'}, 'image': {'@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#primaryimage'}, 'thumbnailUrl': 'https://cdn.geekwire.com/wp-content/uploads/2019/11/191120-jobs3.jpg', 'datePublished': '2019-11-20T22:56:12+00:00', 'dateModified': '2019-11-20T23:32:45+00:00', 'description': 'When experts talk about the disruptive effects of artificial intelligence, they tend to focus on low-paid laborers — but a newly published study suggests', 'breadcrumb': {'@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#primaryimage', 'url': 'https://cdn.geekwire.com/wp-content/uploads/2019/11/191120-jobs3.jpg', 'contentUrl': 'https://cdn.geekwire.com/wp-content/uploads/2019/11/191120-jobs3.jpg', 'width': 1113, 'height': 677, 'caption': 'A map by the Brookings Institution uses shades of pink and red to indicate which cities are expected to be hard-hit by job disruption related to AI. (Brookings Institution Graphic / Source: Brookings Analysis of Webb, 2019)'}, {'@type': 'BreadcrumbList', '@id': 'https://www.geekwire.com/2019/study-suggests-ais-disruptive-effect-jobs-will-hit-higher-economic-education-scales/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.geekwire.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Bot or Not', 'item': 'https://www.geekwire.com/bot-or-not/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Study suggests AI&#8217;s disruptive effect on jobs will hit higher on economic and education scales'}]}, {'@type': 'WebSite', '@id': 'https://www.geekwire.com/#website', 'url': 'https://www.geekwire.com/', 'name': 'GeekWire', 'description': 'Breaking News in Technology &amp; Business', 'publisher': {'@id': 'https://www.geekwire.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.geekwire.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.geekwire.com/#organization', 'name': 'GeekWire', 'url': 'https://www.geekwire.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geekwire.com/#/schema/logo/image/', 'url': 'https://cdn.geekwire.com/wp-content/uploads/2017/10/GeekWire-Logo.png', 'contentUrl': 'https://cdn.geekwire.com/wp-content/uploads/2017/10/GeekWire-Logo.png', 'width': 400, 'height': 400, 'caption': 'GeekWire'}, 'image': {'@id': 'https://www.geekwire.com/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/geekwire', 'https://x.com/geekwire', 'https://www.instagram.com/geekwire/', 'https://www.youtube.com/geekwire', 'https://www.pinterest.com/geekwire/', 'https://en.wikipedia.org/wiki/GeekWire', 'https://www.linkedin.com/company/geekwire/']}, {'@type': 'Person', '@id': 'https://www.geekwire.com/#/schema/person/9641ab4b8b61dd1b5a6d0ebc6236d5ab', 'name': 'Alan Boyle', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geekwire.com/#/schema/person/image/fe58e4d63645669100f65958faeb8f2f', 'url': 'https://secure.gravatar.com/avatar/8aeeff7d40870308f9c2c5796cd026d0?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/8aeeff7d40870308f9c2c5796cd026d0?s=96&d=mm&r=g', 'caption': 'Alan Boyle'}, 'description': 'Alan Boyle is an award-winning science writer and space reporter and GeekWire contributing editor. Formerly of NBCNews.com, he is the author of ""The Case for Pluto: How a Little Planet Made a Big Difference."" Follow him via CosmicLog.com, on Twitter @b0yle,\xa0on Facebook and on MeWe.', 'sameAs': ['https://cosmiclog.com/', 'https://www.facebook.com/alan.boyle', 'https://instagram.com/alanb0yle', 'https://www.linkedin.com/in/alan-boyle-97a874', 'https://x.com/b0yle', 'https://en.wikipedia.org/wiki/Alan_Boyle'], 'url': 'https://www.geekwire.com/author/alanboyle/'}]",N/A,N/A,"


Study suggests AI’s disruptive effect on jobs will hit higher on economic and education scales
by Alan Boyle on November 20, 2019 at 2:56 pmNovember 20, 2019 at 3:32 pm




 Share  11
 Tweet
 Share
 Reddit
 Email




Subscribe to GeekWire Newsletters today!






 
 

BOT or NOT? This special series explores the evolving relationship between humans and machines, examining the ways that robots, artificial intelligence and automation are impacting our work and lives.






A map by the Brookings Institution uses shades of pink and red to indicate which cities are expected to be hard-hit by job disruption related to AI. (Brookings Graphic / Source: Brookings Analysis of Webb, 2019)
When experts talk about the disruptive effects of artificial intelligence, they tend to focus on low-paid laborers — but a newly published study suggests higher-paid, more highly educated workers will be increasingly exposed to job challenges.
The study puts Seattle toward the top of the list for AI-related job disruption.
The analysis, which draws on work by researchers at Stanford University and the Brookings Institution, makes use of a novel technique that connects AI-related patents with the job descriptions for different professions.
Stanford researcher Michael Webb extracted entries from the tens of millions of patents in a Google database, as well as the 964 job descriptions indexed by the U.S. Department of Labor.
The goal was to match up noun-verb descriptions from the patents related to automation and AI with the job descriptions, on the theory that a particular job might face more exposure to future disruption if there are more patents associated with the description of that job.
For example, a patent for a device that monitors the operating conditions of power equipment could affect someone whose job description mentions monitoring operating conditions of equipment at a wastewater treatment plant. If there are a lot of patents for such devices, that would increase the job exposure index in Webb’s analytical model.
Webb looked at job exposure assessments for robotics, software applications and AI, and ranked the highest and lowest job exposure for each category:

Robotics: High-exposure occupations included forklift drivers, crane operators and janitors. Low-exposure occupations included payroll clerks, art/entertainment performers and clergy.
Software: High-exposure occupations: broadcast equipment operators, water and sewage treatment plant operators, parking lot attendants, and packers and packagers by hand. Low-exposure: barbers, podiatrists, college instructors and postal carriers.
AI: High-exposure occupations: clinical lab technicians, chemical engineers, optometrists, power plant operators. Low-exposure: non-farm animal caretakers, food preparation workers, college instructors, art/entertainment performers.

Why did highly skilled workers such as lab technicians end up on top of the AI list? Webb noted that AI is getting better and better at matching human performance in some of these jobs. In contrast, some job categories that may seem closely related — for example, lab researchers — “involve reasoning about situations that have never been seen before.” Others, such as food preparation or massage therapy, require the sorts of interpersonal skills that aren’t as suited for AI.
Lawyers who interact with the public have less to worry about than paralegals who merely review documents. Podiatrists who make judgment calls about patients’ feet have less to worry about optometrists whose diagnostic processes are easier to computerize. “Optometry is the area of medicine that has seen perhaps the most success of AI algorithms to date,” Webb wrote.
The Brookings Institution built upon Webb’s study by looking at how job exposure was distributed geographically, demographically and across occupational groups. For AI, the exposure scores are highest for agriculture, engineering and science. They’re lowest for education, food service and personal care.
Job exposure is relatively higher for men as opposed to women, for white and Asian-American workers as opposed to black and Hispanic/Latino workers, and for workers in the 25-64 age bracket as opposed to workers who are younger or older.
Bigger, higher-tech metro areas and manufacturing centers are more prone to AI job disruption. As a result, the Seattle area and California’s Silicon Valley have high scores on the disruption scale — but not as high as, say, Elkhart, an Indiana city that’s considered the nation’s capital of RV manufacturing.

Webb’s calculations suggest that AI’s effect on high-wage occupations will have an impact on the nation’s income inequality — up to a point. “Under the assumption that the historical pattern of long-run substitution will continue, I estimate that AI will reduce 90-10 wage inequality, but will not affect the top 1%,” he said.
He and his colleagues at Brookings acknowledged that there’s still a lot of uncertainty about how AI will play out. Changes in population trends, investment levels or education programs could reduce job exposure, or heighten it. And the rise of AI could well lead to new products, services and occupations.
“While the present assessment predicts areas of work in which some kind of impact is expected, it doesn’t specifically predict whether AI will substitute for existing work, complement it, or create entirely new work for humans,” the Brookings Institution’s research team said. “That means much more inquiry — qualitative and empirical — is needed to tease out AI’s special genius and coming impacts.”
For the full details, check out Webb’s study, “The Impact of Artificial Intelligence on the Labor Market,” and the Brookings report, “What Jobs Are Affected by AI?”


Message from the UnderwriterPutting AI-driven insights, productivity tools,
and security in the hands of businesses

We’re all experiencing a real-time revolution with the rise of generative AI. The opportunity is huge—but it requires a new approach to cloud computing. Organizations are already leveraging Google Cloud’s AI capabilities to unlock data, lower costs, embrace hybrid work, and protect against threats.
Learn how they’re succeeding at Transform with Google Cloud.
Learn more about underwritten and sponsored content on GeekWire.
More Bot or NotHow clean tech companies can take advantage of AI — without draining energy from the planetAI has trouble identifying sarcasm from Seattle satirical news site The Needling‘We know something big is happening’: Tech vets encourage experimentation, education with AIGeekWire contributing editor Alan Boyle is an award-winning science writer and veteran space reporter. Formerly of NBCNews.com, he is the author of ""The Case for Pluto: How a Little Planet Made a Big Difference."" Follow him via CosmicLog.com, on Twitter @b0yle, and on Facebook and MeWe. Reach him via email at alan@geekwire.com. 
 Share  11
 Tweet
 Share
 Reddit
 Email




Previous StoryAmazon defends Ring’s relationship with police as U.S. senator resurfaces privacy concerns 

Next StorySpotify opens its free tier to Amazon Alexa devices, as Sonos acquires voice assistant startup Snips 

 Filed Under: Bot or Not • Science  Tagged With: AI • artificial intelligence • Employment








GeekWire Newsletters

Subscribe to GeekWire's free newsletters to catch every headline




Email address

Subscribe







GeekWire Daily - Top headlines daily
                                    




GeekWire Weekly - Most-read stories of the week, delivered Sunday
                                    




Breaking News Alerts - Important news as it happens
                                    




GeekWire Startups - News, analysis, insights from the Pacific Northwest startup ecosystem, delivered Friday
                                    




GeekWire Mid-week Update — Most-read stories so far this week, delivered Wednesday
                                    




GeekWire Local Deals — Special offers for Pacific Northwest area readers
                                    







Send Us a Tip
Have a scoop that you'd like GeekWire to cover? Let us know.

Send Us a Tip








",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifmh0dHBzOi8vd3d3LnNlYXR0bGV0aW1lcy5jb20vYnVzaW5lc3MvdGVjaG5vbG9neS9zdHVkeS1zdWdnZXN0cy13aGl0ZS1jb2xsYXItd29ya2Vycy13aWxsLWJlLW1vc3QtYWZmZWN0ZWQtaW4tdGhlLW5ldy1lY29ub215L9IBAA?oc=5,"White collar workers will be most affected by AI in the new economy, study suggests - The Seattle Times",2019-11-19,The Seattle Times,https://www.seattletimes.com,"The Brookings Institution report revealed that workers with higher education and wages will experience the greatest changes in their jobs due to AI, for better or worse. Washington state stood...",N/A,"The Brookings Institution report revealed that workers with higher education and wages will experience the greatest changes in their jobs due to AI, for better or worse. Washington state stood out in the state-by-state comparison of the role of AI,...","The Brookings Institution report revealed that workers with higher education and wages will experience the greatest changes in their jobs due to AI, for better or worse. Washington state stood out in the state-by-state comparison of the role of AI,...",https://schema.org,,N/A,N/A,"


BusinessTechnology 

    White collar workers will be most affected by AI in the new economy, study suggests  

 Nov. 19, 2019 at 9:01 pm  Updated Nov. 25, 2019 at 7:47 pm  







 

Skip Ad 


 
By 

Melissa Hellmann

Seattle Times staff reporter 


In 2015, Stanford University Ph.D. student Michael Webb watched as his classmates in the computer science program became increasingly interested in the advancements of artificial intelligence (AI). An outlier, he was more focused on the economic implications of the technologies.But there was a dearth of reports that solely focused on the impact AI has on different sectors and occupations. Instead, most analyses generalized technology to include a wide range of automation and software.Although often used interchangeably, AI and automation are not one and the same: AI technologies are designed to mimic human thinking and actions, while automation performs repetitive tasks.The A.I. Age | This 12-month series of stories explores the social and economic questions arising from the fast-spreading uses of artificial intelligence. The series is funded with the help of the Harvard-MIT Ethics and Governance of AI Initiative. Seattle Times editors and reporters operate independently of our funders and maintain editorial control over the coverage.Then it struck Webb to analyze the overlap between the language in patents — which offer insight into the commercial use of technologies — and the text of job descriptions, which provide a wide-ranging view of the labor market.Equipped with a blueprint, Webb set about to create a machine learning system that would accurately shed light on the economic impacts of a rapidly changing field. His results were the basis of a new report by the D.C.-based research group Brookings Institution focused on the potential impact of AI on the workforce.Webb’s findings revealed that workers with higher education and wages will experience the greatest changes in their jobs due to AI, for better or worse.
Advertising

Skip AdSkip AdSkip Ad 

“While earlier waves of automation have led to disruption across the lower half of the wage distribution, AI appears likely to have different impacts, with its own windfalls and challenges,” wrote the authors of the Brookings Institution report.Washington state stood out in the Brookings Institution’s state-by-state comparison of the role of AI, which the researchers attributed to the Puget Sound region’s focus on technology and manufacturing.In the report titled “What Jobs are Affected by AI?”, released Wednesday, Brookings researchers extrapolated on Webb’s statistics and analyzed the impact of AI on various industries, demographics and geographies.



        Related
        More on The A.I. Age


Seattle faith groups reckon with AI — and what it means to be ‘truly human’ These AI earbuds could help travelers seamlessly translate conversations Video: Will artificial intelligence make work better — or worse? Experts explore the future of work


AI 101: What is artificial intelligence and where is it going? Face-scanning algorithm increasingly decides whether you deserve the job 



More



Using over 16,000 patents that contained words describing AI technologies, Webb trained a natural language processing algorithm — a mathematical formula that amounts to a set of processing instructions — to identify thousands of verb-object pairs to quantify the usage of AI in nearly 800 job applications from a U.S. Department of Labor database.For example, his system would extract the words “diagnose disease” in the description of an AI patent, and then find a similar phrase in job descriptions to determine if, say, a doctor’s task will be affected by the technology.Webb’s analysis revealed that 740 out of the 769 job posts he analyzed matched with AI patent language, showing that the occupations may be impacted by AI technologies.
Advertising

Skip Ad 

Higher paid and educated workers, as well as some agriculture and manufacturing positions will be the most affected by AI, the report noted. The motor vehicle manufacturing and textile industries will experience the most changes due to AI deployment, which is already seen in production lines where AI systems are used to identify defective clothes.Demographic groups including men, white and Asian Americans, as well as workers ages 25 to 54, will be disproportionately involved with or impacted by AI, the study found.Webb believes the difference in impact between automation and AI on different demographics is because AI is particularly adept at performing tasks that involve optimization, judgment, and learning from experience. Meanwhile, AI is less skilled at roles that involve human interaction – tasks typically fulfilled by less-educated workers in service jobs.Will interacting with AI be helpful or detrimental to highly paid workers? That’s hard to say, says Webb: “On the one hand, AI could make them more productive, and increase their wages. On the other, it could reduce employment.”Mark Muro, one of the report’s authors, stressed that involvement with AI doesn’t equate to job replacement. In fact, higher-wage workers may be the best equipped to weather the changes brought about by AI.“White-collar workers with higher education levels may have better resources to roll with it and adjust more than, say lower-educated manufacturing workers in the U.S. with fewer skills,” said Muro.
Advertising

Skip AdSkip AdSkip Ad 

The report’s findings drastically differed from the results of a January Brookings Institution report that looked at the impact of automation and robotics on jobs; it found that less-educated, lower-wage workers would potentially face more change due to automation technologies and robotics than employees in higher-wage jobs.Joseph Wilcox, co-manager of the Future of Work Task Force, finds it surprising that AI could potentially affect higher-wage workers more than than lower-wage ones. “It doesn’t surprise me that the nature of the job would change, but I would be surprised if there’s a lot of displacement,” Wilcox said.Over the past year, Wilcox has attended hundreds of meetings, held formal outreach in rural areas, and talked with workers, educators and unions throughout Washington about how new technology is impacting them and gathering suggestions for best practices.Launched last September, the task force was the result of a 2018 legislative mandate designed to explore policies for businesses and communities to thrive in what is referred to as the fourth industrial revolution, a time heavily dependent upon AI. He said he’s seen firsthand that lower-paying jobs are the most likely to be automated.“The impact on each industry is going to be different, but it is significant,” said Wilcox. “It’s going to require lifelong learning and continuous upskilling to keep yourself competitive in the job market.” 



         Melissa Hellmann      

 


Most Read Business Stories


European Union adds porn site XNXX to list of online platforms facing strictest digital scrutiny  
 
Kroger-Albertsons deal would remake Seattle-area grocery map  
 
Amazon sold a used diaper. It tanked a mom-and-pop business  
 
Turnaround for ‘really strange’ housing market is a year off | Analysis  
 
Boeing 777X finally gets FAA green light for certification flights   WATCH






 View Comments

Posting comments is now limited to subscribers only. View subscription offers here. For more information, visit our FAQ's.

The opinions expressed in reader comments are those of the author only and do not reflect the opinions of The Seattle Times.

",BreadcrumbList,,,,,,,,,,,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Business', 'item': 'https://www.seattletimes.com/business/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Technology', 'item': 'https://www.seattletimes.com/business/technology/'}, {'@type': 'ListItem', 'position': 3, 'name': 'White collar workers will be most affected by AI in the new economy, study suggests'}]",,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiggFodHRwczovL3d3dy5yb3V0ZS1maWZ0eS5jb20vZGlnaXRhbC1nb3Zlcm5tZW50LzIwMTkvMTEvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtY291bGQtaGF2ZS1iaWdnZXN0LWltcGFjdC13aGl0ZS1jb2xsYXItam9icy8xNjE0NDUv0gEA?oc=5,Artificial Intelligence Could Have Biggest Impact on White-Collar Jobs - Route Fifty,2019-11-20,Route Fifty,https://www.route-fifty.com,New research suggests that white collar jobs are more likely to feel the impacts of artificial intelligence in the workplace than blue-collar positions. ,"artificial intelligence, jobs, economy, ai impact",New research suggests that white collar jobs are more likely to feel the impacts of artificial intelligence in the workplace than blue-collar positions.,New research suggests that white collar jobs are more likely to feel the impacts of artificial intelligence in the workplace than blue-collar positions.,http://schema.org,,N/A,N/A,"

Artificial Intelligence Could Have Biggest Impact on White-Collar Jobs
                    Shutterstock /  XiXinXing 
                  
Sponsor Message

Sponsor Message


      Sign up for Route Fifty Today
    
      Your daily read on state and local government
    email
Sponsor Message

Sponsor Message

Connect with state & local government leadersJoin Our CommunityFeatured eBooks





            
              
                
                  




  


By

Andrea Noble,Staff Correspondent








 
Connecting state and local government leaders








































            
              
                
                  



  By


Andrea Noble

|

             November 20, 2019
            

New research suggests that white collar jobs are more likely to feel the impacts of artificial intelligence in the workplace than blue-collar positions. 






                  New Technology
                









                  Innovation
                












































The fear used to be that robots would entirely replace manufacturing and production jobs.But new research shows that developments in artificial intelligence could also have drastic effects on the white-collar workforce, particularly in cities and states with a high number of technology-related jobs. The new report from the Brookings Institution examines the jobs that are most likely to change or—in the worst case scenario—be eliminated because of the evolution of artificial intelligence in the workplace. Researchers believe that many well-paying, white-collar professions, ranging from market research analysts and sales managers to programmers and engineers, will see encroachment by artificial intelligence in their fields.“Often analytic or supervisory, these roles appear heavily involved in pattern-oriented or predictive work, and may therefore be especially susceptible to the data-driven inroads of AI,” the report states.To analyze the potential impact of artificial intelligence in the economy, researchers compared language included in artificial intelligence-related patents with job descriptions to determine which type of jobs are most likely to encounter artificial intelligence.The researchers found that “workers with graduate or professional degrees will be almost four times as exposed to AI as workers with just a high school degree.” By contrast, low-paying jobs that require hands-on services, such as health care or food preparation, are unlikely to be as vulnerable to artificial intelligence.Jacob Whiton, a research analyst with the Metropolitan Policy Program at Brookings, stressed that the research does not provide a definitive guide of the type of jobs that will be replaced by artificial intelligence, rather the research informs the industries in which AI has the potential to be used in the future.“This is an indication of the direction towards which a technology could be used and is not meant to imply anything about how in particular it is going to change a given job,” Whiton said.One key insight in the report is that the effects of artificial intelligence on the workforce will not be felt evenly across the country.Cities identified as being at the highest risk of exposure to workforce disruption include those with large technology industries like San Jose, California; Seattle, Washington; Boulder, Colorado; and Salt Lake City and Ogden, Utah. Other cities that are highly reliant on the agriculture and manufacturing industries—including Bakersfield, California; Greenville, South Carolina; Detroit, Michigan; Columbus, Ohio; and Louisville, Kentucky—were also identified as having a high risk for disruption.Smaller, rural communities “are significantly less exposed to technological disruption than larger, dense urban ones,” the report states.
Brookings Institution
When it comes to the impact on manufacturing industries, states in the Midwest and south—including Wisconsin, Michigan, Indiana, Kentucky, Alabama and Georgia—are likely to see higher levels of exposure to artificial intelligence, specifically in the textile and auto manufacturing sectors, where machine learning is making significant advancements, according to the report.

Story Continues Below Sponsor Message

Story Continues Below Sponsor Message


The findings may provide new insight for state and local government officials looking to develop or improve workforce development programs, Whiton said.“So much of the discourse has been equipping workers with tech skills,” he said.The new analysis on the impact that artificial intelligence can have on white-collar jobs may prompt some officials to reexamine the type of durable skills that office workers need to be competitive in the modern workplace, he said.






Andrea Noble is a staff correspondent with Route Fifty. 








              New technology Case Studies
            




                  Powered By
                  










Browse The Atlas full case study database or read more case studies about
            New technology.
          














                            Smart Loading Zones in Pittsburgh Manage Curbside Congestion
                          



                          Pittsburgh, PA
                      

















                            Applied Learning at Wichita State Gives Students Real-World Technology Experience
                          



                          Wichita, KS
                      

















                            Pilot project comparing Utilis to traditional acoustic: Prince William County, VA
                          



                          Prince William County, VA
                      










                BROWSE LOCAL GOV CASE STUDIES
              









Share This:



NEXT STORY:

              When not to automate
            













Midwest states launch new rail service, 12 years in the making







Why the fight over abortion pills isn’t over yet







The 'silver tsunami' is here. Is government ready?







Auditing reimagined: Looking beyond the public dollar







Utah Gov. Cox to homeless providers: Produce results, or you could lose funding






sponsor content

Transforming Lives by Transforming Government: Enterprise Performance Management for Strategic and Operational Excellence in State Government








Midwest states launch new rail service, 12 years in the making






Why the fight over abortion pills isn’t over yet






The 'silver tsunami' is here. Is government ready?






Auditing reimagined: Looking beyond the public dollar






Utah Gov. Cox to homeless providers: Produce results, or you could lose funding





sponsor content

Transforming Lives by Transforming Government: Enterprise Performance Management for Strategic and Operational Excellence in State Government






",Article,Artificial Intelligence Could Have Biggest Impact on White-Collar Jobs,https://www.route-fifty.com/digital-government/2019/11/artificial-intelligence-could-have-biggest-impact-white-collar-jobs/161445/,"{'url': 'https://cdn.route-fifty.com/media/img/cd/2019/11/20/shutterstock_380832685/route-fifty-lead-image.jpg?1627439893', 'width': 1200, '@type': 'ImageObject', 'height': 550}",2019-11-20T18:03:00,2023-07-10T21:04:16,"{'url': '/voices/andrea-noble/14689/', '@type': 'Person', 'name': 'Andrea Noble'}","{'logo': {'url': 'https://cdn.route-fifty.com/media/logos/route-fifty-logo.png', 'width': 241, '@type': 'ImageObject', 'height': 60}, '@type': 'Organization', 'name': 'Route Fifty'}",https://www.route-fifty.com,https://cdn.route-fifty.com/b/route_fifty/img/social/route-fifty-logo.png,"['https://www.facebook.com/RouteFifty', 'https://twitter.com/routefifty', 'https://www.linkedin.com/company/route-fifty/']",,,,,,,,,,,Route Fifty,,,,,,,,,,
https://news.google.com/rss/articles/CBMilgFodHRwczovL3d3dy5idXNpbmVzc3dpcmUuY29tL25ld3MvaG9tZS8yMDE5MTExOTAwNTA0Ni9lbi9DZXJlYnJhcy1TeXN0ZW1zLVVudmVpbHMtQ1MtMS10aGUtSW5kdXN0cnklRTIlODAlOTlzLUZhc3Rlc3QtQXJ0aWZpY2lhbC1JbnRlbGxpZ2VuY2UtQ29tcHV0ZXLSAQA?oc=5,"Cerebras Systems Unveils CS-1, the Industry's Fastest Artificial Intelligence Computer - Business Wire",2019-11-19,Business Wire,https://www.businesswire.com,"Cerebras Systems Unveils CS-1, the Industry’s Fastest Artificial Intelligence Computer",N/A,"Cerebras Systems Unveils CS-1, the Industry’s Fastest Artificial Intelligence Computer","Cerebras Systems Unveils CS-1, the Industry’s Fastest Artificial Intelligence Computer",,,N/A,N/A,"




Cerebras Systems Unveils CS-1, the Industry’s Fastest Artificial Intelligence Computer






Optimized for AI, CS-1 Delivers Unprecedented Compute Performance at a Fraction of the Size and Power of Conventional Systems














Download







Cerebras Systems Unveils CS-1, the Industry’s Fastest Artificial Intelligence Computer (Photo: Business Wire)












Cerebras Systems Unveils CS-1, the Industry’s Fastest Artificial Intelligence Computer (Photo: Business Wire)





Full Size








Small








Preview








Thumbnail


















Full Size








Small








Preview








Thumbnail














November 19, 2019 08:00 AM Eastern Standard Time



LOS ALTOS, Calif.--(BUSINESS WIRE)--Cerebras Systems, a company dedicated to accelerating Artificial Intelligence (AI) compute, today unveiled its CS-1 system, the world’s fastest AI computer. With every component optimized for AI work, the CS-1 delivers more compute performance at less space and less power than any other system. At only 26 inches tall, the CS-1 fits in one-third of a standard data center rack, but replaces clusters of hundreds or thousands of graphics processing units (GPUs) that consume dozens of racks and use hundreds of kilowatts of power.

.@CerebrasSystems Unveils CS-1, the Industry’s Fastest Artificial Intelligence ComputerPost this

In August, Cerebras delivered the Wafer Scale Engine (WSE), the only trillion transistor wafer scale processor in existence. The Cerebras WSE is 56.7 times larger and contains 78 times more compute cores than the largest GPU, setting a new bar for AI processors. The CS-1 system design and Cerebras software platform combine to extract every ounce of processing power from the 400,000 compute cores and 18 Gigabytes of high performance on-chip memory on the WSE.


In AI compute, chip size is profoundly important. Big chips process information more quickly, producing answers in less time. However, exceptional processor performance is necessary but not sufficient to guarantee industry leading AI performance. Innovative, high performance processors, like the WSE, must be combined with dedicated hardware systems and extraordinary software to achieve record-breaking performance. For this reason, every aspect of the Cerebras CS-1 system and the Cerebras software platform was designed for accelerated AI compute.


“The CS-1 is the industry’s fastest AI computer, and because it is easy to install, quick to bring up and integrates with existing AI models in TensorFlow and PyTorch, it delivers value the day it is deployed,” said Andrew Feldman, Founder and Chief Executive Office, Cerebras Systems. “Depending on workload, the CS-1 delivers hundreds or thousands of times the performance of legacy alternatives at one-tenth the power draw and one-tenth the space per unit compute.”


Cerebras is the only company to undertake the ambitious task of building a dedicated system from the ground up. By optimizing every aspect of chip design, system design, and software, the CS-1 delivers unprecedented performance. With the CS-1, AI work that today takes months can now be done in minutes, and work that takes weeks now can be completed in seconds. Not only does the CS-1 radically reduce training time, but also it sets a new bar for latency in inference. For deep neural networks, single image classification can be accomplished in microseconds, thousands of times faster than alternative solutions.


Early customer deployments include Argonne National Laboratory where the CS-1 is being used to accelerate neural networks in pathbreaking cancer studies, to better understand the properties of black holes, and to help understand and treat traumatic brain injuries. The sheer performance of the CS-1 makes it an exceptional solution for the largest and most complex problems in AI.


“The CS-1 is a single system that can deliver more performance than the largest clusters, without the overhead of cluster set up and management,” said Kevin Krewell, Principal Analyst, TIRIAS Research. “By delivering so much compute in a single system, the CS-1 not only can shrink training time but also reduces deployment time. In total, the CS-1 could substantially reduce overall time to answer, which is the key metric for AI research productivity.”


About the CS-1


The CS-1 solution is the fastest AI computer system. It is comprised of three major technical innovations: the CS-1 system, the Cerebras software platform and the WSE.


The CS-1 System


Optimized exclusively for accelerating AI work and built from the ground up by Cerebras Systems, the CS-1 is 15 rack units (26 inches) tall and fits in one-third of a standard data center rack. The CS-1 integrates the Cerebras WSE and feeds its massive 400,000 AI optimized compute cores with 1.2 Terabits per second of data. The combination of the massive Input/Output bandwidth – 12 x 100 Gigabit Ethernet lanes – and the 18 Gigabytes of on-chip memory enable the CS-1 to deliver vastly more calculations per unit time than legacy offerings. Since all of the compute and communication remains on-chip, the CS-1 uses less than one-tenth the power and takes one-tenth the space of alternative solutions.


Unlike clusters of GPUs, which can take weeks or months to set up, require extensive modifications to existing models, consume dozens of data center racks and require complicated proprietary InfiniBand to cluster, the CS-1 takes minutes to set up. Users can simply plug in the standards-based 100 Gigabit Ethernet links to a switch and are ready to start training models at record-breaking speed.


Cerebras Software Platform 


The CS-1 is easy to deploy and simple to use. Cerebras’s mission is to accelerate not only time-to-train, but also the end-to-end time it takes for researchers to achieve new insights – from model definition to training to debugging to deployment.


The Cerebras software platform is designed for Machine Learning (ML) researchers to leverage CS-1 performance without changing their existing workflows. Users can define their models for the CS-1 using industry-standard ML frameworks such as TensorFlow and PyTorch. A powerful graph compiler automatically converts these models into optimized executables for the CS-1, and a rich set of tools enables intuitive model debugging and profiling.


The Cerebras software platform is comprised of four primary elements:



Integration with common ML frameworks like TensorFlow and PyTorch


Optimized Cerebras Graph Compiler (CGC)


Flexible library of high-performance kernels and a Kernel API


Development tools for debug, introspection, and profiling



The Cerebras Graph Compiler


The Cerebras Graph Compiler (CGC) takes as input a user-specified neural network. For maximum workflow familiarity and flexibility, researchers can use both existing ML frameworks and well-structured graph algorithms written in other general-purpose languages, such as C and Python, to program for the CS-1.


CGC begins the translation of a deep learning network into an optimized executable by extracting a static graph representation from the source language and converting it into the Cerebras Linear Algebra Intermediate Representation (CLAIR). As ML frameworks evolve rapidly to keep up with the needs of the field, this consistent input abstraction allows CGC to quickly support new frameworks and features, without changes to the underlying compiler.


Using its knowledge of the unique WSE architecture, CGC then allocates compute and memory resources to each part of the graph and then maps them to the computational array. Finally, a communication path, unique to each network, is configured onto the fabric.


Because of the massive size of the WSE, every layer in the neural network can be placed onto the fabric at once, and run simultaneously in parallel. This approach to whole-model acceleration is unique to the WSE -- no other device has sufficient on-chip memory to hold all layers at once on a single chip, or the enormous high-bandwidth and low-latency communication advantages that are only possible on silicon.


The final result is a CS-1 executable, customized to the unique needs of each neural network, so that all 400,000 compute cores and 18 Gigabytes of on-chip SRAM can be used at maximum utilization towards accelerating the deep learning application.


Development Tools and APIs


CGC’s integrations with popular ML frameworks means that industry-standard tools such as TensorBoard are supported out of the box. In addition, Cerebras provides a fully-featured set of debugging and profiling tools to make deeper introspection and development easy.


For ML practitioners, Cerebras provides a debugging suite that allows visibility into every step of the compilation and training run. This enables visual introspection into details like:



Validity of the compilation on the fabric


Latency evaluations across a single kernel versus through the entire program


Hardware utilization on a per-kernel basis to help identify bottlenecks



For advanced developers interested in deeper flexibility and customization, Cerebras provides a Kernel API and a C/C++ compiler based on LLVM that allows users to program custom kernels for CGC. Combined with extensive hardware documentation, example kernels, and best practices for kernel development, Cerebras provides users with the tools they need to create new kernels for unique research needs.


The WSE


The Cerebras WSE is the largest chip ever made and the industry’s only trillion transistor processor. It contains more cores, with more local memory, and more fabric bandwidth, than any chip in history. This enables fast, flexible computation at lower latency and with less energy. The WSE is 46,255 millimeters square, which is 56 times larger than the largest GPU. In addition, with 400,000 cores, 18 Gigabytes of on-chip SRAM, 9.6 Petabytes/sec of memory bandwidth, and 100 Petabits/sec of interconnect bandwidth, the WSE contains 78 times more compute cores, 3,000 times more high speed, on-chip memory, 10,000 times more memory bandwidth and 33,000 times more fabric bandwidth than its GPU competitors.


For more information on Cerebras Systems and the Cerebras CS-1, please visit www.cerebras.net. Imagery and digital photography for the Cerebras CS-1 can be found linked here.


About Cerebras Systems


Cerebras Systems is a team of pioneering computer architects, computer scientists, deep learning researchers, and engineers of all types. We have come together to build a new class of computer to accelerate artificial intelligence work by three orders of magnitude beyond the current state of the art. The CS-1 is the fastest AI computer in existence. It contains a collection of industry firsts, including the Cerebras Wafer Scale Engine (WSE). The WSE is the largest chip ever built. It contains 1.2 trillion transistors and covers more than 46,225 square millimeters of silicon. The largest graphics processor on the market has 21.1 billion transistors and covers 815 square millimeters. In artificial intelligence work, large chips process information more quickly producing answers in less time. As a result, neural networks that in the past took months to train, can now train in minutes on the Cerebras WSE.



Contacts

Kim Ziesemer
Email: pr@zmcommunications.com




",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiMmh0dHBzOi8vd3d3Lm5hdHVyZS5jb20vYXJ0aWNsZXMvZDQyNDczLTAxOS0wMDMzMS0w0gEA?oc=5,Artificial intelligence turns to antibody selection - Nature.com,2019-11-17,Nature.com,https://www.nature.com,A new machine-learning platform scours the scientific literature in search of experimentally suitable antibodies.,N/A,A new machine-learning platform scours the scientific literature in search of experimentally suitable antibodies.,N/A,,,N/A,N/A,"




ADVERTISEMENT FEATURE Advertiser retains sole responsibility for the content of this article

Artificial intelligence turns to antibody selection


                    A new machine-learning platform scours the scientific literature in search of experimentally suitable antibodies.
                





Produced by














Twitter





Facebook





Email














Bench Sci marries a database of 6.9 million commercially available antibodies with a machine learning algorithm that searches scientific literature for relevant, high-affinity antibodies.Credit: MF3d/Getty Images


Getting through graduate school is hard enough. But Tom Leung encountered a particularly frustrating obstacle while studying epigenetic changes in cultured cells during his PhD at the University of Toronto. “I had to stay in the lab for 12 hours at a time to collect all the samples, and in the end, when I ran the western blot to analyze my results, it didn’t work,” he says. “It wasn’t because I did anything wrong in the experiment. It was because the antibody quality was poor.”Leung’s experience is common. A 2008 high-profile, proteome-scale study of antibody performance from 2008 found that nearly half of the thousands of reagents tested by the authors did not deliver the expected affinity or specificity. Leung wanted a solution, and along with University of Toronto colleagues, Elvis Wianda, David Chen and Liran Belenzon, founded BenchSci, an artificial intelligence (AI) platform that allows researchers to search for optimal reagents based on figures from published experiments and make data-driven antibody choices.The burden of choiceThere are millions of antibodies available, sold by hundreds of vendors. One recent article reports that more than 5,000 antibodies exist for the human epidermal growth factor receptor protein (EGFR) alone. “Scientists know that every antibody is not going to work in every experimental context,” says Casandra Mangroo, Head of Science at BenchSci. “Even if the vendor has done some sort of testing, they don't have the capacity to test every antibody in every single experimental context.”David Rimm, a pathologist at Yale University, has been a vocal advocate for antibody quality testing and validation. In 2016, he was part of a push to get makers of antibodies to agree to a set of best practices for quality control.“At first, they all agreed to a sort of ‘scoring system’, but after thinking about it for a bit, they changed their mind,” he says. Without a clear-cut way to confidently rank and compare antibodies, scientists can only rely on the data in the literature, but extracting this information is labor intensive.“PubMed and Google Scholar let you quickly look for results, and you can find a lot of papers,” Leung says. “But if you have to go down to the supplementary data, or one of the figures, to find a reagent, it’s not that easy.”Matchmaking for antibodiesBenchSci’s AI-Assisted Antibody Selection platform automates this work, scouring text and figures in the literature to identify antibodies that might support a particular experiment. Leung notes that the company initially benefited from the wealth of publications in repositories such as PubMed Central.“We were able to get a lot of high-tier journals and open-access journals and then use them to train a machine learning model,” he says. “Then later on, we forged a lot of partnerships with different closed-access publishers.” Springer Nature, which publishes Nature, Wiley, and Wolters Kluwer, are among the publishers that have since agreed to share article data with BenchSci.Leung says that BenchSci’s timing was also fortuitous. “If this idea was hatched two years earlier, it probably wasn't going to work because deep learning and machine learning were not as mature yet,” he says. It helped that Chan and Wianda had extensive familiarity with these tools from their own graduate research.BenchSci’s database contains 6.9 million antibodies from over 220 vendors, with coupled data drawn from 10 million research papers. That dataset is continually growing, both in the number of reagents and journals, as are the demands on BenchSci’s computational capacity. “We do monthly runs to update data on the platform and train our algorithms. We basically had to create a whole wall of computers to do that,” Mangroo says.When scientists search BenchSci’s database for antibodies against a particular protein, the AI assembles a simple set of figures depicting the use of various products in different experimental contexts. One can just look for antibodies that have been used in immunohistochemistry, ELISA or flow cytometry experiments, and then assess the performance of different reagents in different studies. Today more than 15 of the top 20 pharmaceutical companies are using the BenchSci platform, as are more than 31,000 scientists at 3,600 institutions.Rimm has found the platform quite valuable. “Because it gives you access to the figures in which an antibody was used, you can have your own criteria for what you accept as validation,” he says. “It just saves a lot of time spent digging in the literature.”In addition to being helpful for product selection, Rimm says the platform can also streamline the experimental process. If, for example, BenchSci uncovers a published track record for a given antibody in a particular validation assay, researchers can cite prior work rather than wasting effort repeating the experiment. Conversely, Mangroo says that image search makes it easy to recognize reagents that are low quality or poorly matched for a given assay, should a figure show results that are inconsistent with other experimental data.Network effectsMany antibody manufacturers have now lined up behind BenchSci’s efforts, sharing their catalogues and associated validation data for incorporation into the company’s database. Rimm says that even though these reagent manufacturers were hesitant about universal validation standards, many still recognized the need for better quality control. “The system sort of policed itself, and made the competitive marketplace the scoring system, where many vendors competed with each other in who could show the most validated antibody,” he says.BenchSci builds on that sentiment. Users benefit from access to validation data while antibody companies benefit from having their products directly linked with successful published experiments. Importantly, BenchSci remains a neutral platform in terms of recommendations, allowing the data to speak for itself. “I like that BenchSci doesn’t have skin in the game,” Rimm says. “They don’t advertise or rate antibodies.”The ability to make informed decisions about antibodies can reduce waste and lost time, but antibodies are only one part of the reproducibility problem. Researchers also rely on a host of other reagents, including molecular probes, protein-specific inhibitors and activators, and primers for sequencing and PCR amplification. Leung ultimately hopes to turn BenchSci’s AI platform toward this broader constellation of products as well.“By linking these other reagents together, we can provide a much more complete picture of what has transpired in different publications,” Leung says. “Then we'll be able to help scientists in a much fuller regard to planning their experiments.”
To learn more about how AI could help researchers more accurately select suitable antibodies, visit benchsci.com.


Related Articles




                        
                        For researchers, antibody selection is a risky decision
                    



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWGh0dHBzOi8vd3d3LnNocm0ub3JnL3RvcGljcy10b29scy9uZXdzL2FsbC10aGluZ3Mtd29yay9maXZlLXJlY3J1aXRpbmctdHJlbmRzLW5ldy1kZWNhZGXSAQA?oc=5,Five Recruiting Trends for the New Decade - SHRM,2019-11-16,SHRM,https://www.shrm.org,"From relying on predictive analytics and chatbots to using enhanced candidate vetting strategies—and myriad other up-and-coming trends—the recruiting industry is shedding old habits and trying some potentially game-changing new approaches as a new, tec...","News,Workforce Planning,Feature,All Things Work,Vendors and Software,Talent Acquisition,Technology,Recruiting","From relying on predictive analytics and chatbots to using enhanced candidate vetting strategies—and myriad other up-and-coming trends—the recruiting industry is shedding old habits and trying some potentially game-changing new approaches as a new, tec...","From relying on predictive analytics and chatbots to using enhanced candidate vetting strategies—and myriad other up-and-coming trends—the recruiting industry is shedding old habits and trying some potentially game-changing new approaches as a new, tec...",https://schema.org,,N/A,N/A,"
 














 




Share












 







Linked In


 
 
Facebook


 
 
Twitter


  

Email



 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus
            convallis sem tellus, vitae egestas felis vestibule ut.  





 

 
Error message details.

 
 




Copy button
 



 


















 



 
 
 
Reuse Permissions
              













Request permission to republish or redistribute SHRM content and materials.
            






 
 

 
Learn More
  

  



 





          Feature

Five Recruiting Trends for the New Decade

New technologies and practices are changing the way companies recruit talent, and it's all happening at lightning speed.

November 16, 2019

          | 



          

          
                      Brian O’Connell
                             







Share



Bookmark


i
Reuse
                            Permissions








",NewsArticle,Five Recruiting Trends for the New Decade,,,,2024-05-31T17:40:24.699Z,"{'@type': 'Person', 'name': 'Brian O’Connell', 'url': ''}",,https://www.shrm.org/,https://shrm-res.cloudinary.com/image/upload/v1703622970/shrm-logo.png,"['http://twitter.com/SHRM', 'http://www.linkedin.com/company/shrm', 'https://www.facebook.com/SHRMHQ', 'http://www.youtube.com/shrmofficial', 'https://instagram.com/shrmofficial/', 'https://en.wikipedia.org/wiki/Society_for_Human_Resource_Management', 'https://www.wikidata.org/wiki/Q1527909', 'https://www.crunchbase.com/organization/shrm']",,,,,"{'@type': 'WebPageElement', 'isAccessibleForFree': 'false', 'cssSelector': '.content-metering-wrapper'}",,,,false,,SHRM,,,,,,,Society for Human Resource Management,1-800-283-7476,"{'@type': 'PostalAddress', 'streetAddress': '1800 Duke Street', 'addressLocality': 'Alexandria', 'postalCode': '22314', 'addressCountry': 'United States'}","{'@type': 'SearchAction', 'target': 'https://www.shrm.org/search-results#q={search_term_string}', 'query-input': 'required name=search_term_string'}"
https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vZm9ydHVuZS5jb20vMjAxOS8xMS8xOS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1jZXJlYnJhcy1zdXBlcmNvbXB1dGVyL9IBAA?oc=5,Artificial Intelligence: Cerebras Unveils A Speedy and Small Supercomputer - Fortune,2019-11-19,Fortune,https://fortune.com,"The CS-1 could rapidly speed up the research involved in cancer-drug development, among other disciplines.","ai artificial intelligence, a.i., artificial intelligence, supercomputers, ultrafast computing, machine learning, best artificial intelligence, a.i. solutions, Cerebras, Cerebras Systems, deep learning, neural nets, neural networks, drug research, drug development, cancer drugs, cancer research, Argonne Labs, Argonne National Laboratory","The CS-1 from Cerebras could rapidly speed up the research involved in cancer-drug development, among other disciplines.","The CS-1 from Cerebras could rapidly speed up the research involved in cancer-drug development, among other disciplines.",,,N/A,N/A,"Newsletters - Data SheetThe security company that Alphabet may buy for $23 billion launched at the perfect timeBYDavid MeyerJuly 16, 2024",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiZWh0dHBzOi8vZGFpbHl0aW1lcy5jb20ucGsvNTAzODY3L2lzLXRoZS1yaXNlLWluLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXN1cGVyc2VkaW5nLWh1bWFuLWVtcGxveW1lbnQv0gFpaHR0cHM6Ly9kYWlseXRpbWVzLmNvbS5way81MDM4NjcvaXMtdGhlLXJpc2UtaW4tYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utc3VwZXJzZWRpbmctaHVtYW4tZW1wbG95bWVudC9hbXAv?oc=5,Is the rise in Artificial Intelligence superseding human employment? - Daily Times,2019-11-20,Daily Times,https://dailytimes.com.pk,N/A,N/A,This century belongs to the advancement of technology. The technology of the present era baffles human beings with its wonderful services. Invention of machines with an association of Artificial Intelligence (AI) and automation are the products of technology. These days amenities provided by AI are invaluable. One couldn’t contemplate in the past that one would […],N/A,https://schema.org,"[{'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'HOME', 'url': 'https://dailytimes.com.pk/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Latest', 'url': 'https://dailytimes.com.pk/tag/latest/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Pakistan', 'url': 'https://dailytimes.com.pk/blog/pakistan-blog/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Balochistan', 'url': 'https://dailytimes.com.pk/balochistan/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Gilgit Baltistan', 'url': 'https://dailytimes.com.pk/gilgit-baltistan/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Khyber Pakhtunkhwa', 'url': 'https://dailytimes.com.pk/khyber-pakhtunkhwa/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Punjab', 'url': 'https://dailytimes.com.pk/punjab/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Sindh', 'url': 'https://dailytimes.com.pk/sindh/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'World', 'url': 'https://dailytimes.com.pk/blog/world-blog/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Editorials & Opinions', 'url': 'https://dailytimes.com.pk/editorials/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Editorials', 'url': 'https://dailytimes.com.pk/editorial/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Op-Eds', 'url': 'https://dailytimes.com.pk/opeds/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Commentary / Insight', 'url': 'https://dailytimes.com.pk/commentary/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Perspectives', 'url': 'https://dailytimes.com.pk/perspectives/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Cartoons', 'url': 'https://dailytimes.com.pk/cartoons/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Letters to the Editor', 'url': 'https://dailytimes.com.pk/letters/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Featured', 'url': 'https://dailytimes.com.pk/features/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Blogs', 'url': 'https://dailytimes.com.pk/blog/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Ramblings', 'url': 'https://dailytimes.com.pk/blog/ramblings/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Lifestyle', 'url': 'https://dailytimes.com.pk/lifestyle/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Culture', 'url': 'https://dailytimes.com.pk/blog/culture/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Sports', 'url': 'https://dailytimes.com.pk/sports/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Business', 'url': 'https://dailytimes.com.pk/business/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Arts, Culture &amp; Books', 'url': 'https://dailytimes.com.pk/dtculture/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'E-PAPER', 'url': 'https://dailytimes.com.pk/epapers/'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Lahore', 'url': 'https://dailytimes.com.pk/e-paper/?city=lhr'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Islamabad', 'url': 'https://dailytimes.com.pk/e-paper/?city=isb'}, {'@context': 'https://schema.org', '@type': 'SiteNavigationElement', 'id': 'site-navigation', 'name': 'Karachi', 'url': 'https://dailytimes.com.pk/e-paper/?city=khi'}]",N/A,N/A,"This century belongs to the advancement of technology. The technology of the present era baffles human beings with its wonderful services. Invention of machines with an association of Artificial Intelligence (AI) and automation are the products of technology. These days amenities provided by AI are invaluable. One couldn’t contemplate in the past that one would make visible contact with one’s dear ones sitting in far-flung areas. Now even a low-income citizen can make video contact-a widespread phenomenon-withhis relatives or friends.Theexample that I have cited is a very ordinary one of the services provided by technology.
However, with the rise in technologically sophisticated machines, coupled with AI and automation, there is an imminent hazard of growing unemployment. The concerned debate revolves around the pivot that is this idea paranoia or is it categorically conquering the prospects of human employment.
Before examining the effects of technology, let’s discuss what brought about advancement in technology to such a great extent.The answer to this question is globalisation. It is globalisation that has helped in the propagation of technology at a wide scale. Interconnectivity among countries and relaxed trade conditions-some of the outcomes of globalisation-have allowed manufactured tech-products from the developed world to reach developing and under-developing world.Now the question emanates whether the rampant advancement in technology is contributing to unemployment or not.



The advancement in technology has enlarged the invention of robots with the help of automation and AI. There is no denying that robots can perform activities and tasks with ample accuracy and efficiency than human beings. However, the rise of robots has led to some scary warnings about the future of work. These warnings have brought forth the growing risks of unemployment. A recent study foundthat up to 0.7 million jobs were lost to robots in the US from 1990 to 2007. This dilemma doesn’t start from the 1990s as it began when the Industrial Revolution caught momentum during the late 19th century.
The World Economic Forum predicts that millions of jobs will be lost to robots by 2020. The Industrial Revolution brought ease for humans, but it is also important to mention that each invention came at a cost for humans. Let’s take the example of bronze’s arrival; stones tools were replaced, and when iron tools were made, workers who used bronze tools lost their job. With industrial advancement, millions of cobblers and weavers’ jobs were put at stake. They had to endeavour a lot to keep pace with the ever-advancing technology.
Our daily routine jobs are getting automated. One prime illustration is the introduction of live railway tracking software in Pakistan Railways, which has provided passengers with unmatched comfort as they don’t have to wait for hours. They just track the train on their mobiles and reach the station on time. However, jobs of many inquiry providers havebeen affected because of this technology. Another example is ATM-Automated Teller Machine-whichhas replaced the job of scores of bankers.



Seven in ten Americans, six in ten Canadians, and six in ten UK residents believe the advent of artificial intelligence will eliminate more jobs than it creates
There isample evidence that the rise in AI and automated appliances have affected human jobs. Various reports published by prestigious institutions shed light on the effects on jobs due to rise in AI technology. McKinsey reckons tha depending upon various adoption scenarios, automation will displace between 400 and 800 million jobs by 2030, requiring as many as 375 million people to switch job categories entirely. According to Oxford Economics, up to 20 million manufacturing jobs worldwide will be lost to robots by 2030.
There is another impact of technology that can affect our jobs by increasing the demand for labour in industries or jobs that arise or develop due to technological progress. The World Economic Forum says that automation will displace 75 million jobs but generate 133 million new ones worldwide by 2022. According to Gartner, AI-related job creation will reach two million net-new jobs in 2025.AI is creating “a surge in new career opportunities,” says a ZipRecruiter report.A recent Gallup and Northeastern University online survey of 4,394 Americans, 3,049 Canadians and 3,208 UK adults found mixed attitudes towards AI, reflecting the mix of apocalyptic and human-friendly scenarios.There is a general pessimistic attitude about the impact of AI adoption on the overall economy, but adults in the US, UK and Canada remainoptimistic about their own jobs.Seven in ten Americans, six in ten Canadians, and six in ten UK residents believe the advent of artificial intelligence will eliminate more jobs than it creates.
It is obvious that rampant use of technology can cause reduction in employment and can also boost it. But the need of the hour is to harness the opportunities that are practical for supporting the employment sector. For this purpose, government needs to take steps that boost employment.One of the best ways to fight redundancy created by technology is to provide the masses opportunities to gain technical and updated education.
Human beings are the supreme creatures, adorned with an intellectual power that no technology can replace. Technology is the upshot of human efforts. Humans create machines, it is not machines that create humans. Machines are operated and controlled by human beings. However, the current chaos in employment is the result of miscalculation of the rise in technology. If the problems in jobs created by automation are addressed in due time, technology will create employment prospects. And if the issues are not addressed,it would create a void in the area of employment.
The writer is a freelancer
0Shares
 
                            Read More
                            
            


",BreadcrumbList,Is the rise in Artificial Intelligence superseding human employment?,"{'@type': 'WebPage', '@id': 'https://dailytimes.com.pk/503867/is-the-rise-in-artificial-intelligence-superseding-human-employment/'}",,2019-11-20T23:31:45,2019-11-20T23:31:46,"{'@type': 'Person', 'name': 'DailyTimes.pk'}","{'@type': 'Organization', 'name': 'Daily Times', 'logo': {'@type': 'ImageObject', 'url': 'https://dailytimes.com.pk/assets/uploads/2020/08/app-icon-192x192-1-60x60.png'}}",https://dailytimes.com.pk,,,,"[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://dailytimes.com.pk/', 'name': 'Home'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://dailytimes.com.pk/503867/is-the-rise-in-artificial-intelligence-superseding-human-employment/', 'name': 'Is the rise in Artificial Intelligence superseding human employment?'}}]",,,,,,,,,Daily Times,,,,,,,,,,"[{'@type': 'SearchAction', 'target': 'https://dailytimes.com.pk/?s={search_term_string}', 'query-input': 'required name=search_term_string'}]"
