URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@graph,article:section,article:summary,article text,@type,url,image,author,publisher,headline,datePublished,dateModified,articleSection,name,isAccessibleForFree,itemListElement,dateCreated,identifier,creator,mainEntityOfPage,thumbnailUrl,inLanguage,alternativeHeadline,video,hasPart,comment,commentCount,copyrightHolder,sourceOrganization,copyrightYear,isPartOf,logo,@id,diversityPolicy,ethicsPolicy,masthead,foundingDate,sameAs,articleBody,speakable,potentialAction
https://news.google.com/rss/articles/CBMiTGh0dHBzOi8vYzMuYWkvYmxvZy9yaXNrcy1hbmQtcmVtZWRpZXMtZm9yLWJsYWNrLWJveC1hcnRpZmljaWFsLWludGVsbGlnZW5jZS_SAQA?oc=5,Risks and Remedies for Black Box Artificial Intelligence - C3 AI,2020-08-31,C3 AI,https://c3.ai,N/A,N/A,There is no denying that artificial intelligence (AI) and machine learning technologies will shape our future. Advanced AI systems can solve the most complex business problems and create significant economic benefits for organizations. But coupling such systems with poor interpretability – also referred to as explainability – can create a recipe for disaster. End users... Read more »,N/A,https://schema.org,"[{'@type': 'WebPage', '@id': 'https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/', 'url': 'https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/', 'name': 'Risks and Remedies for Black Box Artificial Intelligence', 'isPartOf': {'@id': 'https://c3.ai/#website'}, 'primaryImageOfPage': {'@id': 'https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/#primaryimage'}, 'image': {'@id': 'https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://c3.ai/wp-content/uploads/2020/08/blog-hero-background-risks-and-remedies-for-black-box-artificial-intelligence.jpg', 'datePublished': '2020-08-31T18:49:54+00:00', 'dateModified': '2022-06-09T06:34:16+00:00', 'breadcrumb': {'@id': 'https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/#primaryimage', 'url': 'https://c3.ai/wp-content/uploads/2020/08/blog-hero-background-risks-and-remedies-for-black-box-artificial-intelligence.jpg', 'contentUrl': 'https://c3.ai/wp-content/uploads/2020/08/blog-hero-background-risks-and-remedies-for-black-box-artificial-intelligence.jpg', 'width': 1920, 'height': 850}, {'@type': 'BreadcrumbList', '@id': 'https://c3.ai/blog/risks-and-remedies-for-black-box-artificial-intelligence/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://c3.ai/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Blog', 'item': 'https://c3.ai/blog/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Risks and Remedies for Black Box Artificial Intelligence'}]}, {'@type': 'WebSite', '@id': 'https://c3.ai/#website', 'url': 'https://c3.ai/', 'name': 'C3 AI', 'description': '', 'publisher': {'@id': 'https://c3.ai/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://c3.ai/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://c3.ai/#organization', 'name': 'C3.ai', 'url': 'https://c3.ai/', 'sameAs': ['https://www.linkedin.com/company/c3-ai/', 'https://www.youtube.com/channel/UCrG82oiDaJa5-43cdfMYY5w', 'https://www.facebook.com/C3.ai.posts', 'https://twitter.com/C3_AI'], 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://c3.ai/#/schema/logo/image/', 'url': 'https://c3.ai/wp-content/uploads/2019/07/C3_Logo_500_black.jpg', 'contentUrl': 'https://c3.ai/wp-content/uploads/2019/07/C3_Logo_500_black.jpg', 'width': 500, 'height': 278, 'caption': 'C3.ai'}, 'image': {'@id': 'https://c3.ai/#/schema/logo/image/'}}]",N/A,N/A,"  There is no denying that artificial intelligence (AI) and machine learning technologies will shape our future. Advanced AI systems can solve the most complex business problems and create significant economic benefits for organizations. But coupling such systems with poor interpretability – also referred to as explainability – can create a recipe for disaster. End users often have no insight into how complex AI systems work, so they are commonly viewed as black boxes. They take information and parameters as inputs, perform calculations on them in an unknown fashion, and generate predictions, recommendations, and classifications that do not always make sense to end users. The black box phenomenon can lead to issues such as unrealistic expectations for AI capabilities, poorly informed decision making, or a lack of overall trust in AI systems. These issues can jeopardize adoption across an organization and potentially cause AI implementations to fail altogether. A recent survey by IDC shows that most global organizations report some failures among their AI projects – a quarter of them report up to a 50 percent failure rate – citing as contributing factors such things as the black box phenomenon and interpretability challenges. Despite these failures, many organizations continue to rely on AI to drive business decisions in the belief that it can exceed human capabilities. Examples include predicting asset and equipment failure in advance, optimizing supply chains and inventory, and detecting fraudulent activity. Organizations place big bets on AI to help solve business problems. What executives don’t realize is that it can adversely affect their business and end users when AI systems are not interpretable. In this article, we outline the issues that can be addressed through implementing interpretable AI, the challenges to AI interpretability, and how the C3 AI® Platform supports AI interpretability. Issues Interpretable AI Can Help Address Interpretability provides the reasoning behind AI-generated predictions or recommendations to allow users to discern whether AI is “right for the right reasons.” Under this umbrella, interpretability provides benefits and reduces the risk of failure caused by the black box phenomenon on three fronts:  Interpretability sheds light on the inner workings of black-box AI systems. Interpretability augments the predictions and recommendations already provided, spelling out additional insights. Interpretability helps identify potential issues with how the AI system was designed.  Here are some issues and key questions interpretable AI can help address: Is your system learning the correct objective? Interpretability is critical to ensure that AI systems are being asked to address the correct business objectives. If an AI system has poorly defined objectives, it will learn the wrong criteria, leading to unwanted outcomes and decisions. In one case, researchers applied AI to simulate a mechanism that would help decelerate fighter jets as they landed on an aircraft carrier. The AI system counterintuitively exploited the fact that maximizing the aircraft’s landing force – essentially crashing it into the carrier’s deck – would lead to an overflow and flip the speed instantly to zero, thus perfectly solving for the deceleration problem. But this only optimized for stopping the aircraft as quickly as possible while disregarding aircraft safety altogether, that clearly was meant to be a critical component of the broader objective function. Using interpretability, researchers could uncover the system’s inner workings, identify the fact that it was using the wrong objective function, and ultimately correct this problem. Is your system trustworthy? AI interpretability is key to developing user trust and is an integral part of change management across organizations. To develop trust in AI systems, users expect the systems to establish logical relationships between the input data and the resulting predictions. Interpretability is paramount in presenting and describing the relationships and associations between the input data and model output. Interpretable models showcase where predictive power comes from, allowing subject matter experts to determine whether that association is logically correct according to their human knowledge and experience. In one example, researchers exploring intelligible models for health care developed a model to predict pneumonia risk across a group of patients. Through interpretability, researchers identified that the AI model counterintuitively correlated having asthma with a lower risk of dying from pneumonia. Upon investigation, researchers found that patients with asthma tended to receive treatment earlier, hence lowering the risk of severe pneumonia. The AI system did not know about this interrelation. Researchers improved the model by adding additional features, subsequently demonstrating that a positive correlation exists between the level of treatment a patient receives and the lower risk of death caused by pneumonia. This AI system was untrustworthy until interpretability surfaced illogical discrepancies between the input data and model predictions, ultimately helping to fix it. Does your system bring new insights? Intelligible models performing beyond human capabilities can identify previously unforeseen patterns, motivating user adoption of AI. This was seen when AlphaGo beat top world players in a Go tournament. AlphaGo showcased what some described as beautiful and creative moves that Go players now are trying out as new strategies. Interpretability helps explain how such strategies or knowledge came to be, leading to fresh takes and improved capabilities for existing solutions. Is your system ethical? AI interpretability is an essential part of ethical AI, ensuring auditability and traceability of outcomes. Auditing model decisions allows assessment of liability and avoids legal or ethical problems that can have a huge impact on people’s lives. For example, in January 2020, the Detroit Police Department deployed a facial recognition system to identify a thief. A suspect was arrested and held for 30 hours based solely on the system’s recommendation. Charges were dropped after later comparisons between the security camera footage used and the suspect made it clear it was a false positive. If the police had been told that the identification was based only on a skin color match, the improper arrest may not have occurred. Incidents such as this will lead to needed strict ethical guidelines for AI systems, as already is being seen in the EU’s recent GDPR legislation, that requires any decision-making system to provide citizens with an explanation. Researchers have proposed some high-level criteria for successfully integrating interpretability:  The interpretable AI system needs to showcase what factors led to the outcomes – in other words, what data led to the prediction or recommendation.  The interpretable AI system must permit ongoing effective control through interactive machine learning such as adding new training examples or correcting erroneous labels.  Challenges to AI Interpretability The first challenge comes from potential conflicts between predictive and descriptive accuracy within AI-enabled solutions. Improving descriptive accuracy – interpretability – can come at the expense of predictive accuracy and, therefore, model performance.  Simple AI systems that use linear regression or tree-based models are more interpretable and understandable than their more complicated, deep learning counterparts, but simpler systems tend to provide worse predictive accuracy. This often leads to a trade-off in AI system design. But both interpretability and predictive accuracy can be achieved using model-based and post-hoc approaches. The model-based approach improves descriptive accuracy by: Favoring simpler models  The C3 AI Readiness application helps improve the mission capability and readiness of military aircraft, such as helicopters. In one deployment of C3 AI Readiness, the application predicts when specific parts of the aircraft are at risk of failure – hence require maintenance – using gradient-boosted trees. For model training, we leverage data from maintenance logs and onboard sensors. By using a tree-based model – with high interpretability – maintainers now have more targeted information to troubleshoot aircraft and prioritize maintenance. In another case, we configured the C3 AI Predictive Maintenance application to reduce downtime in a drug manufacturing plant by predicting failure of clean-in-place (CIP) systems. These complicated pieces of equipment show little consistency over time, making it hard to model normal behavior. In this example, the application predicts the risk of failure after each cycle level using linear models. For model training, we construct sensor-based features; instead of using raw sensor values, we measure the relative deviation from the expected set points. Thanks to model interpretability, technicians can quickly check specific problematic parts to reduce the risk of overall machine failure.  Favoring features that rely on domain-specific and business-driven rules  In the C3 AI Readiness example mentioned earlier, the critical step was finding the right representation of the input data. Working with 1,800 features from disparate sensor and maintenance systems, the team determined that crafted features – some as simple as time-since-last-maintenance or average turbine speed – drove model performance while algorithm hyperparameter optimization provided only incremental gains. Within the C3 AI Inventory Optimization application, supply chain customers can designate the optimal time and quantity buffer to be used for future procurement of certain SKUs within specific facilities. Because the application employs a simulation-based optimization approach, it is heavily dependent on domain knowledge and the physical dynamics of a supply chain. In order to maintain the optimization’s robustness while still providing sufficient insights to end users, the different uncertainty sources are perturbated, causing some variation in the output. This analysis of the sensitivity caused by the perturbations directly helps to derive the importance of each uncertainty factor.  The post-hoc approach explains predictions and recommendations through generating feature contributions. This is typically accomplished using third-party libraries that either A) apply a surrogate model to approximate the predictions of the underlying model, or B) directly extract contributions from the model structure itself (for example, decision paths in tree-based methods or model parameters in linear regression algorithms). We describe some of these frameworks in the final section of this post.  A second challenge of interpretable AI systems is that they need to build trust at two levels: For the model itself, known as global interpretability, and for each individual prediction, known as local interpretability. The former prevents biased models, while the latter guarantees actionable and sensical predictions. Data scientists can use these considerations to evaluate their AI systems in accordance with the needs of end users.  This figure illustrates when global and local interpretability are most needed as the human perception of problem complexity and the trust needed in the system vary. Global interpretability always plays an important role because AI developers need to ensure that the model is learning the right objective and is unbiased. Local interpretability is critical when problems are hard to solve, and when humans need to be in the loop to make high-stakes decisions. This is the case in anti-money-laundering applications, where large amounts of money are involved, or in precision medicine, where people’s lives are at stake. Each use case has its own unique needs and requires thorough examination to see what works best. Interpretability on the C3 AI Platform The C3 AI Platform leverages some of the widely used and most impactful post-hoc interpretability frameworks during development and production stages of AI systems:  TreeInterpreter ELI5 (Explain Like I’m 5) SHAP (SHapley Additive exPlanations)  These commonly used techniques can be configured using ML Pipelines in the C3 AI ML Studio, a low-code, lightweight interface for configuring multi-step machine learning models. Users can call a single and consistent API to interpret model predictions and do not need to worry either about specific interfaces or writing custom code. These techniques are interchangeable, increasing user flexibility to pick the best diagnosis tool. TreeInterpreter is an interpretability framework focused on tree-based models such as random forest. This framework allows for the decomposition of decision trees into a series of decision paths from the root of the tree to the leaf. This makes interpretation of the model’s output straightforward, given the inherent interpretability of a decision tree. For example, we applied TreeInterpreter in a predictive maintenance application to detect boiler leakages in power plants. The underlying model in this AI system is a random forest classifier that predicts the likelihood of leakage. The features represent sound intensities of microphone sensors located at different points on the boiler. The spatial coordinates of the microphones are weighted with the corresponding feature contributions of each prediction to localize the potential leakage. Maintainers are preventing days of downtime by inspecting specific sections of the boiler to confirm and fix leakages more quickly. ELI5 is another interpretability framework that supports widely used machine learning libraries such as scikit-learn, XGBoost, and LightGBM. We have leveraged ELI5 in financial services, helping financial institutions detect and prevent money laundering. Here we used a gradient-boosted classifier to learn suspicious patterns in transactions from historical cases. For regulatory and transparency reasons, our application needs to provide compliance investigators with prediction-level interpretability insights to help them make more informed decisions about client risk. Low-level features are usually neither useful nor actionable, so we constructed mapping to meaningful, high-level categories that aggregate individual feature contributions into money laundering typologies and scenarios such as shell accounts, trade finance, or structuring. This drove faster user adoption and supported regulatory approval. Lastly, the SHAP framework is the most general approach. We used SHAP in a securities lending application to predict securities that would be shorted by the bank’s clients. Automatically and quickly estimating client demand increased inventory visibility for each stock, allowing the bank to broker more deals and increase revenue. The feature contributions helped build confidence in the machine learning model amongst trading desk personnel, who are now better equipped to issue shares of securities as loans to clients. AI interpretability is a hot research area today and is evolving fast. At C3.ai, we contribute to global research and make use of the latest interpretability frameworks across our software stack to deliver value and trustworthy AI applications for our clients.   About the authors Ryan Compton joined C3.ai as a senior data scientist in the spring of 2020. He has worked on projects ranging from financial securities lending to assembly yield optimization. He earned his PhD in computer science from the University of California, Santa Cruz. Romain Juban is a principal data scientist at C3.ai and has worked on multiple projects across several industries (utilities, oil and gas, aerospace and defense, banking, and manufacturing) since 2014. He earned his bachelor’s in applied mathematics from Ecole Polytechnique in France and his master’s in civil and environmental engineering from Stanford University. Sina Pakazad is a lead data scientist at C3.ai. Since 2017, he has worked with different problems involving predictive maintenance, anomaly detection, optimization, and decision making under uncertainty within a variety of industries, including financial services, oil and gas, process industry and manufacturing. Before C3.ai, he was an experienced researcher at Ericsson Research in Stockholm. He received his master’s in systems, control and mechatronics from Chalmers University in Gothenburg, Sweden, and his PhD in automatic control from Linköping University in Sweden.       ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiZ2h0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvY2FsdW1jaGFjZS8yMDIwLzA4LzMxL3RoZS1pbXBhY3Qtb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Utb24td29ya3NwYWNlcy_SAQA?oc=5,The Impact of Artificial Intelligence on Workspaces - Forbes,2020-08-31,Forbes,https://www.forbes.com,Artificial intelligence will change everything - even big office real estate,"AI,Artificial Intelligence,Workspaces,Offices",Artificial intelligence will change everything - even big office real estate,Artificial intelligence will change everything - even big office real estate,http://schema.org,,AI,N/A,"More From ForbesJul 16, 2024,09:30am EDTIn Superconvergence, Jamie Metzl Unravels AI MysteriesJul 15, 2024,09:30pm EDTAnswering Your Most Frequently Asked Questions (FAQs) About Artificial Intelligence In Honor Of National AI Appreciation DayJul 15, 2024,06:06pm EDTNot Just A Maker Space: Fab Labs Spark Innovation WorldwideJul 15, 2024,02:57pm EDTIBM InstructLab And Granite Models Revolutionizing LLM TrainingJul 15, 2024,09:42am EDTHow Generative AI Is Driving HyperpersonalizationJul 15, 2024,08:00am EDTThe Clever ‘Rephrase And Respond’ Prompting Strategy Provides Big Payoffs For Prompt EngineeringJul 13, 2024,09:00am EDTBig Tech Involvement With OpenAI Sparks Unease Among RegulatorsEdit StoryForbesInnovationAIThe Impact of Artificial Intelligence on WorkspacesCalum ChaceContributorOpinions expressed by Forbes Contributors are their own.""The AI guy""FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got itAug 31, 2020,12:00pm EDTUpdated Aug 31, 2020, 12:00pm EDTThis article is more than 3 years old.Share to FacebookShare to TwitterShare to LinkedinIntelligent and intelligible office buildingsgetty
Big buildings move slowly
It is a truth universally acknowledged that artificial intelligence will change everything. In the next few decades, the world will become intelligible, and in many ways, intelligent. But insiders suggest that the world of big office real estate will get there more slowly - at least in the world’s major cities.


The real estate industry in London, New York, Hong Kong and other world cities moves in cycles of 10 or 15 years. This is the period of the lease. After a tense renewal negotiation, and perhaps a big row, landlord and tenant are generally happy to leave each other alone until the next time. This does not encourage innovation, or investment in new services in between the renewals. There are alternatives to this arrangement. In Scandinavia, for instance, lease durations are shorter - often three years or so. This encourages a more collegiate working relationship, where landlord and tenant are more like business partners.

Another part of the pathology of major city real estate is the landmark building. With the possible exception of planners, everyone likes grand buildings: certainly, architects, developers, and the property managers and CEOs of big companies do. A mutual appreciation society is formed, which is less concerned about the impact on a business than about appearing in the right magazines, and winning awards.

PROMOTED
Bottom-up demand
Outside the big cities, priorities are different. To attract a major tenant to Dixons’ old headquarters in Hemel Hempstead, for instance, the landlord will need to seduce with pragmatism rather than glamour.

Tim Oldman is the founder and CEO of Leesman, a firm which helps clients understand how to manage their workspaces in the best interests of their staff and their businesses. He says there is plenty of opportunity for AI to enhance real estate, and much of the impetus for it to happen will come from the employees who work in office buildings rather than the developers who design and build them. Employees, the actual users of buildings, will be welcoming AI into many corners of their lives in the coming years and decades, often without realising it. They will expect the same convenience and efficiency at work that they experience at home and when travelling. They will demand more from their employers and their landlords.
MORE FROMFORBES ADVISORBest High-Yield Savings Accounts Of September 2023ByKevin PayneContributorBest 5% Interest Savings Accounts of September 2023ByCassidy HortonContributor
Christina Wood is responsible for two of Emap's conferences on the office sector: Property Week's annual flagship event WorkSpace, and AV Magazine's new annual event AVWorks, which explores the changing role of AV in the workspace. She says that “workspaces are undergoing an evolution that increasingly looks like a revolution, powered by technology innovation and driven by workforce demands for flexibility, connectivity, safety and style.”









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



Responsive buildings
Buildings should be smart, and increasingly they will be. Smart buildings will be a major component of smart cities, a phenomenon which we have been hearing about since the end of the last century, and which will finally start to become a reality in the coming decade, enabled in part by 5G.


1/100:01Forbes Innovation





Skip Ad
 
Continue watchingCall Of Duty Black Ops 6 Beta Dates And More Revealedafter the adVisit Advertiser websiteGO TO PAGE
Buildings should know what load they are handling at any given time. They should provide the right amount of heat and light: not too little and not too much. The air conditioning should not go off at 7pm when an after-hours conference is in full flow. They should monitor noise levels, and let occupants know where the quiet places are, if they ask. They should manage the movement of water and waste intelligently. All this and much more is possible, given enough sensors, and a sensible approach to the use of data.
Imagine we are colleagues who usually work in different buildings. Today we are both in the head office, and our calendars show that we have scheduled a meeting. An intelligent building could suggest workspaces near to each other. Tim Oldman calls this “assisted serendipity”.
Generation Z is coming into the workplace. They are not naive about data and the potential for its mis-use, but they are more comfortable with sharing it in return for a defined benefit. Older generations are somewhat less trusting. We expect our taxi firm to know when we will be exiting the building, and to have a car waiting. But we are suspicious if the building wants to know our movements. Employees in Asian countries show more trust than those in France and Germany, say, with the US and the UK in between.
RPA and helpdesks
Robotic process automation, or RPA, can make mundane office interactions smoother and more efficient. But we will want it to be smart. IT helpdesks should not be rewarded for closing a ticket quickly, but for solving your problem in a way which means you won’t come back with the same problem a week later – and neither will anyone else.
That said, spreadsheet-driven efficiency is not always the best solution. Face-to-face “genius bar”-style helpdesks routinely deliver twice the level of customer satisfaction as the same service delivered over the phone, even when they use exactly the same people, the same technology, and the same infrastructure. There is a time and place for machines, and a time and a place for humans.
Predictive maintenance 
Rolls Royce is said to make more money from predictive maintenance plans than it makes by selling engines. Sensors in their engines relay huge volumes of real-time data about each engine component to headquarters in Derby. If a fault is developing, they can often have the relevant spare part waiting at the next airport before the pilot even knows there’s a problem. One day, buildings will operate this way too.
The technology to enable these services is not cheap today, and an investment bank or a top management consultancy can offer their employees features which will not be available for years to workers in the garment industry in the developing world. There will be digital divides, but the divisions will be constantly changing, with laggards catching up, and sometimes overtaking, as they leapfrog legacy infrastructures. China is a world leader in smartphone payment apps partly because its banking infrastructure was so poor.
Accelerated by Covid
Covid will bring new pressure to bear on developers and landlords. Employees will demand biosecurity measures such as the provision of air which is fresh and filtered air, not re-circulated. They may want to know how many people are in which parts of the building, to help them maintain physical distancing. This means more sensors, and more data.
The great unplanned experiment in working from home which we are all engaged in thanks to covid-19 will probably result in a blended approach to office life in the future. Working from home suits some people very well, reducing commuting time, and enabling them to spend more time with their families. But others miss the decompression that commuting allows, and many of us don’t have good working environments at home. In the winter, many homes are draughty, and the cost of heating them all day long can be considerable.
Tim Oldman thinks the net impact on demand for office space will probably be a slight reduction overall, and a new mix of locations. There are indications that companies will provide satellite offices closer to where their people live, perhaps sharing space with workers from other firms. This is the same principle as the co-working facilities provided by WeWork and Regus, but whereas those companies have buildings in city centres, there will be a new demand for space on local High Streets.
Retail banks have spotted this as an opportunity, a way of using the branch network which they have been shrinking as people shift to online banking. Old bank branches can be transformed into safe and comfortable satellite offices, and restore some life to tired suburban streets. Companies will have to up their game to co-ordinate this more flexible approach, and landlords will need to help them. They will need to collect and analyse information about where their people are each day, and develop and refine algorithms to predict where they will be tomorrow.
Some employers will face a crisis of trust as we emerge from the pandemic. Millions of us have been been trusted to work from home, and to the surprise of more than a few senior managers, it has mostly worked well. Snatching back the laptop and demanding that people come straight back to the office is not a good idea. Companies will adopt different approaches, and some will be more successful than others. Facebook has told its staff they can work from wherever they want, but their salary will be adjusted downwards if they leave the Bay Area. Google has simply offered every employee $1,000 to make their home offices more effective.
The way we work is being changed by lessons learned during the pandemic, and by the deployment of AI throughout the economy. Builders and owners of large office buildings must not get left behind.
Follow me on Twitter or LinkedIn. Check out my website or some of my other work here. Calum ChaceFollowingFollowCalum Chace is a keynote speaker, and a best-selling author on artificial intelligence. Calum argues that, in the course of this century, AI will... Read MoreEditorial StandardsPrintReprints & Permissions",BreadcrumbList,https://www.forbes.com/sites/calumchace/2020/08/31/the-impact-of-artificial-intelligence-on-workspaces/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/specials-images/imageserve/5f4d1be7cdfe2e7803a8595c/0x0.jpg?format=jpg&crop=5689,3299,x810,y0,safe&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Calum Chace', 'url': 'https://www.forbes.com/sites/calumchace/', 'description': 'Calum Chace is a keynote speaker, and a best-selling author on artificial intelligence. Calum argues that, in the course of this century, AI will change pretty much everything about being human. Calum’s book ”The Economic Singularity"" addresses the coming wave of cognitive automation; ”Surviving AI"" looks further ahead to the arrival of superintelligence; and ""Pandora\'s Brain"" is a novel about the first superintelligence on earth. Before becoming a full-time speaker and author, Calum had a 30-year career in business and journalism, working for the BBC, BP, and KPMG among others. Before that, he studied philosophy at Oxford, and was delighted to discover that science fiction is philosophy in fancy dress.', 'sameAs': ['https://www.linkedin.com/in/calum-chace-bb68168/', 'https://www.twitter.com/cccalum', 'https://calumchace.com/']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",The Impact of Artificial Intelligence on Workspaces,2020-08-31T12:00:05-04:00,2020-08-31T12:00:05-04:00,AI,The Impact of Artificial Intelligence on Workspaces,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'AI', 'item': 'https://www.forbes.com/ai/'}]",,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiO2h0dHBzOi8vYmVjb21pbmdodW1hbi5haS9haS10aGUtZnV0dXJlLW9mLXdvcmstYTkwYjE0MzhjYzVi0gEA?oc=5,AI & The Future of Work. Will you still have a job in the… | by Raj Hayer - Becoming Human: Artificial Intelligence Magazine,2020-08-31,Becoming Human: Artificial Intelligence Magazine,https://becominghuman.ai,"This is where the journey had to inevitably end up. Artificial Intelligence (“AI”) is interesting and all, but what does it mean to me personally? Will I still have a job? What does the future look…",N/A,Will you still have a job in the future?,Will you still have a job in the future?,http://schema.org,,N/A,N/A,"AI & The Future of WorkWill you still have a job in the future?Raj Hayer·FollowPublished inBecoming Human: Artificial Intelligence Magazine·7 min read·Aug 31, 2020102ListenShareWhat is the Future of AI — MediumThis is where the journey had to inevitably end up. Artificial Intelligence (“AI”) is interesting and all, but what does it mean to me personally? Will I still have a job? What does the future look like?Truth is, no one knows for sure. But there are a few indicators that help us envision where the Future of Work may lead when it comes to AI. What does AI mean to our livelihoods?AI in the Future“The Role Digital Plays in the HR IT Landscape “— Future of Work MediaWhat jobs will AI replace?Some jobs are relatively easily replaced by AI, not because they are easy tasks, but more so because they are formulated and run by a set of rules and regulations, which can be learned from, adapted to, and used by an AI.There is lots of speculation on which roles will be replaced, and perhaps some are far fetched, but I reviewed a few legitimate and realistic options that I found on HubSpot.Telemarketing — unfortunately, this role already feels like it has been taken over by robots! Surprisingly though, even with automation, the conversion rate is barely impacted.Bookkeeping Clerks — cheaper equivalents such as Quickbooks or Microsoft office software does a lot of the work for you, and there are standardized bookkeeping practices for the AI to follow.Compensation and Benefits Managers — automation here is being increasingly adopted as companies across the globe need to reduce costs.Receptionists — there have been scheduling applications and automated message receivers for years, but the number of businesses operating online, along with the reduced in-person office operations for this year and 2021, the need for this role has dramatically decreased.Couriers — it seems only a matter of time before drones and robots take over, and with automated driving, in test phases, this appears to be edging closer.AI JobsProofreaders — in my experience Grammarly and these types of applications can do the job with less error and more quickly than a person!Computer Support Specialists — with step-by-step guides, it will be easy to program software to answer simple questions. Hmm, machines helping to fix themselves, sound familiar?Market Research Analysts — technology can manage and organize more data than any human and therefore is capable of deriving insights more efficiently.Even in my limited knowledge, this list covers only a segment of jobs that are considered replaceable, and it is evident that there is an opportunity to simplify and replace the role responsibilities with automation or machine learning beyond these.The larger impact is however that some jobs are unlikely to even exist in the coming years because of reduced demand and the non necessity of them.Which jobs will be gone by 2030?Automation, AI, and self-determined processes have already begun to replace some of these roles per Career Addict.Job Loss from AI — ForbesWe have all influenced the transition or experienced the impact of the following:Travel Agents — most people book online themselves these days directly through the companies or hotel web services.Cashiers — it is common to use self-service checkouts, and some new retail models no longer have any personnel in store.Librarians — most content is online and while there is a great benefit to studying at the library for students, a self-check-in and check-out of the building can be automated and online books make re-filing unnecessary.Bank Tellers — most services are available through the ATMs and tellers are a residual vestigial service that we have continued for the elder demographic. Most business services are not available in branch and only through a central remote office.Textile Workers — machinery can replace even the most intricate work requirements these days.Postal Couriers, The Print Industry, Sports Referees and Umpires…There are roles that may well stay around, not from necessity, but as a vestigial and sentimental course of action, such as librarians or bank tellers. But eventually, anywhere there are rules and regulations for the AI to follow, job loss is likely to be experienced.Trending AI Articles:1. Microsoft Azure Machine Learning x Udacity — Lesson 4 Notes2. Fundamentals of AI, ML and Deep Learning for Product Managers3. Roadmap to Data Science4. Work on Artificial Intelligence ProjectsThe Future of WorkSo if these jobs are disappearing, the next question is, what happens to the people in these jobs?Funniest Movies — The GuardianWhile I am no expert in this field I would like to think it will not all be downhill. I want to believe there will be an opportunity within this fourth industrial revolution as there was after each previous industrial revolution.Industry 4.0 will bring new AI-specific roles and roles on the periphery of AI.Roles within the field of AIThere will inevitably be roles within the field of AI, and it is important to understand the AI career landscape before we can navigate it. According to Springboard job roles in the field of AI might include:Machine Learning EngineerRobotic ScientistData ScientistResearch ScientistBusiness Intelligence DeveloperThe knowledge and educational prerequisites will include skills and training that are readily available.Computer ScienceCoding expertise with popular programming languages such as Python, Java, Julia, Lisp, etc.Physics, engineering, and roboticsMathematicsAlgebra, calculus, logic and algorithms, probability, statisticsBayesian networking (including neural nets)Cognitive science theoryWe have considered these subjects only individually in the past; so there is sometimes not a clear route to a comprehensive or holistic understanding of AI. There are three top-tier online AI resources to learn from, including:Udacity — is an online platform with unlimited learning opportunities.MIT — continues to lead in the field of technology and development.Google AI — is launching its college certification program for in-demand roles.How Will the Fourth Industrial Revolution Impact the Future of Work — Change Recruitment GroupRoles on the periphery or edge of AIIf a role within AI does not appeal to you, then do not fear as there are possibilities for roles surrounding AI. One of which is related to business, and that is the role of an AI translator.Automation AI Robots Jobs — InsightsWhat is an AI translator?Nope, it’s not DeepL or a translation tool for languages! Let’s start with an understanding of the Analytics Translator.Analytics Translator — Product Owner with Data Science Skills — GoDataDrivenAn analytics translator enables the execution of your company’s AI strategy. Data engineers are good at developing…godatadriven.comAn Analytics Translator translates information from data scientists and professionals to help management teams make strategic and operational decisions.Mckinsey already offers comprehensive training information for the role of Analytics Translator.How to train your analytics translatorsAnalytics translators perform some of the essential functions for integrating analytics capabilities in a company…www.mckinsey.comAn AI Translator, on the other hand, can take on the role of translating how AI can be used to further business strategy and implementation. This role can be the bridge between the technology team and the C-level management team.Top 22 Best AI and Robotics Movies of All Time — New World AISure, there is still the possibility for AI to go the apocalyptic in the future, but for now, the future is still within our hands.The biggest learning from this 30-day journey is, we must be part of the conversation about AI, we must find our place within the journey, and help shape the Future of Work…or it will be shaped for us.Perhaps the Future of Work is not an either/or solution, but rather a combination of Human and Digital workers.For your company to get to the next level, at least in the next 20 years, someone has to connect the business to the AI — and that person could be you.Don’t forget to give us your 👏 !",NewsArticle,https://becominghuman.ai/ai-the-future-of-work-a90b1438cc5b,['https://miro.medium.com/v2/resize:fit:1200/1*V-B5nJYe9c0eAAJrzVZ98Q.jpeg'],"{'@type': 'Person', 'name': 'Raj Hayer', 'url': 'https://rajhayer.medium.com'}","{'@type': 'Organization', 'name': 'Becoming Human: Artificial Intelligence Magazine', 'url': 'becominghuman.ai', 'logo': {'@type': 'ImageObject', 'width': 146, 'height': 60, 'url': 'https://miro.medium.com/v2/resize:fit:292/1*1fYpRTTpKQNa0zuEPe3itg.png'}}",AI & The Future of Work - Becoming Human: Artificial Intelligence Magazine,2020-08-31T09:57:56.602Z,2022-03-30T21:09:38.651Z,,AI & The Future of Work - Becoming Human: Artificial Intelligence Magazine,,,2020-08-31T09:57:56.602Z,a90b1438cc5b,['Raj Hayer'],https://becominghuman.ai/ai-the-future-of-work-a90b1438cc5b,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVGh0dHBzOi8vbmV3cy50dWxhbmUuZWR1L3ByL3R1bGFuZS1yZXNlYXJjaGVyLXJlY29nbml6ZWQtd29yay1hcnRpZmljaWFsLWludGVsbGlnZW5jZdIBAA?oc=5,Tulane researcher recognized for work in artificial intelligence - Tulane University,2020-08-31,Tulane University,https://news.tulane.edu,"Nicholas Mattei, an assistant professor of computer science who specializes in artificial intelligence, is the recipient of two recent awards. Mattei received an IBM University Research Award for research that will help the next generation of AI technologies to learn, adapt and decide in complex environments.  He was also selected as one of only 14 researchers in the world to deliver an Early Career Spotlight Talk at the International Joint Conference on AI (IJCAI), to be held in Kyoto, Japan (or virtually) in January 2021.",N/A,Tulane University News and Press Releases,N/A,,,N/A,N/A,"

Tulane researcher recognized for work in artificial intelligence


August 31, 2020 10:45 AM


 | 

Barri Bronston

bbronst@tulane.edu


  

View PDF





 



	                            		 Nicholas Mattei, an assistant professor of computer science at Tulane University, focuses much of his research on ethics in artificial intelligence. (Photo by Paula Burch-Celentano)
	                        		


Nicholas Mattei, an assistant professor of computer science who specializes in artificial intelligence, is the recipient of two recent awards.
Mattei received an IBM University Research Award for research that will help the next generation of AI technologies to learn, adapt and decide in complex environments.  He was also selected as one of only 14 researchers in the world to deliver an Early Career Spotlight Talk at the International Joint Conference on AI (IJCAI), to be held in Kyoto, Japan (or virtually) in January 2021.
The one-year IBM University Award, valued at $40,000, will allow Mattei and his team to advance their research work on providing machines with the capability to combine features of symbolic and formal reasoning with data interpretation and machine learning in order to make better, more transparent, and ethical decisions.
While modern AI systems have demonstrated amazing performance in a number of domains including image processing, text translation, and simple question answering, Mattei said, many of the systems developed for these applications are narrow in scope and often don’t leverage multiple types of reasoning and learning for particular applications.
“The complex tasks that we often ask modern AI decision making algorithms to perform, expose a number of limitations of current AI systems including the lack of deep understanding of information coming from data, the absence of common sense reasoning, the difficulty in dealing with causality, and the inability to learn general concepts from small amounts of data,” he said.
Mattei said the goal of this project is to advance basic research in computer science that will hopefully have impacts in a variety of areas that rely on AI enabled decision making.
At the IJCAI Conference in Japan, Mattei will present his paper “Closing the Loop: Bringing Humans into Empirical Computational Social Choice and Preference Reasoning.”
Presenters are by invitation based on nominations from the IJCAI program committee. The organization started the Early Career Spotlight track in 2016 to introduce the work of some of the world’s most active early career researchers in AI.
“The paper is a summary of my work in what’s called preference reasoning and computational social choice.  In preference reasoning we want to understand, mathematically, how to model human preferences in terms of desires, constraints, complementarities, and other features.
“We then use these formal models of preference to feed into algorithms for group decision making / social choice. For instance, selecting matchings for ride sharing, allocating things like computing resources to individuals, or even picking proposals to fund.”




Other Related Articles


AI professor at Tulane wins CAREER award from National Science Foundation 
Tulane receives $2 million gift to establish faculty chair in AI at School of Science and Engineering 
Office of the Provost and Innovation Institute fund three $50,000 technology development projects 
Tulane part of record-setting $160 million grant to transform Louisiana’s energy industry  
Five fun facts about cicadas, a ‘biological phenomenon to be relished’ 





",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMihAFodHRwczovL3d3dy5uZXdzd2lzZS5jb20vYXJ0aWNsZXMvd2ljaGl0YS1zdGF0ZS1qb2lucy1wcmVzdGlnaW91cy1uYXRpb25hbC1yZXNlYXJjaC1pbnN0aXR1dGUtdG8tYm9vc3QtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtZmllbGTSAYIBaHR0cHM6Ly9tLm5ld3N3aXNlLmNvbS9hcnRpY2xlcy93aWNoaXRhLXN0YXRlLWpvaW5zLXByZXN0aWdpb3VzLW5hdGlvbmFsLXJlc2VhcmNoLWluc3RpdHV0ZS10by1ib29zdC1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1maWVsZA?oc=5,Wichita State joins prestigious national research | Newswise - Newswise,2020-08-27,Newswise,https://www.newswise.com,"Wichita State University has been named a founding member of a newly formed AI Institute for Foundations of Machine Learning (IFML), established by a $20 millio",['AI;Artificial Intelligence;Research;National Science Foundation (NSF) ;Transportation;Entertainment;Healthcare;Diversifying economy;University Of Texas At Austin;theoretical machine learning'],"Wichita State University has been named a founding member of a newly formed AI Institute for Foundations of Machine Learning (IFML), established by a $20 m",N/A,https://schema.org,,N/A,N/A,"Newswise — Wichita State University has been named a founding member of a newly formed AI Institute for Foundations of Machine Learning (IFML), established by a $20 million grant from the National Science Foundation.Machine learning is the technology that allows computers to acquire knowledge and make predictions in complex environments. This technology has the potential to transform everything from transportation to entertainment to health care.The institute is one of five that the NSF announced Wednesday as part of a $100 million, five-year, national investment in the advancement of artificial intelligence research and workforce development.“In effect, over the next five years, some of the best minds in the country will be tackling some of the grandest challenges that we face, both in terms of new AI techniques as well as breakthroughs in fields of science and engineering and sectors of our economy,” said Michael Kratsios, U.S. Chief Technology Officer. “And along the way, they will nurture the future American workforce in AI research and practice.”Dr. Jay Golden, president of Wichita State, says this partnership and the research that will come from it are a direct fit with WSU’s goal of focusing on technology’s key role in diversifying the economy and further exemplifies our growing reputation as a national leader for innovation and research.“This institute is an important early milestone for our newly launched National Institute for Digital Transformation and will serve as an important driver in our priorities to diversify Kansas' economy and provide applied learning opportunities and careers for our students.""Dr. Kaushik Sinha, an associate professor in the Department of Electrical Engineering and Computer Science at Wichita State, will work on a team that includes researchers from the University of Texas, Austin, where the institute will be based; the University of Washington, and Microsoft Research.Sinha was invited to join the team in its early stage of formation because of his expertise in theoretical machine learning. This field includes research in next-generation algorithms for deep learning, neural architecture optimization, and efficient robust statistics.“The primary focus of the AI Institute for Foundations of Machine Learning is to solve some of these fundamental questions over the next five years to ensure continued success of Machine Learning and AI over the next decade,” Sinha said.“Dr. Sinha has been important in forming our computer science, computing and data science degrees,” said Dr. Gergely Záruba, chair of WSU’s Electrical Engineering and Computer Science Department. “This work will enable us to focus even more efforts on providing cutting-edge education and research-exposure to our undergraduate and graduate students.”Research opportunities for undergrad, grad studentsWichita State’s share of the grant is $700,000 over five years. According to Sinha, the grant will create many opportunities for students to participate in research. Graduate students will be funded from this grant and will work under the supervision of Sinha on the foundational machine learning research problems. Undergraduate students will have the opportunity to participate in theoretical or empirical research projects directed by faculty at their respective universities for a semester or a year.“Expanding our efforts in computing, data science and artificial intelligence is one of my biggest priorities,” said Dr. Dennis Livesay, dean of the College of Engineering at Wichita State University. “Dr. Sinha’s accomplishment, along with other recent computing-related research grants, highlights that our faculty and programs are garnering significant national recognition.”Once a year, IFML researchers will meet for a week-long retreat in Kansas where they will deliver public technical talks aimed at graduate students and faculty. Sinha believes this will provide Wichita state faculty and students the opportunity to learn about current technological advances in computer science and AI from world-renowned researchers.In addition, IFML will create opportunities for high school students to get exposure to AI as a discipline and career path. High schools often face challenges in providing upper-level electives in computer science because of the difficulty in recruiting qualified computer science teachers.To address this, IFML will develop a month-long mini course in “Introductory AI” that will be taught remotely by IFML-trained math and computer science teachers across Kansas, Texas and Washington. Five high school computer science/math teachers from the Wichita area will be invited annually to the Wichita State campus for a three-day summer workshop where the teachers will be given training in implementing the AI mini-course content. Sinha predicts this will ultimately boost enrollment in computer science undergraduate programs.Wichita State is distinctive for opening pathways to applied learning, applied research and career opportunities, alongside unsurpassed classroom, laboratory and online education. The university's beautiful 330-acre main campus is a supportive, rapidly expanding learn-work-live-play environment, where students gain knowledge and credentials to prepare for fulfilling lives and careers. Students enjoy a wide selection of day, evening and summer courses in more than 200 areas of study at the main campus and other locations throughout the metro area and online. WSU's approximately 16,000 students come from every state in the U.S. and more than 100 other countries. Wichita State's Innovation Campus is an interconnected community of partnership buildings, laboratories and mixed-use areas where students, faculty, staff, entrepreneurs and businesses have access to the university's vast resources and technology. For more information, follow us on Twitter at www.twitter.com/wichitastate and Facebook at www.facebook.com/wichita.state.# # # # #",NewsArticle,https://www.newswise.com/articles/wichita-state-joins-prestigious-national-research-institute-to-boost-artificial-intelligence-field,"{'@type': 'ImageObject', 'url': 'https://www.newswise.com/legacy/image.php?image=/images/uploads/2022/01/28/download 1.jpg', 'width': 1200, 'height': 600}","{'@type': 'Organization', 'name': 'Wichita State University', 'identifier': '2650', 'url': 'https://www.newswise.com/institutions/newsroom/2650', 'description': '', 'image': {'@type': 'ImageObject', 'url': 'https://www.newswise.com/legacy/image.php?image=/images/institutions/logos/WSU_logo.jpg'}}","{'@type': 'Organization', 'name': 'Newswise', 'logo': {'@type': 'ImageObject', 'url': 'https://www.newswise.com/assets/new/img/Newswise-Logo.png'}}",Wichita State joins prestigious national research  | Newswise,2020-08-27,2020-08-27,,,,,,,,"{'@type': 'WebPage', '@id': 'https://www.newswise.com/articles/wichita-state-joins-prestigious-national-research-institute-to-boost-artificial-intelligence-field'}",https://www.newswise.com/legacy/image.php?image=/images/uploads/2022/01/28/download 1.jpg,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifGh0dHBzOi8vd3d3LmZkYS5nb3YvbmV3cy1ldmVudHMvZmRhLXZvaWNlcy9pbXBvcnQtc2NyZWVuaW5nLXBpbG90LXVubGVhc2hlcy1wb3dlci1kYXRhLWFuZC1sZXZlcmFnZXMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2XSAQA?oc=5,Import Screening Pilot Unleashes the Power of Data - FDA.gov,2020-08-31,FDA.gov,https://www.fda.gov,FDA is leveraging our use of artificial intelligence as part of the FDA’s New Era of Smarter Food Safety initiative.,N/A,FDA is leveraging our use of artificial intelligence as part of the FDA’s New Era of Smarter Food Safety initiative. ,FDA is leveraging our use of artificial intelligence as part of the FDA’s New Era of Smarter Food Safety initiative. ,https://schema.org,"[{'@type': 'Article', 'headline': 'Import Screening Pilot Unleashes the Power of Data and Leverages Artificial Intelligence', 'name': 'Import Screening Pilot Unleashes the Power of Data and Leverages Artificial Intelligence', 'description': 'FDA is leveraging our use of artificial intelligence as part of the FDA’s New Era of Smarter Food Safety initiative.', 'image': {'@type': 'ImageObject', 'representativeOfPage': 'True', 'url': 'https://www.fda.gov/files/Seafood-Safety-New-Era-AI-Pilot-Aug2020-FDA-Voices-1600x900.png'}, 'datePublished': 'Tue, 03/08/2022 - 13:50', 'dateModified': 'Mon, 08/31/2020 - 00:00', 'author': {'@type': 'Organization', 'name': 'Office of the Commissioner'}, 'publisher': {'@type': 'Organization', 'name': 'FDA'}}]",N/A,N/A,"



Import Screening Pilot Unleashes the Power of Data and Leverages Artificial Intelligence 

Subscribe to Email Updates


Share






        Post
      


Linkedin


Email


Print












Image
 












By: Stephen M. Hahn, M.D., Commissioner of Food and Drugs 
I frequently emphasize the importance of data in the U.S. Food and Drug Administration’s work as a science-based regulatory agency, and the need to “unleash the power of data” through sophisticated mechanisms for collection, review and analysis so that it may become preventive, action-oriented information. 
As one example of this commitment, I would like to tell you about cross-cutting work the agency is undertaking to leverage our use of artificial intelligence (AI) as part of the FDA’s New Era of Smarter Food Safety initiative. This work promises to equip the FDA with important new ways to apply available data sources to strengthen our public health mission. The ultimate goal is to see if AI can improve our ability to quickly and efficiently identify products that may pose a threat to public health. 

 

Stephen M. Hahn, M.D.

One area in which the FDA is assessing the use of AI is in the screening of imported foods. Americans want to enjoy a diverse and available food supply. They also want their food to be safe, whether it’s domestically produced or imported from abroad. 
So we launched a pilot program in the spring of 2019 to learn the added benefits of using AI, specifically machine learning (ML), in our import-screening processes. Machine learning is a type of AI that makes it possible to rapidly analyze data, automatically identifying connections and patterns in data that people or even our current rules-based screening system cannot see.  
The first phase of this pilot was a “proof of concept” to validate the approach we’re taking. We decided to test this approach on imported seafood to assess the utility of using AI/ML to better target seafood at the border that may be unsafe.  
Why seafood? Because the U.S. imports so much of it. Upwards of 94 percent of the seafood Americans consume each year is imported. 
Strengthening Our Predictive Capabilities, a Proof of Concept  
We embarked on the proof of concept by training the ML screening tool, using years of retrospective data from past seafood shipments that were refused entry or subjected to additional scrutiny, such as a field exam, label exam or laboratory analysis of a sample. This gave us an idea of how much our surveillance efforts might be improved using these technologies.  
The results are exciting, suggesting that this approach has real potential to be a tool that expedites the clearance of lower risk seafood shipments, and identifies those that are higher risk. In fact, this is great news. The proof of concept demonstrated that AI/ML could almost triple the likelihood that we will identify a shipment containing products of public health concern. 
The implementation team is now working to apply the AI/ML model algorithm to field conditions as part of the second phase of this work, an in-field pilot again focusing on imported seafood, and that’s where we are now. As part of the in-field pilot, the model will be applied to the screening methods used to help FDA staff decide which shipments to examine and will then provide information about which food in the shipment to sample for laboratory testing. We will then compare the results to the recommendations made by our current system. 
We see this opportunity as a critical step in the FDA employing the power of AI across the spectrum of product and process challenges facing the agency. Our initial proof of concept results indicate that such innovative approaches hold great promise in further strengthening protections for consumers.
Unleashing the Power of Data to Keep Americans Safe 
The pilot taps into two important new initiatives at the FDA. In addition to the New Era of Smarter Food Safety, it also reflects the priorities embodied in our Technology Modernization Action Plan – or TMAP.
On July 13, the FDA released a blueprint for the New Era of Smarter Food Safety outlining how the agency plans to leverage new technologies and approaches to create a more digital, traceable and safer food system. 
When we developed the blueprint, we knew that AI technology could be a game changer in expanding the FDA’s predictive analytics capabilities, enabling us to mine data to anticipate and mitigate foodborne risks. The pilot is revealing the specific, immediate benefits that this technology could have in helping us ensure the safety of imported foods. 
The TMAP describes important actions we are taking to modernize our technology information systems — computer hardware, software, data, analytics, advanced technology tools and more — in ways that accelerate the FDA’s pursuit of our public health mission.
Additionally, the plan lays out how the agency intends to transform our computing and technology infrastructure to position the FDA to close the gap between rapid advances in product and process technology and the technology solutions needed to ensure those advances translate into meaningful results for American consumers and patients. The TMAP provides a foundation for the development of the FDA’s ongoing strategy around data itself — a strategy for the stewardship, security, quality control, analysis and real-time use of data — that will illuminate the brightest path and the best tools for the FDA to enhance and promote public health.  
While both of these initiatives were well underway before the COVID-19 pandemic, lessons learned during this time of crisis have underscored the need for more real-time, data-driven approaches to protecting public health.
Scaling a Mountain of Data
The pilot also gives us the opportunity to learn how to untether the knowledge we need from the huge volume of data we have from screening millions of import shipments every year. In 2019, the FDA screened nearly 15 million food shipments offered for import into our country for sale to American consumers. Last year, the U.S. imported about 15% of the food we consume and that percentage continues to increase.
The FDA has a massive amount of data about these shipments and about the companies that are producing and processing the food, offering it for import, and selling it in the U.S. marketplace. In fact, every year the FDA collects tens of millions of data points on imports alone, and we screen all the data associated with every shipment of food against the information in our internal databases. One of the major goals of our pilot is to assess the ability of AI/ML to more quickly, efficiently, and comprehensively take advantage of all the data and information residing in our systems.  
In fact, we believe that we can use the knowledge that ML provides to know where best to concentrate our resources to find potentially unsafe products. In addition to improved import surveillance resources, the intelligence that ML can extract from the stores of data the FDA collects can also inform decisions about which facilities we inspect, what foods are most likely to make people sick and other risk prioritization questions.  
The bottom line is this: times and technologies change, and the FDA is changing with them, but the goal remains the same – to do everything in our power to strengthen the way we protect public health.




 Was this helpful?YesNo









Content current as of:
08/31/2020





Regulated Product(s)

Food & Beverages















                                        FDA Voices
                      












",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMieGh0dHBzOi8vd3d3LmRhaWx5c2FiYWguY29tL3BvbGl0aWNzL2xlZ2lzbGF0aW9uL3R1cmtpc2gtanVzdGljZS1zeXN0ZW0taW4tbmVlZC1vZi1hcnRpZmljaWFsLWludGVsbGlnZW5jZS10cmFuc2Zvcm1hdGlvbtIBfGh0dHBzOi8vd3d3LmRhaWx5c2FiYWguY29tL3BvbGl0aWNzL2xlZ2lzbGF0aW9uL3R1cmtpc2gtanVzdGljZS1zeXN0ZW0taW4tbmVlZC1vZi1hcnRpZmljaWFsLWludGVsbGlnZW5jZS10cmFuc2Zvcm1hdGlvbi9hbXA?oc=5,‘Turkish justice system in need of artificial intelligence transformation’ | Daily Sabah - Daily Sabah,2020-08-28,Daily Sabah,https://www.dailysabah.com,"The coronavirus outbreak has transformed almost every aspect of our lives, showing that integration with technology is no longer a luxury but a must as it...","turkish justice system,legislation,rule of law,ARTIFICIAL INTELLIGENCE,Coronavirus outbreak,COVID-19","The coronavirus outbreak has transformed almost every aspect of our lives, showing that integration with technology is no longer a luxury but a must as it...","The coronavirus outbreak has transformed almost every aspect of our lives, showing that integration with technology is no longer a luxury but a must as it...",,, Legislation,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiYWh0dHBzOi8vd3d3LmdvdnRlY2guY29tL2VkdWNhdGlvbi9oaWdoZXItZWQvV2ljaGl0YS1TdGF0ZS1OYW1lZC10by1GZWRlcmFsLUFJLVJlc2VhcmNoLUdyb3VwLmh0bWzSAQA?oc=5,Wichita State Named to Federal AI Research Group - Government Technology,2020-08-28,Government Technology,https://www.govtech.com,"Wichita State is taking part in a $100 million, five-year federal initiative to advance artificial intelligence research and workforce development supported by the National Science Foundation.",N/A,"Wichita State is taking part in a $100 million, five-year federal initiative to advance artificial intelligence research and workforce development supported by the National Science Foundation.","Wichita State is taking part in a $100 million, five-year federal initiative to advance artificial intelligence research and workforce development supported by the National Science Foundation.",http://schema.org,,Higher Education,N/A,N/A,Article,https://www.govtech.com/education/higher-ed/Wichita-State-Named-to-Federal-AI-Research-Group.html,"[{'@context': 'http://schema.org', '@type': 'ImageObject', 'height': 853, 'url': 'https://erepublic.brightspotcdn.com/dims4/default/6933e36/2147483647/strip/false/crop/1280x853+0+0/resize/1280x853!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2F2c%2F5d%2Fb38c2b46a0698976adffa1e32c40%2Fartificial-intelligence-4469138-1280.jpg', 'width': 1280}, {'@context': 'http://schema.org', '@type': 'ImageObject', 'height': 675, 'url': 'https://erepublic.brightspotcdn.com/dims4/default/0666907/2147483647/strip/false/crop/1280x720+0+67/resize/1200x675!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2F2c%2F5d%2Fb38c2b46a0698976adffa1e32c40%2Fartificial-intelligence-4469138-1280.jpg', 'width': 1200}]",,"{'@type': 'Organization', 'name': 'GovTech', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'url': 'https://erepublic.brightspotcdn.com/bc/a8/3ad2250148b8a28b31d4bd4edd24/gt-with-block.svg'}}",Wichita State Named to Federal AI Research Group,"August 28, 2020","April 20, 2021",,Wichita State Named to Federal AI Research Group,,,,,,"{'@type': 'WebPage', '@id': 'https://www.govtech.com/education/higher-ed/Wichita-State-Named-to-Federal-AI-Research-Group.html'}",,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWGh0dHBzOi8vbmV3cy5jbGVhcmFuY2Vqb2JzLmNvbS8yMDIwLzA4LzMxL2FkdmFuY2UtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtYXQtYm9vei1hbGxlbi_SAQA?oc=5,Advance Artificial Intelligence at Booz Allen - ClearanceJobs - ClearanceJobs,2020-08-31,ClearanceJobs,https://news.clearancejobs.com,Protect lives and help shape the future with AI.  ,N/A,Protect lives and help shape the future with AI.  ,N/A,https://schema.org,"[{'@type': 'Article', '@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#article', 'isPartOf': {'@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/'}, 'author': {'name': 'ClearanceJobs', '@id': 'https://news.clearancejobs.com/#/schema/person/6f33262edf2bf0f68a41f6188afc2cd7'}, 'headline': 'Advance Artificial Intelligence at Booz Allen', 'datePublished': '2020-08-31T21:21:46+00:00', 'dateModified': '2020-09-01T03:13:32+00:00', 'mainEntityOfPage': {'@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/'}, 'wordCount': 670, 'commentCount': 0, 'publisher': {'@id': 'https://news.clearancejobs.com/#organization'}, 'image': {'@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#primaryimage'}, 'thumbnailUrl': 'https://news.clearancejobs.com/wp-content/uploads/2020/08/Booz-Allen-AI.jpg', 'keywords': ['AI', 'artificial intelligence', 'booz allen', 'sponsored'], 'articleSection': ['Sponsored'], 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'CommentAction', 'name': 'Comment', 'target': ['https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#respond']}]}, {'@type': 'WebPage', '@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/', 'url': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/', 'name': 'Advance Artificial Intelligence at Booz Allen - ClearanceJobs', 'isPartOf': {'@id': 'https://news.clearancejobs.com/#website'}, 'primaryImageOfPage': {'@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#primaryimage'}, 'image': {'@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#primaryimage'}, 'thumbnailUrl': 'https://news.clearancejobs.com/wp-content/uploads/2020/08/Booz-Allen-AI.jpg', 'datePublished': '2020-08-31T21:21:46+00:00', 'dateModified': '2020-09-01T03:13:32+00:00', 'description': 'Protect lives and help shape the future with AI.\xa0\xa0', 'breadcrumb': {'@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#primaryimage', 'url': 'https://news.clearancejobs.com/wp-content/uploads/2020/08/Booz-Allen-AI.jpg', 'contentUrl': 'https://news.clearancejobs.com/wp-content/uploads/2020/08/Booz-Allen-AI.jpg', 'width': 1150, 'height': 732, 'caption': 'artificial intelligence'}, {'@type': 'BreadcrumbList', '@id': 'https://news.clearancejobs.com/2020/08/31/advance-artificial-intelligence-at-booz-allen/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Defense News', 'item': 'https://news.clearancejobs.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Sponsored', 'item': 'https://news.clearancejobs.com/category/sponsored/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Advance Artificial Intelligence at Booz Allen'}]}, {'@type': 'WebSite', '@id': 'https://news.clearancejobs.com/#website', 'url': 'https://news.clearancejobs.com/', 'name': 'ClearanceJobs', 'description': 'Security Clearance News &amp; Career Advice', 'publisher': {'@id': 'https://news.clearancejobs.com/#organization'}, 'alternateName': 'ClearanceJobs.com', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://news.clearancejobs.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://news.clearancejobs.com/#organization', 'name': 'ClearanceJobs', 'url': 'https://news.clearancejobs.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://news.clearancejobs.com/#/schema/logo/image/', 'url': 'https://news.clearancejobs.com/wp-content/uploads/2019/02/color-logo.png', 'contentUrl': 'https://news.clearancejobs.com/wp-content/uploads/2019/02/color-logo.png', 'width': 1706, 'height': 1706, 'caption': 'ClearanceJobs'}, 'image': {'@id': 'https://news.clearancejobs.com/#/schema/logo/image/'}, 'sameAs': ['http://www.facebook.com/ClearanceJobs', 'https://x.com/ClearanceJobs', 'https://www.youtube.com/user/ClearanceJobs']}, {'@type': 'Person', '@id': 'https://news.clearancejobs.com/#/schema/person/6f33262edf2bf0f68a41f6188afc2cd7', 'name': 'ClearanceJobs', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://news.clearancejobs.com/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/2c220f42c4902ab4ecfb9b3c4c06dd36?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/2c220f42c4902ab4ecfb9b3c4c06dd36?s=96&d=mm&r=g', 'caption': 'ClearanceJobs'}, 'description': 'ClearanceJobs.com, the largest security-cleared career network, specializes in defense jobs for professionals with security clearances. Search thousands of jobs from pre-screened, registered defense industry employers.', 'sameAs': ['http://www.clearancejobs.com', 'https://x.com/ClearanceJobs'], 'url': 'https://news.clearancejobs.com/author/clearancejobscom/'}]",N/A,N/A,"



ClearanceJobs
News & Career Advice





Advance Artificial Intelligence at Booz Allen




ClearanceJobs / Aug 31, 2020 














Sponsored  













 

Protect lives and help shape the future of AI at Booz Allen
From using data to support citizens during the heart of a pandemic to playing a key role in national security, the opportunities to do impactful, innovative work with artificial intelligence (AI) at Booz Allen are practically endless. “As we understand it, we’re the largest single provider of AI and data science services to the Federal Government,” says Joe Rohner, director of AI for defense. “Here, you’d have the chance to impact some of the most groundbreaking projects in the world.”
Graham Gilmer, Joint Artificial Intelligence Center (JAIC) program manager, agrees. “We work with unique, important data sets that no one else can access. Our clients trust us with their most sensitive information, and in turn, we bring them the finest people and the best technology.”
Build AI Capabilities for National Security and Defense
The missions Booz Allen serves with their work in AI are of national and global importance. Booz Allen recently won a contract to support several of the most high-profile AI development efforts across the Department of Defense (DoD). Together, they’ll work with industry experts to source and integrate the most fitting AI technologies for an enormous variety of defense missions. “AI has transformational potential,” says Steve Escaravage, analytics and AI business leader. “On this project, we’ll combine our deep understanding of both mission and technology to help the JAIC develop and operationalize AI-enabled capabilities across the DoD.”
Help Transform Healthcare with AI
To support our nation during this challenging time, the Booz Allen team used AI and specialized modeling software to build a COVID-19 forecasting platform. The goal? To help states and cities make more informed decisions about how, where, and when to loosen social distancing restrictions while keeping their populations safe.
The platform also serves as a repository for models that predict the spread of the disease and related demand for equipment and services. “We have the expertise to directly impact people’s health and safety during the current pandemic and beyond,” says Lauren Neal, director of health AI.
By joining their team, you’ll not only get to solve some of the toughest challenges clients face in the health sector, you’ll work closely with them to create pilot solutions using new technology. “From computer vision to natural language processing, we implement and operationalize these emerging technologies to see them at work in the real world,” adds Lauren. “Between the brand-new tech and the real impact we’re having on big health challenges, you can say what we do is exclusively Booz Allen.”
Break New Ground with Quantum Computing
“Few people in the world understand what quantum computing is, let alone know how to use it,” says Isabella Martinez, a quantum computing software researcher. “Here at Booz Allen, we are lucky to have an entire team dedicated to the field.”
Their team of Ph.D. quantum computing researchers represents diverse skills and interests within the realm. They work with clients to identify which of their needs are met by AI and which may, someday, be better solved using quantum computing. The team includes hardware and software researchers who focus on everything from writing software for quantum computers to vetting others’ software to figuring out the correct composition of a quantum bit or qubit.
Team members also play a role in creating market reports and leading educational workshops. “We do what we do because we honestly believe this technology will lead to a global paradigm shift, especially when it comes to AI” says Isabella.
If you’re fascinated by the latest advances in AI and want to be part of a team that employs cutting-edge technologies to protect lives and make an impact, there’s a place for you at Booz Allen.
Ready to work for a company on the forefront of AI?
Join booz allen.
The world can’t wait.
 
SPONSORED CONTENT: This article is written on or behalf of our Sponsor.





Related News






 SponsoredHow Booz Allen Is Shaping the Future of Energy Innovation


 SponsoredTips from Booz Allen's Military Recruiting Team


 SponsoredBooz Allen Pax River: Making a Difference with Artificial Intelligence


 SponsoredLaw Enforcement and Intelligence Jobs Intersect at Booz Allen Quantico
 
  ClearanceJobs.com, the largest security-cleared career network, specializes in defense jobs for professionals with security clearances. Search thousands of jobs from pre-screened, registered defense industry employers.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjAvMDgvMjgvb3Bpbmlvbi9zdW5kYXkvYnJhaW4tbWFjaGluZS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS5odG1s0gEA?oc=5,Opinion | The Brain Implants That Could Change Humanity (Published 2020) - The New York Times,2020-08-28,The New York Times,https://www.nytimes.com,"Brains are talking to computers, and computers to brains. Are our daydreams safe?",N/A,"Brains are talking to computers, and computers to brains. Are our daydreams safe?","Brains are talking to computers, and computers to brains. Are our daydreams safe?",https://schema.org,,Opinion,N/A,"Credit...By Derrick SchultzSkip to contentSkip to site indexSearch & Section NavigationSection NavigationThe Brain Implants That Could Change HumanityBrains are talking to computers, and computers to brains. Are our daydreams safe?Credit...By Derrick SchultzSupported bySKIP ADVERTISEMENTShare full article391Read in appBy Moises Velasquez-ManoffContributing Opinion WriterAug. 28, 2020Leer en españolJack Gallant never set out to create a mind-reading machine. His focus was more prosaic. A computational neuroscientist at the University of California, Berkeley, Dr. Gallant worked for years to improve our understanding of how brains encode information — what regions become active, for example, when a person sees a plane or an apple or a dog — and how that activity represents the object being viewed.By the late 2000s, scientists could determine what kind of thing a person might be looking at from the way the brain lit up — a human face, say, or a cat. But Dr. Gallant and his colleagues went further. They figured out how to use machine learning to decipher not just the class of thing, but which exact image a subject was viewing. (Which photo of a cat, out of three options, for instance.)One day, Dr. Gallant and his postdocs got to talking. In the same way that you can turn a speaker into a microphone by hooking it up backward, they wondered if they could reverse engineer the algorithm they’d developed so they could visualize, solely from brain activity, what a person was seeing.The first phase of the project was to train the AI. For hours, Dr. Gallant and his colleagues showed volunteers in fMRI machines movie clips. By matching patterns of brain activation prompted by the moving images, the AI built a model of how the volunteers’ visual cortex, which parses information from the eyes, worked. Then came the next phase: translation. As they showed the volunteers movie clips, they asked the model what, given everything it now knew about their brains, it thought they might be looking at.AdvertisementSKIP ADVERTISEMENTThe experiment focused just on a subsection of the visual cortex. It didn’t capture what was happening elsewhere in the brain — how a person might feel about what she was seeing, for example, or what she might be fantasizing about as she watched. The endeavor was, in Dr. Gallant’s words, a primitive proof of concept.And yet the results, published in 2011, are remarkable.The reconstructed images move with a dreamlike fluidity. In their imperfection, they evoke expressionist art. (And a few reconstructed images seem downright wrong.) But where they succeed, they represent an astonishing achievement: a machine translating patterns of brain activity into a moving image understandable by other people — a machine that can read the brain.Dr. Gallant was thrilled. Imagine the possibilities when better brain-reading technology became available? Imagine the people suffering from locked-in syndrome, Lou Gehrig’s disease, the people incapacitated by strokes, who could benefit from a machine that could help them interact with the world?VideoReconstruction of viewed movies from human brain activity measured by fMRI (Nishimoto et al., 2011)CreditCredit...By Derrick SchultzHe was also scared because the experiment showed, in a concrete way, that humanity was at the dawn of a new era, one in which our thoughts could theoretically be snatched from our heads. What was going to happen, Dr. Gallant wondered, when you could read thoughts the thinker might not even be consciously aware of, when you could see people’s memories?AdvertisementSKIP ADVERTISEMENT“That’s a real sobering thought that now you have to take seriously,” he told me recently.The ‘Google Cap’For decades, we’ve communicated with computers mostly by using our fingers and our eyes, by interfacing via keyboards and screens. These tools and the bony digits we prod them with provide a natural limit to the speed of communication between human brain and machine. We can convey information only as quickly (and accurately) as we can type or click.Voice recognition, like that used by Apple’s Siri or Amazon’s Alexa, is a step toward more seamless integration of human and machine. The next step, one that scientists around the world are pursuing, is technology that allows people to control computers — and everything connected to them, including cars, robotic arms and drones — merely by thinking.Dr. Gallant jokingly calls the imagined piece of hardware that would do this a “Google cap”: a hat that could sense silent commands and prompt computers to respond accordingly.The problem is that, to work, that cap would need to be able to see, with some detail, what’s happening in the nearly 100 billion neurons that make up the brain.Technology that can easily peer through the skull, like the MRI machine, is far too unwieldy to mount on your head. Less bulky technology, like electroencephalogram, or E.E.G., which measures the brain’s electrical activity through electrodes attached to the scalp, doesn’t provide nearly the same clarity. One scientist compares it to looking for the surface ripples made by a fish swimming underwater while a storm roils the lake.AdvertisementSKIP ADVERTISEMENTOther methods of “seeing” into the brain might include magnetoencephalography, or M.E.G., which measures magnetic waves emanating outside the skull from neurons firing beneath it; or using infrared light, which can penetrate living tissue, to infer brain activity from changes in blood flow. (Pulse oximeters work this way, by shining infrared light through your finger.)What technologies will power the brain-computer interface of the future is still unclear. And if it’s unclear how we’ll “read” the brain, it’s even less clear how we’ll “write” to it.This is the other holy grail of brain-machine research: technology that can transmit information to the brain directly. We’re probably nowhere near the moment when you can silently ask, “Alexa, what’s the capital of Peru?” and have “Lima” materialize in your mind.Even so, solutions to these challenges are beginning to emerge. Much of the research has occurred in the medical realm, where, for years, scientists have worked incrementally toward giving quadriplegics and others with immobilizing neurological conditions better ways of interacting with the world through computers. But in recent years, tech companies — including Facebook, Microsoft and Elon Musk’s Neuralink — have begun investing in the field.Some scientists are elated by this infusion of energy and resources. Others worry that as this tech moves into the consumer realm, it could have a variety of unintended and potentially dangerous consequences, from the erosion of mental privacy to the exacerbation of inequality.AdvertisementSKIP ADVERTISEMENTRafael Yuste, a neurobiologist at Columbia University, counts two great advances in computing that have transformed society: the transition from room-size mainframe computers to personal computers that fit on a desk (and then in your lap), and the advent of mobile computing with smartphones in the 2000s. Noninvasive brain-reading tech would be a third great leap, he says.“Forget about the Covid crisis,” Dr. Yuste told me. “What’s coming with this new tech can change humanity.”Dear BrainNot many people will volunteer to be the first to undergo a novel kind of brain surgery, even if it holds the promise of restoring mobility to those who’ve been paralyzed. So when Robert Kirsch, the chairman of biomedical engineering at Case Western Reserve University, put out such a call nearly 10 years ago, and one person both met the criteria and was willing, he knew he had a pioneer on his hands.VideoCreditCredit...By Derrick SchultzThe man’s name was Bill Kochevar. He’d been paralyzed from the neck down in a biking accident years earlier. His motto, as he later explained it, was “somebody has to do the research.”AdvertisementSKIP ADVERTISEMENTAt that point, scientists had already invented gizmos that helped paralyzed patients leverage what mobility remained — lips, an eyelid — to control computers or move robotic arms. But Dr. Kirsch was after something different. He wanted to help Mr. Kochevar move his own limbs.The first step was implanting two arrays of sensors over the part of the brain that would normally control Mr. Kochevar’s right arm. Electrodes that could receive signals from those arrays via a computer were implanted into his arm muscles. The implants, and the computer connected to them, would function as a kind of electronic spinal cord, bypassing his injury.Once his arm muscles had been strengthened — achieved with a regimen of mild electrical stimulation while he slept — Mr. Kochevar, who at that point had been paralyzed for over a decade, was able to feed himself and drink water. He could even scratch his nose.About two dozen people around the world who have lost the use of limbs from accidents or neurological disease have had sensors implanted on their brains. Many, Mr. Kochevar included, participated in a United States government-funded program called BrainGate. The sensor arrays used in this research, smaller than a button, allow patients to move robotic arms or cursors on a screen just by thinking. But as far as Dr. Kirsch knows, Mr. Kochevar, who died in 2017 for reasons unrelated to the research, was the first paralyzed person to regain use of his limbs by way of this technology.This fall, Dr. Kirsch and his colleagues will begin version 2.0 of the experiment. This time, they’ll implant six smaller arrays — more sensors will improve the quality of the signal. And instead of implanting electrodes directly in the volunteers’ muscles, they’ll insert them upstream, circling the nerves that move the muscles. In theory, Dr. Kirsch says, that will enable movement of the entire arm and hand.AdvertisementSKIP ADVERTISEMENTThe next major goal is to restore sensation so that people can know if they’re holding a rock, say, or an orange — or if their hand has wandered too close to a flame. “Sensation has been the longest ignored part of paralysis,” Dr. Kirsch told me.A few years ago, scientists at the University of Pittsburgh began groundbreaking experiments on that front with a man named Nathan Copeland who was paralyzed from the upper chest down. They routed sensory information from a robotic arm into the part of his cortex that dealt with his right hand’s sense of touch.Every brain is a living, undulating organ that changes over time. That’s why, before each of Mr. Copeland’s sessions, the AI has to recalibrate — to construct a new brain decoder. “The signals in your brain shift,” Mr. Copeland told me. “They’re not exactly the same every day.”And the results weren’t perfect. Mr. Copeland described them to me as “weird,” “electrical tingly” but also “amazing.” The sensory feedback was immensely important, though, in knowing that he’d actually grasped what he thought he’d grasped. And more generally, it demonstrated that a person could “feel” a robotic hand as his or her own, and that information coming from electronic sensors could be fed into the human brain.Preliminary as these experiments are, they suggest that the pieces of a brain-machine interface that can both “read” and “write” already exist. People cannot only move robotic arms just by thinking; machines can also, however imperfectly, convey information to the brain about what that arm encounters.AdvertisementSKIP ADVERTISEMENTWho knows how soon versions of this technology will be available for kids who want to think-move avatars in video games or think-surf the web. People can already fly drones with their brain signals, so maybe crude consumer versions will appear in coming years. But it’s hard to overstate how life-changing such tech could be for people with spinal cord injuries or neurological diseases.Edward Chang, a neurosurgeon at the University of California, San Francisco, who works on brain-based speech recognition, said that maintaining the ability to communicate can mean the difference between life or death. “For some people, if they have a means to continue to communicate, that may be the reason they decide to stay alive,” he told me. “That motivates us a lot in our work.”In a recent study, Dr. Chang and his colleagues predicted with up to 97 percent accuracy — the best rate yet achieved, they say — what words a volunteer had said (from about 250 words used in a predetermined set of 50 sentences) by using implanted sensors that monitored activity in the part of the brain that moves the muscles involved in speaking. (The volunteers in this study weren’t paralyzed; they were epilepsy patients undergoing brain surgery to address that condition, and the implants were not permanent.)Dr. Chang used sensor arrays similar to those Dr. Kirsch used, but a noninvasive method may not be too far away.Facebook, which funded Dr. Chang’s study, is working on a brain-reading helmet-like contraption that uses infrared light to peer into the brain. Mark Chevillet, the director of brain-computer interface research at Facebook Reality Labs, told me in an email that while full speech recognition remains distant, his lab will be able to decode simple commands like “home,” “select” and “delete” in “coming years.”AdvertisementSKIP ADVERTISEMENTThis progress isn’t solely driven by advances in brain-sensing technology — by the physical meeting point of flesh and machine. The AI matters as much, if not more.Trying to understand the brain from outside the skull is like trying to make sense of a conversation taking place two rooms away. The signal is often messy, hard to decipher. So it’s the same types of algorithms that now allow speech-recognition software to do a decent job of understanding spoken speech — including individual idiosyncrasies of pronunciation and regional accents — that may now enable brain-reading technology.Zap That UrgeNot all the applications of brain reading require something as complex as understanding speech, however. In some cases, scientists simply want to blunt urges.When Casey Halpern, a neurosurgeon at Stanford, was in college, he had a friend who drank too much. Another was overweight but couldn’t stop eating. “Impulse control is such a pervasive problem,” he told me.VideoCreditCredit...By Derrick SchultzAs a budding scientist, he learned about methods of deep brain stimulation used to treat Parkinson’s disease. A mild electric current applied to a part of the brain involved in movement could lessen tremors caused by the disease. Could he apply that technology to the problem of inadequate self control?AdvertisementSKIP ADVERTISEMENTWorking with mice in the 2010s, he identified a part of the brain, called the nucleus accumbens, where activity spiked in a predictable pattern just before a mouse was about to gorge on high-fat food. He found he could reduce how much the mouse ate by disrupting that activity with a mild electrical current. He could zap the compulsion to gorge as it was taking hold in the rodents’ brains.Earlier this year, he began testing the approach in people suffering from obesity who haven’t been helped by any other treatment, including gastric-bypass surgery. He implants an electrode in their nucleus accumbens. It’s connected to an apparatus that was originally developed to prevent seizures in people with epilepsy.As with Dr. Chang or Dr. Gallant’s work, an algorithm first has to learn about the brain it’s attached to — to recognize the signs of oncoming loss of control. Dr. Halpern and his colleagues train the algorithm by giving patients a taste of a milkshake, or offering a buffet of the patient’s favorite foods, and then recording their brain activity just before the person indulges.He’s so far completed two implantations. “The goal is to help restore control,” he told me. And if it works in obesity, which afflicts roughly 40 percent of adults in the United States, he plans to test the gizmo against addictions to alcohol, cocaine and other substances.AdvertisementSKIP ADVERTISEMENTDr. Halpern’s approach takes as fact something that he says many people have a hard time accepting: that the lack of impulse control that may underlie addictive behavior isn’t a choice, but results from a malfunction of the brain. “We have to accept that it’s a disease,” he says. “We often just judge people and assume it’s their own fault. That’s not what the current research is suggesting we should do.”I must confess that of the numerous proposed applications of brain-machine interfacing I came across, Dr. Halpern’s was my favorite to extrapolate on. How many lives have been derailed by the inability to resist the temptation of that next pill or that next beer? What if Dr. Halpern’s solution was generalizable?What if every time your mind wandered off while writing an article, you could, with the aid of your concentration implant, prod it back to the task at hand, finally completing those life-changing projects you’ve never gotten around to finishing?These applications remain fantasies, of course. But the mere fact that such a thing may be possible is partly what prompts Dr. Yuste, the neurobiologist, to worry about how this technology could blur the boundaries of what we consider to be our personalities.Such blurring is already an issue, he points out. Parkinson’s patients with implants sometimes report feeling more aggressive than usual when the machine is “on.” Depressed patients undergoing deep brain stimulation sometimes wonder if they’re really themselves anymore. “You kind of feel artificial,” one patient told researchers. The machine isn’t implanting ideas in their minds, like Leonardo DiCaprio’s character in the movie “Inception,” but it is seemingly changing their sense of self.AdvertisementSKIP ADVERTISEMENTWhat happens if people are no longer sure if their emotions are theirs, or the effects of the machines they’re connected to?Dr. Halpern dismisses these concerns as overblown. Such effects are part of many medical treatments, he points out, including commonly prescribed antidepressants and stimulants. And sometimes, as in the case of hopeless addiction, changing someone’s behavior is precisely the goal.Still the longer-term issue of what could happen when brain-writing technology jumps from the medical into the consumer realm is hard to forget. If my imagined focus enhancer existed, for example, but was very expensive, it could exacerbate the already yawning chasm between those who can afford expensive tutors, cars and colleges — and now grit-boosting technology — and those who cannot.“Certain groups will get this tech, and will enhance themselves,” Dr. Yuste told me. “This is a really serious threat to humanity.”The Brain Business“The idea that you have to drill holes in skulls to read the brains is nuts,” Mary Lou Jepsen, the chief executive and founder of Openwater, told me in an email. Her company is developing technology that, she says, uses infrared light and ultrasonic waves to peer into the body.AdvertisementSKIP ADVERTISEMENTOther researchers are simply trying to make invasive approaches less invasive. A company called Synchron seeks to avoid opening the skull or touching brain tissue at all by inserting a sensor through the jugular vein in the neck. It’s currently undergoing a safety and feasibility trial.Dr. Kirsch suspects that Elon Musk’s Neuralink is probably the best brain-sensing tech in development. It requires surgery, but unlike the BrainGate sensor arrays, it’s thin, flexible and can adjust to the mountainous topography of the brain. The hope is that this makes it less caustic. It also has hairlike filaments that sink into brain tissue. Each filament contains multiple sensors, theoretically allowing the capture of more data than flatter arrays that sit at the brain’s surface. It can both read and write to the brain, and it’s accompanied by a robot that assists with the implantation.A major challenge to implants is that, as Dr. Gallant says, “your brain doesn’t like having stuff stuck in your brain.” Over time, immune cells may swarm the implant, covering it with goop.One way to try to avoid this is to drastically shrink the size of the sensors. Arto Nurmikko, a professor of engineering and physics at Brown University who’s part of the BrainGate effort, is developing what he calls “neurograins” — tiny, implantable silicon sensors no larger than a handful of neurons. They’re too small to have batteries, so they’re powered by microwaves beamed in from outside the skull.He foresees maybe 1,000 mini sensors implanted throughout the brain. He’s so far tested them only in rodents. But maybe we shouldn’t be so sure that healthy people wouldn’t volunteer for “mental enhancement” surgery. Every year, Dr. Nurmikko poses a hypothetical to his students: 1,000 neurograin implants that would allow students to learn and communicate faster; any volunteers?AdvertisementSKIP ADVERTISEMENT“Typically about half the class says, ‘Sure,’” he told me. “That speaks to where we are today.”Jose Carmena and Michel Maharbiz, scientists at Berkeley and founders of a start-up called Iota Biosciences, have their own version of this idea, which they call “neural dust”: tiny implants for the peripheral nervous system — arm, legs and organs besides the brain. “It’s like a Fitbit for your liver,” Dr. Carmena told me.They imagine treating inflammatory diseases by stimulating nerves throughout the body with these tiny devices. And where Dr. Nurmikko uses microwaves to power the devices, Dr. Carmena and Dr. Maharbiz foresee the use of ultrasound to beam power to them.VideoCreditCredit...By Derrick SchultzGenerally, they say, this kind of tech will be adopted first in the medical context and then move to the lay population. “We’re going to evolve to augmenting humans,” Dr. Carmena told me. “There’s no question.”But hype permeates the field, he warns. Sure, Elon Musk has argued that closer brain-machine integration will help humans compete with ever-more-powerful A.I.s. But in reality, we’re nowhere near a device that could, for example, help you master Kung Fu instantaneously like Keanu Reeves in “The Matrix.”AdvertisementSKIP ADVERTISEMENTWhat does the near future look like for the average consumer? Ramses Alcaide, the chief executive of a company called Neurable, imagines a world in which smartphones tucked in our pockets or backpacks act as processing hubs for data streaming in from smaller computers and sensors worn around the body. These devices — glasses that serve as displays, earbuds that whisper in our ears — are where the actual interfacing between human and computer will occur.Microsoft sells a headset called HoloLens that superimposes images onto the world, an idea called “augmented reality.” A company called Mojo Vision is working toward a contact lens that projects monochrome images directly onto the retina, a private computer display superimposed over the world.And Dr. Alcaide himself is working on what he sees as the linchpin to this vision, a device that, one day, may help you to silently communicate with all your digital paraphernalia. He was vague about the form the product will take — it isn’t market ready yet — except to note that it’s an earphone that can measure the brain’s electrical activity to sense “cognitive states,” like whether you’re hungry or concentrating.We already compulsively check Instagram and Facebook and email, even though we’re supposedly impeded by our fleshy fingers. I asked Dr. Alcaide: What will happen when we can compulsively check social media just by thinking?Ever the optimist, he told me that brain-sensing technology could actually help with the digital incursion. The smart earbud could sense that you’re working, for instance, and block advertisements or phone calls. “What if your computer knew you were focusing?” he told me. “What if it actually removes bombardment from your life?”AdvertisementSKIP ADVERTISEMENTMaybe it’s no surprise that Dr. Alcaide has enjoyed the HBO sci-fi show “Westworld,” a universe where technologies that make communicating with computers more seamless are commonplace (though no one seems better off for it). Rafael Yuste, on the other hand, refuses to watch the show. He likens the idea to a scientist who studies Covid-19 watching a movie about pandemics. “It’s the last thing I want to do,” he says.‘A Human Rights Issue’To grasp why Dr. Yuste frets so much about brain-reading technology, it helps to understand his research. He helped pioneer a technology that can read and write to the brain with unprecedented precision, and it doesn’t require surgery. But it does require genetic engineering.Dr. Yuste infects mice with a virus that inserts two genes into the animals’ neurons. One prompts the cells to produce a protein that make them sensitive to infrared light; the other makes the neurons emit light when they activate. Thereafter, when the neurons fire, Dr. Yuste can see them light up. And he can activate neurons in turn with an infrared laser. Dr. Yuste can thus read what’s happening in the mouse brain and write to the mouse’s brain with an accuracy impossible with other techniques.And he can, it appears, make the mice “see” things that aren’t there.In one experiment, he trained mice to take a drink of sugar water after a series of bars appeared on a screen. He recorded which neurons in the visual cortex fired when the mice saw those bars. Then he activated those same neurons with the laser, but without showing them the actual bars. The mice had the same reaction: They took a drink.He likens what he did to implanting an hallucination. “We were able to implant into these mice perceptions of things that they hadn’t seen,” he told me. “We manipulated the mouse like a puppet.”AdvertisementSKIP ADVERTISEMENTThis method, called optogenetics, is a long way from being used in people. To begin with, we have thicker skulls and bigger brains, making it harder for infrared light to penetrate. And from a political and regulatory standpoint, the bar is high for genetically engineering human beings. But scientists are exploring workarounds — drugs and nanoparticles that make neurons receptive to infrared light, allowing precise activation of neurons without genetic engineering.The lesson in Dr. Yuste’s view is not that we’ll soon have lasers mounted on our heads that play us “like pianos,” but that brain-reading and possibly brain-writing technologies are fast approaching, and society isn’t prepared for them.“We think this is a human rights issue,” he told me.In a 2017 paper in the journal Nature, Dr. Yuste and 24 other signatories, including Dr. Gallant, called for the formulation of a human rights declaration that explicitly addressed “neurorights” and what they see as the threats posed by brain-reading technology before it becomes ubiquitous. Information taken from people’s brains should be protected like medical data, Dr. Yuste says, and not exploited for profit or worse. And just as people have the right not to self-incriminate with speech, we should have the right not to self-incriminate with information gleaned from our brains.Dr. Yuste’s activism was prompted in part, he told me, by the large companies suddenly interested in brain-machine research.Say you’re using your Google Cap. And like many products in the Google ecosystem, it collects information about you, which it uses to help advertisers target you with ads. Only now, it’s not harvesting your search results or your map location; it’s harvesting your thoughts, your daydreams, your desires.AdvertisementSKIP ADVERTISEMENTWho owns those data?Or imagine that writing to the brain is possible. And there are lower-tier versions of brain-writing gizmos that, in exchange for their free use, occasionally “make suggestions” directly to your brain. How will you know if your impulses are your own, or if an algorithm has stimulated that sudden craving for Ben & Jerry’s ice cream or Gucci handbags?“People have been trying to manipulate each other since the beginning of time,” Dr. Yuste told me. “But there’s a line that you cross once the manipulation goes directly to the brain, because you will not be able to tell you are being manipulated.”When I asked Facebook about concerns around the ethics of big tech entering the brain-computer interface space, Mr. Chevillet, of Facebook Reality Labs, highlighted the transparency of its brain-reading project. “This is why we’ve talked openly about our B.C.I. research — so it can be discussed throughout the neuroethics community as we collectively explore what responsible innovation looks like in this field,” he said in an email.Ed Cutrell, a senior principal researcher at Microsoft, which also has a B.C.I. program, emphasized the importance of treating user data carefully. “There needs to be clear sense of where that information goes,” he told me. “As we are sensing more and more about people, to what extent is that information I’m collecting about you yours?”Some find all this talk of ethics and rights, if not irrelevant, then at least premature.Medical scientists working to help paralyzed patients, for example, are already governed by HIPAA laws, which protect patient privacy. Any new medical technology has to go through the Food and Drug Administration approval process, which includes ethical considerations.AdvertisementSKIP ADVERTISEMENT(Ethical quandaries still arise, though, notes Dr. Kirsch. Let’s say you want to implant a sensor array in a patient suffering from locked-in syndrome. How do you get consent to conduct surgery that might change the person’s life for the better from someone who can’t communicate?)Leigh Hochberg, a professor of engineering at Brown University and part of the BrainGate initiative, sees the companies now piling into the brain-machine space as a boon. The field needs these companies’ dynamism — and their deep pockets, he told me. Discussions about ethics are important, “but those discussions should not at any point derail the imperative to provide restorative neurotechnologies to people who could benefit from them,” he added.Ethicists, Dr. Jepsen told me, “must also see this: The alternative would be deciding we aren’t interested in a deeper understanding of how our minds work, curing mental disease, really understanding depression, peering inside people in comas or with Alzheimer’s, and enhancing our abilities in finding new ways to communicate.”There’s even arguably a national security imperative to plow forward. China has its own version of BrainGate. If American companies don’t pioneer this technology, some think, Chinese companies will. “People have described this as a brain arms race,” Dr. Yuste said.Not even Dr. Gallant, who first succeeded in translating neural activity into a moving image of what another person was seeing — and who was both elated and horrified by the exercise — thinks the Luddite approach is an option. “The only way out of the technology-driven hole we’re in is more technology and science,” he told me. “That’s just a cool fact of life.”Moises Velasquez-Manoff, the author of “An Epidemic of Absence: A New Way of Understanding Allergies and Autoimmune Diseases,” is a contributing opinion writer.The Times is committed to publishing a diversity of letters to the editor. We’d like to hear what you think about this or any of our articles. Here are some tips. And here’s our email: letters@nytimes.com.Follow The New York Times Opinion section on Facebook, Twitter (@NYTopinion) and Instagram.A version of this article appears in print on Aug. 30, 2020, Section SR, Page 4 of the New York edition with the headline: The Mind Readers. Order Reprints | Today’s Paper | SubscribeRead 391 CommentsShare full article391Read in appAdvertisementSKIP ADVERTISEMENTComments 391The Brain Implants That Could Change HumanitySkip to CommentsThe comments section is closed.
      To submit a letter to the editor for publication, write to
      letters@nytimes.com.Enjoy unlimited access to all of The Times.6-month Welcome Offeroriginal price:   $6.25sale price:   $1/weekLearn more",BreadcrumbList,https://www.nytimes.com/,"[{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2020/08/30/sunday-review/28manoff-Opener-image/28manoff-Opener-image-videoSixteenByNineJumbo1600-v2.jpg', 'height': 456, 'width': 810, 'contentUrl': 'https://static01.nyt.com/images/2020/08/30/sunday-review/28manoff-Opener-image/28manoff-Opener-image-videoSixteenByNineJumbo1600-v2.jpg', 'creditText': ''}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2020/08/30/sunday-review/28manoff-Opener-image/28manoff-Opener-image-superJumbo.jpg', 'height': 1080, 'width': 810, 'contentUrl': 'https://static01.nyt.com/images/2020/08/30/sunday-review/28manoff-Opener-image/28manoff-Opener-image-superJumbo.jpg', 'creditText': ''}, {'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/2020/08/30/sunday-review/28manoff-Opener-image/28manoff-Opener-image-mediumSquareAt3X-v2.jpg', 'height': 810, 'width': 810, 'contentUrl': 'https://static01.nyt.com/images/2020/08/30/sunday-review/28manoff-Opener-image/28manoff-Opener-image-mediumSquareAt3X-v2.jpg', 'creditText': ''}]","[{'@context': 'https://schema.org', '@type': 'Person', 'url': '', 'name': 'Moises Velasquez-Manoff'}]","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",Opinion | The Brain Implants That Could Change Humanity,2020-08-28T09:00:36.000Z,2020-08-28T22:10:05.000Z,,The New York Times,False,"[{'@context': 'https://schema.org', '@type': 'ListItem', 'name': 'Opinion', 'position': 1, 'item': 'https://www.nytimes.com/section/opinion'}, {'@context': 'https://schema.org', '@type': 'ListItem', 'name': 'Sunday Opinion', 'position': 2, 'item': ''}]",,,,https://www.nytimes.com/2020/08/28/opinion/sunday/brain-machine-artificial-intelligence.html,,en-US,Opinion | The Mind Readers,"[{}, {'@id': 'https://www.nytimes.com/video/embedded/sunday-review/100000007313659/28ManoffVideo.html'}, {'@id': 'https://www.nytimes.com/video/embedded/sunday-review/100000007311944/28ManoffHouse.html'}, {'@id': 'https://www.nytimes.com/video/embedded/sunday-review/100000007311963/28ManoffHand.html'}, {'@id': 'https://www.nytimes.com/video/embedded/sunday-review/100000007311999/28ManoffFish.html'}]","{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.meteredContent'}",{'@id': '#commentsContainer'},391.0,"{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}","{'@id': 'https://www.nytimes.com/#publisher', 'name': 'The New York Times'}",2024.0,"{'@type': ['CreativeWork', 'Product'], 'name': 'The New York Times', 'productID': 'nytimes.com:basic'}","{'@context': 'https://schema.org', '@type': 'ImageObject', 'url': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'height': 291, 'width': 291, 'contentUrl': 'https://static01.nyt.com/images/icons/t_logo_291_black.png', 'creditText': 'The New York Times'}",https://www.nytimes.com/#publisher,https://www.nytco.com/company/diversity-and-inclusion/,https://www.nytco.com/company/standards-ethics/,https://www.nytimes.com/interactive/2023/01/28/admin/the-new-york-times-masthead.html,1851-09-18,https://en.wikipedia.org/wiki/The_New_York_Times,,,
https://news.google.com/rss/articles/CBMiigFodHRwczovL20uZWNvbm9taWN0aW1lcy5jb20vdGVjaC9zb2Z0d2FyZS9jaWktbGF1bmNoZXMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtZm9ydW0tY2hhaXJlZC1ieS1pYm1zLXNhbmRpcC1wYXRlbC9hcnRpY2xlc2hvdy83NzgwMTU5MC5jbXPSAY4BaHR0cHM6Ly9tLmVjb25vbWljdGltZXMuY29tL3RlY2gvc29mdHdhcmUvY2lpLWxhdW5jaGVzLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWZvcnVtLWNoYWlyZWQtYnktaWJtcy1zYW5kaXAtcGF0ZWwvYW1wX2FydGljbGVzaG93Lzc3ODAxNTkwLmNtcw?oc=5,CII launches Artificial Intelligence Forum chaired by IBM's Sandip Patel - The Economic Times,2020-08-28,The Economic Times,https://m.economictimes.com,"The CII AI Forum will focus on building a strong AI ecosystem in India by building awareness at scale and enhancing capabilities by skilling/reskilling workforce for the future, CII said.","['cii', 'confederation of indian industry', 'ai', 'artificial intelligence', 'bfsi', 'Research and Development', 'Financial services', 'retail', 'Banking']","The CII AI Forum will focus on building a strong AI ecosystem in India by building awareness at scale and enhancing capabilities by skilling/reskilling workforce for the future, CII said.","The CII AI Forum will focus on building a strong AI ecosystem in India by building awareness at scale and enhancing capabilities by skilling/reskilling workforce for the future, CII said.",https://schema.org/,,N/A,N/A,"AgenciesNew Delhi: Confederation of Indian Industry (CII) on Friday said it has established a new forum on artificial intelligence chaired by IBM's India/South Asia Managing Director Sandip Patel.The forum prioritises artificial intelligence (AI) as a driver of economic and business revival.Elevate Your Tech Prowess with High-Value Skill CoursesOffering CollegeCourseWebsiteMIT xPROMIT Technology Leadership and InnovationVisitIIT DelhiCertificate Programme in Data Science & Machine LearningVisitIndian School of BusinessISB Product ManagementVisitThe CII AI Forum will focus on building a strong AI ecosystem in India by building awareness at scale and enhancing capabilities by skilling/reskilling workforce for the future, CII said.    by Taboola by Taboola Sponsored Links Sponsored Links Promoted Links Promoted Links You May LikeG4 By GolpaNew and Permanent Teeth in 24 Hours - Life Changing!G4 By GolpaLearn MoreUndoIt will also work with the government in shaping conducive policy and regulatory regime; encourage public-private partnerships in research and development, and facilitate pilot implementation of AI solutions in priority sectors, it added.The sectors that have been prioritised for this year are: banking and financial services industries (BFSI), retail, social (healthcare) and manufacturing (automotive).Play VideoPlaySkip BackwardSkip ForwardUnmuteCurrent Time 0:00/Duration 51:42Loaded: 0.82%00:00Stream Type LIVESeek to live, currently behind liveLIVERemaining Time -51:42 1xPlayback RateChaptersChaptersDescriptionsdescriptions off, selectedCaptionscaptions settings, opens captions settings dialogcaptions off, selectedAudio Trackdefault, selectedPicture-in-PictureFullscreenThis is a modal window.Beginning of dialog window. Escape will cancel and close the window.TextColorWhiteBlackRedGreenBlueYellowMagentaCyanOpacityOpaqueSemi-TransparentText BackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanOpacityOpaqueSemi-TransparentTransparentCaption Area BackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanOpacityTransparentSemi-TransparentOpaqueFont Size50%75%100%125%150%175%200%300%400%Text Edge StyleNoneRaisedDepressedUniformDrop shadowFont FamilyProportional Sans-SerifMonospace Sans-SerifProportional SerifMonospace SerifCasualScriptSmall CapsReset restore all settings to the default valuesDoneClose Modal DialogEnd of dialog window.Advertisement“As the economy moves into the recovery and revival phase, the transformational potential of responsible AI-driven solutions can be used to fuel India's growth story in a big way. CII AI Forum will look at initiatives to spur local innovations,"" Patel said.Discover the stories of your interestBlockchain5 StoriesCyber-safety7 StoriesFintech9 StoriesE-comm9 StoriesML8 StoriesEdtech6 StoriesHe observed that this will make AI adoption a reality and further the national agenda of Digital India and Make in India for India and the world.""More importantly, the forum will work on policies to embed trust and transparency into AI applications and processes, a critical step in realising true promise of the technology for business, society and the world,” said Patel.At the first meeting of the CII AI Forum for 2020-21 convened earlier this month, the members delved into the role of AI across diversified sectors to boost productivity and set the agenda to create new opportunities to galvanise the growth across the priority sectors.Read More News onciiconfederation of indian industryaiartificial intelligencebfsiResearch and DevelopmentFinancial servicesretailBanking",WebPage,https://economictimes.indiatimes.com/tech/software/cii-launches-artificial-intelligence-forum-chaired-by-ibms-sandip-patel/articleshow/77801590.cms,"{'@type': 'ImageObject', 'url': 'https://img.etimg.com/thumb/msid-77801590,resizemode-4,width-1200,height-900,imgsize-25305,overlay-ettech/photo.jpg', 'width': 1200, 'height': 900}","{'@type': 'Thing', 'name': 'PTI'}","{'@type': 'NewsMediaOrganization', 'name': 'Economic Times', 'logo': {'@type': 'ImageObject', 'url': 'https://img.etimg.com/thumb/msid-76939477,width-600,height-60,quality-100/economictimes.jpg', 'width': 600, 'height': 60}}",CII launches Artificial Intelligence Forum chaired by IBM's Sandip Patel,2020-08-28T14:36:00+05:30,2020-08-28T14:36:00+05:30,Tech,CII launches Artificial Intelligence Forum chaired by IBM's Sandip Patel,,,,,,https://economictimes.indiatimes.com/tech/software/cii-launches-artificial-intelligence-forum-chaired-by-ibms-sandip-patel/articleshow/77801590.cms,,en,,,,,,,,,,"{'@type': 'ImageObject', 'url': 'https://img.etimg.com/thumb/msid-76939477,width-600,height-60,quality-100/economictimes.jpg', 'width': 600, 'height': 60}",,,,,,,"New Delhi: Confederation of Indian Industry (CII) on Friday said it has established a new forum on artificial intelligence chaired by IBM's India/South Asia Managing Director Sandip Patel.The forum prioritises artificial intelligence (AI) as a driver of economic and business revival.The CII AI Forum will focus on building a strong AI ecosystem in India by building awareness at scale and enhancing capabilities by skilling/reskilling workforce for the future, CII said.It will also work with the government in shaping conducive policy and regulatory regime; encourage public-private partnerships in research and development, and facilitate pilot implementation of AI solutions in priority sectors, it added.The sectors that have been prioritised for this year are: banking and financial services industries (BFSI), retail, social (healthcare) and manufacturing (automotive).“As the economy moves into the recovery and revival phase, the transformational potential of responsible AI-driven solutions can be used to fuel India's growth story in a big way. CII AI Forum will look at initiatives to spur local innovations,  Patel said.He observed that this will make AI adoption a reality and further the national agenda of Digital India and Make in India for India and the world. More importantly, the forum will work on policies to embed trust and transparency into AI applications and processes, a critical step in realising true promise of the technology for business, society and the world,” said Patel.At the first meeting of the CII AI Forum for 2020-21 convened earlier this month, the members delved into the role of AI across diversified sectors to boost productivity and set the agenda to create new opportunities to galvanise the growth across the priority sectors.","{'@type': 'SpeakableSpecification', 'cssSelector': ['.article_wrap h1', '.artSyn h2']}",
https://news.google.com/rss/articles/CBMic2h0dHBzOi8vd3d3LmlidGltZXMuc2cvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtaGVscHMtYnJpbmctYmFjay0yMDAwLXllYXItb2xkLWNoaW5lc2UtdGVycmFjb3R0YS13YXJyaW9yLWxpZmUtNTA3NzfSAQA?oc=5,"Artificial Intelligence Helps to Bring Back 2,000-Year-Old Chinese Terracotta Warrior to Life - International Business Times, Singapore Edition",2020-08-28,"International Business Times, Singapore Edition",https://www.ibtimes.sg,A Chinese coder who released the video said that the inspiration came from famous YouTuber Denis Shiryaev's work,N/A,A Chinese coder who released the video said that the inspiration came from famous YouTuber Denis Shiryaev's work,A Chinese coder who released the video said that the inspiration came from famous YouTuber Denis Shiryaev's work,https://schema.org,,Technology,N/A,"
Artificial Intelligence Helps to Bring Back 2,000-Year-Old Chinese Terracotta Warrior to LifeA Chinese coder who released the video said that the inspiration came from famous YouTuber Denis Shiryaev's work

By Bhaswati Guha Majumder

August 28, 2020 16:05 +08








 VideoRelated VideosMore videos Video Player is loading.00:31 COPY LINKPlayUnmuteCurrent TimeÂ 0:00/DurationÂ -:-Loaded: 0%Stream TypeÂ LIVESeek to live, currently playing liveLIVERemaining TimeÂ --:-Â Playback Rate1xChaptersChaptersDescriptionsdescriptions off, selectedCaptionscaptions settings, opens captions settings dialogcaptions offEnglish
        
 Captions
, selectedAudio TrackQualityHDSD, selectedSDFullscreenRepeatShareThis is a modal window.The media could not be loaded, either because the server or network failed or because the format is not supported.Beginning of dialog window. Escape will cancel and close the window.TextColorWhiteBlackRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentBackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentTransparentWindowColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyTransparentSemi-TransparentOpaqueFont Size50%75%100%125%150%175%200%300%400%Text Edge StyleNoneRaisedDepressedUniformDropshadowFont FamilyProportional Sans-SerifMonospace Sans-SerifProportional SerifMonospace SerifCasualScriptSmall CapsReset restore all settings to the default valuesDoneClose Modal DialogEnd of dialog window.Advertisement 0:30Clearview AI lawsuit
Close

Clearview AI lawsuit   


The discovery of Chinese terracotta warriors is one of the greatest findings of the 20th century. It is a collection of terracotta sculptures depicting the armies of Chinese emperor Qin Shi Huang—the founder of the Qin dynasty. Now, with the help of technology, one of the warriors has been brought to life.Hu Wengu, a Chinese coder and Indie game developer has created a new video where he used Artificial Intelligence to generate realistic moving faces of real and fictional figures in Chinese history. These characters were recreated based on images or paintings of the historic sculptures that impressed many netizens, even though some facial movements still appear wobbly and distorted.AI Brought Them Back to Life 

Terracotta Warrior
Bilibili

One of the figures, featured in the video which was uploaded in the Chinese video-sharing website, was a terracotta warrior from the huge collection of sculptures created in the Qin dynasty that was buried along with the Chinese emperor to protect him in his afterlife.powered by AdSparcPlayback speed1x NormalQualityAutoBack360pAutoBack0.25x0.5x1x Normal1.5x2x/SkipAds by Wengu told media that to create the face of the warrior and make it look like real, he first used photoshop to replace the original metal-like face with color to make it look like human skin. Then the creator used Artbreeder—a website that can combine images using machine learning—to manipulate portraits and create a realistic face image of the terracotta warrior. He also made some adjustments and ran the image through the First-Order-Model algorithm to make an animation.The Indie game developer said the success rate of making such videos was not very high. But he spent two months to create this almost four-minute-long video during his spare time.From History The video created by Wengu also portrayed the founding emperor of the Ming dynasty, Zhu Yuanzhang, and one of the principal characters of Cao Xueqin's classic 18th-century Chinese novel Dream of the Red Chamber, Lin Daiyu.Many netizens who watched the video said that they were amazed after seeing the work of Wengu, while some of them had pointed out that the AI-restored Dauyu looks too beautiful from her original appearance that had deprived her the charm.

Lin Daiyu
Wikimedia commons

He also used synthetic media generating tools such as Stylegan-Art and Realistic-Neural-Talking-Head-Models. To make the video play smoothly, Wengu used the DAIN algorithm—the next-generation AI platform and specialized in addressing and solving AI problems—to add artificial frames, while Topaz Labs was used to sharpen the pictures.However, Wengu was also behind a video, which went viral in China in May. It used AI to revive color to black-and-white footage of old Beijing. He said the inspiration to create the previous video and the recent one came from YouTuber Denis Shiryaev's work. The YouTuber had created videos where he turned Leonardo da Vinci's Mona Lisa and other famous paintings into what looked like moving images.Read more Hackers Can Access Elon Musk's Neuralink to Steal Memories, Skills and Thoughts, Warn Experts AI Could Overtake Humans in 5 Years, Says Elon Musk, Whose 'Top Concern' is Google-Owned DeepMind British scientists researching on bees to develop new drone technology

Related topics : Artificial intelligence
",WebSite,https://www.ibtimes.sg/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{'@type': 'SearchAction', 'target': 'https://www.ibtimes.sg/search?q={search_term_string}', 'query-input': 'required name=search_term_string'}"
