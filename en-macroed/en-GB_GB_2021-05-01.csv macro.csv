URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,article:section,article:summary,article text,@context,@graph,url,@type,about,image,author,headline,publisher,dateCreated,thumbnailUrl,datePublished,articleSection,articleBody,isBasedOn,dateModified,isPartOf,isAccessibleForFree,alternativeHeadline,mainEntityOfPage,itemListElement,name
https://news.google.com/rss/articles/CBMiYmh0dHBzOi8vbWl0c2xvYW4ubWl0LmVkdS9pZGVhcy1tYWRlLXRvLW1hdHRlci9oaWRkZW4td29yay1jcmVhdGVkLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXByb2dyYW1z0gEA?oc=5,The hidden work created by artificial intelligence programs - MIT Sloan News,2021-05-04,MIT Sloan News,https://mitsloan.mit.edu,Successful and ethical artificial intelligence programs take into account behind-the-scenes ‘repair work’ and ‘ghost workers.’,N/A,Successful and ethical artificial intelligence programs take into account behind-the-scenes ‘repair work’ and ‘ghost workers.’,Successful and ethical artificial intelligence programs take into account behind-the-scenes ‘repair work’ and ‘ghost workers.’,N/A,N/A,"
 










recent




7 hours ago
Federal spending was responsible for the 2022 spike in inflation




Jul 16, 2024
Making generative AI work in the enterprise: New from MIT SMR




Jul 15, 2024
How to tap AI’s potential while avoiding its pitfalls 


















Credit: Rob Dobi















Ideas Made to Matter


                        Artificial Intelligence


The hidden work created by artificial intelligence programs

By
 Sara Brown


May 4, 2021






Why It Matters		



As AI programs become more prevalent, researchers say that companies need to prioritize behind-the scenes workers and those affected by the programs.



















Share













































































facebook
X
linkedin
email
print

open share links
close share links

Artificial intelligence is often framed in terms of headline-grabbing technology and dazzling promise. But some of the workers who enable these programs — the people who do things like code data, flag pictures, or work to integrate the programs into the workplace — are often overlooked or undervalued.
“This is a common pattern in the social studies of technology,” said Madeleine Clare Elish, SM ’10, a senior research scientist at Google. “A focus on new technology, the latest innovation, comes at the expense of the humans who are working to actually allow that innovation to function in the real world.""
Speaking at the recent EmTech Digital conference hosted by MIT Technology Review, Elish and other researchers said artificial intelligence programs often fail to account for the humans who incorporate AI systems into existing workflow, workers doing behind-the-scenes labor to make the programs run, and the people who are negatively affected by AI outcomes.
“This is a challenge for ethical AI, because overlooking the role of humans misses what’s actually going on,” Elish said.  
Here are their insights about making AI systems more effective and more ethical.
Consider how AI will be integrated into a workplace
In her previous job leading the AI on the Ground initiative at Data & Society, Elish and Elizabeth Anne Watkins, SM ’12, studied an initiative by Duke University and Duke Health System called Sepsis Watch. This is a clinical decision support system that uses AI to predict a patient’s risk of sepsis, a leading cause of death in hospitals that is notoriously difficult to diagnose and treat quickly.



Work smart with our Thinking Forward newsletterInsights from MIT experts, delivered every Tuesday morning.
Email Address









Leave this field blank





The program has dramatically improved the cases of patients with sepsis, Elish said, but for workers in the hospital, Sepsis Watch was disruptive. It changed the way rapid response nurses and doctors typically communicate, and nurses had to figure out the best ways to relay risk scores to doctors. Nurses also had to fit the Sepsis Watch information into existing emergency department practices.
“This hadn’t even crossed the minds of the tech development team, but this strategy proved essential,” Elish said. “We saw skilled individuals performing essential but overlooked and undervalued work.”
The nurses ended up doing what Elish and her fellow researchers call repair work — the work required to make a technology actually effective in a specific context, and to weave that technology into existing work practices, power dynamics, and cultural contexts.
The way many people focus on the technology and development of AI programs leaves out people doing on-the-ground innovation to make them work.
“So much of the actual day-to-day work that is required to make AI function in the world is rendered invisible, and then undervalued,” Elish said.
Even the language used to talk about launching AI systems tends to discount the importance of this work, Elish said.
“I actually try to avoid talking about ‘deploying systems’,” she said. “Deploy is a military term. It connotes a kind of contextless dropping in. And what we actually need to do with systems is to integrate them into particular context. And when you use words like ‘integrate,’ it requires you to say, ‘Integrate into what, or with whom?’”
In the case of Sepsis Watch, people respected the autonomy of the nurses, and they were allowed the discretion and flexibility to improvise and create ways to communicate about sepsis risk scores, Elish said. Those creating AI systems need to allocate resources toward supporting the people who will be doing this type of repair work, and making sure they are part of the project from beginning to end.
“An AI solution, in theory, doesn’t actually get us very far,” Elish said. “Responsible implementation, effective implementation, comes from focusing on how individuals will be empowered to use the solution in a particular context.”


An AI solution, in theory, doesn’t actually get us very far. Responsible implementation comes from focusing on how individuals will be empowered to use the solution in a particular context.

Madeleine Clare Elish
SM ’10, senior research scientist at Google




Share

























Don’t forget about ‘ghost workers’ behind the scenes 
Repair work to integrate AI into the workplace isn’t the only unseen labor. AI has created millions of new jobs, including for human workers who do things like labeling images so a machine learning model can learn, said Saiph Savage, director of the Civic Innovation Lab at the National Autonomous University of Mexico. Other human tasks might include transcribing audio, which helps voice assistants understand the world, or flagging violent content or disinformation on social media platforms.
The workers operating behind the scenes, often called ghost workers or invisible workers, are usually hidden to the end user, Savage said. Her research shows that these workers often earn below minimum wage and have limited opportunities for career growth and development.
Taking workers into account requires understanding systemic challenges they face, and what values they have. Savage said there are several ways AI programs can be used to help ghost workers, including some tools she’s created:
AI programs that can detect when an employer is being unfair to a worker, perhaps through things like negative feedback, and then nudge the employer to reconsider. AI programs can also be used to guide workers to achieve different goals.
Studying workers who have been able to grow and thrive, and building programs based on their strategies. “I computationally find those workers and I organize them to share tips and strategies for other workers, so that those other workers can follow [these] strategies and also increase their wages,” Savage said. She has developed web plug-ins that allow workers to share advice.
Auditing tools to understand the conditions workers are exposed to — things like hourly wages and invisible labor.
“We really need to think about the risks that we are exposing workers to,” Savage said. “For instance, the amount of invisible labor that we're forcing workers to do.”
Ask who’s not at the table, and whom AI might harm 
Abeba Birhane, a PhD candidate in cognitive science at University College Dublin, questioned the assumption that AI is a universally good thing that can solve any problem. AI and algorithmic systems “carry actual, tangible consequences to real people, whether it’s in policing or in the health care system,” she said.
A recurring theme across AI tools is that “individuals in communities that are at the margins of society, people who are the most vulnerable, are always the ones who pay the highest price,” Birhane said.
Things like facial recognition systems, health care algorithms, and privacy violations tend to disproportionately affect and disadvantage Black and transgender people, immigrants, and LGBTQ children.
Related Articles


Human-centered AI fights bias in machines and people 


3 ways to make technology more equitable


Machine learning, explained 


“If we ask who benefits, we find that people who are creating and deploying these systems benefit, while the cost falls heavily on the marginalized,” she said.
People creating artificial intelligence systems tend to be from privileged backgrounds she said, and are often ill-equipped to understand potential problems and provide solutions. (For example, AI programs used to look for welfare fraud have had inaccurate and disastrous results for some welfare recipients.)
Even conferences like EmTech, she pointed out, are aimed toward company presidents, CEOs, and directors.
“The idea of including or thinking of the end user, or those impacted as the stakeholders, seems a bit of a radical stance,” Birhane said, but those developing AI should consider talking to people in communities where the technology is going to be used.
To be ethical, AI should benefit the most marginalized and the most impacted, Birhane said. This starts with recognizing unjust structural and social power dynamics and hierarchies that benefit some and marginalize others, and thinking of ethics and fairness as concrete and urgent matters.
Companies should also think of ethics as an integral part of the process, from ideation to deployment, rather than an add-on.
“It's an ecology, it's a culture, it's a habit, it's a safe and supportive environment that is not hostile to critique, but that actively seeks critique,” Birhane said. 









































For more info
Sara Brown
Senior News Editor and Writer


sbrown1@mit.edu






Related Articles












Ideas Made to Matter

Making generative AI work in the enterprise: New from MIT SMR












Ideas Made to Matter

How to tap AI’s potential while avoiding its pitfalls 












Ideas Made to Matter

How can we preserve human ability in the age of machines?



",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiUGh0dHBzOi8vcGh5c2ljc3dvcmxkLmNvbS9hL2ZpZ2h0aW5nLWFsZ29yaXRobWljLWJpYXMtaW4tYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Uv0gEA?oc=5,Fighting algorithmic bias in artificial intelligence – Physics World - physicsworld.com,2021-05-04,physicsworld.com,https://physicsworld.com,N/A,N/A,Julianna Photopoulos explores the issues of racial and gender bias in AI – and what physicists can do to recognize and tackle the problem,N/A,N/A,N/A,"



								Taken from the May 2021 issue of Physics World. 						


Physicists are increasingly developing artificial intelligence and machine learning techniques to advance our understanding of the physical world but there is a rising concern about the bias in such systems and their wider impact on society at large. Julianna Photopoulos explores the issues of racial and gender bias in AI – and what physicists can do to recognize and tackle the problem







Built-in bias 
As artificial intelligence permeates many aspects of science and society, researchers must be aware of bias that creeps into these seemingly neutral systems, and the negative impacts 
on the already marginalized. (Courtesy: iStock/imaginima)

In 2011, during her undergraduate degree at Georgia Institute of Technology, Ghanaian-US computer scientist Joy Buolamwini discovered that getting a robot to play a simple game of peek-a-boo with her was impossible – the machine was incapable of seeing her dark-skinned face. Later, in 2015, as a Master’s student at Massachusetts Institute of Technology’s Media Lab working on a science–art project called Aspire Mirror, she had a similar issue with facial analysis software: it detected her face only when she wore a white mask. Was this a coincidence?
Buolamwini’s curiosity led her to run one of her profile images across four facial recognition demos, which, she discovered, either couldn’t identify a face at all or misgendered her – a bias that she refers to as the “coded gaze”. She then decided to test 1270 faces of politicians from three African and three European countries, with different features, skin tones and gender, which became her Master’s thesis project “Gender Shades: Intersectional accuracy disparities in commercial gender classification” (figure 1). Buolamwini uncovered that three commercially available facial-recognition technologies made by Microsoft, IBM and Megvii misidentified darker female faces nearly 35% of the time, while they worked almost perfectly (99%) on white men (Proceedings of Machine Learning Research 81 77).
Machines are often assumed to make smarter, better and more objective decisions, but this algorithmic bias is one of many examples that dispels the notion of machine neutrality and replicates existing inequalities in society. From Black individuals being mislabelled as gorillas or a Google search for “Black girls” or “Latina girls” leading to adult content to medical devices working poorly for people with darker skin, it is evident that algorithms can be inherently discriminatory (see box below).
“Computers are programmed by people who – even with good intentions – are still biased and discriminate within this unequal social world, in which there is racism and sexism,” says Joy Lisi Rankin, research lead for the Gender, Race and Power in AI programme at the AI Now Institute at New York University, whose books include A People’s History of Computing in the United States (2018 Harvard University Press). “They only reflect and amplify the larger biases of the world.”
Our main goal is to develop tools and algorithms to help physics, but unfortunately, we don’t anticipate how these can also be deployed in society to further oppress marginalized individuals
Jessica Esquivel
Physicists are increasingly using artificial intelligence (AI) and machine learning (ML) in a variety of fields, ranging from medical physics to materials. While they may believe their research will only be applied in physics, their findings can also be translated to society. “As particle physicists, our main goal is to develop tools and algorithms to help us find physics beyond the Standard Model, but unfortunately, we don’t step back and anticipate how these can also be deployed in technology and used every day within society to further oppress marginalized individuals,” says Jessica Esquivel, a physicist and data analyst from the Fermi National Accelerator Laboratory (Fermilab) in Chicago, Illinois, who is working on developing AI algorithms to enhance beam storage and optimization in the Muon g-2 experiment.
What’s more, the lack of diversity that exists in physics affects both the work carried out and the systems that are being created. “The huge gender and race imbalance problem is definitely a hindrance to rectifying some of these broader issues of bias in AI,” says Savannah Thais, a particle-physics and machine-learning researcher at Princeton University in New Jersey. That’s why physicists need to be aware of their existing biases and more importantly, as a community, need to be asking what exactly they should be doing.
1 Accuracy across the spectrum Joy Buolamwini, Timnit Gebru, Deborah Raji and colleagues work on the Gender Shades project to evaluate the accuracy of AI gender-classification products. The study looked at three companies’ commercial products and how they classified 1270 images of subjects from African and European countries. Subjects were grouped by gender, skin type, and the intersection of gender and skin type. The study found that while the products appear to have relatively high accuracy overall, there are notable differences in the error rates between different groups. All the companies performed better on male than female faces; and on lighter subjects than those with darker skin colour. The worst recognition was on darker females, failing on over 1 in 3 women of colour. A key factor in the accuracy differences is the lack of diversity in training images and benchmark data sets. (Courtesy: Joy Buolamwini)
‘Intelligent being’ beginnings
The idea that machines could become intelligent beings has existed for centuries, with myths about robots in ancient Greece and automatons in numerous civilizations. But it wasn’t until after the Second World War that scientists, mathematicians and philosophers began to discuss the possibility of creating an artificial mind. In 1950 the British mathematician Alan Turing famously asked whether machines could think and proposed the Turing Test for measuring their intelligence. Six years later, the research field of AI was formally founded during the Dartmouth Summer Research Project on Artificial Intelligence in Hanover, New Hampshire. Based on the notion that human thought processes could be defined and replicated in a computer program, the term “artificial intelligence” was coined by US mathematician John McCarthy – replacing the previously used “automata studies”.
Although the groundwork for AI and machine learning was laid down in the 1950s and 1960s, it took a while for the field to really take off. “It’s only in the past 10 years that there has been the combination of vast computing power, labelled data and wealth in tech companies to make artificial intelligence on a massive scale feasible,” says Rankin. And although Black and Latinx women in the US were discussing issues of discrimination and inequity in computing as far back as the 1970s, as highlighted by the 1983 Barriers to Equality in Academia: Women in Computer Science at MIT report, the problems of bias in computing systems have only become more widely discussed over the last decade.
Turning tides In the early days of computing, it was low-paid work largely performed by women. As the field became more prestigious, it was increasingly dominated by white men. This photo shows a US government employee using an NCR 796-201 video terminal, c. 1972. (Courtesy: National Archives at College Park)
The bias is all the more surprising given that women in fact formed the heart of the computing industry in the UK and the US from 1940s to the 1960s. “Computers used to be people, not machines,” says Rankin. “And many of those computers were women.” But as they were pushed out and replaced by white men, the field changed, as Rankin puts it, “from something that was more feminine and less valued to something that became prestigious, and therefore, also more masculine”. Indeed, in the mid-1980s nearly 40% of all those who graduated with a degree in computer science in the US were women, but that proportion had fallen to barely 15% by 2010.
Computer science, like physics, has one of the largest gender gaps in science, technology, engineering, mathematics and medicine (PLOS Biol. 16 e2004956). Despite increases in the number of women earning physics degrees, the proportion of women is about 20% across all degree levels in the US. Black representation in physics is even lower, with barely 3% of physics undergraduate degrees in the US being awarded to Black students in 2017. There is a similar problem in the UK, where women made up 57.5% of all undergraduate students in 2018, but only 1.7% of all physics undergraduate students were Black women.
This under-representation has serious consequences for how research is built, conducted and implemented. There is a harmful feedback loop between the lack of diversity in the communities building algorithmic technologies and the ways these technologies can harm women, people of colour, people with disabilities and the LGBTQ+ community, says Rankin. One example is Amazon’s experimental hiring algorithms, which – based as they were on their past hiring practices and applicant data – preferentially rejected women’s job applications. Amazon eventually abandoned the tool because gender bias was embedded too deeply in their system from past hiring practices and could not ensure fairness.
Many of these issues were tackled in Discriminating Systems – a major report from the AI Now Institute in 2019, which demonstrated that diversity and AI bias issues should not be considered separately because “they are two sides of the same problem”. Rankin adds that harassment within the workplace is also tied to discrimination and bias, noting that it has been reported by the National Academies of Sciences, Engineering, and Medicine that over 50% of female faculty and staff in scientific fields have experienced some form of harassment.
Having diverse voices in physics is essential for a number of reasons, according to Thais, who is currently developing accelerated ML-based reconstruction algorithms for the High-Luminosity Large Hadron Collider at CERN. “A large portion of physics researchers do not have direct lived experience with people of other races, genders and communities, which are impacted by these algorithms,” she says. That’s why marginalized individual scientists need to be involved in developing algorithms to ensure they are not inundated with bias, argues Esquivel.
It’s a message echoed by Pratyusha Kalluri, an AI researcher from Stanford University in the US, who co-created the Radical AI Network, which advocates anti-oppressive technologies, and gives a voice to those marginalized by AI. “It is time to put marginalized and impacted communities at the centre of AI research – their needs, knowledge and dreams should guide development,” she wrote last year in Nature (583 169).






Artificial intelligence technologies can reinforce inequalitiesPhysicists have key role to play in understanding societal impacts and regulating the industryVideo Player is loading.Play VideoPlayMuteCurrent Time 0:00/Duration 7:16Loaded: 2.26%0:00Stream Type LIVESeek to live, currently behind liveLIVERemaining Time -7:16 1xPlayback RateChaptersChaptersDescriptionsdescriptions off, selectedSubtitlessubtitles settings, opens subtitles settings dialogsubtitles offEnglish, selectedAudio Tracken (Main), selectedPicture-in-PictureFullscreenThis is a modal window.Beginning of dialog window. Escape will cancel and close the window.TextColorWhiteBlackRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentBackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentTransparentWindowColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyTransparentSemi-TransparentOpaqueFont Size50%75%100%125%150%175%200%300%400%Text Edge StyleNoneRaisedDepressedUniformDropshadowFont FamilyProportional Sans-SerifMonospace Sans-SerifProportional SerifMonospace SerifCasualScriptSmall CapsReset restore all settings to the default valuesDoneClose Modal DialogEnd of dialog window.Close Modal DialogThis is a modal window. This modal can be closed by pressing the Escape key or activating the close button.


Role of physicists
Back at Fermilab, Brian Nord is a cosmologist using AI to search for clues about the origins and evolution of the universe. “Telescopes scan the sky in multi-year surveys to collect very large amounts of complex data, including images, and I analyse that data using AI in pursuit of understanding dark energy which is causing space–time expansion to accelerate,” he explains. 
But in 2016 he realized that AI could be harmful and biased against Black people, after reading an investigation by ProPublica that analysed a risk-assessment software known as COMPAS, which is used in US courts to predict which criminals are most likely to reoffend and make decisions about bail setting. The investigation found that Black people were almost twice as likely as whites to be labelled a higher risk; irrespective of the severity of the crime committed, or the actual likelihood of re-offending. “I’m very concerned about my complicity in developing algorithms that could lead to applications where they’re used against me,” says Nord, who is Black and knows that facial-recognition technology, for example, is biased against him, often misidentifies Black men, and is under-regulated. So while physicists may have developed a certain AI technology to tackle purely scientific problems, its application in the real world is beyond their control and may be used for insidious purposes. “It’s more likely to lead to an infringement on my rights, to my disenfranchisement from communities and aspects of society and life,” he says.
Nord decided not to “reinvent the wheel” and is instead building a coalition of physicists and computer scientists to fight for more scrutiny when developing algorithms. He points to companies such as Clearview AI – a US facial-recognition outfit used by law-enforcement agencies and other private institutions – that are scraping social-media data and then selling a surveillance service to law enforcement without explicit consent. Countries around the world, including China, are using such surveillance technology for widespread oppressive purposes, he warns. “Physicists should be working to understand power structures – such as data privacy issues, how data and science have been used to violate civil rights, how technology has upheld white supremacy, the history of surveillance capitalism – in which data-driven technologies disenfranchise people.”
To bring this issue to wider attention, Nord, Esquivel and other colleagues wrote a letter to the entire particle-physics community as part of the Snowmass process, which regularly develops a scientific vision for the future of the community, both in the US and abroad. Their letter, which discussed the “Ethical implications for computational research and the roles of scientists”, emphasizes why physicists, as individuals or at institutions and funding agencies, should care about the algorithms they are building and implementing.
Thais also urges physicists to actively engage with AI ethics, especially as citizens with deep technical knowledge (APS Physics 13 107). “It’s extremely important that physicists are educated on these issues of bias in AI and machine learning, even though it typically doesn’t come up in physics research applications of ML,” she says. One reason for this, Thais explains, is that many physicists leave the field to work in computer software, hardware and data science companies. “Many of these companies are using human data, so we have to prepare our students to do that work responsibly,” she says. “We can’t just teach the technical skills and ignore the broader societal context because many are eventually going to apply these methods beyond physics.”
Both Thais and Esquivel also believe that physicists have an important role to play in understanding and regulating AI because they often have to interpret and quantify systematic uncertainties using methods that produce more accurate output data, which can then counteract the inherent bias in the data. “With a machine-learning algorithm that is more ‘black boxy’, we really want to understand how accurate the algorithm is, how it works on edge cases, and why it performs best for that certain problem,” Thais says, “and those are tasks that a physicist did before.”
Another researcher who is using physics to improve accuracy and reliability in AI is Payel Das, a principal research staff manager with IBM’s Thomas J Watson Research Center. To design new materials and antibiotics, she and her team are developing ML algorithms that can combine learning from both data and physics principles, increasing the success rate of a new scientific discovery up to 100-fold. “We often enhance, guide or validate the AI models with the help of prior scientific or other form of knowledge, for example, physics-based principles, in order to make the AI system more robust, efficient, interpretable and reliable,” says Das, further explaining that “by using physics-driven learning, one can crosscheck the AI models in terms of accuracy, reliability and inductive bias”.
The real-world impact of algorithmic bias 
Big brother Algorithmic decision-making tools may be developed for scientific research and then later used in commercial surveillance situations where any biases in the data have real-life consequences. (Courtesy: Mark Schiefelbein/AP/Shutterstock)


In 2015 a Black software developer Tweeted that Google Photos had labelled images of him with a friend as “gorillas”. Google managed to fix this issue by deleting the word “gorilla”, and some others referring to primates, from its vocabulary. By censoring these searches, the service can no longer find primates such as “gorilla”, “chimp”, “chimpanzee” or “monkey.”
When searching for the terms “Black girls”, “Latina girls” or “Asian girls”, the Google Ad portal would offer keyword suggestions related to pornography. Searches for “boys” of those same ethnicities also mostly returned suggestions related to pornography, but searches for “white girls” or “white boys” offered no suggested terms. In June 2020 the Google Ad portal was still perpetuating the objectification of Black, Latinx and Asian people, and has now solved the issue by blocking results from these terms.
Infrared technology, such as that in pulse oximeters, does not work properly on darker skin because less light passes through the skin. This can lead to inaccurate readings that may mean not getting the medical care needed. The same infrared technology has also been shown to fail in soap dispensers.



Auditing algorithms
In 2020 the Centre for Data Ethics and Innovation, the UK government’s independent advisory body on data-driven technologies, published a review on bias in algorithmic decision-making. It found that there has been a notable growth in algorithmic decision-making over the last few years across four sectors – recruitment, financial services, policing and local government – and discovered clear evidence of algorithmic bias. The report calls for organizations to actively use data to identify and mitigate bias, making sure to understand the capabilities and limitations of their tools. It’s a sentiment echoed by Michael Rovatsos, AI professor and director of the Bayes Centre at the University of Edinburgh. “It’s very hard to actually get access to the data or to the algorithms used,” he explains, adding that companies should be required by government to audit and be transparent about their systems applied in the real world.
Some researchers, just like Buolamwini, are trying to use their scientific experience in AI to uncover bias in commercial algorithms from the outside. They include the mathematician Cathy O’Neil, who wrote Weapons of Math Destruction in 2016 about her work on data-driven biases and who in 2018 founded a consultancy to work privately with companies and audit their algorithms. Buolamwini also continues her work trying to create more equitable and accountable technology through her non-profit Algorithmic Justice League – an interdisciplinary research institute that she founded in 2016 to understand the social implications of AI technologies.
Mirror mirror Computer scientist Joy Buolamwini tested this image of herself across a few facial-analysis demos. Microsoft and Face++ did not detect her face, while IBM and Kairos misgendered her. (Courtesy: TJ Rak)
Following her 2018 Gender Shades study, published with computer scientist Timnit Gebru who co-founded Black in AI, the findings were sent to the companies involved. A year later, a follow-up study was carried out to rerun the audits and added two more companies: Amazon and Kairos (AIES ’19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society 429). Led by Deborah Raji, a computer scientist and currently a Mozilla Foundation fellow, the follow-up found that the two had huge accuracy errors – Amazon’s facial recognition software even failed to classify Michelle Obama’s face correctly – but the original three companies had improved significantly, suggesting their dataset was trained with more diverse images.

These two studies had a profound real-world impact, leading to two US federal bills – the Algorithmic Accountability Act and No Biometric Barriers Act – as well as state bills in New York and Massachusetts. The research also helped persuade Microsoft, IBM and Amazon to put a hold on their facial-recognition technology for the police. “We designed an ‘actionable audit’ to lead to some accountability action, whether that be an update or modification to the product or its removal – something that past audits had struggled with,” says Raji. She is continuing her work on algorithmic evaluation and in 2020 developed with colleagues at Google a framework of algorithmic audits (arXiv:2001.00973) for AI accountability. “Internal auditing is essential as it allows for changes to be made to a system before it gets deployed out in the world,” she says, adding that sometimes the bias involved can be harmful for a particular population “so it’s important to identify the groups that are most vulnerable in these decisions and audit for the moments in the development pipeline that could introduce bias”.
In 2019 the AI Now Institute published a detailed report outlining a framework for public agencies interested in adopting algorithmic decision-making tools responsibly, and subsequently released an Algorithmic Accountability Policy Toolkit. The report called for AI and ML researchers to know what they are building; to account for potential risks and harms; and to better document the origins of their models and data. Esquivel points out the importance of physicists knowing where their data has come from, especially the data sets used to train ML systems. “Many of the algorithms that are being used on particle-physics data are fine-tuned architectures that were developed by AI experts and trained on industry standard data sets – data sets that have been shown to be racist, discriminatory and sexist,” she says, using the example of MIT permanently taking down a widely used, 80-million-image AI data set because existing images were labelled in offensive, racist and misogynistic ways.
Gebru and her colleagues also recently highlighted problems with large data sets such as the Common Crawl open repository of web crawl data, where there is an over-representation of white supremacist, ageist and misogynistic views (FAccT ’21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency) – a paper for which she was recently fired from Google’s AI Ethics team. Consequently, Esquivel is clear that academics “have the opportunity to act as an objective third party to the development of these tools”.
Removing bias
The 2019 AI Now Institute report also recommends AI bias research to move beyond technical fixes. “It’s not just that we need to change the algorithms or the systems; we need to change institutions and social structures,” explains Rankin. From her perspective, to have any chance of removing or minimizing and regulating bias and discrimination, there would need to be “massive, collective action”. Involving people from beyond the natural sciences community in the process would also help.
It’s not just that we need to change the algorithms or the systems; we need to change institutions and social structures
Joy Lisi Rankin
Nord agrees that physicists need to work with scientists from other disciplines, as well as with social scientists and ethicists. “Unfortunately, I don’t see physical scientists or computer scientists engaging sufficiently with the literature and communities of these other fields that have spent so much time and energy in studying these issues,” he says, noting that “it seems like every couple of weeks there is a new terrible, harmful and inane machine-learning application that tries to do the impossible and the unethical.” For example, the University of Texas in Austin only recently stopped using an ML system to predict success in graduate school, whose data was based on previous admissions cycles and so would have carried biases. “Why do we pursue such technocratic solutions in a necessarily humanistic space?” asks Nord.
Thais insists that physicists must become better informed about the current state of these issues of bias, and then understand the approaches adopted by others for mitigating them. “We have to bring these conversations into all of our discussions around machine learning and artificial intelligence,” she says, hoping physicists might attend relevant conferences, workshops or talks. “This technology is impacting and already ingrained in so many facets of our lives that it’s irresponsible to not contextualize the work in the broader societal context.”
Nord is even clearer. “Physicists should be asking whether they should, before asking whether they could, create or implement some AI technology,” he says, adding that it is also possible to stop using existing, harmful technology. “The use of these technologies is a choice we make as individuals and as a society.”









Want to read more?
Registration is free, quick and easy
Note: The verification e-mail to complete your account registration should arrive immediately. However, in some cases it takes longer. Don't forget to check your spam folder.
If you haven't received the e-mail in 24 hours, please contact customerservices@ioppublishing.org.




E-mail Address







Register











",https://schema.org,"[{'@type': 'WebPage', '@id': 'https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/', 'url': 'https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/', 'name': 'Fighting algorithmic bias in artificial intelligence &#8211; Physics World', 'isPartOf': {'@id': 'https://physicsworld.com/#website'}, 'primaryImageOfPage': {'@id': 'https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/#primaryimage'}, 'image': {'@id': 'https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://physicsworld.com/wp-content/uploads/2021/05/Photopoulos-artificial-intelligence-1310293181-iStock_imaginima.jpg', 'datePublished': '2021-05-04T08:03:13+00:00', 'dateModified': '2023-11-03T15:54:21+00:00', 'author': {'@id': 'https://physicsworld.com/#/schema/person/9c3a671a40bae47f4281c2e58b89a730'}, 'breadcrumb': {'@id': 'https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/#breadcrumb'}, 'inLanguage': 'en-GB', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-GB', '@id': 'https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/#primaryimage', 'url': 'https://physicsworld.com/wp-content/uploads/2021/05/Photopoulos-artificial-intelligence-1310293181-iStock_imaginima.jpg', 'contentUrl': 'https://physicsworld.com/wp-content/uploads/2021/05/Photopoulos-artificial-intelligence-1310293181-iStock_imaginima.jpg', 'width': 1200, 'height': 900, 'caption': 'Built-in bias As artificial intelligence permeates many aspects of science and society, researchers must be aware of bias that creeps into these seemingly neutral systems, and the negative impacts on the already marginalized. (Courtesy: iStock/imaginima)'}, {'@type': 'BreadcrumbList', '@id': 'https://physicsworld.com/fighting-algorithmic-bias-in-artificial-intelligence/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://physicsworld.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Fighting algorithmic bias in artificial intelligence'}]}, {'@type': 'WebSite', '@id': 'https://physicsworld.com/#website', 'url': 'https://physicsworld.com/', 'name': 'Physics World', 'description': '', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://physicsworld.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-GB'}, {'@type': 'Person', '@id': 'https://physicsworld.com/#/schema/person/9c3a671a40bae47f4281c2e58b89a730', 'name': 'No Author', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-GB', '@id': 'https://physicsworld.com/#/schema/person/image/', 'url': 'https://secure.gravatar.com/avatar/59c7726d03bb570d63a0310935ced1de?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/59c7726d03bb570d63a0310935ced1de?s=96&d=mm&r=g', 'caption': 'No Author'}, 'url': 'https://physicsworld.com/author/no-author/'}]",,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vd3d3Lm1hcmtldHBsYWNlLm9yZy8yMDIxLzA1LzA0L3RoZS1odW1hbi1sYWJvci1iZWhpbmQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2Uv0gEA?oc=5,The human labor behind artificial intelligence - Marketplace,2021-05-04,Marketplace,https://www.marketplace.org,N/A,N/A,Marketplace speaks to data labelers in central China.,Marketplace speaks to data labelers in central China.,N/A,N/A,N/A,https://schema.org,"[{'@type': 'Article', '@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/#article', 'isPartOf': {'@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/'}, 'author': [{'@id': 'https://www.marketplace.org/#/schema/person/8797a6e418263efbbaec9659d8340236'}], 'headline': 'The human labor behind artificial intelligence', 'datePublished': '2021-05-04T11:34:44+00:00', 'dateModified': '2021-05-04T17:27:19+00:00', 'mainEntityOfPage': {'@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/'}, 'wordCount': 932, 'publisher': {'@id': 'https://www.marketplace.org/#organization'}, 'image': {'@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://www.marketplace.org/wp-content/uploads/2021/04/DSC02390.jpg?fit=3532%2C2354', 'keywords': ['artificial intelligence', 'China', ""China's Economy"", 'data', 'labels'], 'inLanguage': 'en-US', 'copyrightYear': '2021', 'copyrightHolder': {'@id': 'https://www.marketplace.org/#organization'}}, {'@type': 'WebPage', '@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/', 'url': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/', 'name': 'The human labor behind artificial intelligence - Marketplace', 'isPartOf': {'@id': 'https://www.marketplace.org/#website'}, 'primaryImageOfPage': {'@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/#primaryimage'}, 'image': {'@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://www.marketplace.org/wp-content/uploads/2021/04/DSC02390.jpg?fit=3532%2C2354', 'datePublished': '2021-05-04T11:34:44+00:00', 'dateModified': '2021-05-04T17:27:19+00:00', 'breadcrumb': {'@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/#primaryimage', 'url': 'https://www.marketplace.org/wp-content/uploads/2021/04/DSC02390.jpg?fit=3532%2C2354', 'contentUrl': 'https://www.marketplace.org/wp-content/uploads/2021/04/DSC02390.jpg?fit=3532%2C2354', 'width': 3532, 'height': 2354, 'caption': 'Data labeling firms like this one in Henan province are the new factory floor of the digital age.'}, {'@type': 'BreadcrumbList', '@id': 'https://www.marketplace.org/2021/05/04/the-human-labor-behind-artificial-intelligence/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Latest Stories', 'item': 'https://www.marketplace.org/latest-stories/'}, {'@type': 'ListItem', 'position': 2, 'name': 'The human labor behind artificial intelligence'}]}, {'@type': 'WebSite', '@id': 'https://www.marketplace.org/#website', 'url': 'https://www.marketplace.org/', 'name': 'Marketplace', 'description': 'Raising your economic intelligence', 'publisher': {'@id': 'https://www.marketplace.org/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.marketplace.org/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.marketplace.org/#organization', 'name': 'Marketplace', 'url': 'https://www.marketplace.org/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.marketplace.org/#/schema/logo/image/', 'url': 'https://www.marketplace.org/wp-content/uploads/2020/07/marketplace_pod.jpg?fit=1000%2C1001', 'contentUrl': 'https://www.marketplace.org/wp-content/uploads/2020/07/marketplace_pod.jpg?fit=1000%2C1001', 'width': 1000, 'height': 1001, 'caption': 'Marketplace'}, 'image': {'@id': 'https://www.marketplace.org/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/marketplaceapm/', 'https://twitter.com/Marketplace', 'https://www.youtube.com/user/marketplacevideos/featured']}, {'@type': 'Person', '@id': 'https://www.marketplace.org/#/schema/person/8797a6e418263efbbaec9659d8340236', 'name': 'Jennifer Pak', 'description': 'Jennifer is Marketplace’s China correspondent, based in Shanghai. She tells stories about the world’s second-biggest economy and why Americans should care about it. She arrived in Beijing in 2006 with few journalism contacts but quickly set up her own news bureau. Her work has appeared in many news outlets, including the BBC, NPR and The Financial Times. After covering the 2008 Beijing Olympics, Jennifer moved to Kuala Lumpur to be the BBC’s Malaysia correspondent. She reported on the disappearance of Malaysia Airlines Flight MH370 and Edward Snowden’s brief escape to Hong Kong. Jennifer returned to China in 2015, based in the high-tech hub of Shenzhen, before joining Marketplace two years later. In 2022, Jennifer, along with 25 million Shanghai residents, was locked down for over 60 days and had to scramble for food. The coverage of the pandemic she and her team produced helped earn them a Gracie and a National Headliner Award in 2023. You can see the food Jennifer was able to get during the Shanghai lockdown here\xa0and keep up with her tasty finds across China on Instagram at @jpakradio.', 'sameAs': ['https://www.instagram.com/jpakradio/?hl=en'], 'url': 'https://www.marketplace.org/author/jennifer-pak/'}]",,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiYWh0dHBzOi8vd3d3LmFhYXMub3JnL25ld3MvYmlwbGF2LXNyaXZhc3RhdmEtYXBwbGllcy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1yZWFsLXdvcmxkLWNoYWxsZW5nZXPSAQA?oc=5,Biplav Srivastava Applies Artificial Intelligence to Real-World Challenges - AAAS,2021-05-05,AAAS,https://www.aaas.org,"One of Biplav Srivastava’s long-term goals is to help people use technology to solve the problems they face. This wasn’t always his top priority, though – for many years after completing his Ph.D., he didn’t really worry about connecting his research to real-world impacts. Then, he says, he started to become very concerned that while his work was having significant commercial success, “it meant nothing on the street… it has limited value if you can’t see it around you.”",N/A,"One of Biplav Srivastava’s long-term goals is to help people use technology to solve the problems they face. This wasn’t always his top priority, though – for many years after completing his Ph.D., he didn’t really worry about connecting his research to real-world impacts. Then, he says, he started to become very concerned that while his work was having significant commercial success, “it meant nothing on the street… it has limited value if you can’t see it around you.”","One of Biplav Srivastava’s long-term goals is to help people use technology to solve the problems they face. This wasn’t always his top priority, though – for many years after completing his Ph.D., he didn’t really worry about connecting his research to real-world impacts. Then, he says, he started to become very concerned that while his work was having significant commercial success, “it meant nothing on the street… it has limited value if you can’t see it around you.”",N/A,N/A,"

5 May 2021
by: Elana Kimbrell




 










        Biplav Srivastava organized a virtual/in-person event in October 2020 on the ethical use of digital assistants like chatbots.
      

Photo credit: Abe Danaher








1




        Biplav Srivastava organized a virtual/in-person event in October 2020 on the ethical use of digital assistants like chatbots.
      

Photo credit: Abe Danaher









One of Biplav Srivastava’s long-term goals is to help people use technology to solve the problems they face. This wasn’t always his top priority, though – for many years after completing his Ph.D., he didn’t really worry about connecting his research to real-world impacts. Then, he says, he started to become very concerned that while his work was having significant commercial success, “it meant nothing on the street… it has limited value if you can’t see it around you.”
Srivastava is a 2020-21 AAAS Leshner Leadership Institute Public Engagement Fellow and last July, he became a professor of computer science at the University of South Carolina’s Artificial Intelligence (AI) Institute after many years at IBM. Starting in 2011 while still at IBM, he began applying his knowledge of AI to sustainability projects around the world, particularly related to developing smarter cities through addressing issues like water access, electricity, transportation, and public health. One example of this work is Water Advisor, a chat-based tool he and his team at IBM developed, which uses machine learning techniques to help users easily find information about water quality in different locations. 
This turn towards impact-oriented research eventually led Srivastava to the AAAS Leshner Leadership Institute, which supports scientists and engineers in public engagement that has broader societal impacts. The fellowship cohorts are focused on science and engineering issues with societal significance. And the importance of AI, the focus of Srivastava’s cohort, only increased with the onset of the COVID-19 pandemic, when AI was put to use in multitude of response efforts. Srivastava put together a collection of resources on AI and COVID, and created a tool to analyze the effect of mask-wearing. He is featured in a video and article on The Conversation about this tool, which can calculate, for specific locations, how many lives were saved or lost depending on when mask mandates were implemented – showing that early mandates had exponentially greater benefits.
As one of his key activities for the public engagement fellowship, Srivastava organized a hybrid virtual/in-person event at his university in October 2020 to promote the ethical use of digital assistants in society. It brought together perspectives on data-driven decision support from across the science, engineering, and societal aspects of AI technologies. Six additional Leshner Fellows from the AI cohort spoke on a panel about the role of AI in handling COVID-19 at universities and in communities, and Emily Therese Cloyd, director of the AAAS Center for Public Engagement with Science, introduced the Leshner Leadership Institute. Srivastava reports that people from across the university are now thinking about how they can work together. Several proposals have arisen as a result, as well as follow-on events including organizing an AI summer program for high school students. 
Moving forward, Srivastava has several related goals -- he is interested in open data collaborations, and has spoken with the local government in Columbia, South Carolina, where his university is located, about the possibility of making more of their data public. Although the local government doesn’t currently have the capacity to do this, his vision is to eventually create fun, competitive events for students to try to solve local problems using local data – such as prioritizing which water pipes need to be replaced. However, he also wants to promote the message that while AI can be extremely useful, it can’t solve everything in isolation: it has to be developed in conversation with end users and the public more broadly, so that it works for the people it’s intended to serve. 
The AAAS Leshner Leadership Institute was founded in 2015 and operates through philanthropic gifts in honor of CEO Emeritus Alan I. Leshner. Each year the Institute provides public engagement training and support to 10-15 mid-career scientists from an area of research at the nexus of science and society.






",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWGh0dHBzOi8vc2xhdGUuY29tL3RlY2hub2xvZ3kvMjAyMS8wNS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1tb29uc2hvdHMtdXN1YWxseS1mYWlsLmh0bWzSAQA?oc=5,Why ambitious predictions about A.I. are always wrong. - Slate,2021-05-04,Slate,https://slate.com,Getting good at chess is one thing. Surpassing the human brain is quite another.,"['artificial-intelligence', 'personal-finance', 'research', 'self-driving-cars', 'robots', 'google', 'health-care', 'redux', 'Section:technology', 'AdNode:technology/future_tense']",Getting good at chess is one thing. Surpassing the human brain is quite another.,Getting good at chess is one thing. Surpassing the human brain is quite another.,N/A,N/A,"
















Why A.I. Moonshots Miss
Ambitious predictions about the future powers of computers keep turning out to be wrong.


By
      
Jeffrey Funk and Gary Smith


May 04, 20215:45 AM











Photo illustration by Slate. Photos by sarawuth702/iStock/Getty Images Plus and allanswart/iStock/Getty Images Plus.








Tweet
  



Share




Share




Comment
    










Tweet
  



Share




Share




Comment
    




Since the very beginning of the computer revolution, researchers have dreamed of creating computers that would rival the human brain. Our brains are information machines that use inputs to generate outputs, and so are computers. How hard could it be to build computers that work as well as our brains?
In 1954 a Georgetown-IBM team predicted that language translation programs would be perfected in three to five years. In 1965 Herbert Simon said that “machines will be capable, within twenty years, of doing any work a man can do.” In 1970 Marvin Minsky told Life magazine, “In from three to eight years we will have a machine with the general intelligence of an average human being.” Billions of dollars have been poured into efforts to build computers with artificial intelligence that equals or surpasses human intelligence. Researchers didn’t know it at first, but this was a moonshot—a wildly ambitious effort that had little chance of a quick payoff.


1/100:11Meeting Cleve Jones, Harvey Milk’s Intern | Slow Burn: Gays Against Briggs





Skip Ad
 
Continue watchingMeeting Cleve Jones, Harvey Milk’s Intern | Slow Burn: Gays Against Briggsafter the adVisit Advertiser websiteGO TO PAGE

So far, it has failed. We still know very little about how the human brain works, but we have learned that building computers that rival human brains is not just a question of computational power and clever code.
A.I. research was launched at a summer conference at Dartmouth in 1956 with the moonshot vision that “every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.” Seventeen years later, the 1973 Lighthill report commissioned by the U.K. Science Research Council concluded that “in no part of the field have the discoveries made so far produced the major impact that was then promised.” Funding dried up and an A.I. winter began. There was a resurgence of A.I. research in the 1980s, fueled by advances in computer memory and processing speed and the development of expert systems, followed by a second A.I. winter as the limitations of expert systems became apparent. Another resurgence began in the 1990s and continues to this day.



Advertisement








Advertisement








Advertisement








Advertisement






We still know very little about how the human brain works, but we have learned that building computers that rival human brains is not just a question of computational power and clever code.

Widely publicized computer victories over world champions in backgammon, checkers, chess, Go, and Jeopardy! have fueled the idea that the initial hopes for A.I. are on the verge of being realized. But just as in the first decades of moonshot hope, ambitious predictions and moving goalposts continue to be the norm.
In 2014, Ray Kurzweil predicted that by 2029, computers will have human-level intelligence and will have all of the intellectual and emotional capabilities of humans, including “the ability to tell a joke, to be funny, to be romantic, to be loving, to be sexy.” As we move closer to 2029, Kurzweil talks more about 2045.
In a 2009 TED talk, Israeli neuroscientist Henry Markram said that within a decade his research group would reverse-engineer the human brain by using a supercomputer to simulate the brain’s 86 billion neurons and 100 trillion synapses.



Advertisement





These failed goals cost money. After being promised $1.3 billion in funding from the European Union, Markram’s Human Brain Project crashed in 2015. In 2016, the market research firm PwC predicted that GDP would be 14 percent or $15.7 trillion higher in 2030 because of A.I. products and services. They weren’t alone. McKinsey, Accenture, and Forrester also forecast similar figures by 2030, with Forrester in 2016 predicting $1.2 trillion in 2020. Four years later, in 2020, Forrester reported that the A.I. market was only $17 billion. It now projects the market to reach $37 billion by 2025. Oops!



Advertisement





The $15 trillion predictions made in 2016 assumed the success of A.I. moonshots such as Watson for health care, DeepMind and Nest for energy use, Level 5 self-driving vehicles on public roads, and humanlike robots and text. When moonshots like these work, they can be revolutionary; when they turn out to be pie in the sky, the failures are costly.



Advertisement








Advertisement





We have learned the hard way that winning a game of Go or Jeopardy! is a lot easier than processing words and images, providing effective health care, and building self-driving cars. Computers are like New Zealander Nigel Richards, who memorized the 386,000 words in the French Scrabble dictionary and won the French-language Scrabble World Championship twice, even though he doesn’t know the meaning of the French words he spells. In the same way, computer algorithms fit mathematical equations to data that they do not understand and consequently cannot employ any of the critical thinking skills that humans have.
If a computer algorithm found a correlation between Donald Trump tweeting the word with and the price of tea in China four days later, it had no way of assessing whether this correlation is meaningful or meaningless. A state-of-the-art image recognition program was 99 percent certain that a series of horizontal black and yellow lines was a school bus, evidently focusing on the color of the pixels and completely ignoring the fact that buses have wheels, doors, and a windshield.



Advertisement








Advertisement








Advertisement





The health care moonshot has also disappointed. Swayed by IBM’s Watson boasts, McKinsey predicted a 30–50 percent productivity improvement for nurses, a 5–9 percent reduction in health care costs, and health care savings in developed countries equal to up to 2 percent of GDP. The Wall Street Journal published a cautionary article in 2017, and soon others were questioning the hype. A 2019 article in IEEE Spectrum concluded that Watson had “overpromised and underdelivered.” Soon afterward, IBM pulled Watson from drug discovery, and media enthusiasm waned as bad news about A.I. health care accumulated. For example, a 2020 Mayo Clinic and Harvard survey of clinical staff who were using A.I.-based clinical decision support to improve glycemic control in patients with diabetes gave the program a median score of 11 on a scale of 0 to 100, with only 14 percent saying that they would recommend the system to other clinics.



Advertisement








Advertisement





Following Watson’s failure, the media moved on to Google health care articles in Nature and other journals that reported black-box results with unreported tweaks that were needed to make the models work well. After Google published its protein folding paper, an expert in structural biology said, “Until DeepMind shares their code, nobody in the field cares and it’s just them patting themselves on the back.” He also said that the idea that protein folding had been solved was “laughable.” An international group of scientists described a Google paper on breast cancer as another “very high-profile journal publishing a very exciting study that has nothing to do with science. … It’s more an advertisement for cool technology. We can’t really do anything with it.” Such cautions are well deserved in light of the flop of Google’s highly touted Flu Trends algorithm. After claiming to be 97.5 percent accurate in predicting flu outbreaks, Google Flu Trends overestimated the number of flu cases for 100 of the next 108 weeks, by an average of nearly 100 percent, before being quietly retired.



Advertisement








Advertisement





The self-driving vehicle moonshot is in a similar state. By late 2018, it was becoming clear that self-driving cars were much harder than originally thought, with one Wall Street Journal article titled “Driverless Hype Collides With Merciless Reality.” In 2020, startups like Zoox, Ike, Kodiak Robotics, Lyft, Uber, and Velodyne began layoffs, bankruptcies, revaluations, and liquidations at deflated prices. Uber sold its autonomous unit in late 2020 after years of claiming that self-driving vehicles were its key to future profitability. An MIT task force announced in mid-2020 that fully driverless systems will take at least a decade to deploy over large areas.
Overall, A.I. moonshots are proving to be an expensive collection of failures. An October 2020 Wired article titled “Companies Are Rushing to Use AI—but Few See a Payoff” reported that only 11 percent of firms that have deployed A.I. are reaping a “sizable” return on their investments. One reason is that costs often turn out to be higher—much higher—than originally assumed. According to a fall 2020 MIT Sloan Management Review article, “A good rule of thumb is that you should estimate that for every $1 you spend developing an algorithm, you must spend $100 to deploy and support it.”



Advertisement








Advertisement








Advertisement








Advertisement





The 2020 edition of the “State of AI Report,” published by A.I. investors Nathan Benaich and Ian Hogarth, concluded that “we’re rapidly approaching outrageous computational, economic, and environmental costs to gain incrementally smaller improvements in model performance.” For example, “Without major new research breakthroughs, dropping the [image recognition] error rate from 11.5% to 1% would require over one hundred billion billion dollars!”
The fact is most moonshots fail: nuclear fusion, synthetic fuels, supersonic flight, maglev, and blockchain for everything. Instead, successful technologies generally begin in small and often overlooked applications and then expand to bigger and more important ones. Transistors were first used in hearing aids and radios before becoming ubiquitous in military equipment, computers, and phones. Computers began with accounting applications and later expanded to every function of a company. LEDs were first used in calculators and automobile dashboards, long before being used for lighting. The internet began as a tool for professors before becoming the most widely used technology since electricity. Solar cells were used in satellites and remote locations long before they were used to generate electricity for urban homes and business. In almost every case, technologies begin in a niche and then incrementally expand to other applications over decades through exponential improvements in price and performance.



Advertisement








Advertisement





Some companies successfully focus their A.I. efforts on solutions to small problems with achievable benefits. For instance, DHL uses A.I.-controlled robots to find packages, move them around warehouses, and load them onto planes. And Microsoft recently acquired Nuance, a company best known for a deep learning voice transcription service that is very popular in the health care sector.
Many similar examples can be found in robotic process automation—software robots that emulate humans interacting with digital systems. It can be used for accounting, manufacturing, financial, and engineering transactions, and it is the fastest-growing segment of the A.I. market.



Advertisement







Recommended for You



The Delegate Whose Trump Tribute Is “Starting a New Fashion Trend” at the RNC



Help! My Friend Came to Me With a Dire Warning About My Fiancé Before the Wedding.



Say What You Will About Age-Gap Relationships. Tobey Maguire Is Not Allowed to Have Them.




The same incremental approach can be used for health care, self-driving vehicles, and more. Mutually beneficial diffusion and progress can come from collaboration among large research hospitals within and across countries as researchers learn from one another and generalize from one case to another. The holy grail of a robotaxi that can operate without a driver in every geographic location no matter the weather remains elusive, but self-driving vehicles are used successfully in constrained environments like mining camps, large factories, industrial parks, theme parks, golf clubs, and university campuses. It is surely better to perfect small solutions before moving on to crowded public roads with a plethora of unforeseen hazards.
One of the reasons A.I. overpromised and underdelivered is that we didn’t anticipate that building a computer that surpasses the human brain is the moonshot of all moonshots. Computers may someday rival human intelligence. In the meantime, we should recognize the limitations of A.I. and take advantage of the real strengths of computers. The failure of A.I. moonshots is not a reason to give up on A.I., but it is a reason to be realistic about what A.I. can do for us.

Future Tense
    is a partnership of
    Slate,
    New America, and
    Arizona State University
    that examines emerging technologies, public policy, and society.





Tweet
  



Share




Share




Comment
    





              Artificial Intelligence
            


              Google
            


              Health Care
            


              Personal Finance
            


              Research
            


              Robots
            


              Self-Driving Cars
            
 





Advertisement






",http://schema.org,,https://slate.com/technology/2021/05/artificial-intelligence-moonshots-usually-fail.html,NewsArticle,Getting good at chess is one thing. Surpassing the human brain is quite another.,"{'url': 'https://compote.slate.com/images/707ebe58-bcf6-4db1-adf1-87438e544d8a.jpeg?width=1200&rect=1560x1040&offset=0x0', '@type': 'ImageObject', 'width': 1200, 'height': 800}","[{'name': 'Jeffrey Funk', '@type': 'Person'}, {'name': 'Gary Smith', '@type': 'Person'}]",Why Ambitious Predictions About A.I. Are Always Wrong,"{'logo': {'url': 'https://dot.cdnslate.com/static/media/sites/slate-com/icon.400x400.09ec623.png', '@type': 'ImageObject', 'width': 400, 'height': 400}, 'name': 'Slate', '@type': 'Organization'}",2021-05-04T09:45:00+00:00,https://compote.slate.com/images/707ebe58-bcf6-4db1-adf1-87438e544d8a.jpeg?width=780&height=520&rect=1560x1040&offset=0x0,2021-05-04T09:45:00+00:00,Future Tense,,,,,,,,,
https://news.google.com/rss/articles/CBMiPGh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9mYWN1bHR5LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlL9IBAA?oc=5,What Faculty is planning next - WIRED,2021-05-05,WIRED,https://www.wired.com,The AI firm’s links to Dominic Cummings made it headline news. Now it’s putting AI to work on businesses and the NHS,"['business', 'ai hub', 'startups', 'regulation', 'may / june 2021 issue', '_wired-uk-migrated', 'textaboveleftsmall', 'magazine']",The AI firm’s links to Dominic Cummings made it headline news. Now it’s putting AI to work on businesses and the NHS,The AI firm’s links to Dominic Cummings made it headline news. Now it’s putting AI to work on businesses and the NHS,tags,N/A,"Matt BurgessBusinessMay 5, 2021 4:44 AMWhat Faculty is planning nextThe AI firm’s links to Dominic Cummings made it headline news. Now it’s putting AI to work on businesses and the NHSSave this storySaveSave this storySaveAs the deadly first wave of the coronavirus pandemic hit the UK in March 2020, Marc Warner stopped going to his usual work meetings. The NHS had put out a plea for help from technology firms, and Warner, the CEO of London-based artificial intelligence firm Faculty, rang his investors and told them he would be focussing on the pandemic response. A symphony of government and NHS contracts ensued – five of them directly linked to the pandemic management. Warner says he threw himself into the work and didn’t attend company meetings for months. “One of the jokes we make is that 2020 was so bad I had to code again,” he says of the pandemic’s earliest days. “No one in Faculty wants that – I was never a great coder.”Faculty’s NHS work focused on two major areas: helping to bring together unruly sprawls of health information, and building AI for the NHS to forecast where supplies are needed. The first of these projects, called the NHS Covid-19 Data Store, is a collection of health datasets, including everything from populations and NHS staff levels to Covid-19 death data and calls to emergency services. The system, which Faculty created alongside Google, Amazon, Microsoft and Palantir, organises and makes data available to decision-makers. “Keeping data in a good state is an incredibly tricky problem in a normal situation,” Warner says. “It was very clear, fairly early on, that it was going to be incredibly powerful to have a really good view of the system, [of] what's going on now.”The second stage was predicting what would happen next. What this looked like was an AI model – based on previous bed use and NHS 111 call volumes, among other data – that lets hospitals predict potential outbreaks for the next three weeks. The model has been given to local NHS Trusts after being used nationally by NHS England and forecasts where ventilators and beds, among other things, would be needed most. As Faculty’s pandemic work progressed, Warner, who has a PhD in quantum computing and a degree in physics, moved to “red teaming” its output: digging into the AI models, trying to break things.Trending NowThe Best of Tech Support: Ken Jeong, Bill Nye, Nicole Stott and MoreThe decision to focus on the pandemic had repercussions, he says: some of Faculty’s work became “unaligned” without the CEO overseeing it. “The other set of consequences that truthfully, we never anticipated going into it, [were] the media repercussions of me being more directly involved,” Warner says. The company’s early work in politics and government connections have made it, to some, one of the most controversial AI companies around.In 2020, The Guardian reported that Faculty had been given 14 government contracts in two years – including for its pandemic work – and that Warner’s brother and former employee Ben, now a Downing Street advisor, was sitting in meetings of the Scientific Advisory Group for Emergencies (Sage). Faculty’s original NHS Covid-19 Data Store contract said it could retain intellectual property rights to train its AI models with NHS data – something that would have allowed the company to profit from the access to usually inaccessible information. However, Faculty says this was later rewritten at its request to make clear it wasn’t using the data.This is on top of questions around the work Faculty did for the Vote Leave campaign during the 2016 EU referendum, and also about £260,000 of work for controversial former government advisor Dominic Cummings. (Details of the work have not been disclosed.) Faculty denies receiving any preferential treatment during the pandemic response and says it stopped working in politics in 2019. “If you want to help solve important problems, you will be exposed to scrutiny and that is a good thing in general,” Warner says. He adds 80 per cent of the company’s contacts are with private companies, rather than the government.Most PopularThe Big StoryPriscila, Queen of the Rideshare MafiaBy Lauren Smiley, WIREDPoliticsThe Right Is Blaming Women and DEI for the Secret Service’s Failure in Trump ShootingBy David Gilbert, WIREDGearThe 29 Best Early Amazon Prime Day DealsBy Simon Hill, WIREDPoliticsTrump Shooting Conspiracies Are Coming From Every DirectionBy David Gilbert, WIREDTwo former members of staff say that, internally, Faculty is highly focused on the ethics of the work it does. They say Warner’s academic background shapes the company’s culture and that individual data scientists can refuse to be involved in projects they find ethically objectionable. This is in addition to an ethics board which assesses projects it does.AdvertisementWarner founded Faculty in 2014 – originally under the name of ASI Data Science – as a fellowship and mentoring company. From the outset, he decided it would train PhD graduates in data science and AI, with a final practical placement taking place at an established company. Since then, more than 300 PhDs have and 200 companies have taken part. While the fellowship sits at the heart of the company, it doesn’t make it a profit, the CEO says. Instead, it allows Faculty to hire PhDs – more than 50 of its 120-plus staff hold one – and make connections with companies.Faculty’s focus, Warner says, is deploying custom AI into businesses and helping them make the technology useful. To do this it has built its own data science software. The tool allows data scientists to import large datasets, run AI against them and then visualise the results. Warner says it’s like Excel, but for data scientists.The real trick, though, is getting more companies to use AI – for this Faculty is deploying its AI within companies focusing on consumer tech, health, engineering and government. Helping a variety of companies use AI has lessons that can be applied across all businesses. Warner stresses that Faculty’s clients own any AI model that has been trained on their data. However, as most AI that’s currently used matches patterns in data, it’s possible for the algorithms and code libraries to be used with different clients. Warner says: “A customer has data, we help them find patterns in it, and we help them use that understanding to sort of push the world in a direction that they want it to.”The hype around AI, which ballooned following DeepMind’s AI victory against Go champion Lee Sedol in 2016, has largely subsided and businesses are faced with the harsh reality that getting returns on investment in AI isn’t guaranteed. Research from the Boston Consulting Group and MIT Sloan Management in 2020 found only 11 per cent of firms using AI are getting a “sizeable” return on their investments.“The typical problem is: it's too broad as a proposition,” says Richard Sargeant, Faculty’s chief operating officer. One unnamed shipping company asked it to predict the price of containers for all of its routes for the next year, a task Sargeant says would have been like predicting exactly what the stock market is going to do. Instead of a broad-brush approach, Sargeant and Warner say deploying AI in narrow situations is best – where it can excel and perform better than humans. To do this they need to understand how a business operates and how the data it has can be used. They cite examples of helping a retailer reduce the number of printed catalogues they send to customers and the UK’s Network Rail using cameras on the front of trains and AI to identify trees and plants encroaching onto railways.For Warner and Sargeant, the secret to companies effectively using AI is building out a whole approach to it. It’s not just having the right data and knowing what can be done with it. The business value of AI comes from the entire system and not how advanced the AI model is, Sargeant says. “There will definitely be some small firms that just want to buy a black box off the shelf,” he says. “But my hunch is that for most enterprises… they're going to need to think in terms of the combination of technology and expertise.”Updated 06.05.21, 09:10 GMT: The headline and standfirst of this article have been updatedMore great stories from WIRED📚 The future of medicine, AI and climate change: get WIRED booksCanary Wharf is trying to reinvent itselfThe hunt for a drug that can stop Covid-19 infectionsHow to get your new puppy ready for post-lockdown lifePolice unions on Facebook are harassing LGBTQ peopleLewis Hamilton opens up about activism and life beyond F1🔊 Subscribe to the WIRED Podcast. New episodes every FridayThis article was originally published by WIRED UKEnter your email to get the Wired newsletterclose dialogRecommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyPlease enter abovesign upUsed consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.Recommended NewsletterFast ForwardA weekly dispatch from the future by Will Knight, exploring advances in AI and other technologies set to change our lives. Delivered on Thursdays.WeeklyYou're signed up!Used consistent with and subject to our Privacy Policy & User Agreement. Read terms of Sign-up.close dialog",https://schema.org/,,https://www.wired.com/story/faculty-artificial-intelligence/,BreadcrumbList,,"['https://media.wired.com/photos/65e713145f2286a91492cd40/16:9/w_1488,h_837,c_limit/wired-uk-default-image-1-1.jpeg', 'https://media.wired.com/photos/65e713145f2286a91492cd40/4:3/w_1500,h_1125,c_limit/wired-uk-default-image-1-1.jpeg', 'https://media.wired.com/photos/65e713145f2286a91492cd40/1:1/w_1500,h_1500,c_limit/wired-uk-default-image-1-1.jpeg']","[{'@type': 'Person', 'name': 'Matt Burgess', 'sameAs': 'https://www.wired.com/author/matt-burgess/'}]",What Faculty is planning next,"{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}",,"https://media.wired.com/photos/65e713145f2286a91492cd40/2:3/w_1000,h_1500,c_limit/wired-uk-default-image-1-1.jpeg",2021-05-05T04:44:53.330-04:00,business,"Faculty’s NHS work focused on two major areas: helping to bring together unruly sprawls of health information, and building AI for the NHS to forecast where supplies are needed. The first of these projects, called the NHS Covid-19 Data Store, is a collection of health datasets, including everything from populations and NHS staff levels to Covid-19 death data and calls to emergency services. The system, which Faculty created alongside Google, Amazon, Microsoft and Palantir, organises and makes data available to decision-makers. “Keeping data in a good state is an incredibly tricky problem in a normal situation,” Warner says. “It was very clear, fairly early on, that it was going to be incredibly powerful to have a really good view of the system, [of] what's going on now.”
The second stage was predicting what would happen next. What this looked like was an AI model – based on previous bed use and NHS 111 call volumes, among other data – that lets hospitals predict potential outbreaks for the next three weeks. The model has been given to local NHS Trusts after being used nationally by NHS England and forecasts where ventilators and beds, among other things, would be needed most. As Faculty’s pandemic work progressed, Warner, who has a PhD in quantum computing and a degree in physics, moved to “red teaming” its output: digging into the AI models, trying to break things.
The decision to focus on the pandemic had repercussions, he says: some of Faculty’s work became “unaligned” without the CEO overseeing it. “The other set of consequences that truthfully, we never anticipated going into it, [were] the media repercussions of me being more directly involved,” Warner says. The company’s early work in politics and government connections have made it, to some, one of the most controversial AI companies around.
In 2020, The Guardian reported that Faculty had been given 14 government contracts in two years – including for its pandemic work – and that Warner’s brother and former employee Ben, now a Downing Street advisor, was sitting in meetings of the Scientific Advisory Group for Emergencies (Sage). Faculty’s original NHS Covid-19 Data Store contract said it could retain intellectual property rights to train its AI models with NHS data – something that would have allowed the company to profit from the access to usually inaccessible information. However, Faculty says this was later rewritten at its request to make clear it wasn’t using the data.
This is on top of questions around the work Faculty did for the Vote Leave campaign during the 2016 EU referendum, and also about £260,000 of work for controversial former government advisor Dominic Cummings. (Details of the work have not been disclosed.) Faculty denies receiving any preferential treatment during the pandemic response and says it stopped working in politics in 2019. “If you want to help solve important problems, you will be exposed to scrutiny and that is a good thing in general,” Warner says. He adds 80 per cent of the company’s contacts are with private companies, rather than the government.
Two former members of staff say that, internally, Faculty is highly focused on the ethics of the work it does. They say Warner’s academic background shapes the company’s culture and that individual data scientists can refuse to be involved in projects they find ethically objectionable. This is in addition to an ethics board which assesses projects it does.
Warner founded Faculty in 2014 – originally under the name of ASI Data Science – as a fellowship and mentoring company. From the outset, he decided it would train PhD graduates in data science and AI, with a final practical placement taking place at an established company. Since then, more than 300 PhDs have and 200 companies have taken part. While the fellowship sits at the heart of the company, it doesn’t make it a profit, the CEO says. Instead, it allows Faculty to hire PhDs – more than 50 of its 120-plus staff hold one – and make connections with companies.
Faculty’s focus, Warner says, is deploying custom AI into businesses and helping them make the technology useful. To do this it has built its own data science software. The tool allows data scientists to import large datasets, run AI against them and then visualise the results. Warner says it’s like Excel, but for data scientists.
The real trick, though, is getting more companies to use AI – for this Faculty is deploying its AI within companies focusing on consumer tech, health, engineering and government. Helping a variety of companies use AI has lessons that can be applied across all businesses. Warner stresses that Faculty’s clients own any AI model that has been trained on their data. However, as most AI that’s currently used matches patterns in data, it’s possible for the algorithms and code libraries to be used with different clients. Warner says: “A customer has data, we help them find patterns in it, and we help them use that understanding to sort of push the world in a direction that they want it to.”
The hype around AI, which ballooned following DeepMind’s AI victory against Go champion Lee Sedol in 2016, has largely subsided and businesses are faced with the harsh reality that getting returns on investment in AI isn’t guaranteed. Research from the Boston Consulting Group and MIT Sloan Management in 2020 found only 11 per cent of firms using AI are getting a “sizeable” return on their investments.
“The typical problem is: it's too broad as a proposition,” says Richard Sargeant, Faculty’s chief operating officer. One unnamed shipping company asked it to predict the price of containers for all of its routes for the next year, a task Sargeant says would have been like predicting exactly what the stock market is going to do. Instead of a broad-brush approach, Sargeant and Warner say deploying AI in narrow situations is best – where it can excel and perform better than humans. To do this they need to understand how a business operates and how the data it has can be used. They cite examples of helping a retailer reduce the number of printed catalogues they send to customers and the UK’s Network Rail using cameras on the front of trains and AI to identify trees and plants encroaching onto railways.
For Warner and Sargeant, the secret to companies effectively using AI is building out a whole approach to it. It’s not just having the right data and knowing what can be done with it. The business value of AI comes from the entire system and not how advanced the AI model is, Sargeant says. “There will definitely be some small firms that just want to buy a black box off the shelf,” he says. “But my hunch is that for most enterprises… they're going to need to think in terms of the combination of technology and expertise.”
Updated 06.05.21, 09:10 GMT: The headline and standfirst of this article have been updated
More great stories from WIRED

📚 The future of medicine, AI and climate change: get WIRED books
Canary Wharf is trying to reinvent itself
The hunt for a drug that can stop Covid-19 infections
How to get your new puppy ready for post-lockdown life
Police unions on Facebook are harassing LGBTQ people
Lewis Hamilton opens up about activism and life beyond F1
🔊 Subscribe to the WIRED Podcast. New episodes every Friday

This article was originally published by WIRED UK",,2021-05-05T04:44:53.330-04:00,"{'@type': 'CreativeWork', 'name': 'WIRED'}",True,The AI firm’s links to Dominic Cummings made it headline news. Now it’s putting AI to work on businesses and the NHS,"{'@type': 'WebPage', '@id': 'https://www.wired.com/story/faculty-artificial-intelligence/'}","[{'@type': 'ListItem', 'position': 1, 'name': 'Business', 'item': 'https://www.wired.com/business/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Startups', 'item': 'https://www.wired.com/tag/startups/'}, {'@type': 'ListItem', 'position': 3, 'name': 'What Faculty is planning next'}]",
https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vd3d3LmJscy5nb3Yvb3B1Yi9idG4vdm9sdW1lLTEwL2ludmVzdGlnYXRpb24tYW5kLXNlY3VyaXR5LXNlcnZpY2VzLmh0bdIBAA?oc=5,Artificial intelligence: taking on a bigger role in our future security - Bureau of Labor Statistics,2021-05-03,Bureau of Labor Statistics,https://www.bls.gov,"Advancements in technology have led to much more advanced security systems that are driven by artificial intelligence. This new technology presents employment opportunities for highly skilled workers, but it also means diminishing opportunities for lower-skilled security workers.",,"Advancements in technology have led to much more advanced security systems that are driven by artificial intelligence. This new technology presents employment opportunities for highly skilled workers, but it also means diminishing opportunities for lower-skilled security workers.",N/A,N/A,N/A,"


An official website of the United States government 
	  
	Here is how you know 


  United States Department of Labor
",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifGh0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2hvdy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS13aWxsLWFmZmVjdC10aGUtZnV0dXJlLW9mLXdvcmstYW5kLWxpZmXSAYYBaHR0cHM6Ly93d3cuYW5hbHl0aWNzaW5zaWdodC5uZXQvYW1wL3N0b3J5L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2hvdy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS13aWxsLWFmZmVjdC10aGUtZnV0dXJlLW9mLXdvcmstYW5kLWxpZmU?oc=5,How Artificial Intelligence will Affect the Future of Work and Life? - Analytics Insight,2021-05-05,Analytics Insight,https://www.analyticsinsight.net,,"Artificial intelligence,Machines,Personalization,Internet,Smart home","If you take a walk down memory lane, you will see the extent to which humanity has advanced in the present era. Back in the olden days, you had to write letters","If you take a walk down memory lane, you will see the extent to which humanity has advanced in the present era. Back in the olden days, you had to write letters",N/A,N/A,What is AI and Data Science Engineering? ,http://schema.org,,https://www.analyticsinsight.net/artificial-intelligence/how-artificial-intelligence-will-affect-the-future-of-work-and-life,NewsArticle,,"{'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/05/Artificial-Intelligence-3.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}","[{'@type': 'Person', 'givenName': 'Market Trends', 'name': 'Market Trends', 'url': 'https://www.analyticsinsight.net/author/market-trends'}]",How Artificial Intelligence will Affect the Future of Work and Life?,"{'@type': 'Organization', '@context': 'http://schema.org', 'name': 'Analytics Insight', 'url': 'https://www.analyticsinsight.net', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'author': 'analyticsinsight', 'contentUrl': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'url': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'name': 'logo', 'width': '', 'height': ''}, 'sameAs': ['https://whatsapp.com/channel/0029VafDe8HCBtxLV2PpRA2l', 'https://twitter.com/analyticsinme', 'https://in.pinterest.com/analyticsinsightsubmissions/_created/', 'https://www.instagram.com/analyticsinsightmagazine/', 'https://www.facebook.com/analyticsinsight.net', 'https://news.google.com/publications/CAAiEDD0Ze78owxVdNti611RNvQqFAgKIhAw9GXu_KMMVXTbYutdUTb0?hl=en-IN&gl=IN&ceid=IN%3Aen', 'https://t.me/analyticsinsightmag', 'https://www.youtube.com/channel/UCgF2J0b46YP0vvVEbgL_GuQ', 'https://www.linkedin.com/company/analytics-insight/'], 'id': 'https://www.analyticsinsight.net'}",2021-05-05T03:59:17Z,https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/05/Artificial-Intelligence-3.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true,2021-05-05T03:59:17Z,Artificial Intelligence,"If you take a walk down memory lane, you will see the extent to which humanity has advanced in the present era. Back in the olden days, you had to write letters to your loved ones, learn about the nationwide events from newspaper columns, walk to the nearest supermarket for buying grocery, stand in long lines at the bank for undertaking a simple transaction, and do a 9-to-5 job, filing boring data day after day. Things have taken a 360° turn, now. You have social media to keep yourself updated about the minute-by-minute lives of your families and friends around the globe. Televisions and smartphone streaming applications entertain you with popular series and blockbusters films around the clock. E-marts like Amazon deliver products to your doorstep with a single click. There are online banking facilities for you to send or receive money at will. Got a question? You can just type it into the Google search bar and the engine will fetch millions of results within mere seconds. And, don't even get me started on the condition of work, which has gone remote, allowing you to fulfill your tasks or attend meetings from the comfort of your home. You can even start your own business without having to invest in a proper office space. So on and so forth..These are just a few examples of the monumental shift in the general lifestyle of people over the bygone decades. The credit goes to technology, and its proud creation – the internet. As you may very well know, the internet is nothing short of a necessity, these days. It has penetrated so deeply into our lives that we cannot imagine a single day without surfing the waves of the World Wide Web. Because of its high demand, ISPs offer competitive internet service plans and packages in the market, and the sales are never-ending. The moment you get your hands on a networking goldmine, like RCN internet plans, you are set for life. The high-speed internet connection will power so many devices in your household, starting from your smartphone to your smart vacuum. There is another piece of technology that a good internet connection will support, and it is called Artificial Intelligence. What is AI and how can it make your work and life easier than ever in the upcoming future? Let's find out below..Artificial Intelligence and Its Possible Impacts.Artificial Intelligence or AI is a budding field of technology, which develops and researches the capabilities of machines to mimic human intelligence and other behavioral traits. AI is the artificial simulation of the human mind and its thought patterns. In simple words, any machine that is programmed to think and act like a human can be called Artificial Intelligence. What is the benefit of AI? Here are several ways it can impact the future of your work and life:.Increased Personalization .The current trajectory of the world has a bad side effect of isolating people. It is evident by the huge number of depressed individuals, which is growing day by day. Technology can help people feel less lonely. How? Through AI-powered chatbots. There are smartphone applications and other mental health programs which utilize Artificial Intelligent chatbots to keep isolated people company during hard times. These chatbots study the user's behavior and adjust the tone of conversation accordingly. Such machine learning traits pave the way to a more personalized experience, which satisfies users largely..Even in the field of commerce, brands that bring AI bots to the front of customer service receive better results than those who do not. Customer service chatbots help buyers throughout the purchasing journey and even after that. Being technological in essence, they are available 24/7 to give customized replies to user queries, based on a prior assessment of the user's tone and attitude. This is just the start. AI has the potential to make people feel special through highly personalized and customized treatment, both in work and in life..Streamlined Operations.Gone are the days of menial drudgery. You no longer have to worry about minimal tasks that eat up your time, resources, and energy. Artificial Intelligence can take care of that. There are state-of-the-art AI programs and software in the making that can complete simple-to-complex assignments and jobs for you. For instance, if you are planning to set up a smart home system this year, consisting of smart lights, smart thermostats, automated locks, smart alarms, smart outdoor lights, smart kitchen appliances, smart stereo, smart switches, and more, then you can rely on the efficiency of an AI-assisted smart home hub to control and manage all these devices for you. A smart home hub can connect to your smart home device grid over the in-home network, and monitor it for optimized performance. It can dial the lights down automatically sensing the absence of human residents to save up energy and reduce your electricity bill, in addition to other things..Similarly, the integration of Artificial Intelligence in work can lead to streamlined business operations. AI can do the smallest of tasks, like data entry, data maintenance, inventory scan, supply chain monitoring, and information security within mere seconds and with greater accuracy, leaving no chances for error. Once AI takes up these tasks, it can open up more avenues of exploration for workers, saving their time and money in the meanwhile..Predictions for a Better Future.Artificial Intelligence cannot only analyze the current statistics but also predict the future curves, based on a set of patterns. Equipped with such information and forecast data, people will be able to prepare for anything, ranging from climatic emergencies like hurricanes, twisters, and wildfires, to business crises. Besides the preparation, they can use AI analysis to strive towards the betterment of the QoL index, or quality of life. This is how AI can help make the world a safer and sweeter place to live in..Wrapping Up.People are always looking for ways to introduce a greater degree of convenience and ease into their lifestyles. Technology has so far enabled them to walk on the path of progress, and with platforms like Artificial Intelligence on the rise, the future of work and life looks brighter than ever..Disclaimer: Analytics Insight does not provide financial advice or guidance. Also note that the cryptocurrencies mentioned/listed on the website could potentially be scams, i.e. designed to induce you to invest financial resources that may be lost forever and not be recoverable once investments are made. You are responsible for conducting your own research (DYOR) before making any investments. Read more here.",,2021-05-05T03:59:17Z,"{'@type': 'WebPage', 'url': 'https://www.analyticsinsight.net/artificial-intelligence/how-artificial-intelligence-will-affect-the-future-of-work-and-life', 'primaryImageOfPage': {'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/05/Artificial-Intelligence-3.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}}",,,"{'@type': 'WebPage', '@id': 'https://www.analyticsinsight.net/artificial-intelligence/how-artificial-intelligence-will-affect-the-future-of-work-and-life'}","[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.analyticsinsight.net'}, {'@type': 'ListItem', 'position': 2, 'name': 'Artificial Intelligence', 'item': 'https://www.analyticsinsight.net/artificial-intelligence'}, {'@type': 'ListItem', 'position': 3, 'name': 'How Artificial Intelligence will Affect the Future of Work and Life?', 'item': 'https://www.analyticsinsight.net/artificial-intelligence/how-artificial-intelligence-will-affect-the-future-of-work-and-life'}]",How Artificial Intelligence will Affect the Future of Work and Life?
https://news.google.com/rss/articles/CBMiZGh0dHBzOi8vbmV3cy5taXQuZWR1LzIwMjEvdW5kZXJncmFkdWF0ZXMtZXhwbG9yZS1wcmFjdGljYWwtYXBwbGljYXRpb25zLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLTA1MDPSAQA?oc=5,Undergraduates explore practical applications of artificial intelligence | MIT News | Massachusetts Institute of Technology - MIT News,2021-05-03,MIT News,https://news.mit.edu,SuperUROP scholars funded by the MIT Quest for Intelligence worked with faculty this past year exploring AI applications ranging from optimized scheduling to modeling ocean dynamics to improve climate model projections.,"MIT students, MIT Quest for Intelligence, Advanced Undergraduate Research Opportunities Program (SuperUROP), MIT-IBM Watson AI lab, Computer Science and Artificial Intelligence Laboratory (CSAIL), Raffaele Ferrari, Ronitt Rubinfeld, Asu Ozdaglar, Pawen Sinha, Spencer Compton, Slobodan Mitrović, Andre Souza, Adeline Hillier, Kristian Georgiev, Alireza Fallah, Ashika Verma, Project Prakash, Kyle Keane",SuperUROP scholars funded by the MIT Quest for Intelligence worked with faculty this past year exploring AI applications ranging from optimized scheduling to modeling ocean dynamics to improve climate model projections.,N/A,N/A,N/A,"


SuperUROP scholars apply deep learning to improve accuracy of climate models, profitably match computers in the cloud with customers, and more.




Kim Martineau
|
MIT Quest for Intelligence


 Publication Date:
 May 3, 2021





Press Inquiries

  Press Contact:



      
            Kim         

            Martineau        

  

      Email:
     kimmarti@mit.edu


      Phone:
              617-710-5216      
  

      
            MIT Quest for Intelligence        

  








 Close














 Caption:
          SuperUROP scholars participating in research projects funded by the MIT Quest for Intelligence and MIT-IBM Watson AI Lab this year included (clockwise from top left) Spencer Compton, Adeline Hillier, Kristian Georgiev, and Ashika Verma.      
          

 Credits:
          Photo collage: Samantha Smiley      
          

















Previous image
Next image






















Deep neural networks excel at finding patterns in datasets too vast for the human brain to pick apart. That ability has made deep learning indispensable to just about anyone who deals with data. This year, the MIT Quest for Intelligence and the MIT-IBM Watson AI Lab sponsored 17 undergraduates to work with faculty on yearlong research projects through MIT’s Advanced Undergraduate Research Opportunities Program (SuperUROP).
Students got to explore AI applications in climate science, finance, cybersecurity, and natural language processing, among other fields. And faculty got to work with students from outside their departments, an experience they describe in glowing terms. “Adeline is a shining testament of the value of the UROP program,” says Raffaele Ferrari, a professor in MIT’s Department of Earth and Planetary Sciences, of his advisee. “Without UROP, an oceanography professor might have never had the opportunity to collaborate with a student in computer science.”
Highlighted below are four SuperUROP projects from this past year.
A faster algorithm to manage cloud-computing jobs
The shift from desktop computing to far-flung data centers in the “cloud” has created bottlenecks for companies selling computing services. Faced with a constant flux of orders and cancellations, their profits depend heavily on efficiently pairing machines with customers.
Approximation algorithms are used to carry out this feat of optimization. Among all the possible ways of assigning machines to customers by price and other criteria, they find a schedule that achieves near-optimal profit.​ For the last year, junior Spencer Compton worked on a virtual whiteboard with MIT Professor Ronitt Rubinfeld and postdoc Slobodan Mitrović to find a faster scheduling method.
“We didn’t write any code,” he says. “We wrote proofs and used mathematical ideas to find a more efficient way to solve this optimization problem. The same ideas that improve cloud-computing scheduling can be used to assign flight crews to planes, among other tasks.”
In a pre-print paper on arXiv, Compton and his co-authors show how to speed up an approximation algorithm under dynamic conditions. They also show how to locate machines assigned to individual customers without computing the entire schedule.
A big challenge was finding the crux of the project, he says. “There’s a lot of literature out there, and a lot of people who have thought about related problems. It was fun to look at everything that’s been done and brainstorm to see where we could make an impact.”​
How much heat and carbon can the oceans absorb?
Earth’s oceans regulate climate by drawing down excess heat and carbon dioxide from the air. But as the oceans warm, it’s unclear if they will soak up as much carbon as they do now. A slowed uptake could bring about more warming than what today’s climate models predict. It’s one of the big questions facing climate modelers as they try to refine their predictions for the future.
The biggest obstacle in their way is the complexity of the problem: today’s global climate models lack the computing power to get a high-resolution view of the dynamics influencing key variables like sea-surface temperatures. To compensate for the lost accuracy, researchers are building surrogate models to approximate the missing dynamics without explicitly solving for them.
In a project with MIT Professor Raffaele Ferrari and research scientist Andre Souza, MIT junior Adeline Hillier is exploring how deep learning solutions can be used to improve or replace physical models of the uppermost layer of ocean, which drives the rate of heat and carbon uptake. “If the model has a small footprint and succeeds under many of the physical conditions encountered in the real world, it could be incorporated into a global climate model and hopefully improve climate projections,” she says.
In the course of the project, Hillier learned how to code in the programming language Julia. She also got a crash course in fluid dynamics. “You’re trying to model the effects of turbulent dynamics in the ocean,” she says. “It helps to know what the processes and physics behind them look like.”
In search of more efficient deep learning models
There are thousands of ways to design a deep learning model to solve a given task. Automating the design process promises to narrow the options and make these tools more accessible. But finding the optimal architecture is anything but simple. Most automated searches pick the model that maximizes validation accuracy without considering the structure of the underlying data, which may suggest a simpler, more robust solution. As a result, more reliable or data-efficient architectures are passed over.
“Instead of looking at the accuracy of the model alone, we should focus on the structure of the data,” says MIT senior Kristian Georgiev. In a project with MIT Professor Asu Ozdaglar and graduate student Alireza Fallah, Georgiev is looking at ways to automatically query the data to find the model that best suits its constraints. “If you choose your architecture based on the data, you’re more likely to get a good and robust solution from a learning theory perspective,” he says.
The hardest part of the project was the exploratory phase at the start, he says. To find a good research question he read through papers ranging from topics in autoML to representation theory. But it was worth it, he says, to be able to work at the intersection of optimization and generalization. “To make good progress in machine learning you need to combine both of these fields.”
What makes humans so good at recognizing faces?
Face recognition comes easily to humans. Picking out familiar faces in a blurred or distorted photo is a cinch. But we don’t really understand why or how to replicate this superpower in machines. To home in on the principles important to recognizing faces, researchers have shown headshots to human subjects that are progressively degraded to see where recognition starts to break down. They are now performing similar experiments on computers to see if deeper insights can be gained
In a project with MIT Professor Pawan Sinha and the MIT Quest for Intelligence, junior Ashika Verma applied a set of filters to a dataset of celebrity photos. She blurred their faces, distorted them, and changed their color to see if a face-recognition model could pick out photos of the same face. She found that the model did best when the photos were either natural color or grayscale, consistent with the human studies. Accuracy slipped when a color filter was added, but not as much as it did for the human subjects — a wrinkle that Verma plans to investigate further.
The work is part of a broader effort to understand what makes humans so good at recognizing faces, and how machine vision might be improved as a result. It also ties in with Project Prakash, a nonprofit in India that treats blind children and tracks their recovery to learn more about the visual system and brain plasticity. “Running human experiments takes more time and resources than running computational experiments,” says Verma’s advisor, Kyle Keane, a researcher with MIT Quest. “We're trying to make AI as human-like as possible so we can run a lot of computational experiments to identify the most promising experiments to run on humans.”
Degrading the images to use in the experiments, and then running them through the deep nets, was a challenge, says Verma. “It’s very slow,” she says. “You work 20 minutes at a time and then you wait.” But working in a lab with an advisor made it worth it, she says. “It was fun to dip my toes into neuroscience.”
SuperUROP projects were funded, in part, by the MIT-IBM Watson AI Lab, MIT Quest Corporate, and by Eric Schmidt, technical advisor to Alphabet Inc., and his wife, Wendy.








Share this news article on:










X











Facebook















LinkedIn




































Reddit


















Print







Related Links

Raffaele FerrariAdeline HillierRonitt RubinfeldAshika VermaAsu OzdaglarSuperUROPMIT Quest for IntelligenceMIT-IBM Watson AI LabDepartment of Electrical Engineering and Computer ScienceMIT Schwarzman College of Computing






Related Topics

Brain and cognitive sciences
EAPS
Electrical Engineering & Computer Science (eecs)
Quest for Intelligence
MIT-IBM Watson AI Lab
Computer Science and Artificial Intelligence Laboratory (CSAIL)
SuperUROP
Artificial intelligence
Algorithms
Machine learning
Students
Undergraduate
Climate change
Climate models
Oceanography and ocean engineering
Data
Computer vision
Education, teaching, academics
School of Engineering
School of Science
MIT Schwarzman College of Computing



Related Articles











An intro to the fast-paced world of artificial intelligence













MIT undergraduates pursue research opportunities through the pandemic













Undergraduates develop next-generation intelligence tools













Bringing artificial intelligence into the classroom, research lab, and beyond













Developing artificial intelligence tools for all

















Previous item
Next item
















",,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMigQFodHRwczovL3d3dy5hbmFseXRpY3NpbnNpZ2h0Lm5ldC9hcnRpZmljaWFsLWludGVsbGlnZW5jZS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1pcy10aGUtbW9zdC1kaXNydXB0aXZlLXRlY2hub2xvZ3ktb2YtdGhlLWNlbnR1cnnSAYsBaHR0cHM6Ly93d3cuYW5hbHl0aWNzaW5zaWdodC5uZXQvYW1wL3N0b3J5L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWlzLXRoZS1tb3N0LWRpc3J1cHRpdmUtdGVjaG5vbG9neS1vZi10aGUtY2VudHVyeQ?oc=5,Artificial Intelligence is the Most Disruptive Technology of the Century - Analytics Insight,2021-05-02,Analytics Insight,https://www.analyticsinsight.net,,"Artificial intelligence,AI is the most disruptive technology,artificial intelligence has revolutionized industries,AI has revolutionized industries,AI",Artificial intelligence seems to be the next big thing in many industries today. The technology is infiltrating every sector and transforming the tasks that com,Artificial intelligence seems to be the next big thing in many industries today. The technology is infiltrating every sector and transforming the tasks that com,N/A,N/A,What is AI and Data Science Engineering? ,http://schema.org,,https://www.analyticsinsight.net/artificial-intelligence/artificial-intelligence-is-the-most-disruptive-technology-of-the-century,NewsArticle,,"{'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/05/Artificial-Intelligence.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}","[{'@type': 'Person', 'givenName': 'Market Trends', 'name': 'Market Trends', 'url': 'https://www.analyticsinsight.net/author/market-trends'}]",Artificial Intelligence is the Most Disruptive Technology of the Century,"{'@type': 'Organization', '@context': 'http://schema.org', 'name': 'Analytics Insight', 'url': 'https://www.analyticsinsight.net', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'author': 'analyticsinsight', 'contentUrl': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'url': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'name': 'logo', 'width': '', 'height': ''}, 'sameAs': ['https://whatsapp.com/channel/0029VafDe8HCBtxLV2PpRA2l', 'https://twitter.com/analyticsinme', 'https://in.pinterest.com/analyticsinsightsubmissions/_created/', 'https://www.instagram.com/analyticsinsightmagazine/', 'https://www.facebook.com/analyticsinsight.net', 'https://news.google.com/publications/CAAiEDD0Ze78owxVdNti611RNvQqFAgKIhAw9GXu_KMMVXTbYutdUTb0?hl=en-IN&gl=IN&ceid=IN%3Aen', 'https://t.me/analyticsinsightmag', 'https://www.youtube.com/channel/UCgF2J0b46YP0vvVEbgL_GuQ', 'https://www.linkedin.com/company/analytics-insight/'], 'id': 'https://www.analyticsinsight.net'}",2021-05-02T06:52:59Z,https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/05/Artificial-Intelligence.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true,2021-05-02T06:52:59Z,Artificial Intelligence,"Artificial intelligence seems to be the next big thing in many industries today. The technology is infiltrating every sector and transforming the tasks that computers perform into a lot of hype. Starting from fitness-focused smartphone apps that adapt to women's menstrual cycle to autonomous vehicles that use sensors and software to dodge at stray animals, artificial intelligence has influenced every part of human life. It has evolved from being just a trend to a core ingredient virtually across every aspect of computing. In the modern world, businesses across diverse sectors use artificial intelligence as a tool to meet their goals, be it customer service through an intuitive chatbot or streamlining video production through synthetic voiceovers. For a term that dates back to 1956 and celebrates its 65th birthday this year, artificial intelligence has performed and revolutionized more than how anybody imagined..As years passed, humans gained great faith in technology and machines, which eventually accelerated artificial intelligence adoption. Today, the role of artificial intelligence in an enterprise has become so important that it has touched every facet of business, and its crucial place will significantly grow over the coming years. Artificial intelligence, though revolutionary in itself, is an enabler that needs to be used effectively to achieve business objectives. Businesses are using AI agents to engage customers, rapidly create content, analyze transactions and detect fraud. Even though it comes with a lot of flaws, the speediness, customized content, and target recommendations, overweigh the cons..AI-driven technologies have the potential to enhance our lives as both learners and workers. Researchers and developers are continuously improving them to mimic human behaviors in routines like learning, problem-solving, and processing language. While they are growing to be strong imitators of humans, they still lack essential human traits such as wisdom, insight, humor, and empathy. Fortunately, the next generation's AI will carry all the objectives that humans think are essential for machines to cope up with them. The technological capabilities will attempt to solve real-world issues, moving beyond doing repetitive and routine works..While big guys like Amazon, Apple, Google, and Microsoft scramble to infuse their products with artificial intelligence, other companies are hard at work developing their own intelligent technologies and services. The rise of digitization and the thirst for automation are fuelling the demand for AI solutions. Not just companies, even the governments are focusing on deep research in the field of finding, investing, and growing local talent to make their country the AI hub. Artificial intelligence companies are also initiating to deliver a robust service to their customers by using sub-technologies like machine learning, deep learning, edge computing, business intelligence, etc. as their prominent business principle. In a nutshell, artificial intelligence is used as a tool to integrate multiple sources of data or a vast amount of data, data security, real-world applications, predictions, cloud operations, etc. With the arena of AI technologies at the beginning, the world has experienced so much so far. The future is anticipated to be more sophisticated and personalized with the help of artificial intelligence..Disclaimer: Analytics Insight does not provide financial advice or guidance. Also note that the cryptocurrencies mentioned/listed on the website could potentially be scams, i.e. designed to induce you to invest financial resources that may be lost forever and not be recoverable once investments are made. You are responsible for conducting your own research (DYOR) before making any investments. Read more here.",,2021-05-02T06:52:59Z,"{'@type': 'WebPage', 'url': 'https://www.analyticsinsight.net/artificial-intelligence/artificial-intelligence-is-the-most-disruptive-technology-of-the-century', 'primaryImageOfPage': {'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/05/Artificial-Intelligence.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}}",,,"{'@type': 'WebPage', '@id': 'https://www.analyticsinsight.net/artificial-intelligence/artificial-intelligence-is-the-most-disruptive-technology-of-the-century'}","[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.analyticsinsight.net'}, {'@type': 'ListItem', 'position': 2, 'name': 'Artificial Intelligence', 'item': 'https://www.analyticsinsight.net/artificial-intelligence'}, {'@type': 'ListItem', 'position': 3, 'name': 'Artificial Intelligence is the Most Disruptive Technology of the Century', 'item': 'https://www.analyticsinsight.net/artificial-intelligence/artificial-intelligence-is-the-most-disruptive-technology-of-the-century'}]",Artificial Intelligence is the Most Disruptive Technology of the Century
