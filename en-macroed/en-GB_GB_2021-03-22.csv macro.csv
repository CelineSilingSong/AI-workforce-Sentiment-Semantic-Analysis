URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@graph,article:section,article:summary,article text,@type,url,publisher,datePublished,dateModified,headline,image,thumbnailUrl,mainEntityOfPage,author,articleBody,isBasedOn,articleSection,isPartOf,isAccessibleForFree,alternativeHeadline,itemListElement
https://news.google.com/rss/articles/CBMiNWh0dHBzOi8vd3d3LnJhY29udGV1ci5uZXQvdGVjaG5vbG9neS9haS1jaGFuZ2luZy13b3Jr0gEA?oc=5,How AI is changing the way we work - Raconteur,2021-03-22,Raconteur,https://www.raconteur.net,"The long-term impact of the robot revolution on the human workforce has been overstated: the truth is AI is changing jobs, not stealing them",N/A,"The long-term impact of the robot revolution on the human workforce has been overstated: the truth is AI is changing jobs, not stealing them",N/A,https://schema.org,"[{'@type': 'Article', '@id': 'https://www.raconteur.net/technology/ai-changing-work#article', 'isPartOf': {'@id': 'https://www.raconteur.net/technology/ai-changing-work'}, 'author': [{'@id': 'https://live-raconteur.pantheonsite.io/#/schema/person/97c746d056e0312313a870db152cf91c'}], 'headline': 'How AI is changing the way we&nbsp;work', 'datePublished': '2021-03-22T08:00:00+00:00', 'dateModified': '2024-05-08T11:15:44+00:00', 'mainEntityOfPage': {'@id': 'https://www.raconteur.net/technology/ai-changing-work'}, 'wordCount': 1770, 'publisher': {'@id': 'https://www.raconteur.net/#organization'}, 'image': {'@id': 'https://www.raconteur.net/technology/ai-changing-work#primaryimage'}, 'thumbnailUrl': 'https://www.raconteur.net/wp-content/uploads/2021/03/ai-.jpg', 'keywords': ['Artificial Intelligence', 'HR', 'Outsourcing', 'Talent Management'], 'articleSection': ['Future of Work 2021'], 'inLanguage': 'en-GB'}, {'@type': 'WebPage', '@id': 'https://www.raconteur.net/technology/ai-changing-work', 'url': 'https://www.raconteur.net/technology/ai-changing-work', 'name': 'How AI is changing the way we work', 'isPartOf': {'@id': 'https://www.raconteur.net/#website'}, 'primaryImageOfPage': {'@id': 'https://www.raconteur.net/technology/ai-changing-work#primaryimage'}, 'image': {'@id': 'https://www.raconteur.net/technology/ai-changing-work#primaryimage'}, 'thumbnailUrl': 'https://www.raconteur.net/wp-content/uploads/2021/03/ai-.jpg', 'datePublished': '2021-03-22T08:00:00+00:00', 'dateModified': '2024-05-08T11:15:44+00:00', 'description': 'The long-term impact of the robot revolution on the human workforce has been overstated: the truth is AI is changing jobs, not stealing them', 'breadcrumb': {'@id': 'https://www.raconteur.net/technology/ai-changing-work#breadcrumb'}, 'inLanguage': 'en-GB', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.raconteur.net/technology/ai-changing-work']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-GB', '@id': 'https://www.raconteur.net/technology/ai-changing-work#primaryimage', 'url': 'https://www.raconteur.net/wp-content/uploads/2021/03/ai-.jpg', 'contentUrl': 'https://res.cloudinary.com/yumyoshojin/image/fetch/https://www.raconteur.net/wp-content/uploads/2021/03/ai-.jpg', 'width': 1200, 'height': 675}, {'@type': 'BreadcrumbList', '@id': 'https://www.raconteur.net/technology/ai-changing-work#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Technology', 'item': 'https://www.raconteur.net/topic/technology'}, {'@type': 'ListItem', 'position': 2, 'name': 'How AI is changing the way we&nbsp;work'}]}, {'@type': 'WebSite', '@id': 'https://www.raconteur.net/#website', 'url': 'https://www.raconteur.net/', 'name': 'Raconteur', 'description': 'Publisher of special-interest content to the world’s leading media brands', 'publisher': {'@id': 'https://www.raconteur.net/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.raconteur.net/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-GB'}, {'@type': 'Organization', '@id': 'https://www.raconteur.net/#organization', 'name': 'Raconteur Media Ltd', 'url': 'https://www.raconteur.net/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-GB', '@id': 'https://www.raconteur.net/#/schema/logo/image/', 'url': 'https://www.raconteur.net/wp-content/uploads/2022/11/raconteur-yoast.png', 'contentUrl': 'https://www.raconteur.net/wp-content/uploads/2022/11/raconteur-yoast.png', 'width': 2000, 'height': 314, 'caption': 'Raconteur Media Ltd'}, 'image': {'@id': 'https://www.raconteur.net/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/raconteur.net', 'https://x.com/raconteur', 'https://www.linkedin.com/company/raconteur-media']}, {'@type': 'Person', '@id': 'https://live-raconteur.pantheonsite.io/#/schema/person/97c746d056e0312313a870db152cf91c', 'name': 'Rebecca Stewart', 'description': 'Business writer, she covers media, technology and advertising as a senior reporter at The Drum.', 'url': 'https://www.raconteur.net/contributors/rebecca-stewart'}]",N/A,N/A,"

The coronavirus crisis has given rise to fresh concerns about automation in labour markets. With people locked down and social distancing still in place, many businesses around the world have scaled up their investment in artificial intelligence (AI). 
AI’s ability to identify and learn from data patterns, and translate them into useful technologies, has proven to be indispensable for many organisations, from healthcare providers to delivery subscription services, for example, in responding to the pandemic.Business applications of AI over the past 12 months have ranged from those designed to increase productivity and yield, through to customer-service functions.Robots have been rolling in to sanitise UK and US hospital corridors and deliver crucial supplies such as blood samples. Cincinnati/Northern Kentucky International Airport has enlisted mechanical floor scrubber Neo to whizz around and clean its floors every night, freeing up staff to make sure high-touch areas of the terminal are spotless. Following its $300-million acquisition of so-called decision engine Dynamic Yield in 2020, McDonald’s has also got in on the AI action, trailling drive-through systems chaired by smart speakers.Goldman Sachs, L’Oréal and Procter & Gamble are among those that have bet big on less-robotic looking AI tools to meet the challenges posed by a move away from physical consumerism to digital, including technology that predicts people’s financial outlook and “try-before-you-buy” product-matching systems.However, AI’s business impact will outlast COVID-19. Recent data from Grand View predicts the market will be worth $390 billion by 2025, marking 46.2 per cent growth since 2019. McKinsey is showing an appetite to invest in machine learning, with C-suite leaders who are seeing strong results from AI adoption gearing up to boost their spend over the next three years and develop their own solutions in-house.There are implications for humans, though. According to a World Economic Forum forecast, in a shift likely to worsen inequality, half of all work tasks will be handled by machines by 2025. A separate report from the Massachusetts Institute of Technology projects AI could replace as many as two million more workers in manufacturing alone by 2025.This shift, coupled with an economic downturn, is paving the way for fresh concerns from employees about how many jobs will be lost to automation, especially for those in the services, electronic and manufacturing industries. In the United States alone, a little over a quarter (27 per cent) of all workers say they are worried the job they have now will be eliminated within the next five years as a result of new technology, robots or AI.However, AI’s automation effect may well be overstated. Analysis using data from the European Centre for the Development of Vocational Training reveals that, in the European Union, the share of jobs facing a very high risk of being automated by new digital technologies is close to 14 per cent. However, what two in five EU jobs do face is a high probability of “substantial transformation”.So, while AI might change how humans work, the reality is it’s unlikely to replace most jobs entirely. And, although “robophobia” is on the up, there are several examples of how machine learning is already positively impacting employees across a host of industries.
AI’s role in supporting the workforce
Dr Samer Al Moubayed, chief executive at Furhat Robotics, a conversational-AI social robotics startup that builds tech designed to interact with humans in a natural, fluid way, has overseen the conception of the firm’s namesake robot that can speak, show emotions and maintain eye contact.The past few years have seen Moubayed work with the likes of human resources company Tengai to develop a robot that autonomously performs job interviews, scores the interview according to an established framework and summarises the output for a human recruiter.
Furhat Robotics is also collaborating with Deutsche Bahn to place intuitive travel assistants in train stations that can answer questions related to departure times, delays and more in over 35 languages.Moubayed is firm in his belief that such AI investments can support humans in their day-to-day jobs, working in harmony with the workforce instead of against it. Robotics, he argues, will open up more job opportunities.“Looking at innovations like Tengai, though the solution might reduce the need for recruiters, it won’t remove them from the process entirely,” he explains. “It will also increase the need for robot operators.”The same applies to Deutsche Bahn travel assistants. “Long term, it might lessen the need for hiring multiple customer service agents to cover a broad range of languages, but it will require developers and translators to a higher extent,” says Moubayed.The AI expert cautions that no one is in a position to understand the lasting net effect automation will have on the workforce. But, done right, many of the risks associated with automation can be mitigated in a similar fashion to previous industrial revolutions, through upskilling staff and also upholding a sense of social welfare.“Businesses should focus their AI investments on increasing the average quality of life for workers and customers, rather than simply relying on automation as a tool to cut costs,” he says.
Identifying human gaps, instead of filling them
Marketing giant Publicis Groupe credits the use of its internal AI application Marcel for helping it save 2,000 jobs during the pandemic. As a result, it plans to spend more heavily on the tool, having seen an encouraging impact on its workforce.The smart intranet works by assigning relevant briefs and projects to Publicis’ 80,000 staff. As ad execs everywhere from Tokyo to Toronto shifted to remote working at the outset of 2020, Marcel hosted an internal job mobility platform that allowed people to change agencies, move to different markets and stretch their skillset. 
Suddenly, as some Publicis-owned agencies and markets found themselves working harder and faster to respond to COVID, others were experiencing a slowdown as clients in the hardest-hit sectors, like travel, were incapacitated. 
Marcel enabled those overstretched agencies to post jobs and requests, making use of talent that had time on their hands. Dan Murray, Marcel’s chief executive, says as well as automating the mundane, the tech is enabling the business to identify human gaps for things robots can’t do, like think creatively or communicate effectively. 
“It’s allowed our staff to focus on things that might take a higher-level thought process or more creative mind. It’s also opened opportunities we might not otherwise have had,” he says.
The business is now plotting a lot more additional uses for Marcel’s future, based on AI and machine learning, that will connect people and allow staff to better flex their creative muscle.
Improving the working experience
Tech behemoth Cisco is leveraging AI, machine learning and smart data to enhance the quality of remote working for its 75,000 staff, from engineers to sales reps.With 96 per cent of employees currently working away from the office and a hybrid working model here to stay for the long-term, it has been using deep-learning algorithms to power an “empathetic, inclusive, secure, work-from-anywhere” experience, says senior director of people and communities Gianpaolo Barozzi.Solutions include use of intelligent video so people can move around freely in their workspace and still be available in meetings, as well as real-time meeting transcription and automated debriefs.An AI tool, dubbed People Insights, is also providing individuals and teams with analytics around personal and collective work practices.“This means staff can make better use of their time and take care of their peers,” says Barozzi. “The 21st-century world of work is about connecting people to people, wherever they are, safely and securely. Productivity is no longer a sole matter of efficiency, it’s increasingly achieved through inclusive collaboration, adaptability to unforeseen change and the effective leverage of collective strengths. AI is helping our teams in all these regards.”With most jobs set to involve some form of AI by 2025, he stresses education is key to ensuring staff are comfortable with these types of technologies, rather than fearful of them.Cisco’s standing as a tech firm means its people are largely open to being early adopters, but his advice to other businesses is clear. “Fears are real, so first of all we have to treat them with empathy. We need to be transparent and clear about the opportunities, while not underestimating the risks and challenges,” says Barozzi.
“It’s time to move beyond the paralysing utopia-dystopia dichotomy: as always the future is ours to shape. Digital has already turned from a tool to the environment where our lives happen. Humans and artificial agents will increasingly work together, enhancing and amplifying each other’s capabilities.”

How AI helped Publicis save 2000 jobs
In the immediate months after Covid-19, Publicis Groupe’s revenues declined by 13%. The French advertising network, which owns agencies such as Saatchi&Saatchi, quickly implemented a €500m cost-cutting drive that included a recruitment freeze, elimination of freelance work and cuts to general expenses.
As the business considered the impact of the pandemic on its workforce, however, AI emerged as an unlikely saviour. Its Marcel management tool, aimed at reinventing how the agency communicates internally, was launched in 2018, but in the crisis has truly come into its own.
“We woke up one day and instead of having 80,000 employees we had 80,000 offices,” says Carla Serrano, chief strategy officer of Publicis Groupe. She reveals that Marcel’s global roll-out was accelerated as the company realised the importance of connecting staff with relevant briefs and each other. “We didn’t know it would end up being the saviour of so many people’s jobs,” she says.
The key driver was Marcel’s ‘Gigs’ function. Pre-pandemic it hosted an internal job mobility platform that allowed people to change agencies, move to different markets and stretch their skillset. Amid the crisis, Gigs pivoted to use algorithms to match individuals with briefs based on their skillset and allocating resource to clients where it was most needed.
Key to its has been educating staff on the platform and ensuring the digital experience runs smoothly. Publicis design teams have focused on making the platform as “digestible and personalised” as possible, with work ongoing around the user experience.
Prior to 2020, Serrano admits she had reservations about AI’s impact on the workforce, but is now “bullish” about the benefits it can bring. “It’s immensely benefited our people and culture, as well as learning and discovery. We can use AI for good. There are things we need to be cautious of – like privacy or bias – but we’re certainly learning a lot more about it.”

Future of WorkGrowth StrategiesTalent & CultureTechnologyFuture of Work 2021Artificial IntelligenceHROutsourcingTalent Management
",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibGh0dHBzOi8vd3d3LnVuaXNvbi5vcmcudWsvbmV3cy9hcnRpY2xlLzIwMjEvMDMvbmV3LW1hbmlmZXN0by1kZWZlbmRzLXdvcmtlcnMtcmlnaHRzLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlL9IBAA?oc=5,"New manifesto defends workers’ rights from artificial iIntelligence | Article, News | News - UNISON",2021-03-25,UNISON,https://www.unison.org.uk,UNISON backs the TUC's warning that employment law is failing to keep pace with the rapid expansion of AI in the workplace,N/A,UNISON backs the TUC's warning that employment law is failing to keep pace with the rapid expansion of AI in the workplace,N/A,https://schema.org,"[{'@type': 'WebPage', '@id': 'https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/', 'url': 'https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/', 'name': 'New manifesto defends workers’ rights from artificial iIntelligence | Article, News | News | UNISON National', 'isPartOf': {'@id': 'https://www.unison.org.uk/#website'}, 'primaryImageOfPage': {'@id': 'https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/#primaryimage'}, 'image': {'@id': 'https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/#primaryimage'}, 'thumbnailUrl': 'https://www.unison.org.uk/content/uploads/2021/03/Screenshot-2021-03-25-at-16.41.35.png', 'datePublished': '2021-03-25T17:36:21+00:00', 'dateModified': '2021-03-29T09:45:06+00:00', 'author': {'@id': 'https://www.unison.org.uk/#/schema/person/973d0069b7dcf4cc60a90eb8499d1895'}, 'description': ""UNISON backs the TUC's warning that employment law is failing to keep pace with the rapid expansion of AI in the workplace"", 'breadcrumb': {'@id': 'https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/#primaryimage', 'url': 'https://www.unison.org.uk/content/uploads/2021/03/Screenshot-2021-03-25-at-16.41.35.png', 'contentUrl': 'https://www.unison.org.uk/content/uploads/2021/03/Screenshot-2021-03-25-at-16.41.35.png', 'width': 704, 'height': 396}, {'@type': 'BreadcrumbList', '@id': 'https://www.unison.org.uk/news/article/2021/03/new-manifesto-defends-workers-rights-artificial-intelligence/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.unison.org.uk/'}, {'@type': 'ListItem', 'position': 2, 'name': 'News', 'item': 'https://www.unison.org.uk/news/'}, {'@type': 'ListItem', 'position': 3, 'name': 'New manifesto defends workers’ rights from artificial iIntelligence'}]}, {'@type': 'WebSite', '@id': 'https://www.unison.org.uk/#website', 'url': 'https://www.unison.org.uk/', 'name': 'UNISON National', 'description': 'the public service union', 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.unison.org.uk/search/{search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Person', '@id': 'https://www.unison.org.uk/#/schema/person/973d0069b7dcf4cc60a90eb8499d1895', 'name': 'Demetrios Matheou', 'url': 'https://www.unison.org.uk/author/matheoud/'}]",N/A,N/A,N/A,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiZWh0dHBzOi8vbGFib3VybGlzdC5vcmcvMjAyMS8wMy93b3JrLWFuZC10aGUtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtcmV2b2x1dGlvbi10aGUtbW9tZW50LWZvci1hY3Rpb24v0gFpaHR0cHM6Ly9sYWJvdXJsaXN0Lm9yZy8yMDIxLzAzL3dvcmstYW5kLXRoZS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1yZXZvbHV0aW9uLXRoZS1tb21lbnQtZm9yLWFjdGlvbi8_YW1w?oc=5,AI must be used to benefit workers – not as a means to downgrade jobs and pay - LabourList,2021-03-25,LabourList,https://labourlist.org,N/A,N/A,We are at a crucial moment in what has become widely known as the fourth industrial revolution. Artificial…,N/A,https://schema.org,"[{'@type': 'Article', '@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/#article', 'isPartOf': {'@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/'}, 'author': [{'@id': 'https://labourlist.org/#/schema/person/0537bfd887fa7c982ae268db0d7985a3'}], 'headline': 'AI must be used to benefit workers –\xa0not as a means to downgrade jobs and pay', 'datePublished': '2021-03-25T10:46:04+00:00', 'dateModified': '2021-03-26T08:40:49+00:00', 'mainEntityOfPage': {'@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/'}, 'wordCount': 721, 'publisher': {'@id': 'https://labourlist.org/#organization'}, 'image': {'@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/#primaryimage'}, 'thumbnailUrl': 'https://labourlist.org/wp-content/uploads/2021/03/Artificial-intelligence.jpg', 'keywords': [""Worker's Rights"", 'Employment', 'TUC', 'artificial intelligence'], 'articleSection': ['Comment', 'Unions'], 'inLanguage': 'en-GB'}, {'@type': 'WebPage', '@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/', 'url': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/', 'name': 'AI must be used to benefit workers –\xa0not as a means to downgrade jobs and pay - LabourList', 'isPartOf': {'@id': 'https://labourlist.org/#website'}, 'primaryImageOfPage': {'@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/#primaryimage'}, 'image': {'@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/#primaryimage'}, 'thumbnailUrl': 'https://labourlist.org/wp-content/uploads/2021/03/Artificial-intelligence.jpg', 'datePublished': '2021-03-25T10:46:04+00:00', 'dateModified': '2021-03-26T08:40:49+00:00', 'breadcrumb': {'@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/#breadcrumb'}, 'inLanguage': 'en-GB', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-GB', '@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/#primaryimage', 'url': 'https://labourlist.org/wp-content/uploads/2021/03/Artificial-intelligence.jpg', 'contentUrl': 'https://labourlist.org/wp-content/uploads/2021/03/Artificial-intelligence.jpg', 'width': 8088, 'height': 3370, 'caption': '© everything possible/Shutterstock.com'}, {'@type': 'BreadcrumbList', '@id': 'https://labourlist.org/2021/03/work-and-the-artificial-intelligence-revolution-the-moment-for-action/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://labourlist.org/'}, {'@type': 'ListItem', 'position': 2, 'name': 'AI must be used to benefit workers –\xa0not as a means to downgrade jobs and pay'}]}, {'@type': 'WebSite', '@id': 'https://labourlist.org/#website', 'url': 'https://labourlist.org/', 'name': 'LabourList', 'description': 'The latest news and comment on policy, elections, polls and more on Keir Starmer&#039;s Labour Party.', 'publisher': {'@id': 'https://labourlist.org/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://labourlist.org/search/{search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-GB'}, {'@type': 'Organization', '@id': 'https://labourlist.org/#organization', 'name': 'LabourList', 'url': 'https://labourlist.org/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-GB', '@id': 'https://labourlist.org/#/schema/logo/image/', 'url': 'https://labourlist.org/wp-content/uploads/2023/04/LabourList.jpg', 'contentUrl': 'https://labourlist.org/wp-content/uploads/2023/04/LabourList.jpg', 'width': 486, 'height': 508, 'caption': 'LabourList'}, 'image': {'@id': 'https://labourlist.org/#/schema/logo/image/'}, 'sameAs': ['https://en-gb.facebook.com/LabourList/', 'https://twitter.com/LabourList']}, {'@type': 'Person', '@id': 'https://labourlist.org/#/schema/person/0537bfd887fa7c982ae268db0d7985a3', 'name': 'Mary Towers', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-GB', '@id': 'https://labourlist.org/#/schema/person/image/b0a35e914af90fb46a0f72022ad74d3a', 'url': 'https://labourlist.org/wp-content/uploads/2021/03/IMG_20200528_210603-rotated.jpg', 'contentUrl': 'https://labourlist.org/wp-content/uploads/2021/03/IMG_20200528_210603-rotated.jpg', 'caption': 'Mary Towers'}, 'description': ""Mary Towers is a TUC policy officer in its employment rights team and leads the organisation's artificial intelligence taskforce."", 'url': 'https://labourlist.org/author/mary-towers/'}]",N/A,N/A,"


25th March, 2021, 10:46 am

AI must be used to benefit workers – not as a means to downgrade jobs and pay







Mary Towers 









© everything possible/Shutterstock.com



Share this article: 

We are at a crucial moment in what has become widely known as the fourth industrial revolution. Artificial intelligence is already being used to carry out many human tasks and the pace of change has been turbo-charged by the coronavirus pandemic. AI-powered tools are increasingly being used to manage people who are working remotely. But few of us realise the extent to which AI is being used by employers to make important decisions about people at work. We need to seize this moment to make sure that AI is used to benefit workers – not simply as another means to downgrade jobs and pay.
More and more, AI is making life-changing decisions, like who gets a job, who gets paid what salary and who gets made redundant. And when these decisions are unfair, discriminatory or unsafe, this has huge consequences for people’s lives. But workers tell us that they are often unaware of AI being used in this way, that it’s difficult to challenge decisions and how they feel constantly scrutinised. Yet AI technology is being rolled out at work too fast for employment law to catch up. We believe society and policymakers now have a choice to make about whether we put human values at the heart of this technological revolution, or whether we allow commercial interests alone to drive developments. What we choose to do now matters to all of us.
What should we do? The TUC aims to seize this moment in the AI revolution to put human interests at the heart of technological developments, and to ensure that workers benefit from the opportunities offered by AI. Today, we have published a short manifesto that sets out the values employers, technologists and political parties should adopt to ensure that technology at work benefits all. Alongside these values, we make a series of proposals, including reform to the law, based on recommendations made by leading employment lawyers Robin Allen QC and Dee Masters and TUC research.
The importance of collective bargaining is a key principle that underpins our manifesto. Some of our proposals are non-negotiable red lines that we believe are critical to ensuring dignity at work: AI should only be used to make important decisions about people if these decisions are easy to explain and understand; no unlawful discriminatory decisions should be made using technology; everyone at work should have a say in deciding whether AI is introduced to make important decisions about people at work; it should be clear when AI systems are being used at work; and there should be enough information available to people at work and job applicants to satisfy them that the technology is being used fairly.
To secure our red lines, we propose a series of amendments to data protection, employment and equality legislation, as well as better guidance on how to avoid AI-based discrimination, and a statutory duty to consult trade unions. But we propose increased regulation to target high-risk applications only, not harmless uses of AI at work. We do not want to inhibit innovation, but we do want to make sure that innovation benefits everyone at work. And in order to reassert human agency in the face of technological control, we propose a new right to human review of decisions made using AI, as well as a right to in-person engagement and a right to disconnect from digital devices.
The TUC is also calling on workers and trade unions to harness the power of data and AI, proposing a ground-breaking new right of “data reciprocity” whereby workers could collect and combine workplace data. This could help identify patterns in who isn’t receiving their rights and why that could assist union campaigning for better pay and benefits at work. And we highlight ways in which trade unions could help workers achieve data equality, including adopting a formal data gathering role, engaging data scientists and developing AI-powered tools.
Our vision for the future world of work is one where technology is for the benefit of everyone, and humanity is at the heart of all development and application of artificial intelligence at work. You can join us in making this vision a reality by signing up to the values and proposals in our manifesto.

Tags:
Worker's Rights /
Employment /
TUC /
artificial intelligence /








Mary Towers

Mary Towers is a TUC policy officer in its employment rights team and leads the organisation's artificial intelligence taskforce.


View all articles by Mary Towers




Subscribe to our daily email



Value our free and unique service?
LabourList has more readers than ever before - but we need your support. Our dedicated coverage of Labour's policies and personalities, internal debates, selections and elections relies on donations from our readers.
Support LabourList







More from LabourList





News
Child poverty taskforce launched amid pressure to reverse two-child benefit cap
Labour is launching a new “taskforce” to work on a child poverty strategy, after pressure on the government… 


Tom Belger 


17th July, 2024, 2:47 pm









News
King’s Speech 2024: What policies didn’t make it into the King’s address?
King Charles delivered the first King’s Speech of the new Labour government today, a jam-packed address that featured… 


Katie Neame 


17th July, 2024, 2:07 pm









News
King’s Speech vows ‘genuine’ living wage and flexible working ‘default’
Labour has committed in its first King’s Speech of the new parliament to deliver a “genuine” living wage… 


Katie Neame 


17th July, 2024, 12:04 pm







",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiLGh0dHBzOi8vd3d3LmJiYy5jb20vbmV3cy90ZWNobm9sb2d5LTU2NTE1ODI30gEwaHR0cHM6Ly93d3cuYmJjLmNvbS9uZXdzL3RlY2hub2xvZ3ktNTY1MTU4MjcuYW1w?oc=5,AI at work: Staff 'hired and fired by algorithm' - BBC.com,2021-03-24,BBC.com,https://www.bbc.com,The TUC is calling for fresh legal protections to cover artificial intelligence in the workplace.,N/A,The TUC is calling for fresh legal protections to cover artificial intelligence in the workplace.,The TUC is calling for fresh legal protections to cover artificial intelligence in the workplace.,http://schema.org,,N/A,N/A,"AI at work: Staff 'hired and fired by algorithm'24 March 2021ShareGetty ImagesThe Trades Union Congress (TUC) has warned about what it calls “huge gaps” in UK employment law over the use of artificial intelligence at work.The TUC said workers could be “hired and fired by algorithm”, and new legal protections were needed. Among the changes it is calling for is a legal right to have any “high-risk” decision reviewed by a human.TUC general secretary Frances O’Grady said the use of AI at work stood at “a fork in the road”. “AI at work could be used to improve productivity and working lives. But it is already being used to make life-changing decisions about people at work - like who gets hired and fired.  “Without fair rules, the use of AI at work could lead to widespread discrimination and unfair treatment – especially for those in insecure work and the gig economy,” she warned. 2:06How will AI change the future jobs market?Many workplaces already use automated decision making for simple tasks. For example, Uber assigns driving jobs to its drivers automatically, by computer, and Amazon is known to use  AI monitoring systems to watch its staff in its warehouses.  And many firms already use an automated system with no human oversight in the first stage of the hiring process, to narrow the field. The computers rejecting your job application Computer says go: Taking orders from an AI bossBut as AI becomes more sophisticated, the fear is that it will be entrusted with more serious, high-risk decisions, such as analysing those performance metrics to figure out who should be first in line for promotion – or being let go. That can happen even when a human is involved, a TUC report warns, thanks to automated decision making. Human agency“A human might undertake some formal task, such as handling a document, but the human agency in the decision is minimal,” the authors write. “Sometimes the human decision making is largely illusory, for instance where a human is ultimately involved only in some formal way in the decision what to do with the output from the machine.” The TUC’s report, written with the aid of employment rights lawyers and the AI Law Consultancy, argues that the law has failed to stay abreast of quick progress in AI in recent years. The union body is calling for: An obligation on employers to consult unions on the use of “high risk” or “intrusive” AI at workThe legal right to have a human review decisionsA legal right to “switch off” from work and not be expected to answer calls or emailsChanges to UK law to protect against discrimination by algorithmDiscrimination by algorithm has been well-documented in recent years, often as an unintentional side-effect of using systems that fail to account for racial bias.One high-profile example is in facial recognition technology, which has in the past been trained to recognise white faces more easily than those from other backgrounds. Such problems led IBM to abandon some of its efforts with the technology last year, labelling it as “biased”. The TUC also pointed to recent reports of allegations from delivery drivers for Uber Eats who claimed they had been fired because the facial recognition software was unable to recognise their faces. That led to drivers with 100% ratings and thousands of deliveries under their belts being fired for failing to complete an ID check, the affected drivers claimed. Uber denies this, saying a human review is always involved before it drops drivers from its platform. 'Exceptionally dangerous'The authors of the report for the TUC, Robin Allen and Dee Masters from Cloisters law firm, said while AI could be beneficial, “used in the wrong way it can be exceptionally dangerous”. “Already important decisions are being made by machines,” the pair said in a joint statement. “Accountability, transparency and accuracy need to be guaranteed by the legal system through the carefully crafted legal reforms we propose. There are clear red lines, which must not be crossed if work is not to become dehumanised.”Computer says go: Taking orders from an AI bossThe computers rejecting your job applicationWill Covid-19 accelerate the use of robots at work?The algorithms that make decisions about your lifeFacial recognitionTrades Union CongressGig economyArtificial intelligenceEmployment",ReportageNewsArticle,https://www.bbc.com/news/technology-56515827,"{'@type': 'NewsMediaOrganization', 'name': 'BBC News', 'publishingPrinciples': 'http://www.bbc.co.uk/news/help-41670342', 'logo': {'@type': 'ImageObject', 'url': 'https://static.files.bbci.co.uk/ws/simorgh-assets/public/news/images/metadata/poster-1024x576.png'}}",2021-03-25T01:21:28.000Z,2021-03-25T01:21:28.000Z,AI at work: Staff 'hired and fired by algorithm',"{'@type': 'ImageObject', 'width': 1024, 'height': 576, 'url': 'https://ichef.bbci.co.uk/news/1024/branded_news/CA1B/production/_117693715_56515827.jpg'}",https://ichef.bbci.co.uk/news/1024/branded_news/CA1B/production/_117693715_56515827.jpg,https://www.bbc.com/news/technology-56515827,"{'@type': 'NewsMediaOrganization', 'name': 'BBC News', 'noBylinesPolicy': 'http://www.bbc.co.uk/news/help-41670342#authorexpertise', 'logo': {'@type': 'ImageObject', 'url': 'https://static.files.bbci.co.uk/ws/simorgh-assets/public/news/images/metadata/poster-1024x576.png'}}",,,,,,,
https://news.google.com/rss/articles/CBMibGh0dHBzOi8vd3d3LnBvbGl0aWNzaG9tZS5jb20vdGhlaG91c2UvYXJ0aWNsZS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1wb3Nlcy1hbi1pbmh1bWFuLXJpc2stdG8td29ya2VyLXJpZ2h0c9IBAA?oc=5,Artificial intelligence poses an inhuman risk to worker rights - PoliticsHome,2021-03-25,PoliticsHome,https://www.politicshome.com,N/A,N/A,This pandemic has transformed the way millions work.  Many of us hadn’t even heard of Zoom until the crisis hit - now that we’re experts we sorely ...,"This pandemic has transformed the way millions work.  Many of us hadn’t even heard of Zoom until the crisis hit - now that we’re experts we sorely miss meeting colleagues face-to-face.

But the huge growth in remote working is not the only technological shift we have witnessed during Covid-19. The use of artificial intelligence (AI) at work has massively accelerated too – albeit under the radar.

",,,N/A,N/A,"

Artificial intelligence poses an inhuman risk to worker rights









































                    Frances O'Grady
                    


@FrancesOGrady




3 min read25 March 2021


This pandemic has transformed the way millions work.  Many of us hadn’t even heard of Zoom until the crisis hit - now that we’re experts we sorely miss meeting colleagues face-to-face.

But the huge growth in remote working is not the only technological shift we have witnessed during Covid-19. The use of artificial intelligence (AI) at work has massively accelerated too – albeit under the radar.



This is not a niche issue. AI is increasingly making life-changing decisions about our working lives - like who gets hired, how much workers are paid, and crucially who is made redundant from their job.
But as advances in technology gather pace, UK employment law is failing to keep up.
This should concern us all.
The TUC has today published a 100 page legal opinion from leading employment rights lawyers Robin Allen QC and Dee Masters from the AI Law Consultancy.

Related









New Labour MPs To Face Immediate Pressure Over Two-Child Cap


                By Sienna Rodgers


11 Jul




Both are hugely respected practitioners in their field and their message to government and policymakers is clear.
Unless urgent new legal protections are put in place, workings will become increasingly and powerless to challenge “inhuman” forms of AI performance management. We are already seeing examples of this happening – especially in the gig economy.

Put bluntly, there needs to be far more transparency over how AI is being used at work.

 






Put bluntly, there needs to be far more transparency over how AI is being used at work.
TUC research published in November revealed that fewer than one in three (31 per cent) workers are consulted when any new forms of technology are introduced. 
And six in ten (60 per cent) said that unless carefully regulated, AI could increase unfair treatment in the workplace. 
That is why the TUC has today issued a joint call to tech companies, employers and ministers to work with us on a new set legal reforms for the ethical use of AI at work. 
These reforms should include: 

a legal duty on employers to consult trade unions on the use of high risk and intrusive forms of AI in the workplace. 
A legal right for all workers to have a human review of decisions made by AI systems so they can challenge decisions that are unfair and discriminatory. 
Amendments to the UK General Data Protection Regulation (UK GDPR) and Equality Act to guard against discrimination by algorithm. 
A legal right to ""switch off"" from work so workers can have proper downtime in their lives. 

None of these calls are a roadblock to innovation. But they will provide much-needed protection for workers.
As legal experts Robin Allen and Dee Masters sum up neatly: “Used properly, AI can change the world of work for good. But used in the wrong way it can be exceptionally dangerous. 
“There are currently huge gaps in the law when it comes to regulating AI at work. They must be quickly plugged quickly to stop workers from being discriminated against and mistreated.”
The TUC feels the same. Which is why, in addition to today’s legal report, we are publishing a short manifesto on the fair and transparent use of AI at work.
We hope all political parties sign up to the values and principles it enshrines.
Make no mistake. AI can be harnessed to transform working lives for the better.
But without proper regulation, accountability and transparency, we risk it being used to set punishing targets, rob workers of human connection and deny them dignity at work.

We are at a fork in the road on workplace AI technology. It’s vital that we pick the right path.

Frances O’Grady is TUC General Secretary


Related









New Labour MPs To Face Immediate Pressure Over Two-Child Cap


                By Sienna Rodgers


11 Jul




PoliticsHome Newsletters
Get the inside track on what MPs and Peers are talking about. Sign up to The House's morning email for the latest insight and reaction from Parliamentarians, policy-makers and organisations. 

Read the most recent article written by Frances O'Grady - Enough is enough – the next prime minister needs a plan to get wages rising across the economy



",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS91cG1peGluZy1hdWRpby1yZWNvcmRpbmdzLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlL9IBAA?oc=5,How Audio Pros 'Upmix' Vintage Tracks and Give Them New Life - WIRED,2021-03-23,WIRED,https://www.wired.com,"Experts are using AI to pick apart classic recordings from the 50s and 60s, isolate the instruments, and stitch them back together in crisp, bold ways.","['the big story', 'gear', 'trends', 'ai hub', 'small company', 'entertainment', 'machine learning', 'longreads', 'audio', 'audio/music', 'artificial intelligence', 'software', 'the beatles', '_no-apple-news', 'textbelowleftfullbleed', 'web']","Experts are using AI to pick apart classic recordings from the 50s and 60s, isolate the instruments, and stitch them back together in crisp, bold ways.","Experts are using AI to pick apart classic recordings from the 50s and 60s, isolate the instruments, and stitch them back together in crisp, bold ways.",https://schema.org/,,tags,N/A,"Jesse JarnowThe Big StoryMar 23, 2021 7:00 AMHow Audio Pros ‘Upmix’ Vintage Tracks and Give Them New LifeExperts are using AI to pick apart classic recordings from the 50s and 60s, isolate the instruments, and stitch them back together in crisp, bold ways.Illustration: Jenny Sharaf; Getty ImagesSave this storySaveSave this storySaveThe AI Database →End UserSmall companySectorEntertainmentTechnologyMachine learningWhen James Clarke went to work at London’s legendary Abbey Road Studios in late 2009, he wasn’t an audio engineer. He’d been hired to work as a software programmer. One day not long after he started, he was having lunch with several studio veterans of the 1960s and ’70s, the pre-computer era of music recording when songs were captured on a single piece of tape. To make conversation, Clarke asked a seemingly innocent question: Could you take a tape from the days before multitrack recording and isolate the individual instruments? Could you pull it apart?The engineers shot him down. It turned into “several hours of the ins and outs of why it’s not possible,” Clarke remembers. You could perform a bit of sonic trickery to transform a song from one-channel mono to two-channel stereo, but that didn’t interest him. Clarke was seeking something more exacting: a way to pick apart a song so a listener could hear just one element at a time. Maybe just the guitar, maybe the drums, maybe the singer.AdChoicesADVERTISEMENT“I kept saying to them that if the human ear can do it, we can write software to do it as well,” he says. To him, this was a challenge. “I’m from New Zealand. We love proving people wrong.”Trending NowWhere the Sounds From the World's Favorite Movies Are BornThe challenge dropped him at the leading edge of a field known as upmixing, in which software and audio engineers work together to transform old recordings in ways that were once unthinkable. Using machine learning, engineers have made inroads into “demixing” the voices and instruments on recordings into completely separate component tracks, often known as stems. Isolating the components of songs is a surprisingly hard problem—more like unswirling paint than using a pair of scissors. But once engineers have stems, they can take the isolated tracks and “upmix” them into something new and perhaps improved. They might enhance a muffled drum track on an old recording, produce an a capella version of a song, or do the opposite and remove a song’s vocals so it can be used as background in a TV show or movie.As an Abbey Road employee, it was only natural that Clarke would soon focus his experimentation on Beatles songs. But he wasn’t the only one trying to pull apart old music. Around the world, other audio aficionados were tackling the same challenge with their own favorite tracks—and converging on some of the same methods. In the years since Clarke’s fateful lunchtime chat, the number of apps and tools for splitting songs has exploded, as has the community of academics and enthusiasts that surround the practice. For creators of sample-based music, demixing is conceivably the greatest sonic invention since the digital sampler that fueled the explosion of hip hop four decades ago. For karaoke fans, it’s a game changer. For the people (or private equity firms) who own the rights to classic but inferior recordings—or enthusiasts willing to wade into legal gray areas—upmixing presents a whole new way to hear the past. After decades of slow advancement, deep learning has now sent both technologies into overdrive. The uncanny valley is alive with the sound of music.Listen to the full story here or on the Curio app.
New Old SoundsTwo decades ago, one of the first people to experiment with demixing was Christopher Kissel, a professional electronics test engineer from Long Island. Kissel didn’t have easy access to a recording studio. But he was a lifelong music fan, and he dreamed of making old tracks sound new.Until the 1960s, almost all popular music was recorded and listened to monaurally—all the instrumental and vocal parts were recorded onto a single track of tape and played back through a single speaker. Once a song was on tape, it was basically finished. But Kissel had an inkling that it might be possible to update old mono recordings in a profound way.In 2000 he purchased his first Mac for the express purpose of transforming single-track pop songs from the ’50s and ’60s into two-channel stereo versions fit for headphones or properly separated speakers. “Compared to mono, stereo sounds more lifelike and allows you to more viscerally hear and appreciate the interplay between the musicians,” he says. Most listeners probably prefer their music on two speakers (or headphones), and Kissel cared enough to try forcing the old recordings into stereo.Most PopularGearThe 29 Best Early Amazon Prime Day DealsBy Simon Hill, WIREDGearThe Top 5 Prime Day Apple Deals on Our Favorite GadgetsBy Brenda Stolyar, WIREDGearThe Top 8 Prime Day Laptop Deals for Work and PlayBy Scott Gilbertson, WIREDGearThe Best Mesh Wi-Fi RoutersBy Simon Hill, WIREDThe first night with his new Mac, Kissel used floppy disks to install an early digital audio workstation called sonicWORX. It was the only software capable of running Pandora Realtime, a plug-in that could selectively boost the volume of vocals on recordings. “It was very advanced for its time,” Kissel says. He wanted to see if the tool could do something more interesting. He loaded up Miss Toni Fisher’s 1959 hit “The Big Hurt” and attempted to pull it apart.AdvertisementTinkering with the song using sonicWORX’s waveform visualizer and settings, Kissel says, he was ""able to separate the lead vocal, backing vocals, and strings and move them to the right side, and the rest of the backing instrumentation to the left.” It was crude and a little glitchy, but the effect was powerful. “It was quite thrilling to hear,” he says. Decades later, Kissel remains blown away by that first experience.He experimented with more ’50s and ’60s classics, including the Del Vikings’ “Whispering Bells,” Johnny Otis’ “Willie and the Hand Jive,” and—perhaps most appropriately—the Tornados’ “Telstar,” a futuristic DIY wonder produced and composed by the influential sound engineer Joe Meek.Kissel immersed himself in the developing fields of demixing and upmixing—though the names came later—by moderating forums and maintaining a website chronicling advances in the disciplines. He started playing around with a technique called spectral editing, which allowed people to treat sound as a visual object. Load a song into a spectral editor and you can see all of the recording’s many frequencies, represented as colorful peaks and valleys, laid out on a graph. At the time, audio engineers employed spectral editing to remove unwanted noise in a recording, but an intrepid user could also zone in on specific frequencies of an audio track and pluck them out. When a freeware spectral editing tool called Frequency popped up, Kissel decided to try it out.He spent about 60 hours using Frequency to craft an upmix of the 1951 mono R&B hit “The Glory of Love,” by the vocal group the Five Keys, using the app to carefully target the separate vocalists and spread their voices across the stereo spectrum. With a final polish by disco legend Tom Moulton, Kissel’s friend, it became one of the first spectrally edited upmixes to get released on a commercial album, in 2005. Several labels soon began releasing collections of upmixed mono-to-stereo hits, sometimes licensed, sometimes in the public domain, and sometimes in between.French software company Audionamix started building professional demixing software to help users pull apart tracks on their own, which made this suite of techniques more accessible. In 2007 the company unveiled a major achievement in upmixing, bringing vintage Édith Piaf recordings from mono to theater-ready surround sound for the biopic La Vie en Rose. In 2009, the company opened a Hollywood office to continue courting film, television, and commercial work.Other times, their projects involved focused demixing. When the British online lending company Sunny wanted to use the song “Sunny” by late American R&B singer Bobby Hebb in a commercial, it found that one of the song’s original vocals interrupted the ad’s narration. With Audionamix’s help, the pesky vocals got zapped from existence. The French company also offers a service—originally called “music disassociation,” but now rebranded slightly less ominously as “music removal”—in which old television series and movies are scrubbed of music that might be too expensive to license, so they can be released in the latest format, be it DVD or streaming. According to Nicolas Cattaneo, a researcher at Audionamix, “This is the first thing that began to be really usable,” at least commercially. (Scholars studying music in old films and television shows should probably rely on releases from before 2009 or so if they want to make sure they’re hearing the original soundtracks.)Most PopularGearThe 29 Best Early Amazon Prime Day DealsBy Simon Hill, WIREDGearThe Top 5 Prime Day Apple Deals on Our Favorite GadgetsBy Brenda Stolyar, WIREDGearThe Top 8 Prime Day Laptop Deals for Work and PlayBy Scott Gilbertson, WIREDGearThe Best Mesh Wi-Fi RoutersBy Simon Hill, WIREDAudioSourceRE and Audionamix’s Xtrax Stems are among the first consumer-facing software options for automated demixing. Feed a song into Xtrax, for example, and the software spits out tracks for vocals, bass, drums, and “other,” that last term doing heavy lifting for the range of sounds heard in most music. Eventually, perhaps, a one-size-fits-all application will truly and instantly demix a recording in full; until then, it’s one track at a time, and it’s turning into an art form of its own.What the Ear Can HearAt Abbey Road, James Clarke began to chip away at his demixing project in earnest around 2010. In his research, he came across a paper written in the ’70s on a technique used to break video signals into component images, such as faces and backgrounds. The paper reminded him of his time as a master’s student in physics, working with spectrograms that show the changing frequencies of a signal over time.Spectrograms could visualize signals, but the technique described in the paper—called non-negative matrix factorization—was a way of processing the information. If this new technique worked for video signals, it could work for audio signals too, Clarke thought. “I started looking at how instruments made up a spectrogram,” he says. “I could start to recognize, ‘That’s what a drum looks like, that looks like a vocal, that looks like a bass guitar.’” About a year later, he produced a piece of software that could do a convincing job of breaking apart audio by its frequencies. His first big breakthrough can be heard on the 2016 remaster of the Beatles’ Live at the Hollywood Bowl, the band’s sole official live album. The original LP, released in 1977, is hard to listen to because of the high-pitched shrieks of the crowd.After unsuccessfully trying to reduce the noise of the crowd, Clarke finally had a “serendipity moment.” Rather than treating the howling fans as noise in the signal that needed to be scrubbed out, he decided to model the fans as another instrument in the mix. By identifying the crowd as its own individual voice, Clarke was able to tame the Beatlemaniacs, isolating them and moving them to the background. That, then, moved the four musicians to the sonic foreground.Clarke became a go-to industry expert on upmixing. He helped rescue the 38-CD Grammy-nominated Woodstock–Back to the Garden: The Definitive 50th Anniversary Archive, which aimed to assemble every single performance from the 1969 mega-festival. (Disclosure: I contributed liner notes to the set.) At one point during some of the festival’s heaviest rain, sitar virtuoso Ravi Shankar took to the stage. The biggest problem with the recording of the performance wasn’t the rain, however, but that Shankar’s then-producer absconded with the multitrack tapes. After listening to them back in the studio, Shankar deemed them unusable and released a faked-in-the-studio At the Woodstock Festival LP instead, with not a note from Woodstock itself. The original festival multitracks disappeared long ago, leaving future reissue producers nothing but a damaged-sounding mono recording off the concert soundboard.Most PopularGearThe 29 Best Early Amazon Prime Day DealsBy Simon Hill, WIREDGearThe Top 5 Prime Day Apple Deals on Our Favorite GadgetsBy Brenda Stolyar, WIREDGearThe Top 8 Prime Day Laptop Deals for Work and PlayBy Scott Gilbertson, WIREDGearThe Best Mesh Wi-Fi RoutersBy Simon Hill, WIREDUsing only this monaural recording, Clarke was able to separate the sitar master’s instrument from the rain, the sonic crud, and the tabla player sitting a few feet away. The result was “both completely authentic and accurate,” with bits of ambiance still in the mix, says the box set’s coproducer, Andy Zax.“The possibilities upmixing gives us to reclaim the unreclaimable are really exciting,” Zax says. Some might see the technique as akin to colorizing classic black-and-white movies. “There’s always that tension. You want to be reconstructive, and you don’t really want to impose your will on it. So that's the challenge.”Heading for the Deep EndAround the time Clarke finished working on the Beatles’ Hollywood Bowl project, he and other researchers were coming up against a wall. Their techniques could handle fairly simple patterns, but they couldn’t keep up with instruments with lots of vibrato—the subtle changes in pitch that characterize some instruments and the human voice. The engineers realized they needed a new approach. “That’s what led toward deep learning,” says Derry Fitzgerald, the founder and chief technology officer of AudioSourceRE, a music software company.Fitzgerald was a lifelong Beach Boys fan; some of the mono-to-stereo upmixes he did of their work, for the fun of it, got tapped for official releases starting in 2012. Like Clarke, Fitzgerald had found his way to non-negative matrix factorization. And, like Clarke, he’d reached the limits of what he could with it. “It got to a point where the amount of hours I spent tweaking the code was very, very time-consuming,” he says. “I thought there had to be a better way.”The WIRED Guide to Artificial IntelligenceSupersmart algorithms won't take all the jobs, But they are learning faster than ever, doing everything from medical diagnostics to serving up ads.By Tom SimoniteThe nearly parallel move to AI by Fitzgerald, James Clarke, and others echoed Clarke’s original instinct that if the human ear can naturally separate the sounds of instruments from one another, it should also be possible to model that same separation by machine. “I started researching deep learning to get more of a neural network approach to it,” Clarke says.He started experimenting with a specific goal in mind: pulling out George Harrison’s guitar from the early Beatles hit “She Loves You.” On the original recording, the instruments and vocals were all laid on a single track, which makes it nearly impossible to manipulate.Clarke started building an algorithm and trained it on every version of the song he could find—radio sessions, live versions, even renditions by tribute bands. “There were quite a few different ones, so plenty of examples to understand how the track should sound,” Clarke says. Using spectrograms, he now also knew how the track should look. The algorithm broke up the audio into individual stems, one for each instrument, but Clarke only had eyes and ears for Harrison’s Gretsch Chet Atkins Country Gentleman guitar.Over nine months, Clarke sifted through the guitar part a few seconds at a time, virtually hand-cleaning the track phrase by phrase. He listened for stray audio artifacts from other instruments and used spectral editing software to find and eliminate them. For the final step, he set out to recapture the track’s original ambience. That part was easy. As an Abbey Road employee, he could book time in the vaunted Studio Two, where “She Loves You” was originally recorded. He played his track into the room through the in-house speakers and recorded it anew, to capture some of the subtleties of the room’s well-preserved acoustics. In August 2018, Clarke showed off his AI demixing work publicly for the first time.Most PopularGearThe 29 Best Early Amazon Prime Day DealsBy Simon Hill, WIREDGearThe Top 5 Prime Day Apple Deals on Our Favorite GadgetsBy Brenda Stolyar, WIREDGearThe Top 8 Prime Day Laptop Deals for Work and PlayBy Scott Gilbertson, WIREDGearThe Best Mesh Wi-Fi RoutersBy Simon Hill, WIREDThe occasion was a sold-out lecture series that offered a rare chance for fans to step inside Studio Two, where the Beatles, Pink Floyd, and plenty of others recorded. Visitors were invited to re-create the clattering E-major chord that ends “A Day in the Life” by playing the studio’s pianos at the same time. The audience also received a glimpse of the future.In front of a packed audience, Clarke played the Beatles’ original 1963 recording of “She Loves You.” Then, to pin-drop silence, he played what should have been impossible: the same recording with everything removed except for Harrison’s guitar.Three days later, excerpts of Clarke’s demo made their way onto the web. The truthers quickly descended. Disbelieving audiophiles started trashing Clarke in online forums. “I think it’s a shame that the demonstration to show how good this new technology is happens to be false,” a user who went by Beatlebug wrote.“It's kind of sad that Abbey Road has to mislead people like that,” RingoStarr39 posted in the same thread.Beatlebug, RingoStarr39, and others insisted that the audio segment in Clarke’s lecture was a more easy-to-isolate bit from a later German version of the song, “Sie Liebt Dich,” recorded in stereo. They insisted that James Clarke was a charlatan.But Clarke had merely demonstrated a proof of concept. Perfecting Harrison’s guitar track of “She Loves You” took him approximately 200 hours. He hadn’t even attempted to isolate John Lennon’s guitar. “Not a viable option for projects,” he admits. It was far from automated. But it could be done. And it would be.Up, Up, and AwayThe dam broke fully when French streaming service Deezer released an open source code library called Spleeter that allowed both casual and professional programmers to build tools for demixing and upmixing. Anybody comfortable enough with their computer’s command line interface could download and install software from Github, select an audio file of their favorite song, and generate their own set of isolated stems. People started putting the code library to creative use. When tech blogger Andy Baio played around with it, he was delighted to discover how easy it now was to create mashups, such as when he crossed the Friends theme and Billy Joel’s “We Didn’t Start the Fire.” “Nobody should have this kind of power,” he tweeted.The first generation of users are demixing and upmixing in creative ways. Some musicians are removing one instrument from a song to create tracks they can practice along to or to generate source material for new music. Podcast producers are cleaning up dialog recorded in noisy environments. Hobbyists are using iPad apps and free sites to create their own mixes or make any song karaoke-ready. Several streaming services in Japan now offer vocal removal in officially licensed form, including Spotify’s SingAlong, where listeners can turn down a song’s vocals, and Line Music, which promises real-time source separation.Most PopularGearThe 29 Best Early Amazon Prime Day DealsBy Simon Hill, WIREDGearThe Top 5 Prime Day Apple Deals on Our Favorite GadgetsBy Brenda Stolyar, WIREDGearThe Top 8 Prime Day Laptop Deals for Work and PlayBy Scott Gilbertson, WIREDGearThe Best Mesh Wi-Fi RoutersBy Simon Hill, WIREDAlong with established players (Audionamix, James Clarke), the newest company offering professional demixing services is the California-based startup Audioshake.The company will soon launch a service where music rights holders—both musicians and labels—can upload their tracks to the cloud and, within minutes, download high-quality stems ready for licensing in film, broadcasting, video games, and elsewhere. Audioshake claims best-in-field ratings for drums, bass, and vocals, according to benchmarks established by the Signal Separation Evaluation Campaign, an organization made up of audio researchers who track the progress of demixing techniques.But Audioshake is also the first company to figure out how to automatically isolate guitars—or, more precisely, a single guitar. The company is tight-lipped about how it achieved this. “We refined the architecture of our deep-learning network to be specially tailored to the harmonics and timbre of the guitar,” says company AI researcher Fabian-Robert Stöter. Basically, when a user uploads a track to Audioshake, a layer in the company’s algorithm converts the song’s waveform into a numerical representation that makes it easier for the AI model to figure out where a guitar ends and everything else begins.Sign Up TodaySign up for our Longreads newsletter for the best features, ideas, and investigations from WIRED.To see it work, I was invited to upload some songs. Within a few minutes, the company’s software was able to pull apart a track of a rock band playing in guitar-bass-drums-vocals power trio format. A track by Talking Heads’ original lineup came back with David Byrne’s 12-string acoustic guitar separated (with minimal artifacts) alongside tracks of Tina Weymouth’s bass and Chris Frantz’s drums. It works equally well on other songs in that exact guitar/bass/drums/vocals configuration. But music is huge, and the power trio format is a tight set of parameters.Outside those parameters is the unclaimed frontier of demixing. The original recording of “She Loves You” comes back from Audioshake with Lennon's and Harrison’s guitars sounding like jangling ghosts. James Clarke’s manual work still can’t be matched by a machine. That said, Audioshake does what couldn’t be done only a few years ago, pointing to a future in which machines will recognize more instruments. They might be unbreachable frontiers. For virtually all producers since the ’60s, a recording studio has been the place to combine unusual instruments and generate wondrous new sounds (and literal overtones) explicitly designed to blend together in the listener’s ears.But what if the artifacts turn out to be art? If a demixing attempt gone awry sounds cool to the right producer, it might become the basis for fantastic new music. Think Cher turning Auto-Tune into a pop trend with “Believe.” As archival producer Andy Zax put it, “Some 16-year-old making hip hop records on a PlayStation is going to figure out some genius use of this thing and create a sound world we've never heard before.”For now, plenty of experimentation is happening in far-flung fan forums, with unofficial upmixes of many equally unofficial recordings. Some fans have been exploring a subgenre that might be called upfakes, fusing, say, George Harrison’s original 1968 demo for “Sour Milk Sea”' with the backing track from a more recent recording by another musician. (Fans are understandably jittery about copyright claims and generally only post their work with quickly expiring links.)As for Clarke, he is still working on the exact AI methodology to pull apart a mono Beatles vocal track. He’s also started an independent company called Audio Research Group to work as demixer-for-hire. Lately he’s been helping to create a set of tracks for a band that lost all its master tapes and has only its LPs.Most PopularGearThe 29 Best Early Amazon Prime Day DealsBy Simon Hill, WIREDGearThe Top 5 Prime Day Apple Deals on Our Favorite GadgetsBy Brenda Stolyar, WIREDGearThe Top 8 Prime Day Laptop Deals for Work and PlayBy Scott Gilbertson, WIREDGearThe Best Mesh Wi-Fi RoutersBy Simon Hill, WIREDEven to Clarke, though, many recordings can’t be pulled apart, especially if the instruments are close in frequency or a recording is particularly compressed, as on a radio broadcast or many audience-sourced live recordings. He once tried to demix a 1991 R.E.M. tape from London. “There’s just not enough from a spectral point of view, it’s so squashed,” Clarke says. “You get really fuzzy results.” For now, some blurry aspects of the past are going to stay blurry. But some are going to sound brighter than ever.Let us know what you think about this article. Submit a letter to the editor at mail@WIRED.com.More Great WIRED Stories📩 The latest on tech, science, and more: Get our newsletters!Sci-fi writer or prophet? The hyperreal life of Chen QiufanNFTs are hot. So is their effect on the Earth’s climateThese sea slugs decapitate themselves and grow new bodiesWhat do TV’s race fantasies actually want to say?So you want to prepare for doomsday🎮 WIRED Games: Get the latest tips, reviews, and more📱 Torn between the latest phones? Never fear—check out our iPhone buying guide and favorite Android phones",BreadcrumbList,https://www.wired.com/story/upmixing-audio-recordings-artificial-intelligence/,"{'@context': 'https://schema.org', '@type': 'Organization', 'name': 'WIRED', 'logo': {'@type': 'ImageObject', 'url': 'https://www.wired.com/verso/static/wired/assets/newsletter-signup-hub.jpg', 'width': '500px', 'height': '100px'}, 'url': 'https://www.wired.com'}",2021-03-23T07:00:00.000-04:00,2021-03-23T07:00:00.000-04:00,How Audio Pros ‘Upmix’ Vintage Tracks and Give Them New Life,"['https://media.wired.com/photos/6059219b7d3068f6d196eaa1/16:9/w_2400,h_1350,c_limit/jenny-sharaf-wired-final.jpg', 'https://media.wired.com/photos/6059219b7d3068f6d196eaa1/4:3/w_1800,h_1350,c_limit/jenny-sharaf-wired-final.jpg', 'https://media.wired.com/photos/6059219b7d3068f6d196eaa1/1:1/w_1349,h_1349,c_limit/jenny-sharaf-wired-final.jpg']","https://media.wired.com/photos/6059219b7d3068f6d196eaa1/1:1/w_1349,h_1349,c_limit/jenny-sharaf-wired-final.jpg","{'@type': 'WebPage', '@id': 'https://www.wired.com/story/upmixing-audio-recordings-artificial-intelligence/'}","[{'@type': 'Person', 'name': 'Jesse Jarnow', 'sameAs': 'https://www.wired.com/author/jesse-jarnow/'}]","The engineers shot him down. It turned into “several hours of the ins and outs of why it’s not possible,” Clarke remembers. You could perform a bit of sonic trickery to transform a song from one-channel mono to two-channel stereo, but that didn’t interest him. Clarke was seeking something more exacting: a way to pick apart a song so a listener could hear just one element at a time. Maybe just the guitar, maybe the drums, maybe the singer.
“I kept saying to them that if the human ear can do it, we can write software to do it as well,” he says. To him, this was a challenge. “I’m from New Zealand. We love proving people wrong.”
The challenge dropped him at the leading edge of a field known as upmixing, in which software and audio engineers work together to transform old recordings in ways that were once unthinkable. Using machine learning, engineers have made inroads into “demixing” the voices and instruments on recordings into completely separate component tracks, often known as stems. Isolating the components of songs is a surprisingly hard problem—more like unswirling paint than using a pair of scissors. But once engineers have stems, they can take the isolated tracks and “upmix” them into something new and perhaps improved. They might enhance a muffled drum track on an old recording, produce an a capella version of a song, or do the opposite and remove a song’s vocals so it can be used as background in a TV show or movie.
As an Abbey Road employee, it was only natural that Clarke would soon focus his experimentation on Beatles songs. But he wasn’t the only one trying to pull apart old music. Around the world, other audio aficionados were tackling the same challenge with their own favorite tracks—and converging on some of the same methods. In the years since Clarke’s fateful lunchtime chat, the number of apps and tools for splitting songs has exploded, as has the community of academics and enthusiasts that surround the practice. For creators of sample-based music, demixing is conceivably the greatest sonic invention since the digital sampler that fueled the explosion of hip hop four decades ago. For karaoke fans, it’s a game changer. For the people (or private equity firms) who own the rights to classic but inferior recordings—or enthusiasts willing to wade into legal gray areas—upmixing presents a whole new way to hear the past. After decades of slow advancement, deep learning has now sent both technologies into overdrive. The uncanny valley is alive with the sound of music.
New Old Sounds
Two decades ago, one of the first people to experiment with demixing was Christopher Kissel, a professional electronics test engineer from Long Island. Kissel didn’t have easy access to a recording studio. But he was a lifelong music fan, and he dreamed of making old tracks sound new.
Until the 1960s, almost all popular music was recorded and listened to monaurally—all the instrumental and vocal parts were recorded onto a single track of tape and played back through a single speaker. Once a song was on tape, it was basically finished. But Kissel had an inkling that it might be possible to update old mono recordings in a profound way.
In 2000 he purchased his first Mac for the express purpose of transforming single-track pop songs from the ’50s and ’60s into two-channel stereo versions fit for headphones or properly separated speakers. “Compared to mono, stereo sounds more lifelike and allows you to more viscerally hear and appreciate the interplay between the musicians,” he says. Most listeners probably prefer their music on two speakers (or headphones), and Kissel cared enough to try forcing the old recordings into stereo.
The first night with his new Mac, Kissel used floppy disks to install an early digital audio workstation called sonicWORX. It was the only software capable of running Pandora Realtime, a plug-in that could selectively boost the volume of vocals on recordings. “It was very advanced for its time,” Kissel says. He wanted to see if the tool could do something more interesting. He loaded up Miss Toni Fisher’s 1959 hit “The Big Hurt” and attempted to pull it apart.
Tinkering with the song using sonicWORX’s waveform visualizer and settings, Kissel says, he was ""able to separate the lead vocal, backing vocals, and strings and move them to the right side, and the rest of the backing instrumentation to the left.” It was crude and a little glitchy, but the effect was powerful. “It was quite thrilling to hear,” he says. Decades later, Kissel remains blown away by that first experience.
He experimented with more ’50s and ’60s classics, including the Del Vikings’ “Whispering Bells,” Johnny Otis’ “Willie and the Hand Jive,” and—perhaps most appropriately—the Tornados’ “Telstar,” a futuristic DIY wonder produced and composed by the influential sound engineer Joe Meek.
Kissel immersed himself in the developing fields of demixing and upmixing—though the names came later—by moderating forums and maintaining a website chronicling advances in the disciplines. He started playing around with a technique called spectral editing, which allowed people to treat sound as a visual object. Load a song into a spectral editor and you can see all of the recording’s many frequencies, represented as colorful peaks and valleys, laid out on a graph. At the time, audio engineers employed spectral editing to remove unwanted noise in a recording, but an intrepid user could also zone in on specific frequencies of an audio track and pluck them out. When a freeware spectral editing tool called Frequency popped up, Kissel decided to try it out.
He spent about 60 hours using Frequency to craft an upmix of the 1951 mono R&B hit “The Glory of Love,” by the vocal group the Five Keys, using the app to carefully target the separate vocalists and spread their voices across the stereo spectrum. With a final polish by disco legend Tom Moulton, Kissel’s friend, it became one of the first spectrally edited upmixes to get released on a commercial album, in 2005. Several labels soon began releasing collections of upmixed mono-to-stereo hits, sometimes licensed, sometimes in the public domain, and sometimes in between.
French software company Audionamix started building professional demixing software to help users pull apart tracks on their own, which made this suite of techniques more accessible. In 2007 the company unveiled a major achievement in upmixing, bringing vintage Édith Piaf recordings from mono to theater-ready surround sound for the biopic La Vie en Rose. In 2009, the company opened a Hollywood office to continue courting film, television, and commercial work.
Other times, their projects involved focused demixing. When the British online lending company Sunny wanted to use the song “Sunny” by late American R&B singer Bobby Hebb in a commercial, it found that one of the song’s original vocals interrupted the ad’s narration. With Audionamix’s help, the pesky vocals got zapped from existence. The French company also offers a service—originally called “music disassociation,” but now rebranded slightly less ominously as “music removal”—in which old television series and movies are scrubbed of music that might be too expensive to license, so they can be released in the latest format, be it DVD or streaming. According to Nicolas Cattaneo, a researcher at Audionamix, “This is the first thing that began to be really usable,” at least commercially. (Scholars studying music in old films and television shows should probably rely on releases from before 2009 or so if they want to make sure they’re hearing the original soundtracks.)
AudioSourceRE and Audionamix’s Xtrax Stems are among the first consumer-facing software options for automated demixing. Feed a song into Xtrax, for example, and the software spits out tracks for vocals, bass, drums, and “other,” that last term doing heavy lifting for the range of sounds heard in most music. Eventually, perhaps, a one-size-fits-all application will truly and instantly demix a recording in full; until then, it’s one track at a time, and it’s turning into an art form of its own.
What the Ear Can Hear
At Abbey Road, James Clarke began to chip away at his demixing project in earnest around 2010. In his research, he came across a paper written in the ’70s on a technique used to break video signals into component images, such as faces and backgrounds. The paper reminded him of his time as a master’s student in physics, working with spectrograms that show the changing frequencies of a signal over time.
Spectrograms could visualize signals, but the technique described in the paper—called non-negative matrix factorization—was a way of processing the information. If this new technique worked for video signals, it could work for audio signals too, Clarke thought. “I started looking at how instruments made up a spectrogram,” he says. “I could start to recognize, ‘That’s what a drum looks like, that looks like a vocal, that looks like a bass guitar.’” About a year later, he produced a piece of software that could do a convincing job of breaking apart audio by its frequencies. His first big breakthrough can be heard on the 2016 remaster of the Beatles’ Live at the Hollywood Bowl, the band’s sole official live album. The original LP, released in 1977, is hard to listen to because of the high-pitched shrieks of the crowd.
After unsuccessfully trying to reduce the noise of the crowd, Clarke finally had a “serendipity moment.” Rather than treating the howling fans as noise in the signal that needed to be scrubbed out, he decided to model the fans as another instrument in the mix. By identifying the crowd as its own individual voice, Clarke was able to tame the Beatlemaniacs, isolating them and moving them to the background. That, then, moved the four musicians to the sonic foreground.
Clarke became a go-to industry expert on upmixing. He helped rescue the 38-CD Grammy-nominated Woodstock–Back to the Garden: The Definitive 50th Anniversary Archive, which aimed to assemble every single performance from the 1969 mega-festival. (Disclosure: I contributed liner notes to the set.) At one point during some of the festival’s heaviest rain, sitar virtuoso Ravi Shankar took to the stage. The biggest problem with the recording of the performance wasn’t the rain, however, but that Shankar’s then-producer absconded with the multitrack tapes. After listening to them back in the studio, Shankar deemed them unusable and released a faked-in-the-studio At the Woodstock Festival LP instead, with not a note from Woodstock itself. The original festival multitracks disappeared long ago, leaving future reissue producers nothing but a damaged-sounding mono recording off the concert soundboard.
Using only this monaural recording, Clarke was able to separate the sitar master’s instrument from the rain, the sonic crud, and the tabla player sitting a few feet away. The result was “both completely authentic and accurate,” with bits of ambiance still in the mix, says the box set’s coproducer, Andy Zax.
“The possibilities upmixing gives us to reclaim the unreclaimable are really exciting,” Zax says. Some might see the technique as akin to colorizing classic black-and-white movies. “There’s always that tension. You want to be reconstructive, and you don’t really want to impose your will on it. So that's the challenge.”
Heading for the Deep End
Around the time Clarke finished working on the Beatles’ Hollywood Bowl project, he and other researchers were coming up against a wall. Their techniques could handle fairly simple patterns, but they couldn’t keep up with instruments with lots of vibrato—the subtle changes in pitch that characterize some instruments and the human voice. The engineers realized they needed a new approach. “That’s what led toward deep learning,” says Derry Fitzgerald, the founder and chief technology officer of AudioSourceRE, a music software company.
Fitzgerald was a lifelong Beach Boys fan; some of the mono-to-stereo upmixes he did of their work, for the fun of it, got tapped for official releases starting in 2012. Like Clarke, Fitzgerald had found his way to non-negative matrix factorization. And, like Clarke, he’d reached the limits of what he could with it. “It got to a point where the amount of hours I spent tweaking the code was very, very time-consuming,” he says. “I thought there had to be a better way.”
The nearly parallel move to AI by Fitzgerald, James Clarke, and others echoed Clarke’s original instinct that if the human ear can naturally separate the sounds of instruments from one another, it should also be possible to model that same separation by machine. “I started researching deep learning to get more of a neural network approach to it,” Clarke says.
He started experimenting with a specific goal in mind: pulling out George Harrison’s guitar from the early Beatles hit “She Loves You.” On the original recording, the instruments and vocals were all laid on a single track, which makes it nearly impossible to manipulate.
Clarke started building an algorithm and trained it on every version of the song he could find—radio sessions, live versions, even renditions by tribute bands. “There were quite a few different ones, so plenty of examples to understand how the track should sound,” Clarke says. Using spectrograms, he now also knew how the track should look. The algorithm broke up the audio into individual stems, one for each instrument, but Clarke only had eyes and ears for Harrison’s Gretsch Chet Atkins Country Gentleman guitar.
Over nine months, Clarke sifted through the guitar part a few seconds at a time, virtually hand-cleaning the track phrase by phrase. He listened for stray audio artifacts from other instruments and used spectral editing software to find and eliminate them. For the final step, he set out to recapture the track’s original ambience. That part was easy. As an Abbey Road employee, he could book time in the vaunted Studio Two, where “She Loves You” was originally recorded. He played his track into the room through the in-house speakers and recorded it anew, to capture some of the subtleties of the room’s well-preserved acoustics. In August 2018, Clarke showed off his AI demixing work publicly for the first time.
The occasion was a sold-out lecture series that offered a rare chance for fans to step inside Studio Two, where the Beatles, Pink Floyd, and plenty of others recorded. Visitors were invited to re-create the clattering E-major chord that ends “A Day in the Life” by playing the studio’s pianos at the same time. The audience also received a glimpse of the future.
In front of a packed audience, Clarke played the Beatles’ original 1963 recording of “She Loves You.” Then, to pin-drop silence, he played what should have been impossible: the same recording with everything removed except for Harrison’s guitar.
Three days later, excerpts of Clarke’s demo made their way onto the web. The truthers quickly descended. Disbelieving audiophiles started trashing Clarke in online forums. “I think it’s a shame that the demonstration to show how good this new technology is happens to be false,” a user who went by Beatlebug wrote.
“It's kind of sad that Abbey Road has to mislead people like that,” RingoStarr39 posted in the same thread.
Beatlebug, RingoStarr39, and others insisted that the audio segment in Clarke’s lecture was a more easy-to-isolate bit from a later German version of the song, “Sie Liebt Dich,” recorded in stereo. They insisted that James Clarke was a charlatan.
But Clarke had merely demonstrated a proof of concept. Perfecting Harrison’s guitar track of “She Loves You” took him approximately 200 hours. He hadn’t even attempted to isolate John Lennon’s guitar. “Not a viable option for projects,” he admits. It was far from automated. But it could be done. And it would be.
Up, Up, and Away
The dam broke fully when French streaming service Deezer released an open source code library called Spleeter that allowed both casual and professional programmers to build tools for demixing and upmixing. Anybody comfortable enough with their computer’s command line interface could download and install software from Github, select an audio file of their favorite song, and generate their own set of isolated stems. People started putting the code library to creative use. When tech blogger Andy Baio played around with it, he was delighted to discover how easy it now was to create mashups, such as when he crossed the Friends theme and Billy Joel’s “We Didn’t Start the Fire.” “Nobody should have this kind of power,” he tweeted.
The first generation of users are demixing and upmixing in creative ways. Some musicians are removing one instrument from a song to create tracks they can practice along to or to generate source material for new music. Podcast producers are cleaning up dialog recorded in noisy environments. Hobbyists are using iPad apps and free sites to create their own mixes or make any song karaoke-ready. Several streaming services in Japan now offer vocal removal in officially licensed form, including Spotify’s SingAlong, where listeners can turn down a song’s vocals, and Line Music, which promises real-time source separation.
Along with established players (Audionamix, James Clarke), the newest company offering professional demixing services is the California-based startup Audioshake.
The company will soon launch a service where music rights holders—both musicians and labels—can upload their tracks to the cloud and, within minutes, download high-quality stems ready for licensing in film, broadcasting, video games, and elsewhere. Audioshake claims best-in-field ratings for drums, bass, and vocals, according to benchmarks established by the Signal Separation Evaluation Campaign, an organization made up of audio researchers who track the progress of demixing techniques.
But Audioshake is also the first company to figure out how to automatically isolate guitars—or, more precisely, a single guitar. The company is tight-lipped about how it achieved this. “We refined the architecture of our deep-learning network to be specially tailored to the harmonics and timbre of the guitar,” says company AI researcher Fabian-Robert Stöter. Basically, when a user uploads a track to Audioshake, a layer in the company’s algorithm converts the song’s waveform into a numerical representation that makes it easier for the AI model to figure out where a guitar ends and everything else begins.
To see it work, I was invited to upload some songs. Within a few minutes, the company’s software was able to pull apart a track of a rock band playing in guitar-bass-drums-vocals power trio format. A track by Talking Heads’ original lineup came back with David Byrne’s 12-string acoustic guitar separated (with minimal artifacts) alongside tracks of Tina Weymouth’s bass and Chris Frantz’s drums. It works equally well on other songs in that exact guitar/bass/drums/vocals configuration. But music is huge, and the power trio format is a tight set of parameters.
Outside those parameters is the unclaimed frontier of demixing. The original recording of “She Loves You” comes back from Audioshake with Lennon's and Harrison’s guitars sounding like jangling ghosts. James Clarke’s manual work still can’t be matched by a machine. That said, Audioshake does what couldn’t be done only a few years ago, pointing to a future in which machines will recognize more instruments. They might be unbreachable frontiers. For virtually all producers since the ’60s, a recording studio has been the place to combine unusual instruments and generate wondrous new sounds (and literal overtones) explicitly designed to blend together in the listener’s ears.
But what if the artifacts turn out to be art? If a demixing attempt gone awry sounds cool to the right producer, it might become the basis for fantastic new music. Think Cher turning Auto-Tune into a pop trend with “Believe.” As archival producer Andy Zax put it, “Some 16-year-old making hip hop records on a PlayStation is going to figure out some genius use of this thing and create a sound world we've never heard before.”
For now, plenty of experimentation is happening in far-flung fan forums, with unofficial upmixes of many equally unofficial recordings. Some fans have been exploring a subgenre that might be called upfakes, fusing, say, George Harrison’s original 1968 demo for “Sour Milk Sea”' with the backing track from a more recent recording by another musician. (Fans are understandably jittery about copyright claims and generally only post their work with quickly expiring links.)
As for Clarke, he is still working on the exact AI methodology to pull apart a mono Beatles vocal track. He’s also started an independent company called Audio Research Group to work as demixer-for-hire. Lately he’s been helping to create a set of tracks for a band that lost all its master tapes and has only its LPs.
Even to Clarke, though, many recordings can’t be pulled apart, especially if the instruments are close in frequency or a recording is particularly compressed, as on a radio broadcast or many audience-sourced live recordings. He once tried to demix a 1991 R.E.M. tape from London. “There’s just not enough from a spectral point of view, it’s so squashed,” Clarke says. “You get really fuzzy results.” For now, some blurry aspects of the past are going to stay blurry. But some are going to sound brighter than ever.

Let us know what you think about this article. Submit a letter to the editor at mail@WIRED.com.

More Great WIRED Stories

📩 The latest on tech, science, and more: Get our newsletters!
Sci-fi writer or prophet? The hyperreal life of Chen Qiufan
NFTs are hot. So is their effect on the Earth’s climate
These sea slugs decapitate themselves and grow new bodies
What do TV’s race fantasies actually want to say?
So you want to prepare for doomsday
🎮 WIRED Games: Get the latest tips, reviews, and more
📱 Torn between the latest phones? Never fear—check out our iPhone buying guide and favorite Android phones",,the big story,"{'@type': 'CreativeWork', 'name': 'WIRED'}",True,"Experts are using AI to pick apart classic recordings from the 50s and 60s, isolate the instruments, and stitch them back together in crisp, bold ways.","[{'@type': 'ListItem', 'position': 1, 'name': 'The Big Story', 'item': 'https://www.wired.com/big-story/'}, {'@type': 'ListItem', 'position': 2, 'name': 'longreads', 'item': 'https://www.wired.com/tag/longreads/'}, {'@type': 'ListItem', 'position': 3, 'name': 'How Audio Pros ‘Upmix’ Vintage Tracks and Give Them New Life'}]"
https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vaGFpLnN0YW5mb3JkLmVkdS9uZXdzL2FydGlzdHMtaW50ZW50LWFpLXJlY29nbml6ZXMtZW1vdGlvbnMtdmlzdWFsLWFydNIBAA?oc=5,Artist's Intent: AI Recognizes Emotions in Visual Art - Stanford HAI,2021-03-22,Stanford HAI,https://hai.stanford.edu,"A team of AI researchers has trained its algorithms to see the emotional intent behind great works of art, possibly leading to computers that see much deeper than current technologies.",N/A,"A team of AI researchers has trained its algorithms to see the emotional intent behind great works of art, possibly leading to computers that see much deeper than current technologies.","A team of AI researchers has trained its algorithms to see the emotional intent behind great works of art, possibly leading to computers that see much deeper than current technologies.",,,N/A,N/A,"


 





            How AI and Art Hold Each Other Accountable
          


by
Beth Jensen


          August 26th, 2020
        


The arts have a major role to play in the fairness of our technological future.




",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vd3d3Mi5kZWxvaXR0ZS5jb20vY2gvZW4vcGFnZXMvdGVjaG5vbG9neS9hcnRpY2xlcy9haS10cmFpbi1pcy1tb3ZpbmctZmFzdC5odG1s0gEA?oc=5,The artificial intelligence (AI) train is moving fast - we have to start running now to catch it - Deloitte,2021-03-24,Deloitte,https://www2.deloitte.com,"Learn about the newest trends in artificial intelligence (AI) and Switzerland as an R&D hub for AI in our interview with Prof. Marco Zaffalon, IDSIA",N/A,"Learn about the newest trends in artificial intelligence (AI) and Switzerland as an R&D hub for AI in our interview with Prof. Marco Zaffalon, IDSIA","Learn about the newest trends in artificial intelligence (AI) and Switzerland as an R&D hub for AI in our interview with Prof. Marco Zaffalon, IDSIA",,,N/A,N/A,"
















                                Article
                            


The AI train is moving fast - we have to start running now to catch it





Prof. Marco Zaffalon, Scientific Director at IDSIA USI-SUPSI, talks about trends in artificial intelligence (AI) and Switzerland as an R&D hub for AI.






































 Let’s make this work.
To view this video, change your targeting/advertising cookie settings.



 

















Marco, you are Professor and Scientific Director at IDSIA, the Institute for Artificial Intelligence in Lugano. What is your main research focus at the institute?
IDSIA has a very broad range of research interests, spanning most of Artificial Intelligence as it is understood today: machine learning, including deep learning/neural networks, control and signal processing, natural language processing, robotics, computer vision, search and optimisation, and more fundamental questions in uncertainty, probability, statistics, causal inference.

To give an example, we have a 4-year Data project funded by the National Science Foundation as part of Switzerland’s National Research Programme 75 “Big Data”. In this project we deal with Gaussian processes, which can be understood as statistical neural networks, which can then provide uncertainty estimates relating to their own predictions – unlike traditional neural nets. This is very important in applications where we are evaluating risks. For example, a self-driving car needs to know whether the car’s sensors are reliably warning of a potential accident ahead rather than a a person safely crossing the street. Therefore Gaussian processes have definite advantages over traditional neural nets, but they also have one big disadvantage: they are far too slow to learn from big data. In this project we are designing powerful new algorithms for Gaussian processes that allow them to scale up to millions of data points far faster than was previously possible, opening up a huge number of potential applications for these tools. For instance, we are working at present with Meteosuisse, the Swiss meteorological body, on improving rain estimates in Switzerland by using Gaussian processes.














IDSIA is known worldwide for its Artificial Intelligence research. What different tech applications and innovations has IDSIA brought to the world? 
IDSIA is very well known for its research on optimisation algorithms – for example, in routing, supply chains and scheduling – inspired by the behaviour of ants: so-called ant-colony optimisation. This research eventually also inspired swarm robotics, where very simple robots create complex swarm behaviour through indirect communication between them, in the way ants do. Technically this process is called stigmergy.
Other than this, we have a very long history of applied projects developed with companies and numerous special innovations have been developed while working on these projects.
A pretty fashionable topic at the moment is LSTM (long short-term memory), a type of neural network invented in 1997 in a joint collaboration between IDSIA and the Technical University of Munich. This is probably the most used type of neural network today in the world, with all the big players using it, such as Apple, Google, Amazon, etc.














What are the most important recent trends in Artificial Intelligence and where do you see growth potential for AI applications?
Natural language processing has been growing a lot recently on the back of the incredible progress achieved by the use of deep learning. I am talking for instance of OpenAI's GPT3. We see numerous requests to work in this field, especially by companies in the banking and insurance sector, and in general where texts and regulations abound. There is also increasing use of AI in Industry 4.0 and the internet of things, which is growing rapidly in manufacturing industries.
More generally speaking, I think all industrial sectors and companies should be growing dramatically at this time because of AI. If that is not happening, there's a problem and we ought to understand better where that problem lies to remove possible roadblocks. In other words, my point is that AI is already ready to help and make businesses grow. It's our fault if we haven't seized the opportunity yet.














Know-how and technology transfer is an important part of your work. That sounds very attractive to companies. How do you collaborate with Swiss and international corporations?
Yes, know-how does excite companies. We usually collaborate with them through Innosuisse, the Swiss innovation agency, which funds joint work between research centres and private companies. This offers a great opportunity for companies, especially those that cannot afford to invest in innovation through applied research. These projects last typically for a year and a half and can greatly help companies to innovate.
But there are also many companies that prefer to give us a direct mandate, to speed up the work, for example. One way or the other we typically have about 20 applied projects in progress at all times. UBS, Mastercard , Novartis, Roche, Georg Fischer and Bystronic are among the companies with which we have worked or are working. 














Switzerland’s success as a business location has a lot to do with strong R&D capabilities and activities – in universities, research institutes and corporates. How would you evaluate Switzerland’s current position as a R&D knowledge hub compared to other countries? 
I can give you many objective reasons why Switzerland is very well positioned:

First and foremost, the government's annual investment in R&D as a percentage of GDP  tops the world ranking;
the quality of its polytechnics and the myriad high-profile research centres distributed all over the country;
the big private sector players, such as Roche, Novartis, UBS and ABB, invest massively in research – so that Switzerland's number of patent applications per inhabitant also tops the world ranking;
Switzerland's dense fabric of small and medium-sized enterprises that are best in class in their fields;
Switzerland's two-track education system, one more academic, the other more applied, which is regularly praised worldwide as an example of a good education system.

But most of all, for me, Switzerland owes its success to its liberal and pragmatic mentality. Bureaucracy is kept to a minimum, the State trusts its citizens, innovation is welcome and favoured, no matter where it originates. I, for example, came here from Italy as a fresh PhD graduate, knowing nobody and not yet well positioned in worldwide research. And yet I have been given freedom to do my research, and given lots of research funding. I have been able to form a large group of researchers, and eventually become scientific director of IDSIA as well as a Professor.













What would you say could be done to increase Switzerland’s attractiveness as an AI research hub? 
Sad to say, money definitely plays a role. Chief scientists in AI companies can earn several million dollars per year in the US. Postgraduate students, even without a PhD, can easily start work on a salary of $150,000. The average salary for researchers is around $350,000. Are these researchers worth that much? That’s debatable but if we want to have top AI scientists in Swiss companies we should consider the financial aspects, too, and not rely on the other benefits of being here.
We also need more aggressive, risk-taking investors. In fact, big investments are key to this process, not just public but also private investment. Look at the very big companies in AI: they are all either in the US (e.g., Amazon, Google, and Facebook) or in China (e.g., Baidu, Tencent, and Alibaba). There are none in Switzerland or Europe. My feeling is that we ought to be asking ourselves why.
My own answer to this question is that what's lacking is a bit of strategic policy-making (as well as vision) that encourages companies and centres to work together to promote strong business growth in this area. That would also involve developing some infrastructure, such as a well consolidated and aggressive investor chain; support and organisation from the Cantonal and Federal government; joint tables where government, academia and big players in the industry and also in the public sector, like the Federal railways, aviation, etc., can continuously discuss means to grow/invest/innovate together. And why not have a ministry for AI? They have had one up and running in the Emirates since 2017.














Can Switzerland be an R&D hub for AI?   
There’s a big opportunity. But the AI train is moving fast and is easy to miss, despite our past successes. We have to start running now to catch it. The answer is not just to appoint new AI professors in a polytechnic. It's much more than that. The country’s system has to embrace change overall. You might ask why I am saying all this about AI and not some other field of innovation. The answer is that AI, unlike other innovation sectors, will pervade every other field. Global AI business revenues in the current decade are estimated at around 13 trillion dollars. We're talking about our future wealth and well-being.











About Marco Zaffalon
Marco Zaffalon is Professor and Scientific Director at the Dalle Molle Institute for Artificial Intelligence (IDSIA USI-SUPSI) in Lugano, Switzerland. He leads a research group made of 30 full-time researchers on probabilistic machine learning. Prof. Zaffalon has published 150 research papers and has co-founded Artificialy, an innovative AI solutions company based in Lugano.
About IDSIA
The Dalle Molle Institute for Artificial Intelligence (IDSIA), based in Lugano, Canton of Ticino, was founded in 1988. It has gained international recognition for the invention and development of long short-term memory (LSTM) in the 1990s, an algorithm that is now used by Google, Facebook, and Apple for speech recognition. IDSIA is also where the key scientists and technologies of DeepMind emanate from, an AI company acquired by Google for 500 million US dollars just four years after its formation.










 




Emerging technologies in Switzerland video series

Watch the videos











Contacts








Michael Grampp 


Research Director & Chief Economist




					   mgrampp@deloitte.ch 





   +41 58 279 6817 











   Michael is Deloitte’s Chief Economist in Switzerland. He has been with Deloitte for over 15 years and leads the Swiss Research team which produces thought leadership publications and studies. Previous... More











Daniel Laude 


Manager




					   dlaude@deloitte.ch 





   +41 58 279 6435 











   Daniel is a member of Deloitte’s Insights Team in Zurich. He advises clients and leadership on economic, business and societal issues and gives evidence-based recommendations for action to corporation... More




















Contact us





Submit request for proposal








",,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibWh0dHBzOi8vd3d3Lm1hc3NhY2h1c2V0dHMuZWR1L3Jlc2VhcmNoL25leHQtZnJvbnRpZXJzLWFwcGxpZWQtc2NpZW5jZXMvbmV4dC1mcm9udGllcnMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2XSAQA?oc=5,Next: Frontiers in Artificial Intelligence | UMass System - University of Massachusetts,2021-03-25,University of Massachusetts,https://www.massachusetts.edu,"Advances in computer science, with their almost limitless applications, are enabling a societal transformation. Artificial intelligence, robotics, and data science extend our brain and body power in ways that promise broad impacts across all areas of applied science and human endeavor, from medicine to manufacturing to municipal services. These technologies have the potential for great human benefit—potential that is quickly becoming a reality.",N/A,N/A,N/A,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,
