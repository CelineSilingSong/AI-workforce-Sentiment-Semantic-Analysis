URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,url,image,author,publisher,headline,datePublished,dateModified,articleSection,name,isAccessibleForFree,itemListElement,article:section,article:summary,article text,mainEntityOfPage,dateCreated,jobTitle,mainEntity,identifier,creator,breadcrumb,articleBody,isPartOf,mentions,hasPart,address,diversityPolicy,email,legalName,leiCode,telephone,logo,brand,thumbnailUrl,alternativeHeadline,speakable,specialty,mainContentOFPage,inLanguage,lastReviewed,target,foundingDate,slogan,contactPoint,sameAs,founder,numberOfEmployees,potentialAction,copyrightYear,contentLocation,associatedMedia
https://news.google.com/rss/articles/CBMimwFodHRwczovL3d3dy5mb3JiZXMuY29tL3NpdGVzL2Jlcm5hcmRtYXJyLzIwMjEvMDEvMjkvdGhlLWFtYXppbmctd2F5cy13aWxkLW1lLXVzZXMtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtYW5kLWNpdGl6ZW4tc2NpZW50aXN0cy10by1oZWxwLXdpdGgtY29uc2VydmF0aW9uL9IBAA?oc=5,The Amazing Ways Wild Me Uses Artificial Intelligence And Citizen Scientists To Help With Conservation - Forbes,2021-01-29,Forbes,https://www.forbes.com,"Wildlife conservation is an enormous challenge. Wild Me, a non-profit organization, is leveraging the photos and videos of citizen scientists and the cloud, artificial intelligence, and computer vision to help track individual animals and to inform wildlife conservation efforts.","ai,artificial intelligence,wildlife,conservation,data sets,big data","Wildlife conservation is an enormous challenge. Wild Me, a non-profit organization, is leveraging the photos and videos of citizen scientists and the cloud, artificial intelligence, and computer vision to help track individual animals and to inform wildlife conservation efforts.","Wildlife conservation is an enormous challenge. Wild Me, a non-profit organization, is leveraging the photos and videos of citizen scientists and the cloud, artificial intelligence, and computer vision to help track individual animals and to inform wildlife conservation efforts.",http://schema.org,BreadcrumbList,https://www.forbes.com/sites/bernardmarr/2021/01/29/the-amazing-ways-wild-me-uses-artificial-intelligence-and-citizen-scientists-to-help-with-conservation/,"{'@type': 'ImageObject', 'url': 'https://imageio.forbes.com/specials-images/imageserve/60139ca26580dd95edd4bbc2/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'width': 542.79, 'height': 304.6}","{'@type': 'Person', 'name': 'Bernard Marr', 'url': 'https://www.forbes.com/sites/bernardmarr/', 'description': 'Bernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society. He has written over 20 best-selling and award-winning books and advises and coaches many of the world’s best-known organisations. He has a combined following of 4 million people across his social media channels and newsletters and was ranked by LinkedIn as one of the top 5 business influencers in the world. Follow Bernard on LinkedIn, X (Twitter) or YouTube. Join his newsletter, check out his website and books.', 'sameAs': ['https://www.linkedin.com/in/bernardmarr/', 'https://www.twitter.com/BernardMarr', 'https://bernardmarr.com/']}","{'@type': 'NewsMediaOrganization', 'name': 'Forbes', 'url': 'https://www.forbes.com/', 'ethicsPolicy': 'https://www.forbes.com/sites/forbesstaff/article/forbes-editorial-values-and-standards/', 'logo': 'https://imageio.forbes.com/i-forbesimg/media/amp/images/forbes-logo-dark.png?format=png&height=455&width=650&fit=bounds'}",The Amazing Ways Wild Me Uses Artificial Intelligence And Citizen Scientists To Help With Conservation,2021-01-29T00:28:40-05:00,2021-01-29T10:37:45-05:00,Enterprise Tech,The Amazing Ways Wild Me Uses Artificial Intelligence And Citizen Scientists To Help With Conservation,False,"[{'@type': 'ListItem', 'position': 1, 'name': 'Forbes Homepage', 'item': 'https://www.forbes.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Innovation', 'item': 'https://www.forbes.com/innovation/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Enterprise Tech', 'item': 'https://www.forbes.com/enterprise-tech/'}]",Enterprise Tech,N/A,"More From ForbesJul 15, 2024,09:02am EDTWhen Will Quantum Computers Affect Your Competitive Landscape?Jul 15, 2024,09:00am EDTAristotle’s Timeless Guide To Mastering AIOpsJul 15, 2024,03:25am EDTIEEE Travels In July (Japan, China And Greece)Jul 14, 2024,03:19am EDTMaking A More Accurate And Sustainable AI ModelJul 12, 2024,09:00am EDT‘Knowledge Is Power’: The Antipattern That Is Holding Your Team BackJul 12, 2024,07:00am EDTHow Generative AI Rattles the WorkplaceJul 11, 2024,12:49pm EDTReality Check: Generative AI’s Impact May Be Overstated, For NowEdit StoryForbesInnovationEnterprise TechThe Amazing Ways Wild Me Uses Artificial Intelligence And Citizen Scientists To Help With ConservationBernard MarrContributorOpinions expressed by Forbes Contributors are their own.FollowingFollowClick to save this article.You'll be asked to sign into your Forbes account.Got itJan 29, 2021,12:28am ESTUpdated Jan 29, 2021, 10:37am ESTThis article is more than 3 years old.Share to FacebookShare to TwitterShare to LinkedinDid you know that scientists have identified only 1.5 million species out of the 10 million estimated on Earth? And many of those species are vulnerable to extinction. Thanks to the efforts of the non-profit organization Wild Me, the gargantuan task of wildlife preservation is getting a much-needed assist from citizen scientists who photograph and video wildlife when traveling the world, plus high-tech solutions such as cloud computing, artificial intelligence, and machine vision.
The Amazing Ways Wild Me Uses Artificial Intelligence And Citizen Scientists To Help With ... [+] ConservationAdobe Stock

Creating Collaborative Data Sets

To make the progress on wildlife conservation that’s necessary, it’s going to take pulling data out of proprietary data sets and joining them into collaborative data sets. This is precisely what Wild Me and its Wildbook platform can do for the effort. The human effort it would take to sort through and classify images and videos of each animal is prohibitively time-consuming. With cloud computing and artificial intelligence, not only does the accuracy improve, but it reduces the time of identifying individual animals from hours when humans do it down to seconds when machines are on the job.

Once the specific animal is identified by the machine analyzing its unique markings, including stripes, spots, and other defining physical features such as scars, the location, time, and date of the image is recorded. Over time, the records for each animal continue to grow as more citizen scientists and researchers add to the image catalog. The data that's collected about each animal can be used by wildlife experts to track the health, migration, and other important facts about a species. Scientists can use this species info along with climate, geographic, behavioral, and environmental data to better understand the ecological and conservation issues they are facing.

PROMOTED
Wild Me started off by tracking whale sharks, a project that launched in 2003 after Jason Holmberg, director of engineering for Wild Me and chief information architect of Wildbook, went scuba diving and saw his first whale shark. After that first encounter, he joined a research expedition and learned how whale sharks were being tracked at the time by using plastic tags. Unfortunately, these plastic tags were rarely re-sighted, therefore making the ability to collect sufficient data on each individual animal nearly impossible.
Holmberg believed there was a more effective way to track animals by using computer vision. He set out to develop the algorithm and the platform to do just that. Wild Me and Wildbook are the result. 
MORE FROMFORBES ADVISORBest High-Yield Savings Accounts Of September 2023ByKevin PayneContributorBest 5% Interest Savings Accounts of September 2023ByCassidy HortonContributor
Today, Wild Me uses computer vision algorithms to identify whale sharks based on their unique markings from the photos taken around the world by tour operators, tourists, and researchers. They have tagged more than 8,100 whale sharks since the project began, thanks to the contributions of citizen scientists. The success of this database has prompted many other researchers to realize the potential of the citizen-scientist model for their conservation efforts, including projects focused on zebras, humpback whales, ragged-tooth sharks, polar bears, and more. Wild Me made its Wildbook platform open source to allow others to use this non-invasive tracking of species. 









DailyDozen
US


Forbes Daily: Join over 1 million Forbes Daily subscribers and get our best stories, exclusive reporting and essential analysis of the day’s news in your inbox every weekday.




                Sign Up
            


By signing up, you agree to receive this newsletter, other updates about Forbes and its affiliates’ offerings, our Terms of Service (including resolving disputes on an individual basis via arbitration), and you acknowledge our Privacy Statement. Forbes is protected by reCAPTCHA, and the Google Privacy Policy and Terms of Service apply.




You’re all set! Enjoy the Daily!


                More Newsletters
            


You’re all set! Enjoy the Daily!

                More Newsletters
            



To make the Wildbook platform better equipped to scale and handle many more endangered species, Wild Me is working with Microsoft and its AI for Earth program. Now that Wild Me and Microsoft are collaborating, they have grand ambitions that could include expanding the artificial intelligence tools to many more species as well as launching a tweet bot. Already an intelligent agent tags ""whale shark"" on YouTube videos nightly by using machine learning and natural language processing to read the video description to determine if the video's description includes info about the animal.


1/1





Skip Ad
 
Continue watchingafter the adVisit Advertiser websiteGO TO PAGE
By 2100, 38 percent of all species will be extinct without action and intervention. Insights from data and the processing power that comes with the cloud and artificial intelligence can be instrumental in understanding the issue and how extinction could be prevented. These efforts are most certainly time-sensitive, so any technology that can speed the efforts are necessary. The beauty of Wild Me and its efforts is the combination of citizen scientists participating to submit the data, images, and videos required for the machines to yield insights about wildlife conservation. The public also has the ability to follow their favorite animals and to contribute to the global conservation effort. 
There’s no doubt Wild Me’s innovations have revolutionized animal identification. It’s a spectacular example of what can result when humans and machines combine efforts for the greater good.
Follow me on Twitter or LinkedIn. Check out my website or some of my other work here. Bernard MarrFollowingFollowBernard Marr is a world-renowned futurist, board advisor and author of Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is... Read MoreEditorial StandardsPrintReprints & Permissions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMihgFodHRwczovL3d3dy5jb2UuaW50L2VuL3dlYi9hcnRpZmljaWFsLWludGVsbGlnZW5jZS8tL2ZhY2lhbC1yZWNvZ25pdGlvbi1zdHJpY3QtcmVndWxhdGlvbi1pcy1uZWVkZWQtdG8tcHJldmVudC1odW1hbi1yaWdodHMtdmlvbGF0aW9uc9IBAA?oc=5,Facial recognition: strict regulation is needed to prevent human rights violations - Council of Europe,2021-01-29,Council of Europe,https://www.coe.int,Latest news of the Council of Europe's work related to AI,"newsroom,ai,dg1 human rights and rule of law,central division dg1,public,compulsory,general, news, ai, council of europe",Latest news of the Council of Europe's work related to AI, Strasbourg 28 January 2021,https://schema.org,WebPage,,"{'@type': 'ImageObject', 'url': 'https://www.coe.int/documents/40452431/43026603/facial+recognition+DP.jpg/75af6869-31a9-b632-bf87-ebe2eff0e263', 'height': 489, 'width': 870}","{'@type': 'Organization', 'name': 'Council of Europe'}","{'@type': 'Organization', 'name': 'Council of Europe', 'logo': {'@type': 'ImageObject', 'url': 'https://static.coe.int/pics/logos/desktop/logo-coe-google-news.png', 'width': 78, 'height': 60}}",Facial recognition: strict regulation is needed to prevent human rights violations,2021-01-29T15:08:00+00:00,2024-05-17,,News,,,N/A,N/A,"

Strasbourg
28 January 2021


                Diminuer la taille du texte
            

                Augmenter la taille du texte
            

                Imprimer la page
            




©Shutterstock

The Council of Europe has called for strict rules to avoid the significant risks to privacy and data protection posed by the increasing use of facial recognition technologies. Furthermore, certain applications of facial recognition should be banned altogether to avoid discrimination.
In a new set of guidelines addressed to governments, legislators and businesses, the 47-state human rights organisation proposes that the use of facial recognition for the sole purpose of determining a person’s skin colour, religious or other belief, sex, racial or ethnic origin, age, health or social status should be prohibited.
This ban should also be applied to “affect recognition” technologies – which can identify emotions and be used to detect personality traits, inner feelings, mental health condition or workers´ level of engagement – since they pose important risks in fields such as employment, access to insurance and education.
“At is best, facial recognition can be convenient, helping us to navigate obstacles in our everyday lives. At its worst, it threatens our essential human rights, including privacy, equal treatment and non-discrimination, empowering state authorities and others to monitor and control important aspects of our lives – often without our knowledge or consent,” said Council of Europe Secretary General Marija Pejčinović Burić.
“But this can be stopped. These guidelines ensure the protection of people’s personal dignity, human rights and fundamental freedoms, including the security of their personal data.”
The guidelines state that a democratic debate is needed on the use of live facial recognition in public places and schools, in light of their intrusiveness, and possibly also on the need for a moratorium pending further analysis.
The use of covert live facial recognition technologies by law enforcement would only be acceptable if strictly necessary and proportionate to prevent imminent and substantial risks to public security that are documented in advance.
Private companies should not be allowed to use facial recognition in uncontrolled environments, such as shopping centres, for marketing or private security purposes.
The guidelines were developed by the Consultative Committee of the Council of Europe Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data, which brings together experts representing the 55 states parties to the Convention as well as 20 observer countries.
The Convention, the first ever binding international treaty addressing the need to protect personal data, was opened for signature in Strasbourg forty years ago today, on 28 January 1981.
 
More information:

Data Protection
Artificial Intelligence

","{'@type': 'WebPage', '@id': 'https://www.coe.int/en/web/artificial-intelligence/-/facial-recognition-strict-regulation-is-needed-to-prevent-human-rights-violations'}",2018-11-28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiOWh0dHBzOi8vd3d3Lm1ja2luc2V5LmNvbS9vdXItcGVvcGxlL2FsZXhhbmRlci1zdWtoYXJldnNredIBAA?oc=5,Alexander Sukharevsky - McKinsey,2021-01-30,McKinsey,https://www.mckinsey.com,"Global leader of QuantumBlack, AI by McKinsey, working across industries to redefine business models and improve performance through the responsible use of artificial intelligence and technology",N/A,"Global leader of QuantumBlack, AI by McKinsey, working across industries to redefine business models and improve performance through the responsible use of artificial intelligence and technology",N/A,https://schema.org,Person,,,,,,,,,Alexander Sukharevsky,,,N/A,N/A,N/A,,,"Senior Partner, London",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiMmh0dHBzOi8vd3d3Lm5hdHVyZS5jb20vYXJ0aWNsZXMvczQxNTk4LTAyMS04MjA5OC0z0gEA?oc=5,A multilayer multimodal detection and prediction model based on explainable artificial intelligence for Alzheimer's ... - Nature.com,2021-01-29,Nature.com,https://www.nature.com,"Alzheimer’s disease (AD) is the most common type of dementia. Its diagnosis and progression detection have been intensively studied. Nevertheless, research studies often have little effect on clinical practice mainly due to the following reasons: (1) Most studies depend mainly on a single modality, especially neuroimaging; (2) diagnosis and progression detection are usually studied separately as two independent problems; and (3) current studies concentrate mainly on optimizing the performance of complex machine learning models, while disregarding their explainability. As a result, physicians struggle to interpret these models, and feel it is hard to trust them. In this paper, we carefully develop an accurate and interpretable AD diagnosis and progression detection model. This model provides physicians with accurate decisions along with a set of explanations for every decision. Specifically, the model integrates 11 modalities of 1048 subjects from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) real-world dataset: 294 cognitively normal, 254 stable mild cognitive impairment (MCI), 232 progressive MCI, and 268 AD. It is actually a two-layer model with random forest (RF) as classifier algorithm. In the first layer, the model carries out a multi-class classification for the early diagnosis of AD patients. In the second layer, the model applies binary classification to detect possible MCI-to-AD progression within three years from a baseline diagnosis. The performance of the model is optimized with key markers selected from a large set of biological and clinical measures. Regarding explainability, we provide, for each layer, global and instance-based explanations of the RF classifier by using the SHapley Additive exPlanations (SHAP) feature attribution framework. In addition, we implement 22 explainers based on decision trees and fuzzy rule-based systems to provide complementary justifications for every RF decision in each layer. Furthermore, these explanations are represented in natural language form to help physicians understand the predictions. The designed model achieves a cross-validation accuracy of 93.95% and an F1-score of 93.94% in the first layer, while it achieves a cross-validation accuracy of 87.08% and an F1-Score of 87.09% in the second layer. The resulting system is not only accurate, but also trustworthy, accountable, and medically applicable, thanks to the provided explanations which are broadly consistent with each other and with the AD medical literature. The proposed system can help to enhance the clinical understanding of AD diagnosis and progression processes by providing detailed insights into the effect of different modalities on the disease risk.",N/A,N/A,Scientific Reports - A multilayer multimodal detection and prediction model based on explainable artificial intelligence for Alzheimer’s disease,https://schema.org,WebPage,,,,,,,,,,,,N/A,N/A,"




Download PDF








Article

Open access

Published: 29 January 2021

A multilayer multimodal detection and prediction model based on explainable artificial intelligence for Alzheimer’s disease
Shaker El-Sappagh1,2, Jose M. Alonso3, S. M. Riazul Islam4, Ahmad M. Sultan5 & …Kyung Sup Kwak6 Show authors

Scientific Reports
volume 11, Article number: 2660 (2021)
            Cite this article




22k Accesses


136 Citations


17 Altmetric


Metrics details






AbstractAlzheimer’s disease (AD) is the most common type of dementia. Its diagnosis and progression detection have been intensively studied. Nevertheless, research studies often have little effect on clinical practice mainly due to the following reasons: (1) Most studies depend mainly on a single modality, especially neuroimaging; (2) diagnosis and progression detection are usually studied separately as two independent problems; and (3) current studies concentrate mainly on optimizing the performance of complex machine learning models, while disregarding their explainability. As a result, physicians struggle to interpret these models, and feel it is hard to trust them. In this paper, we carefully develop an accurate and interpretable AD diagnosis and progression detection model. This model provides physicians with accurate decisions along with a set of explanations for every decision. Specifically, the model integrates 11 modalities of 1048 subjects from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) real-world dataset: 294 cognitively normal, 254 stable mild cognitive impairment (MCI), 232 progressive MCI, and 268 AD. It is actually a two-layer model with random forest (RF) as classifier algorithm. In the first layer, the model carries out a multi-class classification for the early diagnosis of AD patients. In the second layer, the model applies binary classification to detect possible MCI-to-AD progression within three years from a baseline diagnosis. The performance of the model is optimized with key markers selected from a large set of biological and clinical measures. Regarding explainability, we provide, for each layer, global and instance-based explanations of the RF classifier by using the SHapley Additive exPlanations (SHAP) feature attribution framework. In addition, we implement 22 explainers based on decision trees and fuzzy rule-based systems to provide complementary justifications for every RF decision in each layer. Furthermore, these explanations are represented in natural language form to help physicians understand the predictions. The designed model achieves a cross-validation accuracy of 93.95% and an F1-score of 93.94% in the first layer, while it achieves a cross-validation accuracy of 87.08% and an F1-Score of 87.09% in the second layer. The resulting system is not only accurate, but also trustworthy, accountable, and medically applicable, thanks to the provided explanations which are broadly consistent with each other and with the AD medical literature. The proposed system can help to enhance the clinical understanding of AD diagnosis and progression processes by providing detailed insights into the effect of different modalities on the disease risk.



Similar content being viewed by others






An explainable machine learning approach for Alzheimer’s disease classification
                                        


Article
Open access
01 February 2024









In-depth insights into Alzheimer’s disease by using explainable machine learning approach
                                        


Article
Open access
20 April 2022









Machine learning prediction of incidence of Alzheimer’s disease using large-scale administrative health data
                                        


Article
Open access
26 March 2020








IntroductionAlzheimer’s disease (AD) is a chronic neurodegenerative disease. This irreversible disorder is characterized by abnormal accumulation of amyloid plaques and neurofibrillary tangles in the brain, resulting in progressive decline in memory, thinking and language skills, along with behavioral changes. With increased human life expectancy, 11 million to 16 million elderly people are likely to suffer from AD by 20501. As far as we know, there is no effective recovery for this disease. However, early detection is of fundamental importance for timely treatment and progression delay2,3,4. Furthermore, prediction of the probable progression of the disease from mild cognitive impairment (MCI) to AD is of critical importance5,6. MCI is considered an intermediate stage between age-associated cognitive impairment and AD. For effective treatment, it is therefore essential to detect patients with MCI at high risk of progression to AD7. As a result, AD diagnosis and progression detection are multistage in nature. First, physicians determine the category of the patient (MCI or AD). Second, they deeply investigate patient biomarkers to determine progression status to AD from MCI. Most studies in the literature focus either on AD diagnosis1,8,9,10,11,12 or MCI progression, i.e., progressive MCI (pMCI) versus stable MCI (sMCI)13,14,15. Even if it is highly desirable to deal simultaneously with AD diagnosis and MCI progression, this task is extremely hard mainly due to the multimodality nature that also jeopardizes explainability.AD symptomatology is multimodal in nature4,16 correlated with cognitive scores, neuropathology vital signs, symptoms, demographics, medical history, neuropsychological battery, lab tests, etc. Complementary information exists among the modalities, which can be exploited to build powerful classifiers17. Therefore, medically intuitive AD detection methods should not rely only on measurements of a unique domain, such as physiological or behavioral symptoms. Alberdi et al.1 surveyed the AD diagnosis studies based on multimodal data. The combination of multimodalities facilitates the detection of subtle changes in all modalities from the very beginning, which results in reliable diagnoses. Once in the hands of an expert, it is still a challenge to correctly diagnose AD. Usually, medical experts are not able to manually analyze all of these vast and diverse biomarkers, and recognize the so-small behavioral shifts in AD patients until it is too late18. AD could be diagnosed after two years of memory problems19. There is an emerging need for advanced AD detection and prediction models that can serve as a helping hand for medical practitioners to diagnose or detect the disease earlier and more accurately5,19,20. These models can be used to build the inference engines of an AD clinical decision support system (CDSS). However, as far as we know there is no CDSS for AD diagnosis and progression detection ready to use at primary care. In this context, two lines of research have been conducted to address the previous challenges: (1) deep learning (DL) techniques which are able to automatically learn complex, non-linear data transformations that optimize performance metrics21,22,23; and (2) regular machine learning (ML) techniques, especially support vector machine (SVM) and random forest (RF)7,24,25,26,27,28.Unfortunately, all these previous studies focused mainly on improving the system performance while neglecting interpretability issues. Accordingly, although these studies achieved tremendous advances in prediction, they are not expected to be acceptable in the medical environment. There exists a significant gap between academic research outcomes and their effective utilization in medical practice due to several reasons20. The entire patient medical history must be considered to achieve intuitive, stable, and robust decisions20. Most DL-based methods only concentrate on analysis of neuroimaging, i.e., Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). Nevertheless, Oxtoby and Alexander29 asserted that neuroimaging is not sufficient for AD diagnosis and studying its progression. Furthermore, it is frequently the case that physicians do not rely on the latest technical approaches and methodologies (e.g., DL and RF), despite their high accuracy18, because complex model performance and explainability are in apparent conflict- i.e., the search for a good performance-explainability trade-off is required. Most of these approaches and schemes are inherently opaque, not understandable, and unable to easily answer the following straightforward questions. Why/how has it reached a specific decision, and why/how is it medically relevant30? The patterns learned from datasets by using complex ML algorithms do not necessarily carry correct and comprehensible knowledge. Thus, medical experts do not trust decisions provided by black-box models without comprehensive and easy-to-understand explanations31. For these reasons, the ML techniques employed in the clinical domain normally do not consider sophisticated models, resorting instead to simpler and interpretable (e.g., linear) models at the expense of accuracy32. Many studies have tried to open the black box of complex models and provide an explanation of their decisions, either by understanding how the models work or by explaining their decisions33. This new trend is called accountable, transparent, actionable, or explainable Artificial Intelligence (AI), or just XAI for short. Explainability is the ability of ML algorithms to (mathematically) explain or justify their results using terms which are understandable to humans.A CDSS should be based only on ML models that provide a balance between accuracy and explainability. These models are expected to provide sufficient information about the relationship between input features and predictions, and to allow users to answer questions like the following. Which features are the key players in the prediction of interest? Why am I deemed as normal/MCI/AD in the medical diagnosis? For these reasons, the second line of research introduced above (i.e., a CDSS based on regular ML techniques) seems more intuitive and medically acceptable. Regular ML techniques involving linear models and rule-induction algorithms (e.g., a decision tree [DT]34 or a fuzzy rule-based system [FRBS]35,36) are usually preferred when the priority is to generate explainable models18,37,38,39. Unfortunately, these models are not always accurate enough40. One solution is to use an accurate algorithm as an oracle for the classification purpose, and a collection of carefully designed interpretable models (which behave as digital twins of the oracle, i.e., they imitate the classification behavior of the oracle) as candidates to generate explanations of the output provided by the oracle41. The other solution is to open the black box and collect the explanations from the opaque model itself. For example, some studies have extracted interpretable rules from black-box models such as neural networks and SVMs42,43. In the case of RF, Brieman44 asserted that it is an A + predictor for performance, but rates an F on interpretability. More recently, some authors have shown how the behavior of RF can be interpreted to some degree31,45. There exists no study in the literature, which use the RF algorithm in the core of an explainable CDSS system for AD diagnoses and progression detection.Despite the current research effort, AD detection and progression prediction are still openly challenging problems, due to the limited accuracy and limited explainability of existing solutions. The medical domain requires both accurate and explainable AI models. In this paper, we therefore develop a new RF-based explainable AD detection and progression prediction model. Our contributions are as follows:

1.
We demonstrate how to retain interpretability, even when a complex ensemble model like RF is used. The objective of this approach is two-fold: (1) To illustrate the development and validation process of a two-layer computational framework for diagnosing AD patients and predicting pMCI within three years from baseline diagnosis; and (2) to describe how to provide detailed and multiple explanations for the ML decisions. The resulting model provides physicians with a good balance between accuracy and explainability.


2.
We build accurate ML ensemble classifiers based on RF for the two layers; utilizing multimodal AD datasets collected from the Alzheimer’s Disease Neuroimaging Initiative (ADNI). We employ a comprehensive list of modalities to diagnose AD and predict its progression, in agreement with a physician who was taken as domain expert.


3.
We build 22 explainers, based on a set of interpretable ML techniques (i.e. DT and FRBS), ready to explain to physicians the outcome of the two-layer framework. This reverse engineering method is called a black-box outcome explanation33. All explainers’ decisions compatible with RF decisions are used to provide physicians with a pool of plausible explanations. In analogy with a panel of experts who may have different experience and background, each explanation comes from a different explainer which pays attention to the most relevant features for different modalities, and comes along with information about the reliability of the explainer in terms of its accuracy. The consistency and coherence of such explanations are validated by domain experts and ranked according to their explainability-accuracy trade-off. Moreover, they are mapped to a human-friendly language for easy understanding.


4.
We provide physicians with some insights into driving factors of our prediction model from multiple points of view including natural language, visualization, and feature importance based on SHapley Additive exPlanations (SHAP).

The rest of this paper is organized as follows. Section 2 presents and discusses the main reported results. Section 3 introduces the datasets used and goes in depth with technical details of the proposed method. Section 4 concludes the paper.Results and discussionIdentification of informative AD featuresTo reduce computational complexity that comes with the high dimensionality nature of the ADNI, we selected the most relevant feature set using automatic feature selection strategy. For each layer, the full dataset is stratified and randomly divided into a model development set [\(S1\)] and a testing set [\(S2\)]. \(S1\) and \(S2\) are filtered to create the best feature sets \(MS1\) and \(MS2\), respectively (see the Feature Selection and Modeling Approach Section; in Material and Methods). The new sets are used to tune, train, and tests the utilized ML models. Training and tuning of ML models is done with cross-validation over MS1 while MS2 is reserved to provide readers with final test evaluation, mainly regarding some illustrative examples of the explainability of the proposed framework.Figure 1A shows the performance of different subset sizes assessed with RF-RFE (A.1), SVM-RFE (A.2), and GB-RFE (A.3) for the first layer. For different combinations of features, the accuracy from RF, SVM, and GB was measured, and the subset of features with the best performance was detained. As summarized in Table 1, for RF-RFE, we obtained a combination of 28 features [cognitive scores (8), genetics (5), lab tests (1), demographics (3), MRI (2), neuropsychological battery (6), and PET (3)] to attain the highest predictive accuracy of 94.4% (see Supplementary File [part 2], Table T1). Because the optimal subset of features derived using the RFE-RF approach yields the maximum accuracy, we utilized it for training the classification model. These features form about 15% of the whole feature set. Inspired by20, the features selected with RF-RFE are clustered into six modality kinds: (1) cognitive scores (CS) [eight features]; (2) neuropsychological battery (NB) [six features]; (3) MRI [two features], (4) PET [three features], (5) genetics [five features], and (6) medical history (MH) (lab test and demographics) [four features]. It is worth noting that the selected features based on RF-RFE are the most discriminant and informative features for the current classification problem (P < 0.05, Kruskal–Wallis test). The list of non-selected features does not add discriminative values with RFE; however, as asserted by our domain experts, many of these neglected features could provide additional knowledge to understand the made decisions (i.e., they include critical values for model’s explainability in accordance with physicians’ intuition and background). The different modalities were screened to investigate whether a cost-effective and non-invasive subset of features have a higher discriminative power than the whole dataset.Figure 1Selected features for both layers based on three different techniques of SVM, RF, and GB. The first row is for the first layer, and the second row is for the second layer.Full size imageTable 1 Performance of the RFE for the two layers.Full size tableFigure 1B shows the performance of different subset sizes assessed with RF-RFE (B.1), SVM-RFE (B.2), and GB-RFE (B.3) for the second layer. The accuracy of RF, SVM, and GB was calculated for different combinations of features, and the subset of features with the best performance was taken. Similar to the First Layer, the RFE algorithm attained a higher performance when combined with RF than GB and SVM. With RF-RFE (see Table 1), the combination of 36 features [cognitive scores (7), genetics (5), lab tests (6), demographics (1), MRI (5), neuropsychological battery (7), PET (3), and vital signs (2)] achieves the highest predictive accuracy at 86.8%. Accordingly, we used the RF-RFE feature set for training the classification model. These features formed about 19% of the total feature set, (see Supplementary File [part 2], Table T1). Furthermore, we grouped this list of features into five modality types: (1) cognitive and functional assessments (CFA) (CS and NB), (2) MRI, (3) PET, (4) genetics, and (5) MH (lab tests, age, and vital signs). As with the First Layer, we analyzed the performance of different RF classifiers constructed using each modality (as well as their combinations).First layer: early AD detection performanceThe First Layer in the framework is responsible for detecting AD patients from CN and MCI patients. To determine the smallest number of features that produces the most accurate results, we performed a set of experiments using different combination of modalities. Table 2 shows the performance obtained for the multiclass classification problem (i.e., CN, MCI, and AD) by using the whole training dataset and different combinations of six selected modalities (CS, NB, MRI, PET, MH, and genetics) and RF classifier (see the Random Forest for Classification Section). The models’ performance has been evaluated using the area under the receiver operating characteristic curve (AUC), precision, recall, accuracy (AC), and F1-score (F1) metrics (see the Model Performance Evaluation Metrics; in Material and methods). When the whole feature set is used, the model has a multiclass classification accuracy (MCA) of 93.42 ± 2.73% based on tenfold CV. We can see that the CS modality has the highest accuracy (MCA = 92.00 ± 2.26%), compared to other single modalities. As a result, the CS modality was combined with other modalities to test the improvement in the model performance. Please note that, although adding more features could increase the model’s confidence, it also adds additional noises. The two-modality combination CS + NB improved the CS accuracy by about 1%, i.e., MCA = 93.00 ± 2.61%. We notice that the standard deviation of the combined CS + NB data slightly increased compared to the CS dataset alone, but still it is less than the standard deviation of the models based on the whole dataset. After integration of the CS + NB modality with the other types of data, the genetics data improved the accuracy of the system to 93.95 ± 2.30%. We discover that the RF shows more confidence based on the CS + NB + Genetics dataset than CS + NB dataset. This is because its performance has lower standard deviation. The resulting modality of CS + NB + Genetics was tested by combining it with MRI, PET, and MH. However, the performance was not enhanced, and the models become noisier. As a result, the combination of CS, NB, and Genetics was selected as the one producing the best performance.Table 2 Random Forest performance validation for detecting AD patients based on tenfold cross-validation.Full size tableThe next step is to show the generalization capability of the proposed model. As shown in Table 3, we observe the same trend already shown in Table 2. Once again, the combination of NB, CS, and Genetics again achieved the best performance.Table 3 Random Forest performance testing for detecting AD patients (\(MS2\) test dataset;10% of the original data).Full size tableSecond layer: AD progression prediction performanceThe Second Layer in our framework optimizes a binary classification problem to predict the progression to AD within three years from baseline (i.e., sMCI versus pMCI). This classifier is first validated using tenfold CV on the \(MS1\) dataset. As shown in Table 4 with bold typeface, the best performance of this model was observed for the combined CFA, PET, Genetics, and MRI data, i.e. Precision = 88.07 ± 0.70%, Recall = 86.08 ± 1.30%, Accuracy = 87.09 ± 0.80%, F1-score = 87.08 ± 0.90%, and AUC = 87.08 ± 0.80%. In addition, this model achieved the lowest variance in performance compared to all models based on other combinations and the whole feature space. Regarding single modalities, CFA achieves the best performance (see Table 4). In addition, cognitive scores and neuropsychological battery are usually considered in clinical practice. Models built using either MH or PET alone achieved the worst performance and were noisy. Based on the results from single modalities, we combined the best CFA model with each of the other modalities to see if the performance may be improved or not.Table 4 Random Forest performance validation for predicting whether MCI subjects will progress to AD or not (tenfold cross-validation; Second Layer).Full size tableThe addition of PET data improves the predictive performance of our model because PET data provide complementary information about disease progression. The combination of CFA and PET modalities achieves the best performance compared to combinations of other pairs of modalities. However, the resulting model is less confident compared to the model based on CFA alone. This is probably because the PET modality added noise to the combined set. In addition, the CFA + PET modality achieved the smallest variance compared to other two modalities combinations. To check for possible improvement in model performance, the CFA + PET feature set was combined with each of the MRI, Genetics, and MH modalities. The multimodality of CFA, PET, and Genetics enhances the performance of progression prediction by about 2%, compared to the combined CFA and PET modality. In addition, the resulting model is more stable compared to the CFA + PET-based model. This is in accordance with the fact that medically, Amyloid β, PTAU, and TAU are critical biomarkers to monitor the progression of AD46,47,48,49,50,51. Finally, we check the effect of combining MRI and MH with the rest of the modalities (CFA, PET, and Genetics). Again, integrating MRI brain volume features (including the hippocampus, ICV, and others) improves the model accuracy by about 1%. MRI volume features provide vital information for effective prediction of AD progression. According to our domain experts, we believe this is medically promising because it is critical to integrate MRI features in order to measure possible AD progression. With the unseen data in \(MS2\) we verify the good generalization of the generated models that we already observed with tenfold CV (see Table 5).Table 5 Random Forest performance measures for AD progression prediction of MCI subjects based on CFA, MRI, PET, genetics, and MH modalities (\(MS2\) test dataset; Second Layer).Full size tableComparison with other classifiersRecently, Travers et al.21 provided a comprehensive survey of DL techniques in biology and medicine. In this context, Choi and Jin22 utilized a convolutional neural network (CNN) to detect pMCI cases based on positron emission tomography (PET) images. Spasov et al.23 proposed a multimodal DL classification model for AD progression detection based on the late fusion of magnetic resonance imaging (MRI), demographic, neuropsychological, and apolipoprotein E (APOE) e4 genetic data.In addition, many AD studies have considered a single modality, especially MRI, to make a binary classification of sMCI versus pMCI52,53. Li et al.54 used five cognitive scores with a Cox linear regression model to build two prognostic models of AD. Moradi et al.27 achieved an area under the curve (AUC) of 0.77 in discriminating pMCI from sMCI based on RF and MRI data only; after fusing MRI features with baseline cognitive scores and age, they achieved an AUC of 0.90 for the same problem. Jin et al.55 used a Bayesian network to analyze multimodal data from ADNI data including demographics, MRI, PET, neuropsychometrics tests, and genotypes. It is worth noting that RF56,57 is an ensemble classifier that can provide more accurate predictions than other ML techniques. Fernandez-Delgado et al.40 evaluated 179 classifiers using different UCI datasets, and concluded that RF outperforms other classifiers, including SVMs and neural networks. RF works well with a mixture of quantitative and categorical features, and unlike SVM, it handles multiclass problems natively. RF is able to learn wide datasets with a very large number of features, compared to the number of cases. RF has been used intensively in the AD domain26,57,58. For example, Ramírez et al.58 proposed an ML model to predict MCI from normal patients. This model is based on feature standardization, analysis of variance feature selection, partial least squares feature dimension reduction, and an ensemble of one vs. rest RF classifiers. The model achieved accuracy of 56.25% based on MRI data.To verify the goodness and robustness of our approach in each layer, we compared the performance of the RF models with other predictive models, namely the SVM, KNN, Naïve Bayes (NB), and DT models. For each layer, we use the selected features of RFE. For each selected algorithm, we tuned its hyperparameters the same way we tuned the RF algorithm. The results of the best performing parameters are shown in Tables 6 and 7. Our proposal outperforms the rest of classifiers. It is worth noting that we did not compare our model with the artificial neural network approach because they achieved really bad performance in preliminary experiments, mainly due to the small size of the used datasets. In other words, our data is not big enough for training and testing the state-of-the-art DL architectures.Table 6 Comparison of different classifiers (\(MS2\) test dataset; First Layer).Full size tableTable 7 Comparison of different classifiers (\(MS2\) test dataset; Second Layer).Full size tableModels explainabilityExplainability based on random forest internal logicBased on SHAP explainers, we calculate feature contributions of RF models (see the Explainability Capabilities Section; in Material and methods). Figure S1 in Supplementary File (part 2) shows this rank for each class in each layer. The most influential feature for the First Layer is CDRSB followed by MMSE, and the lowest feature is TRABSCOR_PartBTimeToComplete from the neuropsychological battery group (see Supplementary File [part 2], Table T2). For the Second Layer, FAQ plays the main role followed by ADNI_MEM, and Trail4Total has the lowest impact (see Supplementary File [part 2], Table T2). According to our domain experts, it is medically intuitive for cognitive scores to play the main role in detecting AD patients. However, for progression detection, we can see that Hippocampus and MidTerp volumes from MRI images also play significant role, in addition to FDG and SROI from PET images. Table 8 summarizes the sensitivity of the explainer to the different feature values for both layers. For further details about these features and terminologies, readers are invited to see the Supplementary File (part 2) and ADNI at http://adni.loni.usc.edu.Table 8 Examples of the relationship between features and class prediction.Full size tableExplainability of the behavior of individual featuresThe global feature importance gives an abstract view about the role of each feature, but we cannot know the direction of these effects. For example, we cannot know if a high value for CDRSB will increase the probability of selecting the AD, MCI, or CN class. Using SHAP summary plots, we are able to analyze the behavior of our XAI framework with respect to different values of features. Figure 2 shows the summary plots for every class in the first layer. Each dot represents the impact on a particular class of a particular feature for a given instance, and it is colored according to what magnitude of the value contributes to the model impact. The color represents the feature value (red = high, blue = low). We notice a different order for each class.Figure 2SHAP summary plots for the first layer. The upper left figure represents the CN class, the upper right figure represents the MCI class, and the second row represents the AD class.Full size imageIn the First Layer, we find that MMSE is more significant than CDRSB for the AD class, but CDRSB has the highest impact on the CN and MCI classes, see Table 8. The model shows a high degree of non-linearity because the impacts of many features are spread across relatively wide ranges. We notice that the high values of CDRSB have great positive impact on the model for predicting the AD class, meaning CDRSB is a factor that increases AD risk. For the CN class, low values of CDRSB have extreme positive impact on the model. In contrast, high values of ADNI_MEM, DigitalTotalScore, and MOCA have a positive impact on predicting the CN class. For the MCI class, low values of CDRSB have an extreme negative impact on the model. The CDGLOBAL feature is less critical than MOCA for the MCI class. However, in some cases, high values of this feature have a more negative impact on MCI cases than MOCA. The same happens for FAQ, where low values have a more positive impact on a system decision for the MCI class than MOCA and CDGLOBAL. We noticed that AD and MCI classes are related to negative values of ADNI_MEM, but CN is related to positive values. In addition, by plotting the impact of a feature on every sample, we can detect the impact of outliers. For example, in the case of the picture related to AD, although CDGLOBAL is not the most important feature globally, it is critical for a subset of patients. This is indicated by the long-tailed distribution to the right. Again, the same situation applies to the DigitalTotalScore feature for the AD class.In the Second Layer, although HCI is globally less significant than ADNI_MEM for both sMCI and pMCI, in a subset of patients, this feature has more impact than ADNI_MEM, see Table 8 and Fig. 3. The same is true for CDRSB in relation to MOCA for the pMCI class, and ADAS 11 in relation to CDRSB for the sMCI class. A feature with a longer tail to the right means it has a greater positive influence, and vice versa. As a result, understanding the detailed role of each feature alone and in combination with other features is of critical importance. For example, large values of RAVLT_immediate positively impact the model toward selecting the sMCI class, but negatively impact towards the pMCI class. FAQ is the most important feature for both classes, followed by ADNI_MEM, HCI, and ADAS 13. The two classes show symmetric behaviors for all features. It is clear that low values of FAQ negatively affect the prediction of pMCI class, but they have the largest positive impact for the sMCI class. Large values of ADAS 13 have a higher positive impact on the model for predicting the pMCI than ADNI_MEM. Small values of FDG have a greater positive impact for predicting pMCI than MOCA and ADAS 11. As a result, some features are not critical globally, but extreme values for specific cases have a greater impact in the model than the globally important features. Based on the knowledge of our domain experts, this is also medically intuitive, and increases the confidence of medical experts in the behavior of our system.Figure 3SHAP summary plots for the second layer. The left figure shows the pMCI class, and the right figure shows the sMCI class.Full size imageExplainability of individual casesFigure 4 shows examples of prediction for each class in the First Layer, and Figure S2 in Supplementary file (part 2) shows another example from the Second Layer. In addition, the figure illustrates supervised clustering of all cases according to their similarities.Figure 4First layer example predictions for AD (A), CN (B), and MCI (C) and SHAP supervised clustering in model behavior for all cases in each class. Red indicates attributions that push the score higher, while blue indicates contributions that push the score lower. A few of the noticeable subgroups are annotated with the features that define them.Full size imageEach example is a vertical line, and SHAP values for all cases are ordered by similarity. We identify some critical values for each cluster. Figure 4 (A) (part 1) shows a case with a probability of 75% for being AD. It also shows the most significant feature values that have a positive impact for that class, such as MMSE = 24, CDRSB = 3.0, MOCA = 19.3, etc. This is consistent with ADNI data, where the average values of all AD subjects are MMSE = 23.235 ± 2.015, CDRSB = 4.3 ± 1.591, and MOCA = 17.553 ± 3.377. In addition, it shows the features that push the classification away from the AD class including DigitalTotalScore = 33, CDGLOBAL = 0, etc. The features with less impact such as TAU = 347.9, PTAU = 31.64, RAVLT immediate = 27, FAQ = 5, and Trial5Total = 7 are represented with short arrows. Figure 4 (A) (part 2) shows the behavior of the model on all the instances, and the role of each feature to support (red) or not support (blue) classification as AD. Different clusters are defined according to the values of critical features. We find that when MMSE is in the interval [27, 30] and MOCA is in [23.62, 29], this combination has the greatest role in preventing the model from selecting the AD class (blue cluster). On the other hand, when CDRSB is in the range [3.5, 6.0], FAQ is in [6–17], and ADAS 13 is in [20–36], the model will mostly classify cases as AD (red cluster).Figure 4 (B) (part 1) does the same thing for the CN class. The model is 99% confident that the case is CN. Clearly defined clusters explain the model behavior in selecting the CN class. The most critical factors that push the decision towards CN class are CDRSB = 0 and DigitalTotalScore = 50. Figure 4 (B) (part 2) shows the overall logic in detecting CN subjects. We observe some critical values of some clusters from this figure. For example, if CDRSB = 0, FAQ = 0, DigitalTotalScore is in [31, 50], MMSE is in [27, 30], and ADAS 13 is in [1, 15], the patient is mostly classified as CN (red cluster). This means that if MMSE is combined with both CDRSB and FAQ at 0, it loses a lot of its impact on AD class prediction. According to the ADNI data and our experts’ knowledge, these decisions are medically intuitive because the average values of critical factors for CN subjects are CDRSB = 0.039 ± 0.141, FAQ = 0.194 ± 0.720, and DigitalTotalScore = 48.173 ± 7.481. Figure 4 (C) (part 1) shows the prediction of our model for an MCI case. In general, the characteristics of MCI cases are between those of CN and AD classifications. According to the model prediction, the low value of CDRSB (1 in this current case) has a high positive impact on predicting MCI cases. This subject has a negative ADNI_MEM value, which may have a significant impact on the system’s decision. By comparing the feature values of the three cases, we can say that a little change in CDRSB has a great impact on performance, and this is compatible with Fig. 2. Please note that the combination of different values of features could change the role of the related feature, as well as the final decision.Explainability of the interaction between featuresAs shown in the middle of Figs. 3 and 4, many features (such as PTAU for AD class and ABETA for the CN and MCI classes), show a high degree of uncertainty. In addition, some features (such as Entorhinal and PTAU) seem to have less impact, because they are at the bottom of the list. However, these features may have a critical impact if they were combined with specific values of other features. To study the role of these types of features, we need to zoom in and study their behavior in combination with other features. Note that interaction analysis can be studied for other globally important features, as well, like CDRSB and FAQ.Due to space restrictions, in Figure S3 of Supplementary File (part 2), we give a detailed example of the interaction impacts from one of these noisy features (e.g. PTAU) in the First Layer, and we study the impact of less globally critical feature (e.g., Entorhinal) in the Second Layer to highlight its role (see Fig. 2). As can be seen, the domain expert is able to interpret the internal behavior of the ML model and know exactly why it makes specific decisions. We notice that some features may be globally unimportant, but in some cases, they have extreme SHAP values, and that shows the real impact of these features. In addition, the real impact of a feature can be discovered by studying its interactions with other related features. Supplementary File (part 2) (Figure S4 to Figure S8) shows the SHAP interaction summaries for the most important features in both layers and for each class.Explainability based on single explainersIn this section, we provide explanations of the RF model decisions from other explainers and based on other data types. Domain experts often consider these biomarkers to make accountable decisions. For example, the First Layer’s model does not consider MRI and PET data. Furthermore, the Second Layer’s model does not consider medical history. In addition, both models do not consider lab tests, vital signs, and physical examinations. However, all these features are considered by our explainers. It is worth noting that we are not interested in explaining the internal behavior of the RF model but providing physicians with post-hoc explanations of every decision. In the same way, how different physicians may figure out different explanations (in terms of different features) for a given output, our explainers yield complementary, consistent and reliable explanations.Tables 9 and 10 summarize the quality (i.e. the performance-explainability trade-off) of the 22 explainers (11 DT and 11 FURIA) for each layer. Even if some of these explainers exhibit poor performance, they all exhibit complementary explainability because they depend on different features. In practice, these explainers provide physicians with plausible explanations in natural language. It is worth noting that given a specific data instance, only those explainers that point out at the same output class as the RF model are taken into account when generating explanations. Moreover, physicians are provided with explanations along with information about the reliability of each single explainer in terms of its balance between accuracy and explainability. At the end, the physician makes the final decision on which explainers to trust or to discard likewise she may ask for alternative opinions of different colleagues who are likely to have different experience and background. As expected, DT is clever for some modalities, while FURIA is better for others.Table 9 The performance of the explainers on different modalities (First Layer).Full size tableTable 10 The performance of the explainers on different modalities (Second Layer).Full size tableWe analyzed each instance in the test dataset of both layers and recorded how many explainers could predict the same class as their corresponding oracle (i.e. the RF model). The test set in the First Layer was made up of 105 instances. On average, 58.1% of the instances were managed by each single explainer. Regarding the number of explainers that act for each single instance, we found there were 13 (the median value) explainers considered; being 3 the worst case and 22 the best case. Being DT (vital signs-based) the least used explainer (34.4%) and FURIA (cognitive scores based) the most used explainer (92.4%). The test set in the Second Layer was made up of 49 instances. On average, 63% of the instances were managed by each single explainer. Being DT (neurological exams-based) the least used explainer (47%) and both DT and FURIA (cognitive scores-based) the most used explainers (78%). Regarding the number of explainers which act for each single instance, we observed that 14 (the median value) explainers are considered; being 7 the worst case and 20 the best case. All in all, we can conclude that even in the worst cases, we are ready to supply physicians with more than one single explanation. Moreover, explanations are normally rich, thanks to the fact that they involve several modalities. This fact was especially well appreciated by the physicians who collaborated in our study.Case studies for FURIA and DT explainersThe supplementary file (part 3) lists a group of AD cases to tests the system explainability. The supplementary file (part 2) shows the expressiveness of the generated explanations for three illustrative case studies (see Supplementary File [part 2], Table T3 to T7). We tested the following: (1) the ability of explainers to generate supplementary explanations, (2) their consistency with the generated explanations from SHAP, and (3) the quality of the generated natural language explanations. In case study 1, we can see that generated explanations add many values to the interpretability and confidence of the decisions made. First, the explainers reinforce the explanations from SHAP. Second, they increase the confidence physicians have about the decision made. In case study 2, physicians can investigate all the information to understand why the system makes a specific decision. We note perfect matches among SHAP and explainers’ outputs. In case study 3, we observe how explanations related to sMCI and pMCI are somehow in contrast (and in accordance with) physicians’ intuition and background.Model strengths and limitationsThe proposed model is designed to comprehensively integrate high-fidelity Alzheimer’s data to predict AD and detect its possible progression within three years from baseline. We demonstrated the high predictive powers of the proposed models. The First Layer model achieves the best results by combining the NB, CS, and Genetics modalities. These modalities achieved the best cross-validations results. On the other hand, the Second Layer model shows the highest results based on CS, NB, PET, MRI, and Genetics. Both CS and NB have important roles in improving the performance of our model. Similar observations have been reported in the literature20. Note that not all biomarkers of these modalities are used in the training process, but only the features selected by the RFE technique. Using black-box models in the medical domain is very dangerous and not acceptable. Our model achieves superior performance, compared to other ML models; in addition, it combines high-accuracy, complex models (i.e. ensemble RF) with interpretable explanations. This combination allows physicians to receive the best possible predictions, and at the same time, gain insight into why those predictions were made. These actionable decisions increase the confidence and trust in the model’s behavior, help to debug the model, and can work as an educational tool for inexperienced physicians. Note that we used the word “confidence” to indicate that the model provided its results with small variances. In contrast, we used the word “trust” to indicate that our model provided interpretable and explainable results which improved the domain expert’s trust in the model’s decision. Moreover, when the model provided a result with high confidence, it then enhanced the domain expert’s level of trust. Consequently, in our study, more confidence resulted in more trust, in addition to the trust gained from explainability. The quantification of trust for deep learning models has been discussed recently59. Taking this quantification process into account would be an insightful investigation.Training general practitioners, based on educational interventions, to recognize and manage AD has no significant impact on clinical practice60. A CDSS can provide another solution, but current systems are mostly based on a single modality52,53, make use of binary models (e.g., CADi2)61, or are not explainable8,9,10,11,12,13,14,15. As a result, current systems are rarely used routinely in AD management. We believe that a CDSS based on our comprehensive, accurate, and explainable model could make a difference in practice. We provide explanations from different perspectives including CS, NB, MRI, PET, Genetics, medical history, etc. In addition, we provide detailed explanations based on feature contributions. We believe that these explanations provide supplementary knowledge for physicians to fully understand the rationale behind the decisions taken. To the best of our knowledge, this is the first study that provides such a comprehensive model and with such explainability features.Our model has a couple of limitations worth noting. First, we only considered the baseline data for making decisions. Because AD is a chronic disease, a time-series data analysis would be of critical value62,63. A future attempt will study the role of longitudinal data to enhance the model’s accuracy and explainability. We could consider some DL techniques, which are clever at handling time-series data, such as long short-term memory, in such a future study. Second, the ADNI collects data about the roles of a patient’s medication history and comorbidities on AD progression. No such research has been done previously to study these data. Another future enhancement could be the integration into the prediction ML model of semantic intelligence from ontologies. We will consider semantics from the standard ontologies (e.g. RxNorm, Systematized Nomenclature of Medicine-Clinical Terms [SNOMED CT], etc.) to encode these data and to infer hidden knowledge about the relationships between drugs, diseases, and Alzheimer’s. Third, the network science approaches have been used to characterizing the brain activities for AD patients to extract interconnectivity patterns of brain regions based on neuroimaging techniques64,65,66,67. Although these studies provided additional insights into AD pathophysiology, they come with several limitations. For example, Chen et al.,64 used a small cohort of 55 subjects for classifying subjects as AD vs. MCI vs. AD using the large-scale network analysis approach. These data have been collected at baseline visit only, and no longitudinal study has been performed. However, cross-sectional studies cannot dynamically observe changes in network patterns with disease progression. Furthermore, postmortem studies are required as the reference standard when validating the large-scale network methods. In addition, the study used simple linear regression to measure the relationship between changes in network connectivity strengths and behavioral scores. Wang et al.65 utilized a small dataset of 89 subjects to evaluate the impaired network functional connectivity with AD progression. Even though the whole brain network is complex, varied, and interrelated, this study was based on only five networks which put limitations placed on its results. Thus, the entire brain network analysis with finely defined regions is important. Also, this study is based on baseline data only. Besides, longitudinal data of multiple modalities such as functional and structural MRI, PET, genetic genotype, etc. should be fused to follow individuals to differentiate all the severity levels. In future studies, one might explore these network science approaches and integrate them with advanced XAI and deep learning techniques. In this context, we can study the roles of time series data to improve the current literature. Moreover, the role of data fusion of different modalities might be explored using different ML and DL algorithms. Finally, a web based CDSS system based on a user-friendly interface can provide medically intuitive aids for both medical experts and general practitioners. Work is currently in progress to develop such a system, which will be extended to work as a pluggable component of the electronic health record ecosystem. This design facilitates data entry by the physician, online training of the models, and automatic updates on patient status.Material and methodsADNI studyData used in this work was collected from the ADNI database (adni.loni.usc.edu). Subjects have been enrolled from over 57 sites across the U.S. and Canada. The study was conducted according to the Good Clinical Practice guidelines, the Declaration of Helsinki, US 21 CFR part 50 —Protection of Human Subjects—and part 56—Institutional Review Boards. Subjects were willing and able to undergo test procedures, including neuroimaging and follow-up, and written informed consent was obtained from participants. All data are publicly available, at http://adni.loni.usc.edu/.In all, 1048 subjects (54.5% male) participated in the study and were categorized into four groups based on the individual clinical diagnosis at baseline and future visits, as follows: (a) cognitively normal (CN): 294 subjects (28.1%) diagnosed as CN at baseline who remained CN at the time this manuscript is prepared. (b) sMCI: 254 subjects (24.2%) diagnosed as MCI at all-time points. (c) pMCI: 232 subjects (22.1%) evaluated as MCI at baseline visit who had progressed to AD within three years. (d) AD: 268 subjects (25.6%) who had a clinical diagnosis of AD for all visits. Subjects showing improvement in their clinical diagnosis during follow up (i.e., those clinically diagnosed as MCI but reverting to CN, or those clinically diagnosed as AD but reverting to MCI or CN) were excluded from the study because of the potential uncertainty of clinical misdiagnosis, considering that AD is considered irreversible form of dementia. In addition, cases that had a direct conversion from CN to AD were also removed. Patients taking part in this study are anonymized and the actual list of patient IDs in our study can be found in Supplementary File (part 1). The data used in this research are from the baseline visits only, no longitudinal data were considered.Study cohortsEligible participant patients were from 55 to 91 years old, fluent in English or Spanish, and had at least six years of education. Participants were categorized into three groups: CN, MCI (sMCI + pMCI), or AD. CN individuals were free of memory complaints, had a mini-mental state examination (MMSE) score of 24 to 30, and an average clinical dementia rating sum of boxes score (CDR-SB) of 0.04. MCI individuals had MMSE scores of 23 to 30, and an average CDR-SB of 1.582. MMSE and CDR-SB scores for MCI subjects were considerably different from CN subjects (P < 0.0001). The ages of MCI subjects were significantly different from AD and CN subjects (P < 0.005). The years of education for MCI subjects were significantly different from CN subjects (P < 0.01). AD patients fulfill diagnostic criteria for probable AD as set by the National Institute of Neurological and Communicative Disorders and Stroke of the United States and the Alzheimer’s Disease and Related Disorders Association68, with MMSE scores of 19 to 27 and an average CDR-SB of 4.347. MMSE and CDR-SB scores of AD subjects were significantly different from CN and MCI subjects (p < 0.0001). The ages of AD subjects were significantly different from CN subjects (P < 0.05), and the education years of AD subjects were significantly different from CN subjects (P < 0.0001) and MCI subjects (P < 0.01). Available ADNI subjects (n = 1048) with both a T1-weighted MRI scan and a PET–fluorodeoxyglucose (PET-FDG) image upon preparation of this manuscript were used in this study. For the PET data, we collected only three PET-FDG features from Banner Alzheimer’s Institute (BAI)-PET Naval Medical Research Center (NMRC) summaries and University of California, Berkeley, FDG analysis69. The MRI features used in our experiments are based on the imaging data from the ADNI database processed by a team from UCSF, who performed cortical reconstruction and volumetric segmentations with the FreeSurfer version 6.0 image analysis suite (https://surfer.nmr.mgh.harvard.edu/) according to the atlas generated by Desikan et al.70.The FreeSurfer software version 6.0 (https://surfer.nmr.mgh.harvard.edu/) was employed to automatically label cortical and subcortical tissue classes for the structural MRI scan of each subject, and to extract thickness measures of cortical regions of interest and cortical and subcortical volume measures. Based on the 312 features collected from each MRI image, we calculated seven features including ventricles, middle temporal gyrus [midTemp], fusiform, entorhinal, hippocampus, and whole brain volume. The equations used to calculate these features can be found in Supplementary File (part 2). Details of the analysis procedure are available at ADNI (http://adni.loni.usc.edu/methods/mri-tool/mri-analysis/). Detailed descriptions of the ADNI subjects, image acquisition protocol procedures, and post-acquisition preprocessing procedures can be found at ADNI (http://www.adni-info.org/). Demographic and clinical information of the subjects is shown in Table 11. In this study, we utilized multiple modalities that include the followings: (i) Cognitive scores, e.g. 12 features of the Alzheimer’s diseases assessment scale–cognitive subscale (ADAS-Cog) 11, ADAS-Cog 13, global CDR (CDGLOBAL), CDRSB, functional assessment questionnaire (FAQ), geriatric depression scale (GDT), MMSE, Montreal cognitive assessment (MoCA), and the neuropsychiatric inventory questionnaire score (NPISCORE). (ii) PET features, i.e. FDG, hypometabolic convergence index (HCI), and statistical region of interest [SROI]). (iii) Neuropsychological battery, i.e. 35 features of the Rey auditory verbal learning test (RAVLT), CLOCK, COPY, and AVTOT total scores and sub-scores. (iv) Neuropathology vital signs, i.e. seven features including body mass index (BMI), weight, blood pressure, etc. (v) Cerebrospinal fluid (CSF) biomarkers, i.e. TAU, phosphorylated TAU—PTAU, and amyloid-β peptide of 42 amino acids- Aβ1–42. (vi) Demographics, i.e. gender, age, number of years of education, marital status, and ethnic and racial categories. (vii) Medical history, i.e. 22 binary features to check the patient and parents histories, including smoking, allergies, malignancy, gastrointestinal problems, etc. (viii) Symptoms, i.e. 27 binary features asking about diarrhea, dizziness, falls, etc. (ix) Lab tests, i.e. 41 blood lab tests, including vitamin B12, monocytes, platelets, etc. (x) Physical examinations, i.e. 10 feature asking about problems in the head, neck, skin, chest, etc. (xi) Neurological exams, i.e. 12 binary features from the cerebellar exam, gait, motor strength, sensory capabilities, etc. (xii) MRI volumetric features, i.e. volumes of ventricles, MidTemp, fusiform, entorhinal cortex, hippocampus, total intracranial volume (ICV), and whole brain. (xiii) Genetics, i.e. APOE4. To the best of our knowledge, there are no studies in the literature, which study the role of all of these biomarkers. More details about these features can be found in Supplementary File (part 2).Table 11 Descriptive statistics from the dataset used.Full size tableFeature selection and modeling approachThe proposed model has two main layers. Each layer has an oracle classifier based on RF and a set of 22 explainers. The oracle is trained to be as accurate as possible based on the fused dataset. The First Layer’s oracle classifies the patient as CN, MCI, or AD based on the whole dataset. The Second Layer’s oracle concentrates further on the MCI cases, filtered from the previous layer, to predict their probable progression to AD within three years from baseline. As such, the Second Layer classifies the MCI cases into sMCI and pMCI cases. The development process of the proposed oracles has several major steps, as presented in Fig. 5. These steps are applied in the same order for both layers separately. First, after fusing the raw data modalities, for each layer, the full dataset is stratified and randomly divided into a model development set [\(S1\)] (90%) and a testing set [\(S2\)] (10%) that is utilized to evaluate and compare the generality and explainability of models. This split prevents the mixing of model-selection and performance estimation, which supports the estimations of unbiased generalization performance from the models. Second, a feature standardization step is assimilated on numerical features to normalize them in the same way, which is done by standardizing the random variables with zero mean and unitary standard deviation. Note that categorical features are excluded from the normalization process.Figure 5Development process for the oracle model in each layer.Full size imageThird, for enhanced generalization performance of the models, the \(S1\) set is used to implement a feature selection process to identify the most relevant features. Fourth, most ML approaches tend to generate biased models when handling imbalanced datasets. Our Second Layer’s dataset is balanced (52.3% sMCI and 47.7% pMCI). However, the First Layer’s dataset is imbalanced (28.05% CN, 46.37% MCI, and 25.58% AD). Therefore, the synthetic minority oversampling technique (SMOTE) is used to handle the class imbalance in the \(S1\) set of the First Layer by resampling the original data and creating synthetic instances71. Fifth, to guarantee unbiased tuning of model hyperparameters, and because our datasets are relatively small, the model selection and validation process (i.e. hyperparameter optimization) is carried out based on the grid search and nested k-fold stratified cross-validation (CV) where k = 1072. The entire process has two loops: an inner loop for hyperparameter tuning, and an outer loop for evaluation of the model with selected parameters on unseen data73. Model selection without nested CV uses the same data from parameter tuning and model evaluation, where information may leak into the model and overfit the data. The leave-one-out cross-validation (LOOCV), i.e. k-fold CV where k = n72, assures small bias but large variance74. The tenfold CV provides the best trade-off between bias and variance75. Keeping the \(S2\) set untouched helps us to verify that the generalization performance of the selected model thanks to tenfold CV is preserved even with unseen data. In each layer, we develop an RF classification model based on the selected features.RF classifiers are used because they are accurate, and it is possible to get the feature contributions for the whole model (a global explanation) and calculate feature contributions for each specific instance (a local explanation). Although SVM and DL have a huge capability to fit complex nonlinear models to the data and achieve high performance, the resultant models are opaque what makes hard to explain their decisions18. We therefore selected RF as the oracle to classify patients in our two-layer model.After building the RF oracle classifiers, we implement two interpretable classifiers (DT and FRBS) for each of the 11 modalities in each layer. The resulting 22 classifiers play the role of explainers to interpret the oracle decisions at each layer. Thus, we have 11 classifiers as a DT, and 11 classifiers as an FRBS. The FRBS deals naturally with imprecision and uncertainty36. Moreover, an FRBS plays an important role in the quest for XAI76. More precisely, we selected the Fuzzy Unordered Rule Induction Algorithm (FURIA) [51] from among all algorithms available for building an FRBS. FURIA is recognized as one of the most accurate fuzzy classifiers. In addition, FURIA usually yields a compact set of fuzzy IF–THEN rules. FURIA is based on the Repeated Incremental Pruning to Produce Error Reduction (RIPPER) algorithm77. FURIA translates RIPPER rule antecedents into trapezoidal fuzzy sets. These antecedents are related by FURIA weighed rules, which do not necessarily include an antecedent for all the input attributes and can have more than one antecedent for the same attribute. Each FURIA rule is associated with a certainty factor, i.e. a rule weight that FURIA computes regarding the relevance of the rule in accordance with the training data. Given a specific data instance, the min–max fuzzy inference mechanism is applied, and the winning rule, i.e. the one with maximum firing degree, determines the output class. If no rules are fired for a given data instance, then FURIA applies the so-called rule-stretching mechanism, which looks for slight modifications in the rule base with the aim of finding a new rule on-the-fly that is able to manage the given instance. Unfortunately, FURIA rules lack linguistic meaning because they have local semantics, i.e. the most suitable fuzzy sets are defined independently for each rule. This fact may jeopardize the interpretability of FURIA rules.With the aim of paving the way from interpretable to explainable classifiers, we use ExpliClas78. This is a web service ready to provide users with multimodal (textual + graphical) explanations related to the DT and FURIA. As a matter of fact, ExpliClas creates a linguistic layer on top of the DT and FURIA. First, global semantics (whether we consider the DT or FURIA) is set up beforehand. By default, three linguistic terms (e.g., low, medium, high) are defined for each attribute. Next, domain experts (if available) can add/remove/refine the given linguistic terms to assure they are meaningful. Then, given a specific data instance, the actual classification carried out by the DT or FURIA is automatically interpreted by ExpliClas with regard to the linguistic terms previously defined. In practice, both the activated branch of the DT and the winner rule of FURIA are translated into sequences of meaningful words (i.e., each numerical interval in the DT or fuzzy set in FURIA is verbalized by the closest linguistic term in ExpliClas). As a result, users are provided with an explanation in natural language of the output class in terms of the involved attributes. It is worth noting that we substituted the default linguistic terms in ExpliClas by meaningful linguistic terms in agreement with a physician in this study.Figure 6 shows a detailed description of our proposed XAI framework. The first step is preprocessing, which is used to prepare and improve the quality of the datasets. This step has the following four sub-processes.

Preparing biological modalities: For the biological MRI modality, we used ready-made extracted and pre-processed features (http://adni.loni.usc.edu/), done by ADNI. We then used these detailed features to create a list of seven volumetric summary features for the most critical brain regions of interest, including the hippocampus, ventricles, entorhinal, fusiform gyrus, MidTemp, whole brain, and ICV. For biological PET modality, we collected only three FDG-PET features from BAI-PET NMRC summaries and UC Berkeley-FDG analysis69. For instance, to measure FDG, mean levels of glucose metabolisms are first recorded at different regions of interest. The five most common regions are left and right angular gyri, posterior cingulate cortex, and left and right inferior temporal gyri. Then, the summation of the mean glucose metabolisms is considered FDG79. Other PET measures include the HCI to characterize in a single summary metric the extent to which both the magnitude and spatial extent of cerebral glucose hypometabolism in a person’s FDG-PET image corresponds to that in patients with probable AD dementia80. Our prepared PET and MRI features are based on their popularity in studies from the literature, their availability, and their level of accuracy in our current medical problem (see Supplementary File [part 2] for further details).


Multimodal fusion: The AD environment is multimodal in nature, where multiple feature sets are combined. This is called multimodal fusion, where each modality has supplementary information to support the final decision. In this context, two simple strategies are followed: late fusion and early fusion. In late fusion (i.e., decision-level fusion), a different model is trained independently for each modality, and the individual outcomes are merged into a final common decision, as seen in Fig. 7a. In the early fusion strategy (i.e., feature-level fusion), raw features from the individual modalities are integrated to create a common feature vector. The common feature vector is then used to train a classifier as the final prediction model, as seen in Fig. 7b. Each strategy has its own advantages and disadvantages. However, late fusion is based mainly on computing weights associated to which classifiers, which is not an easy process to learn and to explain. Therefore, in this study, we apply the early fusion strategy.


Data standardization: After data splitting, each type of participating data may have a different order of magnitude. These raw data cannot be used directly to train the RF model. To ensure that every feature has the same level of importance, data were standardized using the z-score method (see Eq. 1). The standardized data is therefore normally distributed with mean and standard deviation of 0 and 1, respectively.$$z_{j} = \frac{{x_{j} - \mu_{j} }}{{\sigma_{j} }}$$
                    (1)
                
where \({x}_{j}\) is the old value of feature \(j\), \({z}_{j}\) is the normalized value, \({\mu }_{j}\) is the feature’s mean, and \({\sigma }_{j}\) is the feature’s standard deviation. As a side effect, this method removes outliers.


Handling missing values: For handling missing values, we first removed any feature with more than 30% of the values missing. Then, we use the k-nearest neighbors (KNN) algorithm to impute missing values, where missing values are replaced using information from neighbor subjects that have the same class. After finding \(k\) neighbors, the imputation value is computed by averaging the values of those neighbors. In our study, the mixed Euclidean distance (MED) was used, and k was set to 10 empirically via experiments (for numerical values, the Euclidean distance was used; for categorical values, a distance of 0 was taken if both values were the same, otherwise the distance was set to 1). Please note that the data standardization process has been done before the missing values handling.$$MED\left( {x,y} \right) = \sqrt {\mathop \sum \limits_{i = 1}^{N} d_{i} \left( {x_{i} ,y_{i} } \right)^{2} }$$
                    (2)
                
where \(d_{i} \left( {x_{i} ,y_{i} } \right) = \left\{ {\begin{array}{*{20}l} {overlab\left( {x_{i} ,y_{i} } \right)} \hfill & {if\;i\;is\;categorical} \hfill \\ {diff\left( {x_{i} ,y_{i} } \right)} \hfill & {if\;i\;is\;numerical} \hfill \\ \end{array} } \right.\), \(overlab\left( {x_{i} ,y_{i} } \right) = \left\{ {\begin{array}{*{20}l} 0 \hfill & {if\;x_{i} = y_{i} } \hfill \\ 1 \hfill & {if\;x_{i} \ne y_{i} } \hfill \\ \end{array} } \right.\), and \(diff\left({x}_{i},{y}_{i}\right)=\frac{{x}_{i}-{y}_{i}}{{max}_{i}-{min}_{i}}\)

Figure 6The proposed XAI framework. A variety of data modalities are used to build the predictive model. In addition, a variety of explanations are built for the entire RF behavior and for each prediction. The FreeSurfer version 6.0 is used (https://surfer.nmr.mgh.harvard.edu/).Full size imageFigure 7Multimodal fusion strategies: (a) late fusion, (b) early fusion.Full size imageFor the automatic feature selection, we used wrapper methods, which obtain subsets of features, and offer better performance than filter methods20. The commonly used classifiers in wrapper are naïve Bayes81, SVM82, RF83, and AdaBoost84. Along with greedy search algorithms, these methods find the optimal set of features. It is worth noting that the well-known principal component analysis (PCA) technique cannot be used in our experiments because we need to preserve meaningful medical features, and PCA produces synthetic features that are hard to interpret as a combination of the principal components.Recursive feature elimination (RFE) is famous in the medical domain owing to its efficiency in reducing computational burden85. It maximizes its predictor performance through backward feature elimination as well as its ranking criterion. The literature asserts that RF-RFE outperforms SVM-RFE in finding the best subsets of features, and does not need any parameter regulation to offer reasonable outcomes86. We applied RFE with the stratified tenfold CV related to the \(S1\) dataset. To prevent the bias introduced by randomly partitioning a dataset in CV, the tenfold CV procedure was repeated five times with different data partitions. To evaluate the robustness of the RF-RFE process in selecting the optimal set of features, we utilized the RFE method with RF, SVM, and gradient boosting (GB) classifiers. The initial fused feature set had 188 features combined from 11 different modalities, including MRI, genetics, and symptoms.The two RF models are used as the oracle to make the final decisions. Of course, final decisions are made by physicians in light of the provided information (i.e., both oracle decisions, along with related explanations). The 11 modalities are used separately to build classifiers by using two interpretable ML models, i.e. DT and FURIA. In each layer, the resulting 22 interpretable models are used to support the oracle model by providing interpretations of its decisions. The supplementary explanations extracted from different modalities with different classification algorithms are expected to enhance the medical expert’s confidence in the oracle decisions. As a result, it supports the applicability of the resulting system in real medical environments. It is worth noting that we are not interested in explaining the internal behavior of the oracle but providing physicians with post-hoc explanations of the decision output. Our approach is inspired in the way how different experts who look at the same patient may figure out different explanations for a given output in terms of different features (i.e., in accordance with their own knowledge and background). Similarly, our explainers provide physicians with complementary explanations, all of them consistent and reliable.Random forest for classificationRF is an ensemble classifier formed by a family of \(T\) decision trees,\(h\left({n}_{1}|{\theta }_{1}\right),\dots , h\left({n}_{T}|{\theta }_{T}\right)\), where \({\theta }_{i}=({\theta }_{i1},{\theta }_{i2}{,\dots ,\theta }_{ip})\) is a list of \(p\) features for DT \(i\), and \({n}_{i}\) represents the training instances. Each DT leads to a classifier. Specifically, given data \(D={\{{(\theta }_{i},{y}_{i})\}}_{i=1}^{N}\), we train a family of classifiers,\({h}_{T}\). The predictions of all individual trees are combined by using the majority-voting mechanism. A node is partitioned using the best possible binary split. In our case, information gain is used to define the split point at each node, where \(G\left(S,A\right)=E\left(S\right)-\sum_{v\in values\left(A\right)}\frac{\left|{S}_{v}\right|}{\left|S\right|}E\left({S}_{V}\right)\), and \(E(X)=-\sum_{i=1}^{c}{p}_{i} {log}_{2}({p}_{i})\) is the entropy of set \(X\), in which \({p}_{i}\) is the probability of class \(i\); \(\left|{S}_{v}\right|\) is the number of cases with \(A={S}_{v}\), and \(|S|\) is the number of cases in \(A\). Outliers are likely to be ignored by most trees, which makes RF more stable.Another important feature of RF is its ability to measure the importance of each feature based on the Gini impurity index. Gini impurity is the likelihood of an incorrect classification of a randomly selected case if it was randomly labeled according to the class distribution of the data. From intuitive perspective, Gini impurity helps the algorithm to decide the optimal split from a root node, and subsequent splits. It is calculated as \(G(D)=\sum_{i=1}^{c}p\left(i\right)*(1-p(i))\), where \(c\) is the number of classes and \(p(i)\) is the relative frequency of class \(i\) in \(D\). For an attribute \({\theta }_{m}\), if it splits \(D\) in to \({D}_{1}\) and \({D}_{2}\), then the Gini index for \({\theta }_{m}\) is \({G}_{{\theta }_{m}}\left(D\right)=\frac{\left|{D}_{1}\right|}{\left|D\right|}G\left({D}_{1}\right)+\frac{\left|{D}_{2}\right|}{\left|D\right|}G\left({D}_{2}\right),\) and the reduction in impurity is \(\Delta G\left({\theta }_{m}\right)=G\left(D\right)-{G}_{{\theta }_{m}}\left(D\right)\). A binary DT,\({h}_{T}\), is built from a learning sample of size \({n}_{t}\) drawn from \(D\) using a recursive procedure, which identifies at each node \(t\) the split condition \({s}_{t}={\theta }_{m}<c\) that splits \({n}_{t}\) node samples into \({t}_{L}\), and \({t}_{R}\) maximizes the decrease \(\Delta i\left(s,t\right)=i\left(t\right)-pL*i\left({t}_{L}\right)-pR*i({t}_{R})\); \(\Delta i\) is the importance of node \(t\) based on Gini importance; \(pL={n}_{{t}_{L}}\), and \(pR={n}_{{t}_{R}}\). For each node split, the Gini impurity index values for the two child nodes are less than the value for the parent node. For each variable, the summation of Gini impurity decreases in a dataset over all trees in the RF model and is the corresponding Gini importance measure for that variable. The global importance of a feature, \({\theta }_{m}\), for predicting \(y\) is calculated by adding up the weighted impurity decreases, \(p\left(t\right)\Delta i\left({s}_{t},t\right)\), for all nodes \(t\) where \({\theta }_{m}\) is used, averaged over all \(T\) trees in the forest (see Eq. 3).$$imp\left( {\theta_{m} } \right) = \frac{1}{T}\mathop \sum \limits_{T} \mathop \sum \limits_{{t \in T:v\left( {s_{t} } \right) = \theta_{m} }} p\left( t \right)\Delta i\left( {s_{t} ,t} \right)$$
                    (3)
                Interested readers are referred to56 for further details about the RF algorithm. More details on the Gini variable importance approach in RF can be found in87.Explainability capabilitiesAs RF is an ensemble classifier, it is difficult to get understandable explanation from this complex model. Therefore, we use a collection of simpler models, see Fig. 8, to endow RF with explainability. Each of these models is called “an explainer.” These models provide complementary views and explanations associated to the original RF model. Because AD is a complex disease and RF is a complex model, in order to have a global comprehensive, consistent, and accurate picture about AD progression, several explanatory techniques are required88. Our explainer framework includes SHAP explainer, DT explainer, and fuzzy explainer. Each of these explainers has been carefully designed to exhibit a good balance between accuracy and explainability. All explainers have been tested to verify they provide physicians with consistent and reliable explanations. As a result, medical expert will be more confident regarding the RF decisions.Figure 8Roles of explainers to enhance RF interpretability.Full size imageFor each layer in the proposed model, we provide two types of explanation. The first type gets explanations from the RF black-box model itself. For an RF model \(b\) and a dataset \(D=\left\{\theta ,Y\right\}\), function \(f:\left(\theta \to Y\right)\times \left({\theta }^{n}\times {Y}^{n}\right)\to V\) takes \(b\) and \(D\) as input and returns either global or local approximations \(V\) of the behavior of \(b\),\(f(b,D)=v\in V\), where \(V\) is the set of all possible explanations from RF. List \(V\) includes explanations regarding both global and local issues. We use Eli5 to calculate global feature importance based on the Gini index89,90, i.e. we compute the level of importance for all features based on the entire set of training data and the RF structure. Because model \(b\) is complex, global explanations can sometimes be too approximate to be trustworthy. In addition, medical experts prefer individualized explanations for each specific patient according to his/her own features. Then, we need to take care of local feature contributions too. These explanations, with the contribution directions, are provided for every single patient according to his/her feature vector. We use SHAP tree explainer, which is called the additive feature attribution method42,91. SHAP is based on the Shapely value concept from game theory91,92. Shapely values are used to estimate the magnitude as sign of feature contributions or importance. It is a theoretically justified and model-agnostic approach that builds a local explanation model,\(g\) for the original model \(f\). This model is a linear combination of binary variables \(g\left({x}^{^{\prime}}\right)={\varnothing }_{0}+\sum_{j=1}^{M}{\varnothing }_{j}{x}_{j}^{^{\prime}}\), where \({x}_{j}^{^{\prime}}\) is a simplified input that map to the original input \(x\) using the mapping function \({x=h}_{x}\left({x}^{^{\prime}}\right)\), \({x}^{^{\prime}}\in {\left\{0, 1\right\}}^{M}\) is the coalition vector and the 1 means the features in the new data are the same as those of the original data (the instance \(x\)), and 0 means the features in the new data are different from those of the original data, \(M\) is the total number of features, and \({\varnothing }_{j}\in {\mathbb{R}}\) is the Shapely value that measures the average feature attribution value for feature \(j\) for instance \(x\). SHAP try to ensure that \(g\left({z}^{^{\prime}}\right)\approx f({h}_{x}\left({z}^{^{\prime}}\right))\) when \({z}^{^{\prime}}\approx {x}^{^{\prime}}\). SHAP calculates \({\varnothing }_{j}\) based on the Shapley value from game theory (see Eq. 4)93:$$\emptyset_{j} = \mathop \sum \limits_{{S \in \left\{ {x_{1} , \ldots ,x_{M} } \right\}\backslash \left\{ {x_{j} } \right\}}} \frac{{\left| S \right|!\left( {M - \left| S \right| - 1} \right)!}}{M!}\left( {f\left( {S \cup \left\{ {x_{j} } \right\}} \right) - f\left( S \right)} \right)$$
                    (4)
                
where \(S\) is the subset of set of the features used in the model which have non-zero indexes in \({x}^{^{\prime}}\), \({x}^{^{\prime}}\) is the vector of feature values for the instance to be explained, \((\left|S\right|!\left(M-\left|S\right|-1\right)!)/M!\) is a weighting factor, and \(f\left(S\right)=E[f(x)|{x}_{S}]\) is the expected value of \(f\) for features in subset \(S\) that are marginalized over features not included in subset \(S\). SHAP values are consistent and accurate because they are calculated by averaging the differences in predictions over every possible feature ordering. In addition, the mean magnitude of the SHAP values can be used to estimate the global feature importance. We will compare the Gini index and SHAP-based methods using our datasets and trained RF classifiers.Because an individual decision explanation is critical in the medical domain, and because confidence is very important in order to create a trustworthy model, we add another type of explainability. The second type collects explanations from auxiliary or post-hoc models that try to explain RF decisions. The explainer is a function \(f:\left({\theta }^{m}\to Y\right)\times \left({\theta }^{n\times m}\times {Y}^{n}\right)\to \left({\theta }^{m}\to Y\right)\), which takes \(b\), \(D\) as input and returns local predictor \({p}_{i}\), i.e.\({p}_{i}=f(b,D)\), where \({p}_{i}\) is able to mimic the behavior of \(b\); a local explanatory function \({\varepsilon }_{i}:\left(\left({\theta }^{m}\to Y\right)\times \left({\theta }^{m}\times Y\right)\times {\theta }^{m}\right)\to \varepsilon\) exists, for \(b\),\({p}_{i}\), and \({\theta }^{m}\) instances are inputs; and \({\varepsilon }_{i}\) returns a human interpretable explanation for the patient record \({\theta }^{m}\), i.e.\({ \varepsilon }_{i}=f\left(b, {p}_{i},{\theta }^{m}\right)=e\). We implement interpretable classifiers (i.e. DT and FURIA) for each individual modality. These explainers create simple and easy-to-understand explanations from different dimensions (e.g. MRI, cognitive scores, symptoms, etc.), which help to inform domain experts about the oracle’s decision. By using these 22 explainers, we are confident that each oracle’s decision will have a sufficient number of related explanations. The most important thing regarding these 22 explainers is that they are not affected by the feature selection process, which means more features will participate in the explanation. In addition, the extracted formal knowledge from RF and post-hoc models is represented in natural language form by using ExpliClas78. Accordingly, we resolve the accuracy-explainability trade-off by providing a variety of explanations, while retaining the accuracy of a complex ensemble model (i.e. RF).Model performance evaluation metricsTo evaluate the proposed method, we used the following performance metrics: The area under the receiver operating characteristic curve (AUC), precision, recall, accuracy (AC), and F1-score (F1). In addition to the performance evaluation, the system maximizes the interpretability of the underlying models, and pays special attention to explainability, which can serve as an indispensable tool in the era of precision medicine. To validate the performance of the models, we report both cross-validation as well as test results. In each layer, we compared the performance of the best RF model with other ML models, including SVM, KNN, and decision tree models. The hyperparameters of these algorithms were tuned in the same way as RF.We used several libraries in the Python data science ecosystem to execute the experiments. The scikit-learn 0.21.2 package was used to perform feature selection and to train and evaluate all classifiers. Eli5 0.8.2 and SHAP 0.26.0 were used for explainability, and ExpliClas was used to provide natural language explanations from the 22 explainers. The naturalness and acceptability of generated explanations was validated by the physicians who collaborated in our study.Concluding remarksIn this paper, we proposed a highly accurate and explainable ML model based on a RF classifier. We have shown that multimodal RF classifiers can be successfully applied to AD detection and progression prediction. We proved that predictions based on combined multimodalities are significantly better than any single modality for both binary and multi-class classification tasks. Based on precise selection of the most informative features from 11 multimodalities, the system achieved the highest accuracies. Explainability was achieved using a variety of techniques. First, we provided a set of explanation capabilities for the RF models based on SHAP. For each layer’s model, global feature importance for the whole RF model and feature contributions for each specific patient were provided. For the first layer, we found that MMSE was the most important feature for the AD class, and CDRSB was the most important predictor for CN and MCI classes. For the second layer, FAQ was the most important feature for both sMCI and pMCI classes. Second, we implemented 22 explainers for each layer based on a decision tree classifier and a fuzzy rule-based system. Each explainer is based on a single modality. As a result, in each layer, each output decision comes up along with several complementary, consistent and reliable explanations. To validate the effectiveness of our model, we conducted experiments using the ADNI dataset. The model achieved high performance in each layer. The first layer had cross-validation accuracy of 93.95% and an F1-score of 93.94%, and second layer had cross-validation accuracy of 87.08% and an F1-Score of 87.09%. Moreover, our model exhibits a good accuracy-interpretability tradeoff because it achieved very accurate results as well as high level of interpretability. The resulting two-layer model provided justifiable, medically accurate, and hence, actionable decisions that can enhance physician confidence.The proposed ML model is accurate and explainable. However, it is worth noting that even if we achieved promising results from an academic point of view, we are still far from applying the model in a real-world clinical scenario; what we plan to do in the future. This is a long-term ongoing project. Currently, we are reporting results of the first stage. We have already validated our model with the ADNI dataset; what is a crucial contribution to pave the way towards the application of the model to real clinical data in primary care or general medical practice. Although it is the biggest and most popular real dataset for Alzheimer’s disease, the relevance of our work to direct primary care is limited by the ADNI cohort. Therefore, to translate the outcomes of this study into full-scale clinical practice, further investigations are required to determine its performance characteristics by applying the model to other relevant datasets. We plan to enhance our model with the aim of achieving even higher performance by means of deep learning applied to longitudinal data while preserving explainability issues as we already did in the present manuscript.Ethics statementData used in this study were obtained from the ADNI (http://adni.loni.usc.edu/). The Alzheimer’s Disease Neuroimaging Initiative Data and Publications Committee (ADNI DPC) coordinates patient enrollment and ensures standard practice on the uses and distribution of the data as follows: The ADNI data were previously collected across 50 research sites. To participate in the study, each study subject gave written informed consent at the time of enrollment for imaging and genetic sample collection and completed questionnaires approved by each participating sites’ Institutional Review Board (IRB). All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki declaration and its later amendments or comparable ethical standards. A complete description of ADNI and up-to-date information is available at http://adni.loni.usc.edu/ and data access requests are to be sent to http://adni.loni.usc.edu/data-samples/access-data/. Detailed inclusion criteria for the diagnostic categories can be found at the ADNI website (http://adni.loni.usc.edu/methods). The ethics committees/institutional review board that approved the ADNI study are listed within Supplementary file (part 4).


Data availability
The data that support the findings of this study are openly available at the ADNI web site (http://adni.loni.usc.edu/). In addition, the specific patient RIDs used in our study and the full description of used features can be found in the Supplementary Files.
AbbreviationsAD:
Alzheimer’s disease
ADAS-Cog:
Features of the Alzheimer’s diseases assessment scale-cognitive
ADNI:
Alzheimer’s Disease Neuroimaging Initiative
APOE:
Apolipoprotein E
AUC:
Area under the ROC curve
AV45:
Average AV45 SUVR of frontal
CDSS:
Clinical decision support system
CNN:
Convolutional neural network
CFA:
Cognitive and functional assessments
CS:
Cognitive scores
CDRSB:
Clinical dementia rating sum of boxes
CDGLOBAL:
Global CDR
DL:
Deep learning
DT:
Decision tree
DARPA:
Defense Advanced Research Projects Agency
FRBS:
Fuzzy rule-based system
FAQ:
Functional assessment questionnaire
GB:
Gradient boosting
GDT:
Geriatric depression scale
GDPR:
General data protection regulation
HCI:
Hypometabolic convergence index
ICV:
Intracranial volume
KNN:
K nearest neighbor
MRI:
Magnetic resonance imaging
ML:
Machine learning
MCI:
Mild cognitive impairment
MCA:
Multiclass classification accuracy
MMSE:
Mini-Mental State Examination
MoCA:
Montreal cognitive assessment
MH:
Medical history
NPISCORE:
Neuropsychiatric inventory questionnaire score
NB:
Neuropsychological battery
NB:
Naïve Bayes
pMCI:
Progressive MCI
PTAU:
Phosphorylated TAU
PET:
Positron emission tomography
PCA:
Principal component analysis
RF:
Random forest
RFE:
Recursive feature elimination
RAVLT:
Rey Auditory Verbal Learning Test
SHAP:
SHapley Additive exPlanations
sMCI:
Stable MCI
SVM:
Support vector machine
SNOMED CT:
Systematized Nomenclature of Medicine-Clinical Terms
XAI:
Explainable artificial intelligence
ReferencesAlberdi, A., Aztiria, A. & Basarab, A. On the early diagnosis of Alzheimer’s disease from multimodal signals: a survey. Artif. Intell. Med. 71, 1–29 (2016).Article 
    PubMed 
    
                    Google Scholar 
                Masters, C. L. & Beyreuther, K. Alzheimer’s centennial legacy: prospects for rational therapeutic intervention targeting the Aβ amyloid pathway. Brain 129, 2823–2839 (2006).Article 
    PubMed 
    
                    Google Scholar 
                Zamrini, E., De Santi, S. & Tolar, M. Imaging is superior to cognitive testing for early diagnosis of Alzheimer’s disease. Neurobiol. Aging 25, 685–691 (2004).Article 
    PubMed 
    
                    Google Scholar 
                Liu, M., Zhang, J., Adeli, E. & Shen, D. Joint classification and regression via deep multi-task multi-channel learning for Alzheimer’s disease diagnosis. IEEE Trans. Biomed. Eng. 1 (2018).Lee, G. et al. Predicting Alzheimer’s disease progression using multi-modal deep learning approach. Sci. Rep. 9, 1–12 (2019).ADS 
    CAS 
    
                    Google Scholar 
                Nie, L. et al. Modeling disease progression via multisource multitask learners: a case study with Alzheimer’s disease. IEEE Trans. Neural Netw. Learn. Syst. 28, 1508–1519 (2017).Article 
    MathSciNet 
    PubMed 
    
                    Google Scholar 
                Wee, C. Y., Yap, P. T. & Shen, D. Prediction of Alzheimer’s disease and mild cognitive impairment using cortical morphological patterns. Hum. Brain Mapp. 34, 3411–3425 (2013).Article 
    PubMed 
    
                    Google Scholar 
                Liao, Q. et al. Multi-task deep convolutional neural network for cancer diagnosis. Neurocomputing https://doi.org/10.1016/j.neucom.2018.06.084 (2018).Article 
    
                    Google Scholar 
                Lu, D., Popuri, K., Ding, W., Balachandar, R. & Beg, M. F. Multimodal and multiscale deep neural networks for the early diagnosis of Alzheimer’s disease using structural MR and FDG-PET images. Sci. Rep. 8, 5697 (2018).Article 
    ADS 
    PubMed 
    PubMed Central 
    CAS 
    
                    Google Scholar 
                Liu, M., Cheng, D., Wang, K. & Wang, Y. Multi-modality cascaded convolutional neural networks for Alzheimer’s disease diagnosis. Neuroinformatics 16, 295–308 (2018).Article 
    PubMed 
    
                    Google Scholar 
                Qiu, S. et al. Fusion of deep learning models of MRI scans, mini-mental state examination, and logical memory test enhances diagnosis of mild cognitive impairment. Alzheimer’s Dement. Diagnosis Assess. Dis. Monit. 10, 737–749 (2018).
                    Google Scholar 
                Kim-Han, T., Pew-Thian, Y. & Dinggang, S. Multi-stage diagnosis of Alzheimer’s Disease with incomplete multimodal data via multi-task deep learning. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support 160–168 (Springer, Cham, 2017).Hinrichs, C., Singh, V., Xu, G. & Johnson, S. C. Predictive markers for AD in a multi-modality framework: an analysis of MCI progression in the ADNI population. Neuroimage 55, 574–589 (2011).Article 
    PubMed 
    
                    Google Scholar 
                Zhou, J., Liu, J., Narayan, V. A. & Ye, J. Modeling disease progression via multi-task learning. Neuroimage 78, 233–248 (2013).Article 
    PubMed 
    
                    Google Scholar 
                Wang, T., Qiu, R. G. & Yu, M. Predictive modeling of the progression of Alzheimer’ s disease with recurrent neural networks. Sci. Rep. https://doi.org/10.1038/s41598-018-27337-w (2018).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Ding, X. et al. A hybrid computational approach for efficient Alzheimer’s disease classification based on heterogeneous data. Sci. Rep. 8, 1–10 (2018).Article 
    CAS 
    
                    Google Scholar 
                Gray, K. R., Aljabar, P., Heckemann, R. A. & Hammers, A. Random forest-based similarity measures for multi-modal classification of Alzheimer’ s disease. Neuroimage 65, 167–175 (2013).Article 
    PubMed 
    
                    Google Scholar 
                Das, D., Ito, J., Kadowaki, T. & Tsuda, K. An interpretable machine learning model for diagnosis of Alzheimer’s disease. PeerJ 7, e6543 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Mattila, J. et al. A disease state fingerprint for evaluation of Alzheimer’s disease. J. Alzheimer’s Dis. 27, 163–176 (2011).Article 
    
                    Google Scholar 
                Bucholc, M. et al. A practical computerized decision support system for predicting the severity of Alzheimer’s disease of an individual. Expert Syst. Appl. 130, 157–171 (2019).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Travers, C. et al. Opportunities and obstacles for deep learning in biology and medicine. bioRxiv https://doi.org/10.1101/142760 (2017).Article 
    
                    Google Scholar 
                Choi, H. & Jin, K. H. Predicting cognitive decline with deep learning of brain metabolism and amyloid imaging. Behav. Brain Res. 344, 103–109 (2018).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Spasov, S., Passamonti, L., Duggento, A., Lio, P. & Toschi, N. A parameter-efficient deep learning approach to predict conversion from mild cognitive impairment to Alzheimer’s disease. Neuroimage 189, 276–287 (2019).Article 
    PubMed 
    
                    Google Scholar 
                Zhang, D. & Shen, D. Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer’s disease. Neuroimage 59, 895–907 (2012).Article 
    PubMed 
    
                    Google Scholar 
                Cheng, B., Liu, M., Zhang, D., Munsell, B. C. & Shen, D. Domain transfer learning for MCI conversion prediction. IEEE Trans. Biomed. Eng. 62, 1805–1817 (2015).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Moore, P. J., Lyons, T. J. & Gallacher, J. Random forest prediction of Alzheimer’s disease using pairwise selection from time series data. PLoS ONE 14, 1–14 (2019).Article 
    
                    Google Scholar 
                Moradi, E., Pepe, A., Gaser, C., Huttunen, H. & Tohka, J. Machine learning framework for early MRI-based Alzheimer’s conversion prediction in MCI subjects. Neuroimage 104, 398–412 (2015).Article 
    PubMed 
    
                    Google Scholar 
                Fisher, C. K., Smith, A. M. & Walsh, J. R. Machine learning for comprehensive forecasting of Alzheimer’s disease progression. Sci. Rep. 9, 1–14 (2019).Article 
    CAS 
    
                    Google Scholar 
                Oxtoby, N. P. & Alexander, D. C. Imaging plus X: Multimodal models of neurodegenerative disease. Curr. Opin. Neurol. 30, 371–379 (2017).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Burrell, J. How the machine ‘Thinks:’ understanding opacity in machine learning algorithms. Ssrn https://doi.org/10.2139/ssrn.2660674 (2015).Article 
    
                    Google Scholar 
                Adadi, A. & Berrada, M. Peeking Inside the black-box: a survey on explainable artificial intelligence (XAI). IEEE Access 6, 52138–52160 (2018).Article 
    
                    Google Scholar 
                Lundberg, S. M. et al. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nat. Biomed. Eng. 2, 749–760 (2018).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Guidotti, R. et al. A survey of methods for explaining black box models. ACM Comput. Surv. 51, 93:1-93:42 (2018).
                    Google Scholar 
                Quinlan, J. R. C4.5: Programs for Machine Learning (Morgan Kaufmann Publishers, San Mateo, 1993).
                    Google Scholar 
                El-Sappagh, S. et al. An ontology-based interpretable fuzzy decision support system for diabetes diagnosis. IEEE Access 6, 37371–37394 (2018).Article 
    
                    Google Scholar 
                Trillas, E. & Eciolaza, L. Fuzzy Logic: An Introductory Course for Engineering Students (Springer, New York, 2015).Book 
    
                    Google Scholar 
                Huysmans, J., Dejaeger, K., Mues, C., Vanthienen, J. & Baesens, B. An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models. Decis. Support Syst. 51, 141–154 (2011).Article 
    
                    Google Scholar 
                Alonso, J. M., Castiello, C., Lucarelli, M. & Mencar, C. Modeling interpretable fuzzy rule-based classifiers for medical decision support. In Medical Applications of Intelligent Data Analysis: Research Advancements. 1064–1081 (Hershey, PA, USA: IGI Global, 2013).Alonso, J. M., Castiello, C. & Mencar, C. Interpretability of fuzzy systems: current research trends and prospects. In Springer Handbook of Computational Intelligence 219–237 (Springer, Berlin, 2015).Fernandez-Delgado, M., Cernadas, E., Barro, S. & Amorim, D. Do we need hundreds of classifiers to solve real world classification problems. J. Mach. Learn. Res. 15, 3133–3181 (2014).MathSciNet 
    MATH 
    
                    Google Scholar 
                Alonso, J. M., Ramos-Soto, A., Castiello, C. & Mencar, C. Hybrid data-expert explainable beer style classifier. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence 1–5 (2018).Ribeiro, M. T., Singh, S. & Guestrin, C. ‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier (2016).Zhou, Z.-H., Jiang, Y. & Chen, S.-F. Extracting symbolic rules from trained neural network ensembles. AI Commun. 16, 3–15 (2003).MATH 
    
                    Google Scholar 
                Breiman, L. Statistical modeling: the two cultures. Stat. Sci. 16, 199–231 (2001).Article 
    MathSciNet 
    MATH 
    
                    Google Scholar 
                Zhao, X., Wu, Y., Lee, D. L. & Cui, W. IForest: interpreting random forests via visual analytics. IEEE Trans. Vis. Comput. Graph. 25, 407–416 (2019).Article 
    
                    Google Scholar 
                Matthews, K. Tau protein abnormalities correlate with the severity of dementia in Alzheimer’s disease. Nat. Clin. Pract. Neurol. 2, 178 (2006).Article 
    
                    Google Scholar 
                Murphy, M. P. & Levine, H. Alzheimer’s disease and the amyloid-β peptide. J. Alzheimer’s Dis. 19, 311–323 (2010).Article 
    CAS 
    
                    Google Scholar 
                Sadigh-Eteghad, S. et al. Amyloid-beta: a crucial factor in Alzheimer’s disease. Med. Princ. Pract. 24, 1–10 (2015).Article 
    PubMed 
    
                    Google Scholar 
                Verdile, G. et al. The role of beta amyloid in Alzheimer’ s disease: still a cause of everything or the only one who got caught?. Pharmacol. Res. 50, 397–409 (2004).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Thaweepoksomboon, J. et al. Assessment of cerebrospinal fluid (CSF) beta-amyloid (1–42), phosphorylated tau (ptau-181) and total Tau protein in patients with Alzheimer’s disease (AD) and other dementia at Siriraj Hospital Thailand. J. Med. Assoc. Thai. 94(Suppl 1), S77-83 (2011).PubMed 
    
                    Google Scholar 
                Zetterberg, H. Biomarkers for Alzheimer’ s disease: current status and prospects for the future. J. Intern. Med. 6, 643–663 (2018).
                    Google Scholar 
                Weiner, M. W. et al. Recent publications from the Alzheimer’s disease neuroimaging initiative: reviewing progress toward improved AD clinical trials. Alzheimer’s Dement. 13, e1–e85 (2017).ADS 
    
                    Google Scholar 
                Tong, T. et al. A novel grading biomarker for the prediction of conversion from mild cognitive impairment to Alzheimer’s disease. IEEE Trans. Biomed. Eng. 64, 155–165 (2017).Article 
    PubMed 
    
                    Google Scholar 
                Li, K., O’Brien, R., Lutz, M. & Luo, S. A prognostic model of Alzheimer’s disease relying on multiple longitudinal measures and time-to-event data. Alzheimer’s Dement. 14, 644–651 (2018).Article 
    
                    Google Scholar 
                Jin, Y., Su, Y., Zhou, X. H. & Huang, S. Heterogeneous multimodal biomarkers analysis for Alzheimer’s disease via Bayesian network. Eurasip J. Bioinform. Syst. Biol. 2016, 4–11 (2016).Article 
    CAS 
    
                    Google Scholar 
                Breiman, L. E. O. Random forests. Mach. Learn. 45, 5–32 (2001).Article 
    MATH 
    
                    Google Scholar 
                Lebedev, A. V. et al. Random forest ensembles for detection and prediction of Alzheimer’s disease with a good between-cohort robustness. NeuroImage Clin. 6, 115–125 (2014).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Ramírez, J. et al. Ensemble of random forests one vs. rest classifiers for MCI and AD prediction using ANOVA cortical and subcortical feature selection and partial least squares. J. Neurosci. Methods 302, 47–57 (2018).Article 
    PubMed 
    
                    Google Scholar 
                Cheng, M., Nazarian, S. & Bogdan, P. There is hope after all: quantifying opinion and trustworthiness in neural networks. Front. Artif. Intell. 3 (2020).Dodd, E., Cheston, R. & Ivanecka, A. The assessment of dementia in primary care. J. Psychiatr. Ment. Health Nurs. 22, 731–737 (2015).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Onoda, K. & Yamaguchi, S. Revision of the cognitive assessment for dementia, iPad version (CADi2). PLoS One 9 (2014).El-Sappagh, S., Abuhmed, T., Riazul Islam, S. M. & Kwak, K. S. Multimodal multitask deep learning model for Alzheimer’s disease progression detection based on time series data. Neurocomputing 412, 197–215 (2020).Article 
    
                    Google Scholar 
                El-Sappagh, S. et al. Alzheimer’s disease progression detection model based on an early fusion of cost-effective multimodal data. Futur. Gener. Comput. Syst. 115 (2021).Chen, G. et al. Classification of Alzheimer disease, mild cognitive impairment, and normal cognitive status with large-scale network analysis based on resting-state functional MR imaging. Radiology 259, 213–221 (2011).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Wang, P. et al. Aberrant intra-and inter-network connectivity architectures in Alzheimer’s disease and mild cognitive impairment. Sci. Rep. 5, 1–12 (2015).
                    Google Scholar 
                Yang, R. & Bogdan, P. Controlling the multifractal generating measures of complex networks. Sci. Rep. 10, 1–13 (2020).CAS 
    
                    Google Scholar 
                Zhang, Y., Zhang, H., Chen, X., Lee, S. W. & Shen, D. Hybrid high-order functional connectivity networks using resting-state functional MRI for mild cognitive impairment diagnosis. Sci. Rep. 7, 1–15 (2017).ADS 
    CAS 
    
                    Google Scholar 
                McKhann, G. et al. Clinical diagnosis of Alzheimer’s disease: report of the NINCDS-ADRDA Work Group under the auspices of Department of Health and Human Services Task Force on Alzheimer’s Disease. Neurology 34, 939–939 (2012).Article 
    
                    Google Scholar 
                Jagust, W. J. et al. The Alzheimer’s disease neuroimaging initiative 2 PET core: 2015. Alzheimer’s Dement. 11, 757–771 (2015).Article 
    
                    Google Scholar 
                Desikan, R. S. et al. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. Neuroimage 31, 968–980 (2006).Article 
    PubMed 
    
                    Google Scholar 
                Chawla, N. V., Bowyer, K. W., Hall, L. O. & Kegelmeyer, W. P. SMOTE: Synthetic minority over-sampling technique. J. Artif. Intell. Res. 16, 321–357 (2002).Article 
    MATH 
    
                    Google Scholar 
                Elisseeff, A. & Pontil, M. Leave-one-out error and stability of learning algorithms with applications. In NATO science series sub series iii computer and systems sciences 111–130 (ISO Press, 2003).Krstajic, D., Buturovic, L. J., Leahy, D. E. & Thomas, S. Cross-validation pitfalls when selecting and assessing regression and classification models. J. Cheminform. 6, 1–15 (2014).Article 
    
                    Google Scholar 
                Raschka, S. Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. (2018).Kohavi, R. A study of cross-validation and bootstrap for accuracy estimation and model selection. Ijcai 14, 1137–1145 (1995).
                    Google Scholar 
                Alonso, J. M., Castiello, C. & Mencar, C. A bibliometric analysis of the explainable artificial intelligence research field. In Information Processing and Management of Uncertainty in Knowledge-Based Systems-Theory and Foundations, CCIS853, 1–13 (Springer, 2018).Cohen, W. Fast effective rule induction. In International Conference on Machine Learning (ICML) 115–123 (1995).Alonso, J. M. & Bugar, A. ExpliClas: automatic generation of explanations in natural language for weka classifiers. In IEEE International Conference on Fuzzy Systems 1–6 (2019).Landau, S. M. et al. Associations between cognitive, functional, and FDG-PET measures of decline in AD and MCI. Neurobiol. Aging 32, 1207–1218 (2011).Article 
    PubMed 
    
                    Google Scholar 
                Chen, K. et al. Characterizing Alzheimer’s disease using a hypometabolic convergence index. Neuroimage 56, 52–60 (2011).Article 
    CAS 
    PubMed 
    
                    Google Scholar 
                Cortizo, J. C. & Giraldez, I. Multi criteria wrapper improvements to Naive Bayes learning. In International Conference on Intelligent Data Engineering and Automated Learning 419–427 (Springer, Berlin, Heidelberg, 2006). https://doi.org/10.1007/11875581_51.Maldonado, S., Weber, R. & Famili, F. Feature selection for high-dimensional class-imbalanced data sets using support vector machines. Inf. Sci. (Ny) 286, 228–246 (2014).Article 
    
                    Google Scholar 
                Rodin, A. S. et al. Use of wrapper algorithms coupled with a random forests classifier for variable selection in large-scale genomic association studies. J. Comput. Biol. 16, 1705–1718 (2010).Article 
    CAS 
    
                    Google Scholar 
                Panthong, R. & Srivihok, A. Wrapper feature subset selection for dimension reduction based on ensemble learning algorithm. Procedia Comput. Sci. 72, 162–169 (2015).Article 
    
                    Google Scholar 
                Li, Z., Xie, W. & Liu, T. Efficient feature selection and classification for microarray data. PLoS ONE 13, 1–21 (2018).
                    Google Scholar 
                Granitto, P. M., Furlanello, C., Biasioli, F. & Gasperi, F. Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products. Chemom. Intell. Lab. Syst. 83, 83–90 (2006).Article 
    CAS 
    
                    Google Scholar 
                Menze, B. H. et al. A comparison of random forest and its Gini importance with standard chemometric methods for the feature selection and classification of spectral data. BMC Bioinformatics 10, 1–16 (2009).Article 
    CAS 
    
                    Google Scholar 
                Hall, P. On the Art and Science of Machine Learning Explanations. arXiv Prepr. arXiv 1810.02909 (2018).Ando Saabas. Interpreting Random Forests, treeinterpreter. (2019).Korobov, M. & Lopuhin, K. ELI5 Documentation. (2019).Lundberg, S. & Lee, S.-I. A Unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems 4765–4774 (2017).Molnar, C. Interpretable Machine Learning A Guide for Making Black Box Models Explainable. (2020).Štrumbelj, E. & Kononenko, I. Explaining prediction models and individual predictions with feature contributions. Knowl. Inf. Syst. 41, 647–665 (2014).Article 
    
                    Google Scholar 
                Download referencesAcknowledgementsThe authors would like to thank Farid Badria, a professor of pharmacognosy and head of the Liver Research Lab, Mansoura University, Egypt, and Hosam Zaghloul, a professor in the Clinical Pathology Department, Faculty of Medicine, Mansoura University, Egypt, for their efforts to assist this work. for their assistance as medical experts to finish the experimental part of this study. This work was supported by National Research Foundation of Korea-Grant funded by the Korean Government (Ministry of Science and ICT)-NRF-2020R1A2B5B02002478). In addition, Dr. Jose M. Alonso is Ramon y Cajal Researcher (RYC-2016-19802), and its research is supported by the Spanish Ministry of Science, Innovation and Universities (grants RTI2018-099646-B-I00, TIN2017-84796-C2-1-R, TIN2017-90773-REDT, and RED2018-102641-T) and the Galician Ministry of Education, University and Professional Training (grants ED431F 2018/02, ED431C 2018/29, ED431G/08, and ED431G2019/04), with all grants co-funded by the European Regional Development Fund (ERDF/FEDER program). Data collection and sharing for this project was funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). The ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie; Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.Author informationAuthors and AffiliationsCentro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela, 15782, Santiago de Compostela, SpainShaker El-SappaghInformation Systems Department, Faculty of Computers and Artificial Intelligence, Benha University, Banha, 13518, EgyptShaker El-SappaghCentro Singular de Investigación en Tecnoloxías Intelixentes, Universidade de Santiago de Compostela, 15703, Santiago, SpainJose M. AlonsoDepartment of Computer Science and Engineering, Sejong University, 209 Neungdong-ro, Gwangjin-gu, Seoul, 05006, KoreaS. M. Riazul IslamGastrointestinal Surgical Center, Faculty of Medicine, Mansoura University, Mansura, 35516, EgyptAhmad M. SultanDepartment of Information and Communication Engineering, Inha University, Incheon, 22212, South KoreaKyung Sup KwakAuthorsShaker El-SappaghView author publicationsYou can also search for this author in
                        PubMed Google ScholarJose M. AlonsoView author publicationsYou can also search for this author in
                        PubMed Google ScholarS. M. Riazul IslamView author publicationsYou can also search for this author in
                        PubMed Google ScholarAhmad M. SultanView author publicationsYou can also search for this author in
                        PubMed Google ScholarKyung Sup KwakView author publicationsYou can also search for this author in
                        PubMed Google ScholarContributionsMethodology, S.E.S., and S.M.R.I.; conceptualization, J.M.A. and K.S.K.; formal analysis, S.E.S., A.M.S., J.M.A, S.M.R.I., and K.S.K.; validation, S.E.S., A.M.S., and S.M.R.I.; visualization, J.M.A., and A.M.S.; investigation, S.E.S., and J.M.A.; data curation, K.S.K.; writing—original draft preparation, S.E.S. and J.M.A; writing—review and editing, K.S.K.; supervision, K.S.K. and S.E.S.Corresponding authorsCorrespondence to
                Shaker El-Sappagh or Kyung Sup Kwak.Ethics declarations
Competing interests
The authors declare no competing interests.
Additional informationPublisher's noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary InformationSupplementary Information.Rights and permissions
Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Reprints and permissionsAbout this articleCite this articleEl-Sappagh, S., Alonso, J.M., Islam, S.M.R. et al. A multilayer multimodal detection and prediction model based on explainable artificial intelligence for Alzheimer’s disease.
                    Sci Rep 11, 2660 (2021). https://doi.org/10.1038/s41598-021-82098-3Download citationReceived: 21 October 2019Accepted: 29 December 2020Published: 29 January 2021DOI: https://doi.org/10.1038/s41598-021-82098-3Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        
Subjects

Classification and taxonomyComputational neuroscienceData miningData processingMachine learning





This article is cited by





                                        Interpreting artificial intelligence models: a systematic review on the application of LIME and SHAP in Alzheimer’s disease detection
                                    


Viswan VimbiNoushath ShaffiMufti Mahmud

Brain Informatics (2024)




                                        A multimodal deep learning approach for the prediction of cognitive decline and its effectiveness in clinical trials for Alzheimer’s disease
                                    


Caihua WangHisateru TachimoriYuichi Yamashita

Translational Psychiatry (2024)




                                        Multimodal classification of Alzheimer's disease and mild cognitive impairment using custom MKSCDDL kernel over CNN with transparent decision-making for explainable diagnosis
                                    


V. AdarshG. R. GangadharanPaolo Zanetti

Scientific Reports (2024)




                                        An explainable machine learning approach for Alzheimer’s disease classification
                                    


Abbas Saad AlatranyWasiq KhanDhiya Al-Jumeily

Scientific Reports (2024)




                                        Advanced ensemble machine-learning and explainable ai with hybridized clustering for solar irradiation prediction in Bangladesh
                                    


Muhammad Samee SevasNusrat SharminSaidur Rahaman Sagor

Theoretical and Applied Climatology (2024)





CommentsBy submitting a comment you agree to abide by our Terms and Community Guidelines. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.





",,,,"{'headline': 'A multilayer multimodal detection and prediction model based on explainable artificial intelligence for Alzheimer’s disease', 'description': 'Alzheimer’s disease (AD) is the most common type of dementia. Its diagnosis and progression detection have been intensively studied. Nevertheless, research studies often have little effect on clinical practice mainly due to the following reasons: (1) Most studies depend mainly on a single modality, especially neuroimaging; (2) diagnosis and progression detection are usually studied separately as two independent problems; and (3) current studies concentrate mainly on optimizing the performance of complex machine learning models, while disregarding their explainability. As a result, physicians struggle to interpret these models, and feel it is hard to trust them. In this paper, we carefully develop an accurate and interpretable AD diagnosis and progression detection model. This model provides physicians with accurate decisions along with a set of explanations for every decision. Specifically, the model integrates 11 modalities of 1048 subjects from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) real-world dataset: 294 cognitively normal, 254 stable mild cognitive impairment (MCI), 232 progressive MCI, and 268 AD. It is actually a two-layer model with random forest (RF) as classifier algorithm. In the first layer, the model carries out a multi-class classification for the early diagnosis of AD patients. In the second layer, the model applies binary classification to detect possible MCI-to-AD progression within three years from a baseline diagnosis. The performance of the model is optimized with key markers selected from a large set of biological and clinical measures. Regarding explainability, we provide, for each layer, global and instance-based explanations of the RF classifier by using the SHapley Additive exPlanations (SHAP) feature attribution framework. In addition, we implement 22 explainers based on decision trees and fuzzy rule-based systems to provide complementary justifications for every RF decision in each layer. Furthermore, these explanations are represented in natural language form to help physicians understand the predictions. The designed model achieves a cross-validation accuracy of 93.95% and an F1-score of 93.94% in the first layer, while it achieves a cross-validation accuracy of 87.08% and an F1-Score of 87.09% in the second layer. The resulting system is not only accurate, but also trustworthy, accountable, and medically applicable, thanks to the provided explanations which are broadly consistent with each other and with the AD medical literature. The proposed system can help to enhance the clinical understanding of AD diagnosis and progression processes by providing detailed insights into the effect of different modalities on the disease risk.', 'datePublished': '2021-01-29T00:00:00Z', 'dateModified': '2021-01-29T00:00:00Z', 'pageStart': '1', 'pageEnd': '26', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'sameAs': 'https://doi.org/10.1038/s41598-021-82098-3', 'keywords': ['Classification and taxonomy', 'Computational neuroscience', 'Data mining', 'Data processing', 'Machine learning', 'Science', 'Humanities and Social Sciences', 'multidisciplinary'], 'image': ['https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig1_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig2_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig3_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig4_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig5_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig6_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig7_HTML.png', 'https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-021-82098-3/MediaObjects/41598_2021_82098_Fig8_HTML.png'], 'isPartOf': {'name': 'Scientific Reports', 'issn': ['2045-2322'], 'volumeNumber': '11', '@type': ['Periodical', 'PublicationVolume']}, 'publisher': {'name': 'Nature Publishing Group UK', 'logo': {'url': 'https://www.springernature.com/app-sn/public/images/logo-springernature.png', '@type': 'ImageObject'}, '@type': 'Organization'}, 'author': [{'name': 'Shaker El-Sappagh', 'affiliation': [{'name': 'Universidade de Santiago de Compostela', 'address': {'name': 'Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela, Santiago de Compostela, Spain', '@type': 'PostalAddress'}, '@type': 'Organization'}, {'name': 'Benha University', 'address': {'name': 'Information Systems Department, Faculty of Computers and Artificial Intelligence, Benha University, Banha, Egypt', '@type': 'PostalAddress'}, '@type': 'Organization'}], 'email': 'shaker.elsappagh@usc.es', '@type': 'Person'}, {'name': 'Jose M. Alonso', 'affiliation': [{'name': 'Universidade de Santiago de Compostela', 'address': {'name': 'Centro Singular de Investigación en Tecnoloxías Intelixentes, Universidade de Santiago de Compostela, Santiago, Spain', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'S. M. Riazul Islam', 'affiliation': [{'name': 'Sejong University', 'address': {'name': 'Department of Computer Science and Engineering, Sejong University, Seoul, Korea', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Ahmad M. Sultan', 'affiliation': [{'name': 'Mansoura University', 'address': {'name': 'Gastrointestinal Surgical Center, Faculty of Medicine, Mansoura University, Mansura, Egypt', '@type': 'PostalAddress'}, '@type': 'Organization'}], '@type': 'Person'}, {'name': 'Kyung Sup Kwak', 'affiliation': [{'name': 'Inha University', 'address': {'name': 'Department of Information and Communication Engineering, Inha University, Incheon, South Korea', '@type': 'PostalAddress'}, '@type': 'Organization'}], 'email': 'kskwak@inha.ac.kr', '@type': 'Person'}], 'isAccessibleForFree': True, '@type': 'ScholarlyArticle'}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiuQFodHRwczovL25ld3Nyb29tLmFjY2VudHVyZS5jb20vbmV3cy8yMDIxL2Z1dHVyZS1yZWFkeS1vcmdhbml6YXRpb25zLWxldmVyYWdpbmctZGlnaXRhbC10by1vcGVyYXRlLWZhc3Rlci1hbmQtc21hcnRlci1jb3VsZC1oZWxwLXVubG9jay01LXRyaWxsaW9uLWluLWVjb25vbWljLWdyb3d0aC1zYXlzLWFjY2VudHVyZS1zdHVkedIBAA?oc=5,“Future-Ready” Organizations Leveraging Digital to Operate Faster and Smarter Could Help Unlock $5 Trillion in ... - Newsroom | Accenture,2021-01-27,Newsroom | Accenture,https://newsroom.accenture.com,"NEW YORK; Jan. 27, 2021 - The pandemic-driven acceleration of digital adoption and the resulting new agile ways of operating could unlock $5.4 trillion in profitable growth i","future-ready, fast track","NEW YORK; Jan. 27, 2021 - The pandemic-driven acceleration of digital adoption and the resulting new agile ways of operating could unlock $5.4 trillion in profitable growth i","NEW YORK; Jan. 27, 2021 - The pandemic-driven acceleration of digital adoption and the resulting new agile ways of operating could unlock $5.4 trillion in profitable growth i",,,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiT2h0dHBzOi8vYmVjb21pbmdodW1hbi5haS9hLWNyYXNoLWNvdXJzZS1vbi1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1mNTQyM2EzNmUwOGXSAQA?oc=5,A crash course on Artificial Intelligence | by Veronica Sant | Becoming Human - Becoming Human: Artificial Intelligence Magazine,2021-01-29,Becoming Human: Artificial Intelligence Magazine,https://becominghuman.ai,Artificial intelligence refers to the development of computer-based solutions that can perform tasks that mimic human intelligence. AI collects different technologies that can be brought together…,N/A,"Artificial intelligence is everywhere. We use them into our homes, cars and air traffic controllers.","Artificial intelligence is everywhere. We use them into our homes, cars and air traffic controllers.",http://schema.org,NewsArticle,https://becominghuman.ai/a-crash-course-on-artificial-intelligence-f5423a36e08e,['https://miro.medium.com/v2/resize:fit:1200/1*wB18Ws0IrMZKrmGSr207Rg.jpeg'],"{'@type': 'Person', 'name': 'Veronica Sant', 'url': 'https://versant.medium.com'}","{'@type': 'Organization', 'name': 'Becoming Human: Artificial Intelligence Magazine', 'url': 'becominghuman.ai', 'logo': {'@type': 'ImageObject', 'width': 146, 'height': 60, 'url': 'https://miro.medium.com/v2/resize:fit:292/1*1fYpRTTpKQNa0zuEPe3itg.png'}}",A crash course on Artificial Intelligence - Becoming Human: Artificial Intelligence Magazine,2021-01-29T18:25:09.027Z,2021-12-29T00:31:02.551Z,,A crash course on Artificial Intelligence - Becoming Human: Artificial Intelligence Magazine,,,N/A,N/A,"Top highlightA crash course on Artificial IntelligenceVeronica Sant·FollowPublished inBecoming Human: Artificial Intelligence Magazine·7 min read·Jan 29, 202160ListenShareArtificial intelligence refers to the development of computer-based solutions that can perform tasks that mimic human intelligence.AI collects different technologies that can be brought together, where a machine acts at human-like intelligence levels. This includes learning rules that require simple decisions and reasonings to arrive at certain conclusions, learn from past mistakes, and experience self-correction.There are many steps along the way where the computer can learn like a human. AI systems can be put into complexity buckets. At the moment, simple, weak and narrow systems are everywhere. We use them into our homes, cars and air traffic controllers.The idea is that an AI system is trained for specific tasks. These tasks could include virtual personal assistants such as Apple’s Siri, classifying images, and translation speech. For example, by merely asking Amazon’s Alexa, it can help you answer, set alarms and purchases, and so much more. So you may already be using artificial intelligence every day in your home.There is a race to build robust AI when presented with unfamiliar tasks. A strong AI system would be able to find a solution without human intervention. Its intelligence would not just be able to touch but also to surpass that of humans. Strong AI currently exists with limits but is expected to advance over the next few decades. There are a host of other benefits that AI Brings.Big Data JobsThe origins of Artificial Intelligence TechnologyArtificial Intelligence is not new. It was formally founded in 1956 by a group of scientists in the United States. Artificial Intelligence has gone through many cycles since then, going through significant scientific breakthroughs, followed by ‘AI winters’ — times of disappointment after AI failed to deliver its hype. In recent years, with the more widespread AI application, AI’s responsible use, including ethics and bias, has become a significant factor in its development.The technologies behind Artificial IntelligenceSense lets a machine perceive the world around it by gathering and processing images, sounds, speech and text, facial recognition, image categorisation, sound pattern recognition, and translating speech to text.Computer vision allows machines such as computers or mobile phones to see their surroundings. Computer vision has already made its way to our mobile phones via different e-commerce or camera apps.Trending AI Articles:1. Write Your First AI Project in 15 Minutes2. Generating neural speech synthesis voice acting using xVASynth3. Top 5 Artificial Intelligence (AI) Trends for 20214. Why You’re Using Spotify WrongAudio Processing has to do with detecting and translating audio signals, like Google Cloud vision. It classifies images into thousands of categories such as cats and detects objects and faces within images. The second is Amazon Echo, which acts as a personal DJ to control through your voice.Now a Fourth Industrial Revolution is building on digital technologies. It is thought to be a fusion of technologies that blur the lines between the physical, digital, and biological spheres. (Nkusi, 2020)AI enables a machine to understand the information it collects through pattern recognition, such as finding patterns in social media posts on fraudulent behaviours for insurance claims. This is very similar to how humans interpret information by understanding the patterns presented and their contexts.Some of the technologies that are behind it are Natural language processing or NLP. This technology allows computer programmes to understand spoken language. NLP currently works through a process called deep learning. In essence, language is broken down into shorter elemental pieces to teach the machine to understand their relationships and work together.Knowledge representation is about representing information about the world in a form. A computer system can solve complex tasks such as diagnosing a medical condition or having a natural language dialogue. Speech recognition, this is the translation of speech into text or format that machines can read. For instance, think about automated phone systems that recognise your voice, process your request and put you through the correct department when you call a company.Facebook also uses natural language processing to look for patterns and user posts to understand how people feel about a particular brand or product. Chatbots like IPSoft Amelia or IBM Watson perform service desk roles such as handling customer complaints or solving customer complaints or customer help desk issues.AI can continuously learn, improve the outcomes, and become better at doing the task. It can continually optimise the performance by learning from successes or failures of these actions.Some examples of where AI is acting and learning are Netflix, suggesting movies, TV Shows, and documentaries, based on the viewer’s prior activity, patterns, and behaviours. The more you watch, the more it learns and better suggests relevant content for you to watch. Deep learning is one of the things that makes self-driving cars possible.Artificial Intelligence is also used to assist humans in non-repetitive tasks to find patterns. Learns from experiences and then using machine learning, choose the correct responses. Unlike automation, it does not follow orders or rules.It is used to provide insights. For example, imagine someone has a head injury and needs to determine the level of damage. An AI machine could help diagnose the degree of damage by being ‘trained’ on multiple X-rays of previous head injuries. It would then understand the severity of the current head injury and provide an informed result. This could help doctors provide their overall diagnosis for the patient a lot quicker as they would already have gained insight into the severity of the AI machine’s injury and would therefore have a better understanding of the patient’s condition before doing their checks.The benefits and things to consider relating to Artificial IntelligenceAI brings a new level of efficiency to the use of resources. Machine learning can extract meaning from large and complex data sets. Can, therefore, see patterns and anomalies in data that humans cannot.Secondly, AI has been able to analyse weather patterns and climate data, resulting in more accurate forecasts. Thirdly, a better customer experience, using AI, can improve how it interacts with its customers. This could involve things like customer experience through chatbots and digital assistants, who are available 24/7 to converse with customers. It could also mean that in hospitals, AI can focus on the manual and repetitive tasks such as understanding a patient’s medical history by reading through all the historical records. In contrast, nurses focus on the more human side of their jobs, like forming close interpersonal relationships with patients. AI has also led to other enjoyable and innovative solutions. In certain hotels, hotel guests can now check-in to their hotel using an app by merely using their fingerprints or taking a selfie.Artificial Intelligence is not here to replace us. It is here to help us.The best results are achieved when human experts work hand in hand with AI, each bringing the best of their unique capabilities to a problem.Artificial Intelligence, including machine learning and deep learning systems, are changing each and every industry and could create incredible opportunities for businesses. (Nkusi, 2020)BibliographyAccenture. (n.d.). What is AI exactly? Retrieved November 21, 2020, from https://www.accenture.com/us-en/insights/artificial-intelligence/what-ai-exactlyArtificial Intelligence (AI) Services & Solutions | Accenture. (n.d.). Accenture. Retrieved November 19, 2020, from https://www.accenture.com/us-en/services/ai-artificial-intelligence-indexGlobal, B. (n.d.). The history of artificial intelligence. Bosch Global. Retrieved November 21, 2020, from https://www.bosch.com/stories/history-of-artificial-intelligence/Nkusi, B. F. (2020, December 23). Let’s adapt our skills to work with Artificial Intelligence. The New Times | Rwanda. https://www.newtimes.co.rw/opinions/lets-adapt-our-skills-work-artificial-intelligenceWhat is Machine Learning? (2017, January 11). [Video]. YouTube. https://www.youtube.com/watch?v=f_uwKZIAeM0This blog is a project for Study Unit MCS5460, University of Malta.Don’t forget to give us your 👏 !",https://becominghuman.ai/a-crash-course-on-artificial-intelligence-f5423a36e08e,2021-01-29T18:25:09.027Z,,,f5423a36e08e,['Veronica Sant'],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWWh0dHBzOi8vbmV3cy5taXQuZWR1LzIwMjEvcm9idXN0LWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXRvb2xzLXByZWRpY3QtZnV0dXJlLWNhbmNlci0wMTI40gEA?oc=5,Robust artificial intelligence tools to predict future cancer - MIT News,2021-01-28,MIT News,https://news.mit.edu,"MIT researchers have improved their machine learning system, Mirai, developed to predict cancer risk from mammogram images, and validated their effectiveness with studies across several hospitals.","Institute for Medical Engineering and Science (IMES), J-Clinic, artificial intelligence, machine learning, breast cancer, mammograms, Regina Barzilay, Mirai, Algorithms, Massachusetts General Hospital (MGH), Karolinska Institute, Chang Gung Memorial Hospital, Tyrer-Cuzick model, Adam Yala, Peter G. Mikhael","MIT researchers have improved their machine learning system, Mirai, developed to predict cancer risk from mammogram images, and validated their effectiveness with studies across several hospitals.",N/A,,,,,,,,,,,,,,N/A,N/A,"


Researchers created a risk-assessment algorithm that shows consistent performance across datasets from US, Europe, and Asia.










Watch Video



Rachel Gordon
|
MIT CSAIL


 Publication Date:
 January 28, 2021





Press Inquiries

  Press Contact:



      
            Rachel        

            Gordon        

  

      Email:
     rachelg@csail.mit.edu


      Phone:
              617-258-0675      
  

      
            MIT Computer Science and Artificial Intelligence Laboratory        

  








 Close














 Caption:
          MIT researchers have improved their machine learning system developed to predict cancer risk from mammogram images, and validated their effectiveness with studies across several hospitals.      
          

 Credits:
          Images courtesy of the researchers.      
          

















Previous image
Next image






















To catch cancer earlier, we need to predict who is going to get it in the future. The complex nature of forecasting risk has been bolstered by artificial intelligence (AI) tools, but the adoption of AI in medicine has been limited by poor performance on new patient populations and neglect to racial minorities. 
Two years ago, a team of scientists from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Jameel Clinic demonstrated a deep learning system to predict cancer risk using just a patient’s mammogram. The model showed significant promise and even improved inclusivity: It was equally accurate for both white and Black women, which is especially important given that Black women are 43 percent more likely to die from breast cancer. 
But to integrate image-based risk models into clinical care and make them widely available, the researchers say the models needed both algorithmic improvements and large-scale validation across several hospitals to prove their robustness. 
To that end, they tailored their new “Mirai” algorithm to capture the unique requirements of risk modeling. Mirai jointly models a patient’s risk across multiple future time points, and can optionally benefit from clinical risk factors such as age or family history, if they are available. The algorithm is also designed to produce predictions that are consistent across minor variances in clinical environments, like the choice of mammography machine.  








      

            Robust artificial intelligence tools may be used to predict future breast cancer.        

    



The team trained Mirai on the same dataset of over 200,000 exams from Massachusetts General Hospital (MGH) from their prior work, and validated it on test sets from MGH, the Karolinska Institute in Sweden, and Chang Gung Memorial Hospital in Taiwan. Mirai is now installed at MGH, and the team’s collaborators are actively working on integrating the model into care. 
Mirai was significantly more accurate than prior methods in predicting cancer risk and identifying high-risk groups across all three datasets. When comparing high-risk cohorts on the MGH test set, the team found that their model identified nearly two times more future cancer diagnoses compared the current clinical standard, the Tyrer-Cuzick model. Mirai was similarly accurate across patients of different races, age groups, and breast density categories in the MGH test set, and across different cancer subtypes in the Karolinska test set. 
“Improved breast cancer risk models enable targeted screening strategies that achieve earlier detection, and less screening harm than existing guidelines,” says Adam Yala, CSAIL PhD student and lead author on a paper about Mirai that was published this week in Science Translational Medicine. “Our goal is to make these advances part of the standard of care. We are partnering with clinicians from Novant Health in North Carolina, Emory in Georgia, Maccabi in Israel, TecSalud in Mexico, Apollo in India, and Barretos in Brazil to further validate the model on diverse populations and study how to best clinically implement it.” 
How it works 
Despite the wide adoption of breast cancer screening, the researchers say the practice is riddled with controversy: More-aggressive screening strategies aim to maximize the benefits of early detection, whereas less-frequent screenings aim to reduce false positives, anxiety, and costs for those who will never even develop breast cancer.  
Current clinical guidelines use risk models to determine which patients should be recommended for supplemental imaging and MRI. Some guidelines use risk models with just age to determine if, and how often, a woman should get screened; others combine multiple factors related to age, hormones, genetics, and breast density to determine further testing. Despite decades of effort, the accuracy of risk models used in clinical practice remains modest.  
Recently, deep learning mammography-based risk models have shown promising performance. To bring this technology to the clinic, the team identified three innovations they believe are critical for risk modeling: jointly modeling time, the optional use of non-image risk factors, and methods to ensure consistent performance across clinical settings. 
1. Time 
Inherent to risk modeling is learning from patients with different amounts of follow-up, and assessing risk at different time points: this can determine how often they get screened, whether they should have supplemental imaging, or even consider preventive treatments. 
Although it’s possible to train separate models to assess risk for each time point, this approach can result in risk assessments that don’t make sense — like predicting that a patient has a higher risk of developing cancer within two years than they do within five years. To address this, the team designed their model to predict risk at all time points simultaneously, by using a tool called an “additive-hazard layer.” 
The additive-hazard layer works as follows: Their network predicts a patient’s risk at a time point, such as five years, as an extension of their risk at the previous time point, such as four years. In doing so, their model can learn from data with variable amounts of follow-up, and then produce self-consistent risk assessments. 
2. Non-image risk factors 
While this method primarily focuses on mammograms, the team wanted to also use non-image risk factors such as age and hormonal factors if they were available — but not require them at the time of the test. One approach would be to add these factors as an input to the model with the image, but this design would prevent the majority of hospitals (such as Karolinska and CGMH), which don’t have this infrastructure, from using the model. 
For Mirai to benefit from risk factors without requiring them, the network predicts that information at training time, and if it's not there, it can use its own predictive version. Mammograms are rich sources of health information, and so many traditional risk factors such as age and menopausal status can be easily predicted from their imaging. As a result of this design, the same model could be used by any clinic globally, and if they have that additional information, they can use it. 
3. Consistent performance across clinical environments 
To incorporate deep-learning risk models into clinical guidelines, the models must perform consistently across diverse clinical environments, and its predictions cannot be affected by minor variations like which machine the mammogram was taken on. Even across a single hospital, the scientists found that standard training did not produce consistent predictions before and after a change in mammography machines, as the algorithm could learn to rely on different cues specific to the environment. To de-bias the model, the team used an adversarial scheme where the model specifically learns mammogram representations that are invariant to the source clinical environment, to produce consistent predictions. 
To further test these updates across diverse clinical settings, the scientists evaluated Mirai on new test sets from Karolinska in Sweden and Chang Gung Memorial Hospital in Taiwan, and found it obtained consistent performance. The team also analyzed the model’s performance across races, ages, and breast density categories in the MGH test set, and across cancer subtypes on the Karolinska dataset, and found it performed similarly across all subgroups. 
“African-American women continue to present with breast cancer at younger ages, and often at later stages,” says Salewai Oseni, a breast surgeon at Massachusetts General Hospital who was not involved with the work. “This, coupled with the higher instance of triple-negative breast cancer in this group, has resulted in increased breast cancer mortality. This study demonstrates the development of a risk model whose prediction has notable accuracy across race. The opportunity for its use clinically is high.” 
Here's how Mirai works: 
1. The mammogram image is put through something called an ""image encoder.""
2. Each image representation, as well as which view it came from, is aggregated with other images from other views to obtain a representation of the entire mammogram.
3. With the mammogram, a patient's traditional risk factors are predicted using a Tyrer-Cuzick model (age, weight, hormonal factors). If unavailable, predicted values are used. 
4. With this information, the additive-hazard layer predicts a patient’s risk for each year over the next five years. 
Improving Mirai 
Although the current model doesn’t look at any of the patient’s previous imaging results, changes in imaging over time contain a wealth of information. In the future the team aims to create methods that can effectively utilize a patient's full imaging history.
In a similar fashion, the team notes that the model could be further improved by utilizing “tomosynthesis,” an X-ray technique for screening asymptomatic cancer patients. Beyond improving accuracy, additional research is required to determine how to adapt image-based risk models to different mammography devices with limited data. 
“We know MRI can catch cancers earlier than mammography, and that earlier detection improves patient outcomes,” says Yala. “But for patients at low risk of cancer, the risk of false-positives can outweigh the benefits. With improved risk models, we can design more nuanced risk-screening guidelines that offer more sensitive screening, like MRI, to patients who will develop cancer, to get better outcomes while reducing unnecessary screening and over-treatment for the rest.” 
“We’re both excited and humbled to ask the question if this AI system will work for African-American populations,” says Judy Gichoya, MD, MS and assistant professor of interventional radiology and informatics at Emory University, who was not involved with the work. “We’re extensively studying this question, and how to detect failure.” 
Yala wrote the paper on Mirai alongside MIT research specialist Peter G. Mikhael, radiologist Fredrik Strand of Karolinska University Hospital, Gigin Lin of Chang Gung Memorial Hospital, Associate Professor Kevin Smith of KTH Royal Institute of Technology, Professor Yung-Liang Wan of Chang Gung University, Leslie Lamb of MGH, Kevin Hughes of MGH, senior author and Harvard Medical School Professor Constance Lehman of MGH, and senior author and MIT Professor Regina Barzilay. 
The work was supported by grants from Susan G Komen, Breast Cancer Research Foundation, Quanta Computing, and the MIT Jameel Clinic. It was also supported by Chang Gung Medical Foundation Grant, and by Stockholm Läns Landsting HMT Grant. 
 








Share this news article on:










X











Facebook















LinkedIn




































Reddit


















Print









Paper






Paper: ""Toward robust mammography-based models for breast cancer risk""








Press Mentions


WCVBProf. Regina Barzilay speaks with Nicole Estephan of WCVB-TV’s Chronicle about her work developing new AI systems that could be used to help diagnose breast and lung cancer before the cancers are detectable to the human eye.







 Full story via WCVB →
CNNResearchers at MIT developed a system that uses artificial intelligence to help predict future risk of developing breast cancer, reports Poppy Harlow for CNN. What this work does “is identifies risk. It can tell a woman that you’re at high risk for developing breast cancer before you develop breast cancer,” says Larry Norton, medical director of the Lauder Breast Center at the Memorial Sloan Kettering Cancer Center.







 Full story via CNN →
STATSTAT reporters Katie Palmer and Casey Ross spotlight how Prof. Regina Barzilay has developed an AI tool called Mirai that can identify early signs of breast cancer from mammograms. “Mirai’s predictions were rolled into a screening tool called Tempo, which resulted in earlier detection compared to a standard annual screening,” writes Palmer and Ross.











Full story via STAT →
Good Morning AmericaProf. Regina Barzilay speaks with Good Morning America about her work developing a new AI tool that could “revolutionize early breast cancer detection” by identifying patients at high risk of developing the disease. “If this technology is used in a uniform way,” says Barzilay, “we can identify early who are high-risk patients and intervene.”







 Full story via Good Morning America →
The Washington PostWashington Post reporter Steve Zeitchik spotlights Prof. Regina Barzilay and graduate student Adam Yala’s work developing a new AI system, called Mirai, that could transform how breast cancer is diagnosed, “an innovation that could seriously disrupt how we think about the disease.” Zeitchik writes: “Mirai could transform how mammograms are used, open up a whole new world of testing and prevention, allow patients to avoid aggressive treatments and even save the lives of countless people who get breast cancer.”











Full story via The Washington Post →
WiredWired reporter Will Knight spotlights how MIT researchers built a machine learning system that can help predict which patients are most likely to develop breast cancer. “What the AI tools are doing is they're extracting information that my eye and my brain can't,” says Constance Lehman, a professor of radiology at Harvard Medical School and division chief of breast imaging at MGH.











Full story via Wired →















Previous item
Next item



















Related Links

Adam YalaRegina Barzilay""Learning to Cure"" research group at CSAILJameel ClinicComputer Science and Artificial Intelligence LaboratoryMIT Schwarzman College of Computing






Related Topics

School of Engineering
MIT Schwarzman College of Computing
Biological engineering
Electrical Engineering & Computer Science (eecs)
Computer Science and Artificial Intelligence Laboratory (CSAIL)
Institute for Medical Engineering and Science (IMES)
Health care
Medicine
Research
Women
Cancer
Policy
Disease
Artificial intelligence
Machine learning
Health sciences and technology
Collaboration
Algorithms
Technology and society



Related Articles











Can mammogram screening be more effective?













A new approach to targeting tumors and tracking their spread













Using AI to predict breast cancer and personalize care 













Automated system identifies dense tissue, a risk factor for breast cancer, in mammograms

















Previous item
Next item
















",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiWmh0dHBzOi8vY3VsdHVyZS5wbC9lbi9hcnRpY2xlL2NpY2VyLWN1bS1jYXVsZS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1tZWV0cy1wb2xpc2gtcGllcm9nadIBAA?oc=5,‘Cicer cum Caule’: Artificial Intelligence Meets Polish Pierogi | Article - Culture.pl,2021-01-29,Culture.pl,https://culture.pl,Interdisciplinary curator Anna Desponds and cultural technologist Philo van Kemenade discuss their project ‘Cicer cum Caule’ – a workshop aimed at explaining some fundamental notions of artificial intelligence through the process of making traditional Polish dumplings.,"digital art, digital culture, art & technology, technology, women, women in contemporary art, ami, Digital Cultures",N/A,Interdisciplinary curator Anna Desponds and cultural technologist Philo van Kemenade discuss their project ‘Cicer cum Caule’ – a workshop aimed at explaining some fundamental notions of artificial intelligence through the process of making traditional Polish dumplings.,,,,,,,,,,,,,,N/A,N/A,EnglishpolskiEnglishукраїнськарусскийSearchContrastOpen MenuClose MenuLanguagesEnglishMain Topics#architecture#cuisine#design#film#heritage#literatureMore Topics#music#performing arts#travel in Poland#visual artsPodcastBookAll Our ProjectsArtists & Works IndexMultimedia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiRGh0dHBzOi8vd3d3LmNhbGNhbGlzdGVjaC5jb20vY3RlY2gvYXJ0aWNsZXMvMCw3MzQwLEwtMzg4OTk3OSwwMC5odG1s0gEA?oc=5,Pinecone raises $10 million to develop AI database - CTech,2021-01-27,CTech,https://www.calcalistech.com,"Company is run by former creators of Amazon’s artificial intelligence platform AWS, and plans to use funds for doubling its workforce in coming year","['Pinecone', 'AI', 'Machine Learning', 'Snowflake']","Company is run by former creators of Amazon’s artificial intelligence platform AWS, and plans to use funds for doubling its workforce in coming year","Company is run by former creators of Amazon’s artificial intelligence platform AWS, and plans to use funds for doubling its workforce in coming year",http://schema.org/,NewsArticle,https://www.calcalistech.com/ctechnews/category/5596,"{'@type': 'ImageObject', 'url': 'https://pic1.calcalist.co.il/PicServer3/2021/01/27/1055257/1NL.jpg', 'height': '147', 'width': '270'}","{'@type': 'Person', 'name': 'Meir Orbach'}","{'@type': 'Organization', 'name': 'Ctech', 'logo': {'@type': 'ImageObject', 'url': 'https://www.calcalistech.com/images/1280/header/Desktop%20Logo.png', 'height': '60', 'width': '195'}}",Pinecone raises $10 million to develop AI database ,2021-01-27T20:13:34,2021-05-25T12:30:04,,VC,,,N/A,N/A,N/A,"{'@type': 'WebPage', '@id': 'https://www.calcalistech.com/ctech/articles/0,7340,L-3889979,00.html'}",,,,,,"{'@type': 'BreadcrumbList', 'itemListElement': [{'@type': 'ListItem', 'position': '1', 'item': {'@type': 'WebSite', '@id': 'https://www.calcalistech.com/ctechnews', 'name': 'CTech'}}, {'@type': 'ListItem', 'position': '2', 'item': {'@type': 'WebSite', '@id': 'https://www.calcalistech.com/ctechnews/category/5596', 'name': 'VC'}}]}","Israeli artificial intelligence company Pinecone, the developer of machine learning infrastructure for the cloud, announced on Wednesday that it raised $10 million in seed funding. The round was led by the Wing Venture Capital. Peter Wagner, the Founding Partner of Wing, who was one of the first investors in Snowflake, has joined Pinecone’s board. Pinecone’s system was built by the same team that created Amazon SageMaker, the corporation’s solution for machine learning and artificial intelligence, and was founded by Dr. Edo Liberty, formerly a scientist and director at Amazon AWS. The company employs 10 people, is based out of Tel Aviv, and plans to use the funds to double its workforce over the coming year. 
 
Artificial intelligence companies use embedders to present information - from documents, images and videos, to user characterization. Existing databases and search engines have developed a way to process tabular information or documents, but aren’t yet adapted to process vectors in real-time on a broad scale. As a result, in recent years companies who use AI are forced to build their own solutions.
 
 
 
Pinecone’s database allows infrastructure and ML engineers to process and index billions of high-dimensional vectors in real-time without development efforts. In that way, companies can retrieve information to answer queries, by finding similar vectors, with sharp accuracy and within milliseconds. For example, one of the largest retailers in the world reported that using Pinecon’s system for offering buyer recommendations to customers on its website via its deep learning models, grew its revenues by 18.5% in comparison to existing solutions. “Leading techies are starting to rely on machine learning. Pinecone provides them with the necessary technology which was supposed to only be in the hands of only a few large tech corporations,” said Liberty, founder and CEO of Pinecone. 
 
“Modern organizations and businesses are based on data, and they are transitioning to the future with the help of AI. Similar to the way that companies like Snowflake have set in motion a data revolution, Pinecone is expected to strengthen its data teams and enable companies to adopt AI technologies,” Wagner said. 
 
Pinecone’s founding team has previously built systems for machine learning on a global scale. Liberty has managed Yahoo’s ML platform, after managing Amazon’s AI labs, including the one where his team created Amazon SageMaker. Amir Sadoughi heads the company’s engineering team, and is a former senior engineer at Amazon Cloud Services, and led SageMaker’s engineering development. 
 
Pinecone, which was founded in 2019, includes a team of engineers who have built AI platforms for Amazon AWS AI, Google, Facebook, and Yahoo, and whose scientists have published over 100 academic articles and patents in the field of ML, data, distributed systems, and algorithms. The company has offices in Silicon Valley, New York, and Tel Aviv. One of the company’s investors leads the Wing fund, which invests in company business technologies such as Cohesity, Snowflake, and Gong.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMibmh0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDIxLTAxLTI4L2dvb2dsZS1jZW8tc2F5cy1pbnRlcm5hbC1yYW5jb3Itb3Zlci1haS1kdWUtdG8tdHJhbnNwYXJlbmN50gEA?oc=5,Google CEO Says Internal Rancor Over AI Due to Transparency - Bloomberg,2021-01-28,Bloomberg,https://www.bloomberg.com,Alphabet Inc. Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent.,"ALPHABET INC-CL A,Artificial Intelligence,Sundar Pichai,World Economic Forum,Regulation,Culture,Inclusion,Europe,Australia,Quantum Computing,politics,markets,technology",Alphabet Inc. Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent.,Alphabet Inc. Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent.,http://schema.org,NewsMediaOrganization,https://www.bloomberg.com,"['https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iBcIY0yz5WTA/v0/1200x800.jpg', 'https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iBcIY0yz5WTA/v0/-1x-1.jpg', 'https:/assets.bwbx.io/s3/lightsaber/_next/static/media/social-default.cc6ae30e.jpg']","[{'@type': 'Person', 'name': 'Nico Grant'}]","{'@type': 'Organization', 'name': 'Bloomberg', 'url': 'https://www.bloomberg.com', 'logo': {'@type': 'ImageObject', 'url': 'https:/assets.bwbx.io/s3/lightsaber/_next/static/media/bloomberg-logo-amp.bae0aa0a.png', 'width': 262, 'height': 60}}",Google CEO Says Internal Rancor Over AI Due to Transparency,2021-01-28T20:01:14.162Z,2021-01-28T20:01:14.162Z,,Bloomberg,False,,N/A,N/A,"TechnologyGoogle CEO Says Internal Rancor Over AI Due to TransparencyFacebookTwitterLinkedInEmailLinkGiftExpandSundar PichaiPhotographer: Geert Vanden Wijngaert/BloombergFacebookTwitterLinkedInEmailLinkGiftGift this articleHave a confidential tip for our reporters? Get in TouchBefore it’s here, it’s on the Bloomberg TerminalBloomberg Terminal LEARN MOREFacebookTwitterLinkedInEmailLinkGiftBy Nico GrantJanuary 28, 2021 at 3:01 PM ESTBookmarkSaveLock This article is for subscribers only.Alphabet Inc. Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent.“Part of the reason you see a lot of debate, I mean, we engage as a company,” Pichai said Thursday during a World Economic Forum discussion. “We are a lot more transparent than most other companies, and so you do see us in the middle of these issues. I take it as a sign that we allow for debate to happen around this area and we need to get better as a company. We are committed to doing so.”Have a confidential tip for our reporters? Get in TouchBefore it’s here, it’s on the Bloomberg TerminalBloomberg Terminal LEARN MORE",https://www.bloomberg.com/news/articles/2021-01-28/google-ceo-says-internal-rancor-over-ai-due-to-transparency,2021-01-28T20:01:14.162Z,,,,,,,"{'@type': ['CreativeWork', 'Product'], 'name': 'Bloomberg', 'productID': 'bloomberg.com:basic'}","[{'@type': 'CollectionPage', 'name': 'Technology', 'url': 'https://www.bloomberg.com/technology'}]","{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.paywall'}","{'@type': 'PostalAddress', 'addressCountry': 'USA', 'addressLocality': 'New York', 'addressRegion': 'NY', 'postalCode': '10022', 'streetAddress': '731 Lexington Avenue'}",https://www.bloomberg.com/diversity-inclusion,inquiry1@bloomberg.net,Bloomberg Finance L.P.,5493001KJTIIGC8Y1R12,(212) 318-2000,https://www.bloomberg.com/logo-bloomberg.svg,"[{'@type': 'Brand', 'name': 'Bloomberg markets', 'url': 'https://www.bloomberg.com/markets'}, {'@type': 'Brand', 'name': 'Bloomberg technology', 'url': 'https://www.bloomberg.com/technology'}, {'@type': 'Brand', 'name': 'Bloomberg pursuits', 'url': 'https://www.bloomberg.com/pursuits'}, {'@type': 'Brand', 'name': 'Bloomberg politics', 'url': 'https://www.bloomberg.com/politics'}, {'@type': 'Brand', 'name': 'Bloomberg opinion', 'url': 'https://www.bloomberg.com/opinion', 'logo': 'https://www.bloomberg.com/logo-bloomberg_opinion.svg'}, {'@type': 'Brand', 'name': 'Bloomberg businessweek', 'url': 'https://www.bloomberg.com/businessweek', 'logo': 'https://www.bloomberg.com/logo-bloomberg_businessweek.svg'}, {'@type': 'Brand', 'name': 'Bloomberg green', 'url': 'https://www.bloomberg.com/green'}, {'@type': 'Brand', 'name': 'Bloomberg equality', 'url': 'https://www.bloomberg.com/equality'}, {'@type': 'Brand', 'name': 'Bloomberg citylab', 'url': 'https://www.bloomberg.com/citylab'}, {'@type': 'Brand', 'name': 'Bloomberg crypto', 'url': 'https://www.bloomberg.com/crypto'}, {'@type': 'Brand', 'name': 'Bloomberg industries', 'url': 'https://www.bloomberg.com/industries'}, {'@type': 'Brand', 'name': 'Bloomberg economics', 'url': 'https://www.bloomberg.com/economics'}, {'@type': 'Brand', 'name': 'Bloomberg ai', 'url': 'https://www.bloomberg.com/ai'}, {'@type': 'Brand', 'name': 'Bloomberg wealth', 'url': 'https://www.bloomberg.com/wealth'}]",,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiiwFodHRwczovL25ld3Nyb29tLmlibS5jb20vMjAyMS0wMS0yOC1OZXctSUJNLVRSSVJJR0EtQ2FwYWJpbGl0aWVzLXRvLUhlbHAtU3VwcG9ydC1Pcmdhbml6YXRpb25zLVJldHVybi10by1Xb3JrLXdpdGgtQUktRHJpdmVuLVNwYWNlLVBsYW5uaW5n0gEA?oc=5,New IBM TRIRIGA Capabilities to Help Support Organizations' Return to Work with AI-Driven Space Planning - IBM Newsroom,2021-01-28,IBM Newsroom,https://newsroom.ibm.com,"IBM today introduced capabilities in its IBM TRIRIGA integrated workplace management system that is designed to use artificial intelligence and near real-time insights to help organizations create a safe, productive and efficient workplace. These new capabilities include dynamic space planning, indoor wayfinding and a virtual assistant and are designed to help support organizations' return to work and support employee well-being and productivity.",,"IBM, (NYSE: IBM) today introduced capabilities in its IBM TRIRIGA integrated workplace management system that is designed to use artificial intelligence and near real-time insights to help...","IBM, (NYSE: IBM) today introduced capabilities in its IBM TRIRIGA integrated workplace management system that is designed to use artificial intelligence and near real-time insights to help...",,,,,,,,,,,,,,N/A,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiRGh0dHBzOi8vd3d3LndpY2hpdGEuZWR1L2Fib3V0L3dzdW5ld3MvbmV3cy8yMDIxLzAxLWphbi9xdWFudHVtXzMucGhw0gEA?oc=5,Team explores link between artificial intelligence and quantum computing - Wichita State University,2021-01-27,Wichita State University,https://www.wichita.edu,N/A,N/A,"Jan. 27, 2021 — For three decades, an interdisciplinary team at Wichita State has been exploring how machine learning – also known as artificial intelligence – can maximize the potential of quantum computing.","Jan. 27, 2021 — For three decades, an interdisciplinary team at Wichita State has been exploring how machine learning – also known as artificial intelligence – can maximize the potential of quantum computing.",,,,,,,,,,,,,,N/A,N/A,"
For three decades, an interdisciplinary team at Wichita State has been exploring how
                        machine learning – also known as artificial intelligence – can maximize the potential
                        of quantum computing.
Quantum computing applies the understanding of quantum physics – the behavior of energy
                        and matter at its most basic level – so that computations may be performed at unprecedented
                        speed, solving problems of exceptional complexity that cannot be solved by conventional
                        computers.
Led by Dr. Elizabeth Behrman, a professor of physics, and Dr. James Steck, professor
                        of aerospace engineering, the group currently has seven faculty actively involved.
Behrman, a pioneer in the field of quantum computing, said the two first met in 1990
                        when they were both new faculty at Wichita State.
“Twenty-five years ago, I was doing a lot of research on artificial neural network,
                        which are used in adaptive flight controls,” said Steck, whose area of expertise is
                        flight control systems. Artificial neural networks are structured like the brain,
                        made out of artificial neurons connected together by data pathways modeling synapses
                        in the brain.
“The brain has 10 billion neurons. An ant has 100,000 neurons,” Steck said. “Even
                        today it is hard to build an artificial neural network as big as an ant’s brain, so
                        I was looking at quantum devices, which are very small as a way to build a really
                        big artificial brain.”
Steck and Behrman began looking at the overlap of her research into quantum computing
                        and his interest in artificial neural networks. “If you believe in reincarnation,
                        I suppose you could say I must have been extra virtuous in my previous life I had
                        the good fortune to find such an excellent collaborator as Jim Steck,” she said.

We had incredible difficulty getting published at first. Neural network journals said,
                              ‘What is this quantum stuff?’ and physics journals said, ‘What is this artificial
                              neural network stuff?’Dr. Elizabeth Behrmanprofessor of physics
“Machine learning is used to train artificial neural networks, so we started applying
                        machine learning to train quantum computers to do calculations at a time when everyone
                        else was approaching quantum computers using the same logic gate methods used with
                        traditional PC microprocessors,” Steck said.
 Sometimes, you can be too far ahead of your time. While quantum computing and neural
                        networks have recently become a popular topic of research, in the 1990s the two found
                        it hard to get recognition for their work because it just didn’t “fit” traditional
                        fields of research.
“We had incredible difficulty getting published at first,” Behrman said. “Neural network
                        journals said, ‘What is this quantum stuff?’ and physics journals said, ‘What is this
                        artificial neural network stuff?’”
“We were doing really cool groundbreaking work on using machine learning for quantum
                        computing long before it was cool,” Steck said.
They begin presenting papers and publishing journal articles on topics related to
                        neural networks and quantum computing in the early 1990s. Their landmark work came
                        in 2008 when they published their break-through concepts for machine learning in quantum
                        computers. That paper, “Quantum algorithm design using dynamic learning” in Quantum Information and Computation, has been cited by other researchers dozens of times since.
Generally though, their work is not widely known, Berhman said, but occasionally thanks
                        to the internet, new collaborators seek out the pair. One of these is Dr. Cindy Miller,
                        Joint Staff liaison to NATO Allied Command Transformation. Miller’s job is to seek
                        out areas of promising technological advances from which the United States and our
                        allies would benefit, she said.
""Dr. Behrman and Dr. Steck are advancing a type of research that has the potential
                        to achieve earlier some of the anticipated advantages of quantum computing.  Those
                        advantages include military capabilities for the United States and our allies to ensure
                        our soldiers, sailors, airmen, marines, and guardians have what they need to keep
                        them safe and make them successful,” Miller said.
“Anticipated advantages include the ability to train faster and more precise artificial
                        intelligence in operations; optimizing military logistics; and the development of
                        new materials for weapon systems,” she said. “They have been kind enough to let me
                        learn more about this important field through interaction with their research group.""
In addition to Steck and Behrman, their research group now includes faculty from McPherson
                        College and University of Kansas, in addition to five at Wichita State: Behrman, Steck,
                        and Saideep Nannapaneni, Nathan Thompson and Bill Ingle.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LnBvcHVsYXJtZWNoYW5pY3MuY29tL3RlY2hub2xvZ3kvcm9ib3RzL2EzNTI2NzUwOC9odW1hbnMtY2FudC1jb250YWluLXN1cGVyaW50ZWxsaWdlbnQtbWFjaGluZXMv0gEA?oc=5,Humans Can't Contain Superintelligent Machines | Super AI - Popular Mechanics,2021-01-28,Popular Mechanics,https://www.popularmechanics.com,"New calculations show we won't be able to control superintelligent machines. Well, humans had a pretty good run.",,"Well, we had a pretty good run.",N/A,,,,,,,,,,,,,,Robots,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiTGh0dHBzOi8vaW5kaWFhaS5nb3YuaW4vYXJ0aWNsZS95ZXMtbm9uLXRlY2hpZXMtdG9vLWNhbi13b3JrLWluLWFpLWhlcmUtcy1ob3fSAQA?oc=5,"Yes, non-techies too can work in AI. Here's how! - INDIAai",2021-01-27,INDIAai,https://indiaai.gov.in,"True, Artificial Intelligence is a subfield of Computer Science. It is also true that if you’re a techie trained in the computer sciences, AI is a very lucrative area to work in. But the reverse may not be true.",undefined,"True, Artificial Intelligence is a subfield of Computer Science. It is also true that if you’re a techie trained in the computer sciences, AI is a very lucrative area to work in. But the reverse may not be true.","True, Artificial Intelligence is a subfield of Computer Science. It is also true that if you’re a techie trained in the computer sciences, AI is a very lucrative area to work in. But the reverse may not be true.",,,,,,,,,,,,,,N/A,N/A,"
                            Proforma for submission of nominations for IndiaAI Fellowship under the IndiaAI Mission
                         
                                    Article
                                 
                                4 Min
                            ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifGh0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL3RoZS1yb2xlLW9mLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWFuZC1tbC1pbi1pbnRlbGxpZ2VudC1hbmFseXRpY3PSAYYBaHR0cHM6Ly93d3cuYW5hbHl0aWNzaW5zaWdodC5uZXQvYW1wL3N0b3J5L2FydGlmaWNpYWwtaW50ZWxsaWdlbmNlL3RoZS1yb2xlLW9mLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLWFuZC1tbC1pbi1pbnRlbGxpZ2VudC1hbmFseXRpY3M?oc=5,The Role of Artificial Intelligence and ML in Intelligent Analytics - Analytics Insight,2021-01-27,Analytics Insight,https://www.analyticsinsight.net,,"Artificial Intelligence,AI and ML in intelligent analytics,role of AI and ML,analytics data,Machine Learning",AI and ML in intelligent analytics can drive the efficiency in business Analytics has been changing the way organizations operate for a long while. Since more o,AI and ML in intelligent analytics can drive the efficiency in business Analytics has been changing the way organizations operate for a long while. Since more o,http://schema.org,NewsArticle,https://www.analyticsinsight.net/artificial-intelligence/the-role-of-artificial-intelligence-and-ml-in-intelligent-analytics,"{'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/01/Intelligent-Analysis.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}","[{'@type': 'Person', 'givenName': 'Priya Dialani', 'name': 'Priya Dialani', 'url': 'https://www.analyticsinsight.net/author/priya-dialani'}]","{'@type': 'Organization', '@context': 'http://schema.org', 'name': 'Analytics Insight', 'url': 'https://www.analyticsinsight.net', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'author': 'analyticsinsight', 'contentUrl': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'url': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'name': 'logo', 'width': '', 'height': ''}, 'sameAs': ['https://whatsapp.com/channel/0029VafDe8HCBtxLV2PpRA2l', 'https://twitter.com/analyticsinme', 'https://in.pinterest.com/analyticsinsightsubmissions/_created/', 'https://www.instagram.com/analyticsinsightmagazine/', 'https://www.facebook.com/analyticsinsight.net', 'https://news.google.com/publications/CAAiEDD0Ze78owxVdNti611RNvQqFAgKIhAw9GXu_KMMVXTbYutdUTb0?hl=en-IN&gl=IN&ceid=IN%3Aen', 'https://t.me/analyticsinsightmag', 'https://www.youtube.com/channel/UCgF2J0b46YP0vvVEbgL_GuQ', 'https://www.linkedin.com/company/analytics-insight/'], 'id': 'https://www.analyticsinsight.net'}",The Role of Artificial Intelligence and ML in Intelligent Analytics,2021-01-27T00:59:52Z,2021-01-27T00:59:52Z,Artificial Intelligence,The Role of Artificial Intelligence and ML in Intelligent Analytics,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.analyticsinsight.net'}, {'@type': 'ListItem', 'position': 2, 'name': 'Artificial Intelligence', 'item': 'https://www.analyticsinsight.net/artificial-intelligence'}, {'@type': 'ListItem', 'position': 3, 'name': 'The Role of Artificial Intelligence and ML in Intelligent Analytics', 'item': 'https://www.analyticsinsight.net/artificial-intelligence/the-role-of-artificial-intelligence-and-ml-in-intelligent-analytics'}]",N/A,N/A,What is AI and Data Science Engineering? ,"{'@type': 'WebPage', '@id': 'https://www.analyticsinsight.net/artificial-intelligence/the-role-of-artificial-intelligence-and-ml-in-intelligent-analytics'}",2021-01-27T00:59:52Z,,,,,,"AI and ML in intelligent analytics can drive the efficiency in business.Analytics has been changing the way organizations operate for a long while. Since more organizations are dominating their utilization of analytics, they are diving further into their data to build proficiency, acquire a more prominent upper hand, and lift their bottom lines significantly more..Analytics powers your business, however, what amount of value would you say you are truly harnessing from your data?.Artificial intelligence and machine learning can help. Artificial intelligence is a collection of technologies that extract patterns and valuable insights from huge datasets, then making forecasts dependent on that data. Truth be told, AI exists today that can assist you with getting more value out of the data you as of now have, bind together that data, and make forecasts about customer behaviors based on it..The adoption of AI has been driven not just by increased computational power and new algorithms yet additionally the growth of data now accessible. For intelligence analysts, that multiplication of data implies surefire data over-burden. Human analysts essentially can't adapt to that much information. They need assistance..Intelligence leaders realize that AI can assist to adapt to this data downpour yet they may likewise consider what sway AI will have on their work and staff. For example, Twitter utilizes machine learning and AI to assess tweets in real-time and score them utilizing different measurements to show tweets that can possibly drive the most engagement..Google is researching virtually every part of machine learning and is making advancements in old-style algorithms and different applications like speech translation, prediction systems, natural language processing, and search ranking..Artificial intelligence plays a significant part in assisting organizations with handling data without forfeiting accuracy or speed..With digital transformation widely being embraced, the volume and size of data have expanded significantly. Also, dealing with such gigantic data isn't simple. Artificial intelligence- fueled data-driven innovation can help organizations manage such data to guarantee importance, worth, security, and transparency. They can depend on AI data integration platforms to ingest, change, and use information easily and with accuracy. Such platforms give an end-to-end encrypted environment that protects information from undesirable infringing and breaches, and make them hard to work with..Artificial intelligence and ML frameworks exist that utilize analytics data to assist you with foreseeing results and effective blueprints. Artificial intelligence- empowered frameworks can analyze information from many sources and deliver forecasts about what works and what doesn't. It can likewise deeply jump into information about your customers and offer predictions about buyer inclinations, marketing and sales channels, and product development strategies..Artificial intelligence/ML advances empower companies across various industries to harness value from customer information with no trouble. For instance, AI data integration solutions empower all business users to map information between various fields to make it simpler to incorporate the data into a unified database. Since these arrangements can be effortlessly utilized by non-technical users, IT people need not assume full responsibility. This leaves IT to zero in on other vital tasks..These solutions use ML algorithms to provide predictions of data, which can additionally quicken the data transformation process. Since the decisions are taken utilizing algorithms, the chance of mistakes like missing qualities, deceptions, errors, and so on, reduce. Hence, companies can use AI/ML tools to change the manner in which they deliver customer value. They can plan and integrate data and keep up data integrity, improving decision-making and boosting growth..The advantages of AI and ML, notwithstanding, can go a long way beyond time savings. All things considered, intelligence work is a never-ending process; there is consistently another difficulty that demands attention. So saving time with AI won't decrease the staff or trim intelligence budgets. Or maybe, the more noteworthy value of AI comes from what may be named an &quot;automation dividend&quot;: the better ways experts can utilize their time after these advances reduce their workload..Disclaimer: Analytics Insight does not provide financial advice or guidance. Also note that the cryptocurrencies mentioned/listed on the website could potentially be scams, i.e. designed to induce you to invest financial resources that may be lost forever and not be recoverable once investments are made. You are responsible for conducting your own research (DYOR) before making any investments. Read more here.","{'@type': 'WebPage', 'url': 'https://www.analyticsinsight.net/artificial-intelligence/the-role-of-artificial-intelligence-and-ml-in-intelligent-analytics', 'primaryImageOfPage': {'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/01/Intelligent-Analysis.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}}",,,,,,,,,,,https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/01/Intelligent-Analysis.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiW2h0dHBzOi8vd3d3Lm5oaC5uby9lbi9uaGgtYnVsbGV0aW4vYXJ0aWNsZS1hcmNoaXZlLzIwMjEvamFudWFyL3NlY3VyZWQtaGVyLWRyZWFtLWpvYi1lYXJseS_SAQA?oc=5,Secured her dream job early - nhh.no,2021-01-27,nhh.no,https://www.nhh.no,NHH student Inger Mirjam Madland (26) secured a job at her dream workplace before she started work on her master’s thesis.,N/A,NHH student Inger Mirjam Madland (26) secured a job at her dream workplace before she started work on her master’s thesis. ,NHH student Inger Mirjam Madland (26) secured a job at her dream workplace before she started work on her master’s thesis.,,,,,,,,,,,,,,N/A,N/A,"
Protects companies against cyber attacks

Andreas Orset combined economics and technology studies at NHH to adapt to the future job market. This secured him a permanent job a year and a half before graduating.         
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMiswFodHRwczovL3d3dy51YWxiZXJ0YS5jYS9jb21wdXRpbmctc2NpZW5jZS9uZXdzLWFuZC1ldmVudHMvbmV3cy8yMDIxL2phbnVhcnkvbWlrZS1ib3dsaW5nLWVsZWN0ZWQtYXMtYS1mZWxsb3ctb2YtdGhlLWFzc29jaWF0aW9uLWZvci10aGUtYWR2YW5jZW1lbnQtb2YtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UuaHRtbNIBAA?oc=5,Mike Bowling elected as a Fellow of the Association for the Advancement of Artificial Intelligence - University of Alberta,2021-01-29,University of Alberta,https://www.ualberta.ca,,N/A,N/A,N/A,https://schema.org,Article,,['media-library/news/mike_bowling_amii.jpg'],"{'@type': 'Person', 'name': 'unknown'}","{'@type': 'Organization', 'name': 'University of Alberta', 'logo': {'@type': 'ImageObject', 'url': 'https://content.presspage.com/clients/o_1979.png'}}",Mike Bowling elected as a Fellow of the Association for the Advancement of Artificial Intelligence,2021-01-29T00:00Z,2022-08-15T10:31Z,,,,,N/A,N/A,N/A,"{'@type': 'WebPage', '@id': '//www.ualberta.ca/'}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
https://news.google.com/rss/articles/CBMifmh0dHBzOi8vaW5kaWFuZXhwcmVzcy5jb20vYXJ0aWNsZS90ZWNobm9sb2d5L3RlY2gtbmV3cy10ZWNobm9sb2d5L2dvb2dsZS1jZW8tc2F5cy1pbnRlcm5hbC1yYW5jb3Itb3Zlci1haS1kdWUtdG8tdHJhbnNwYXJlbmN5L9IBgwFodHRwczovL2luZGlhbmV4cHJlc3MuY29tL2FydGljbGUvdGVjaG5vbG9neS90ZWNoLW5ld3MtdGVjaG5vbG9neS9nb29nbGUtY2VvLXNheXMtaW50ZXJuYWwtcmFuY29yLW92ZXItYWktZHVlLXRvLXRyYW5zcGFyZW5jeS9saXRlLw?oc=5,Google CEO says internal rancor over AI due to transparency - The Indian Express,2021-01-29,The Indian Express,https://indianexpress.com,Alphabet Inc Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent.,"['Google’s artificial intelligence ', ' Google AI', ' Google AI dissent', ' Google controversies', ' Google AI news', ' Sundar Pichai news', ' Google artificial intelligence news', '']",Alphabet Inc Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent.,Alphabet Inc Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent.,https://schema.org,NewsArticle,https://indianexpress.com/article/technology/tech-news-technology/google-ceo-says-internal-rancor-over-ai-due-to-transparency/,https://images.indianexpress.com/2021/01/4b17bb2a-8d51-43cf-bc18-500a2408414e-1.jpg,"[{'@type': 'Thing', 'name': 'IE Online', 'url': 'https://indianexpress.com/'}]","{'@type': 'NewsMediaOrganization', 'name': 'The Indian Express', 'url': 'https://indianexpress.com/', 'foundingDate': '1932', 'ethicsPolicy': 'https://indianexpress.com/privacy-policy/', 'SameAs': ['https://www.facebook.com/indianexpress', 'https://twitter.com/IndianExpress', 'https://www.youtube.com/@indianexpress', 'https://www.instagram.com/indianexpress/', 'https://www.linkedin.com/company/indian-express/'], 'logo': {'@type': 'ImageObject', 'url': 'https://images.indianexpress.com/2019/11/ienewlogo3_new.png', 'width': '600', 'height': '60'}}",Google CEO says internal rancor over AI due to transparency,2021-01-29T12:53:28+05:30,2021-01-29T12:53:28+05:30,technology,"['Home', 'Subscribe', 'India News', 'Cities', 'Videos', 'Audio', 'Explained', 'Education', 'Political Pulse', 'Opinion', 'Entertainment', 'Investigations', 'Lifestyle', 'Technology', 'Sports', 'Express Premium', 'Latest News', 'World News', 'Trending', 'Photos', 'Elections 2023', 'Web Stories', 'Bollywood', 'Cricket', 'Science', 'Business', 'Horoscope', 'Delhi News', 'Mumbai News', 'Hyderabad News', 'Bengaluru News', 'What Is', 'When Is', 'Who Is', 'How To', ""Today's Paper"", 'Brand Solutions']",False,"[{'@type': 'ListItem', 'position': 1, 'item': {'@id': 'https://indianexpress.com/', 'name': 'News'}}, {'@type': 'ListItem', 'position': 2, 'item': {'@id': 'https://indianexpress.com/section/technology/', 'name': 'Technology'}}, {'@type': 'ListItem', 'position': 3, 'item': {'@id': 'https://indianexpress.com/section/technology/tech-news-technology/', 'name': 'Tech'}}]",N/A,N/A,N/A,"{'@type': 'WebPage', '@id': 'https://indianexpress.com/article/technology/tech-news-technology/google-ceo-says-internal-rancor-over-ai-due-to-transparency/'}",,,,,,,"Alphabet Inc Chief Executive Officer Sundar Pichai said employee dissent about Google’s artificial intelligence work seems intense because the company is so transparent. “Part of the reason you see a lot of debate, I mean, we engage as a company,” Pichai said Thursday during a World Economic Forum discussion. “We are a lot more transparent than most other companies, and so you do see us in the middle of these issues. I take it as a sign that we allow for debate to happen around this area and we need to get better as a company. We are committed to doing so.” In December, Timnit Gebru, a researcher best known for showing how facial recognition algorithms are better at identifying White people than Black people, left Google in a storm of controversy. She has said she was fired after the company demanded she retract a research paper she co-authored that questioned an AI technology at the heart of Google’s search engine. The company has said she resigned. Close to 2,700 Googlers and more than 4,300 academics and civil society supporters signed a petition in favor of Gebru. Pichai emailed an apology to employees and said he is investigating the incident. The CEO said Thursday that listening to staff is an important part of Google’s culture and he wants to work with employee groups on sustainability and diversity and inclusion efforts. Google is being targeted by regulators from the European Union to Australia to the US on issues such as antitrust and whether the company should pay publishers for news. While the company is fighting many of these efforts, Pichai said he hopes to see more regulation in some areas, including an international accord on AI safety and quantum computing and guidance from governments on content moderation and free speech online. Digital misinformation “is bigger than any single company,” he said. “It is here to stay. As a society, we need to develop the next set of frameworks for us to function through that. That’s the debate we’re in the middle of.”","{'@type': ['CreativeWork', 'Product'], 'name': 'IE Premium', 'productID': 'indianexpress.com:showcase'}",,"{'@type': 'WebPageElement', 'isAccessibleForFree': False, 'cssSelector': '.ie-premium-content-block', 'video': [], 'audio': []}","{'@type': 'PostalAddress', 'streetAddress': 'Express Building, B-1/B, Sector-10', 'addressLocality': 'Noida', 'addressRegion': 'India', 'postalCode': '201301'}",,,The Indian Express,,,"{'@type': 'ImageObject', 'url': 'https://indianexpress.com/wp-content/themes/indianexpress/images/indian-express-logo-n.svg', 'inLanguage': 'en-US', 'width': 600, 'height': 60, 'caption': 'The Indian Express'}",,,,"{'@type': 'SpeakableSpecification', 'xpath': ['//title', ""//meta[@name='description']/@content""]}",https://schema.org/NewsMediaOrganization,News,en,2024-07-17,"{'type': 'EntryPoint', 'urlTemplate': 'https://indexpress.page.link/shareDL'}",1932,JOURNALISM OF COURAGE,"{'@type': 'ContactPoint', 'telephone': '+91-120-6651500', 'areaServed': 'IN', 'availableLanguage': 'English', 'hoursAvailable': {'opens': '09:30', 'closes': '18:30'}}","['https://www.facebook.com/indianexpress', 'https://twitter.com/IndianExpress', 'https://www.linkedin.com/company/indian-express', 'https://www.instagram.com/indianexpress/', 'https://www.youtube.com/channel/UCJEDFSxHHOW1PpBccdSxOTA']","{'@type': 'Person', 'name': 'Ramnath Goenka', 'url': 'https://indianexpress.com/ramnath-goenka/', 'sameAs': 'https://indianexpress.com/ramnath-goenka/'}","{'@type': 'QuantitativeValue', 'value': 153}","{'@type': 'SearchAction', 'target': 'https://indianexpress.com/?s={search_term_string}', 'query-input': 'required name=search_term_string'}",2024,"{'@type': 'AdministrativeArea', 'name': 'Noida, India'}","{'@type': 'ImageObject', 'url': 'https://images.indianexpress.com/2021/01/4b17bb2a-8d51-43cf-bc18-500a2408414e-1.jpg', 'caption': 'Sundar Pichai, chief executive officer of Alphabet Inc said that listening to staff is an important part of Google’s culture ( image source : Bloomberg)', 'description': 'Google’s artificial intelligence , Google AI, Google AI dissent, Google controversies, Google AI news, Sundar Pichai news, Google artificial intelligence news,', 'width': 1200, 'height': 675}"
https://news.google.com/rss/articles/CBMigwFodHRwczovL3d3dy5hbmFseXRpY3NpbnNpZ2h0Lm5ldC9hcnRpZmljaWFsLWludGVsbGlnZW5jZS9pbXBsZW1lbnRpbmctYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtdGFsZW50LXRvLWZpbGwtaW4tY29tcGFuaWVzLXNraWxsLWdhcNIBjQFodHRwczovL3d3dy5hbmFseXRpY3NpbnNpZ2h0Lm5ldC9hbXAvc3RvcnkvYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UvaW1wbGVtZW50aW5nLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXRhbGVudC10by1maWxsLWluLWNvbXBhbmllcy1za2lsbC1nYXA?oc=5,Implementing Artificial Intelligence Talent to Fill in Companies Skill Gap - Analytics Insight,2021-01-27,Analytics Insight,https://www.analyticsinsight.net,,"Artificial Intelligence Talent,AI strategy,AI talent,artificial intelligence,use of AI technologies",How the Implementation of AI Talent feeds Organisations Skill Gap There is an international shortage of artificial intelligence talent and labour markets across,How the Implementation of AI Talent feeds Organisations Skill Gap There is an international shortage of artificial intelligence talent and labour markets across,http://schema.org,NewsArticle,https://www.analyticsinsight.net/artificial-intelligence/implementing-artificial-intelligence-talent-to-fill-in-companies-skill-gap,"{'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/01/Artificial-Intelligence-16.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}","[{'@type': 'Person', 'givenName': 'Puja Das', 'name': 'Puja Das', 'url': 'https://www.analyticsinsight.net/author/puja-das'}]","{'@type': 'Organization', '@context': 'http://schema.org', 'name': 'Analytics Insight', 'url': 'https://www.analyticsinsight.net', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'author': 'analyticsinsight', 'contentUrl': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'url': 'https://images.assettype.com/analyticsinsight/2024-05/2df9abcd-45d0-437f-9a36-167417fe7202/AI_logo_white (2).png', 'name': 'logo', 'width': '', 'height': ''}, 'sameAs': ['https://whatsapp.com/channel/0029VafDe8HCBtxLV2PpRA2l', 'https://twitter.com/analyticsinme', 'https://in.pinterest.com/analyticsinsightsubmissions/_created/', 'https://www.instagram.com/analyticsinsightmagazine/', 'https://www.facebook.com/analyticsinsight.net', 'https://news.google.com/publications/CAAiEDD0Ze78owxVdNti611RNvQqFAgKIhAw9GXu_KMMVXTbYutdUTb0?hl=en-IN&gl=IN&ceid=IN%3Aen', 'https://t.me/analyticsinsightmag', 'https://www.youtube.com/channel/UCgF2J0b46YP0vvVEbgL_GuQ', 'https://www.linkedin.com/company/analytics-insight/'], 'id': 'https://www.analyticsinsight.net'}",Implementing Artificial Intelligence Talent to Fill in Companies Skill Gap,2021-01-27T02:28:59Z,2021-01-27T02:28:59Z,Artificial Intelligence,Implementing Artificial Intelligence Talent to Fill in Companies Skill Gap,,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.analyticsinsight.net'}, {'@type': 'ListItem', 'position': 2, 'name': 'Artificial Intelligence', 'item': 'https://www.analyticsinsight.net/artificial-intelligence'}, {'@type': 'ListItem', 'position': 3, 'name': 'Implementing Artificial Intelligence Talent to Fill in Companies Skill Gap', 'item': 'https://www.analyticsinsight.net/artificial-intelligence/implementing-artificial-intelligence-talent-to-fill-in-companies-skill-gap'}]",N/A,N/A,What is AI and Data Science Engineering? ,"{'@type': 'WebPage', '@id': 'https://www.analyticsinsight.net/artificial-intelligence/implementing-artificial-intelligence-talent-to-fill-in-companies-skill-gap'}",2021-01-27T02:28:59Z,,,,,,"How the Implementation of AI Talent feeds Organisations Skill Gap.There is an international shortage of artificial intelligence talent and labour markets across the world cannot keep up with the demand for developers. There is also a shortage of mathematicians and scientists who can develop new and innovative artificial intelligence (AI) technology..A study by Microsoft and IDC reveals that the shortage of workers with artificial intelligence skills has prevented companies that want to embrace AI from being capable of doing so. Enterprises must discover creative ways to supplement the talent they require to initiate AI projects across industries until highly skilled AI developers enter the workforce. Those projects could involve voice, image, or pattern recognition allowing autonomous movement or simulating realistic conversations. Such innovations can strengthen a new generation of healthcare tools and smart home devices..Companies across all industries have been struggling to secure top AI talent from a pool that is not expanding fast enough. Even during the economic disruption and layoffs due to the COVID-19 pandemic, the demand for AI talent has been robust. Leaders are looking to minimise costs through automation and efficiency. In such a scenario, companies need not solely be looking for fresh graduates. Instead, they need to actively start investing in training current employees and recruiting people with AI-adjacent skills..To get deep insights into it, let's look at how companies can implement AI talent to feed the skill gap effectively:.Build a team of In-house Expertise.Companies in niche areas and smaller organisations should not be too stubborn about recruiting preferences. Part of this requires not looking for people who already have AI experience. Instead, hire employees who show the hard and soft skills which could eventually make them a valuable asset of an artificial intelligence team. They add flexibility and enthusiasm for learning, complex problem-solving skills, data visualisation and analytics, understanding in mathematics, and security..Invest in Learning.Company leadership often talk about a nice game for continuous growth and learning, but they don't tangibly invest time and resources for people to do so. To save time and invest in resources effectively, companies need to focus on robust skills development in the field of AI. However, this might not provide the expected results..Companies can start small, but the leadership team requires creating the time and resources people need to commit to learning artificial intelligence skills and technologies. Ad-hoc opportunities don't likely deliver results a company wants. Besides, someone who is worried about if the manager is looking over the individual's shoulder and wondering the reason why the person is not doing a real job cannot concentrate on meaningful learning..Although skill development reaches its potential when the teachings are grounded in real projects, businesses often are not equipped to train their employees in emerging technologies like AI. Managers should encourage employees to opt for related courses from Coursera, Udacity, Datacamp, etc. They also should support these endeavours by offering employees time to learn the skills they need..Structure Team According to the Problem.The composition of an AI team depends on the problem being solved, the team's approach, and the level of incorporation required with the development team to support production. However, there are a few things to keep in mind. First, hiring solo AI talent is not advisable. Second, AI professionals rely on partnership for ideas and can feel isolated if they are the only member of a larger team. Third, companies may begin with a small team to validate data and the team's ideas, regardless of the domain. Finally, companies need to ensure that they have a robust AI strategy before expanding the team..Disclaimer: Analytics Insight does not provide financial advice or guidance. Also note that the cryptocurrencies mentioned/listed on the website could potentially be scams, i.e. designed to induce you to invest financial resources that may be lost forever and not be recoverable once investments are made. You are responsible for conducting your own research (DYOR) before making any investments. Read more here.","{'@type': 'WebPage', 'url': 'https://www.analyticsinsight.net/artificial-intelligence/implementing-artificial-intelligence-talent-to-fill-in-companies-skill-gap', 'primaryImageOfPage': {'@type': 'ImageObject', 'url': 'https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/01/Artificial-Intelligence-16.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true', 'width': '1200', 'height': '675'}}",,,,,,,,,,,https://media.assettype.com/analyticsinsight/import/wp-content/uploads/2021/01/Artificial-Intelligence-16.jpg?w=1200&h=675&auto=format%2Ccompress&fit=max&enlarge=true,,,,,,,,,,,,,,,,,
