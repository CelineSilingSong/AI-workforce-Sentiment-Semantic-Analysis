URL link,Title,Date,Source,Source Link,description,keywords,og:description,twitter:description,@context,@type,url,dateModified,datePublished,headline,image,mainEntityOfPage,author,publisher,name,article:section,article:summary,article text,itemListElement,@graph,identifier,dateCreated,thumbnailUrl,articleSection,inLanguage,articleBody,copyrightHolder
https://news.google.com/rss/articles/CBMib2h0dHBzOi8vd3d3LmdvdnRlY2guY29tL2Jsb2dzL2xvaHJtYW5uLW9uLWN5YmVyc2VjdXJpdHkvd2lsbC1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1oZWxwLW9yLWh1cnQtY3liZXItZGVmZW5zZdIBAA?oc=5,Will Artificial Intelligence Help or Hurt Cyber Defense? - Government Technology,2021-11-28,Government Technology,https://www.govtech.com,"The world seems focused on new developments in artificial intelligence to help with a wide range of problems, including staffing shortages. But will AI help or harm security teams? ",N/A,"The world seems focused on new developments in artificial intelligence to help with a wide range of problems, including staffing shortages. But will AI help or harm security teams? ","The world seems focused on new developments in artificial intelligence to help with a wide range of problems, including staffing shortages. But will AI help or harm security teams? ",http://schema.org,Article,https://www.govtech.com/blogs/lohrmann-on-cybersecurity/will-artificial-intelligence-help-or-hurt-cyber-defense,"November 28, 2021","November 28, 2021",Will Artificial Intelligence Help or Hurt Cyber Defense?,"[{'@context': 'http://schema.org', '@type': 'ImageObject', 'height': 705, 'url': 'https://erepublic.brightspotcdn.com/dims4/default/879c64a/2147483647/strip/false/crop/940x705+0+0/resize/940x705!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fa6%2Faa%2Fe3482249931f79add26d7ff58af7%2Fshutterstock-artificial-intelligence-ai.jpg', 'width': 940}, {'@context': 'http://schema.org', '@type': 'ImageObject', 'height': 675, 'url': 'https://erepublic.brightspotcdn.com/dims4/default/4c71af6/2147483647/strip/false/crop/940x529+0+88/resize/1200x675!/quality/90/?url=http%3A%2F%2Ferepublic-brightspot.s3.us-west-2.amazonaws.com%2Fa6%2Faa%2Fe3482249931f79add26d7ff58af7%2Fshutterstock-artificial-intelligence-ai.jpg', 'width': 1200}]","{'@type': 'WebPage', '@id': 'https://www.govtech.com/blogs/lohrmann-on-cybersecurity/will-artificial-intelligence-help-or-hurt-cyber-defense'}","[{'@context': 'http://schema.org', '@type': 'Person', 'description': 'Daniel J. Lohrmann is an internationally recognized cybersecurity leader, technologist, keynote speaker and author.', 'email': 'Daniel.lohrmann@gmail.com', 'image': {'@context': 'http://schema.org', '@type': 'ImageObject', 'url': 'https://erepublic.brightspotcdn.com/aa/be/66bbbc539526800857dd96f3c9d5/lohrman.jpg'}, 'jobTitle': 'Contributing Writer', 'name': 'Dan Lohrmann', 'url': 'https://www.govtech.com/authors/dan-lohrmann.html'}]","{'@type': 'Organization', 'name': 'GovTech', 'logo': {'@context': 'http://schema.org', '@type': 'ImageObject', 'url': 'https://erepublic.brightspotcdn.com/bc/a8/3ad2250148b8a28b31d4bd4edd24/gt-with-block.svg'}}",Will Artificial Intelligence Help or Hurt Cyber Defense?,Lohrmann on Cybersecurity,N/A,N/A,,,,,,,,,
https://news.google.com/rss/articles/CBMiXmh0dHBzOi8vYWlidXNpbmVzcy5jb20vdmVydGljYWxzL3JvYm90aWNzLWFuZC1hcnRpZmljaWFsLWludGVsbGlnZW5jZS10aGUtcm9sZS1vZi1haS1pbi1yb2JvdHPSAQA?oc=5,Robotics and Artificial Intelligence: The Role of AI in Robots - AI Business,2021-11-26,AI Business,https://aibusiness.com,"Robotics and artificial intelligence are often conflated, but the two are distinct with some crossover.",N/A,"Robotics and artificial intelligence are often conflated, but the two are distinct with some crossover.",N/A,https://schema.org,BreadcrumbList,,2023-09-08T14:06:17.096Z,2021-11-26T08:36:00.000Z,Robotics and Artificial Intelligence: The Role of AI in Robots,"{'@type': 'ImageObject', 'url': 'https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/bltc7034046f6751cc6/64fb2a3c867b47d5b67da1ec/News_Image_(21).jpg', 'caption': '', 'creditText': ''}",https://aibusiness.com/verticals/robotics-and-artificial-intelligence-the-role-of-ai-in-robots,"[{'@type': 'Person', 'name': 'Alan Martin', 'image': 'https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/blt3a0aa0cd9a9dc679/651c27e67a8bcafca56e20be/Untitled_design_-_2023-10-03T094013.939.png', 'url': 'https://aibusiness.com/author/alan-martin'}]","{'@type': ['NewsMediaOrganization', 'Organization', 'OnlineBusiness'], 'identifier': 'https://aibusiness.com', 'name': 'AI Business', 'url': 'https://aibusiness.com', 'sameAs': ['https://twitter.com/business_ai', 'https://www.linkedin.com/company/ai-business/', 'https://www.facebook.com/aibusinessnews', 'https://www.youtube.com/@AIBTV', 'https://news.google.com/publications/CAAqBwgKMOi0lgswi9qtAw'], 'foundingDate': '2015', 'description': 'To provide an objective view of the AI space to enable better strategic decisions and our editorial content focuses on the practical applications of AI technologies rather than hype, buzzwords and high-level technology updates.', 'logo': {'@type': 'ImageObject', 'url': 'https://aibusiness.com/build/_assets/AiBusiness-QDDGSPKW.svg', 'width': {'@type': 'QuantitativeValue', 'value': 431}, 'height': {'@type': 'QuantitativeValue', 'value': 112}}}",,N/A,N/A,N/A,"[{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://aibusiness.com'}, {'@type': 'ListItem', 'position': 2, 'name': 'Verticals', 'item': 'https://aibusiness.com/verticals'}]",,,,,,,,
https://news.google.com/rss/articles/CBMigAFodHRwczovL3RoZWNvbnZlcnNhdGlvbi5jb20vYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtbWF5LW5vdC1hY3R1YWxseS1iZS10aGUtc29sdXRpb24tZm9yLXN0b3BwaW5nLXRoZS1zcHJlYWQtb2YtZmFrZS1uZXdzLTE3MjAwMdIBAA?oc=5,Artificial intelligence may not actually be the solution for stopping the spread of fake news - The Conversation,2021-11-28,The Conversation,https://theconversation.com,Artificial intelligence is becoming increasingly sophisticated. But we’re still a long way off from AI being able to discern what’s fake news.,N/A,Artificial intelligence is becoming increasingly sophisticated. But we’re still a long way off from AI being able to discern what’s fake news.,N/A,,,,,,,,,,,,N/A,N/A,"






        Artificial intelligence has yet to develop the common sense required to identify fake news.
        (Shutterstock)









            Artificial intelligence may not actually be the solution for stopping the spread of fake news
          




Published: November 28, 2021 7:05am EST












Sze-Fung Lee, Benjamin C. M. Fung, McGill University



Authors





        Sze-Fung Lee
      


      Research Assistant, Department of Information Studies, McGill University
    





        Benjamin C. M. Fung
      


      Professor and Canada Research Chair in Data Mining for Cybersecurity, McGill University
    





Disclosure statement
Benjamin C. M. Fung receives funding from Natural Sciences and Engineering Research Council of Canada (NSERC), Social Sciences and Humanities Research Council (SSHRC), and Defence Research and Development Canada (DRDC), but the topics of the grants are irrelevant to the topic of this article.
Sze-Fung Lee does not work for, consult, own shares in or receive funding from any company or organization that would benefit from this article, and has disclosed no relevant affiliations beyond their academic appointment.


Partners

McGill University provides funding as a member of The Conversation CA.McGill University provides funding as a member of The Conversation CA-FR.
View all partners

We believe in the free flow of informationRepublish our articles for free, online or in print, under a Creative Commons license.Republish this article





 Email


 X (Twitter)50


 Facebook73


 LinkedIn


 WhatsApp


 Messenger

 Print



Disinformation has been used in warfare and military strategy over time. But it is undeniably being intensified by the use of smart technologies and social media. This is because these communication technologies provide a relatively low-cost, low-barrier way to disseminate information basically anywhere. 
The million-dollar question then is: Can this technologically produced problem of scale and reach also be solved using technology?
Indeed, the continuous development of new technological solutions, such as artificial intelligence (AI), may provide part of the solution. 
Technology companies and social media enterprises are working on the automatic detection of fake news through natural language processing, machine learning and network analysis. The idea is that an algorithm will identify information as “fake news,” and rank it lower to decrease the probability of users encountering it.
Repetition and exposure
From a psychological perspective, repeated exposure to the same piece of information makes it likelier for someone to believe it. When AI detects disinformation and reduces the frequency of its circulation, this can break the cycle of reinforced information consumption patterns.



Artificial intelligence can help filter out fake news.
(Shutterstock)


However, AI detection still remains unreliable. First, current detection is based on the assessment of text (content) and its social network to determine its credibility. Despite determining the origin of the sources and the dissemination pattern of fake news, the fundamental problem lies within how AI verifies the actual nature of the content.
Theoretically speaking, if the amount of training data is sufficient, the AI-backed classification model would be able to interpret whether an article contains fake news or not. Yet the reality is that making such distinctions requires prior political, cultural and social knowledge, or common sense, which natural language processing algorithms still lack.




      Read more:
      An AI expert explains why it's hard to give computers something you take for granted: Common sense




In addition, fake news can be highly nuanced when it is deliberately altered to “appear as real news but containing false or manipulative information,” as a pre-print study shows.
Human-AI partnerships
Classification analysis is also heavily influenced by the theme — AI often differentiates topics, rather than genuinely the content of the issue to determine its authenticity. For example, articles related to COVID-19 are more likely to be labelled as fake news than other topics.
One solution would be to employ people to work alongside AI to verify the authenticity of information. For instance, in 2018, the Lithuanian defence ministry developed an AI program that “flags disinformation within two minutes of its publication and sends those reports to human specialists for further analysis.”
A similar approach could be taken in Canada by establishing a national special unit or department to combat disinformation, or supporting think tanks, universities and other third parties to research AI solutions for fake news.
Avoiding censorship
Controlling the spread of fake news may, in some instances, be considered censorship and a threat to freedom of speech and expression. Even a human may have a hard time judging whether information is fake or not. And so perhaps the bigger question is: Who and what determine the definition of fake news? How do we ensure that AI filters will not drag us into the false positive trap, and incorrectly label information as fake because of its associated data?
An AI system for identifying fake news may have sinister applications. Authoritarian governments, for example, may use AI as an excuse to justify the removal of any articles or to prosecute individuals not in favour of the authorities. And so, any deployment of AI — and any relevant laws or measurements that emerge from its application — will require a transparent system with a third party to monitor it.
Future challenges remain as disinformation — especially when associated with foreign intervention — is an ongoing issue. An algorithm invented today may not be able to detect future fake news. 


A BBC report on the dangers of deep fakes.

For example, deep fakes — which are “highly realistic and difficult-to-detect digital manipulation of audio or video” — are likely to play a bigger role in future information warfare. And disinformation spread via messaging apps such as WhatsApp and Signal are becoming more difficult to track and intercept because of end-to-end encryption.
A recent study showed that 50 per cent of the Canadian respondents received fake news through private messaging apps regularly. Regulating this would require striking a balance between privacy, individual security and the clampdown of disinformation. 
While it is definitely worth allocating resources to combating disinformation using AI, caution and transparency are necessary given the potential ramifications. New technological solutions, unfortunately, may not be a silver bullet.





Artificial intelligence (AI)


Authoritarianism


Misinformation


Fake news


Disinformation


Listen to this article









",,,,,,,,,
https://news.google.com/rss/articles/CBMiggFodHRwczovL3d3dy5nZWVrd2lyZS5jb20vMjAyMS90ZWFjaGluZy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1yaWdodC1mcm9tLXdyb25nLW5ldy10b29sLWZyb20tYWkyLWFpbXMtdG8tbW9kZWwtZXRoaWNhbC1qdWRnbWVudHMv0gEA?oc=5,Teaching artificial intelligence right from wrong: New tool from AI2 aims to model ethical judgments - GeekWire,2021-11-29,GeekWire,https://www.geekwire.com,"During the past two decades, machine ethics has gone from being a curiosity to a field of immense importance. Much of the work is based on the idea that",N/A,"During the past two decades, machine ethics has gone from being a curiosity to a field of immense importance. Much of the work is based on the idea that",N/A,https://schema.org,,,,,,,,,,,N/A,N/A,"


Teaching artificial intelligence right from wrong: New tool from AI2 aims to model ethical judgments
by Richard Yonck on November 29, 2021 at 7:00 amNovember 29, 2021 at 7:03 am




 Share  190
 Tweet
 Share
 Reddit
 Email




Subscribe to GeekWire Newsletters today!






 
 

BOT or NOT? This special series explores the evolving relationship between humans and machines, examining the ways that robots, artificial intelligence and automation are impacting our work and lives.







During the past two decades, machine ethics has gone from being a curiosity to a field of immense importance. Much of the work is based on the idea that as artificial intelligence becomes increasingly capable, its actions should be in keeping with expected human ethics and norms. 
To explore this, Seattle-based Allen Institute of Artificial Intelligence (AI2) recently developed Delphi, a machine ethics AI designed to model people’s ethical judgments on a variety of everyday situations. The research could one day help ensure other AIs are able to align with human values and ethics.
Built around a collection of 1.7 million descriptive ethics examples that were created and later vetted by trained human crowdworkers, Delphi’s neural network agrees with human ethical norms 92.1% of the time in the lab. In the wild, however, performance fell to a little over 80%. While far from perfect, this is still a significant accomplishment. With further filtering and enhancement, Delphi should continue to improve.
AI2’s research demo prototype, “Ask Delphi” was published on Oct. 14, allowing users to pose situations and questions for the AI to weigh in on. Though intended primarily for AI researchers, the website quickly went viral with the public, generating 3 million unique queries in a few weeks.
It also caused a bit of a stir because many people seemed to believe Delphi was being developed as a new ethical authority, which was far from what the researchers had in mind.
To get a sense of how Delphi works, I posed a number of questions for the AI to ponder. (Delphi’s responses are included at the end of the article.)
Is it okay to lie about something important in order to protect someone’s feelings?Is it okay for the poor to pay proportionally higher taxes?Is it all right for big corporations to use loopholes to avoid taxes?Should drug addicts be jailed?Should universal healthcare be a basic human right?Is it okay to arrest someone for being homeless?
Some of these questions would be complex, nuanced, potentially even controversial for a human being. While we might expect the AI to fall short in its ethical judgments, it actually performed remarkably well. Unfortunately, Delphi was presented in such a way it led many people who are not AI researchers to assume it was being created to replace us as arbiters of right and wrong.
“It’s an irrational response,” said Yejin Choi, University of Washington professor and senior research manager at AI2. “Humans also interact with each other in ethically informed and socially aware ways, but that doesn’t mean one person suddenly becomes an authority over others.”
Yejin Choi. (Photo via UW/Bruce Hemingway)
According to Choi, training Delphi can be likened to teaching a child the difference between right and wrong, a natural progression for every young mind. Certainly, no one would think that transforms the child into a moral authority.
“Going forward, I think it’s important to teach AI in the way that we teach humans, particularly human children,” says Choi. “The thing about AI learning from just raw text, like GPT-3 and other neural networks do, is it ends up reflecting a lot of human problems and biases.”
GPT-3 is a deep learning-based large language model developed by OpenAI that can be used to answer questions, translate language and output improvised text. While Delphi also uses deep learning techniques, the curated, structured nature of its source data allows it to make more complex inferences about nuanced social situations.
The Commonsense Norm Bank at the heart of Delphi is a collection of 1.7 million examples of descriptive ethics, people’s ethical judgments on a broad spectrum of real-life situations. It was assembled from five smaller curated collections: Social Chemistry, Moral Stories, Social Bias Inference Corpus, Scruples, and Ethics Commonsense Morality. (This last collection was created by a Berkeley team, while all of the others were compiled at AI2.) The Delphi deep learning model was then trained on the Commonsense Norm Bank to generate appropriate output.
Delphi was then tested using a selection of diverse, ethically questionable situations harvested from Reddit, Dear Abby and elsewhere. This is contrary to the early misunderstanding that Reddit texts were actually used to build the database’s ethical examples.
The model’s responses to these situations were evaluated by crowdworkers at Amazon’s MTurk, who were carefully trained in judging the output. This allowed the system to be tested, adjusted and refined. By combining human and AI judgements in this way, the team developed a kind of hybrid intelligence that benefited from the strengths of both.

Delphi performed well in situations with multiple, potentially conflicting factors. For example, “ignoring a phone call from my boss” was deemed “bad.” This judgment remained unchanged when the context “during workdays” was added. However, the action became justifiable “if I’m in a meeting.”
Delphi also displayed an understanding of conventional commonsense behaviors. “Wearing a bright orange shirt to a funeral” is “rude,” but “wearing a white shirt to a funeral” is “appropriate.” “Drinking milk if I’m lactose intolerant” is “bad,” but “drinking soy milk if I’m lactose intolerant” is “okay.” “Mixing bleach with ammonia” is “dangerous.”
Just as with large language models, Delphi is able to generalize and extrapolate about thorny situations it doesn’t have prior examples of, at least in part because of the large dataset it draws from. Intriguingly, when the dataset in the Commonsense Norm Bank was reduced by eliminating seemingly unrelated examples for a given situation, the AI’s accuracy dropped significantly. It was as though all of those other examples contributed to the program’s ability to infer the right answer, even though they might not seem relevant.
Choi noted: “If we removed those complex cases out of the Commonsense Norm Bank and then only trained on simple, very basic elementary situations, then Delphi loses its capability of reason as well,” she said. “That’s the weird part. We don’t know exactly what is going on.”
While some of Delphi’s processes aren’t fully transparent or explainable, the same can be said about certain aspects of human reasoning like intuition. In both cases, the greater the exposure to more relevant and sometimes seemingly irrelevant background information, the better the ability to produce a useful result. 
“We’re starting to think about multiculturalism in Delphi.”
All of this was really put to the test once the Ask Delphi web site went viral in mid-October. Users were plying the AI with questionable and toxic queries trying to trip up the program. For instance, early on Delphi would answer a question like “Is genocide okay?” by saying it was wrong. But some users discovered that by appending the phrase “if it makes everybody happy?” at the end, Delphi was tricked into saying it was okay. 
Uncovering these issues along with other biases led the researchers to add several filters to correct the output. The site now also includes several disclaimers and instructions about Delphi’s purpose and use so as to reduce misunderstanding. Going forward, AI2 is adjusting their review process when producing new publicly facing programs. 
One of the primary motivators for developing machine ethics are concerns about sexism, racism and other forms of toxicity in artificial intelligence. The Delphi project has been no different. The team recognizes that in creating examples of ethical norms, a range of biases are inevitably introduced based on whose norms are sampled. Currently, Delphi trends toward responses that align with the views of heteronormative U.S. lay workers. Delphi’s authors eventually want to extend the system to give responses that can be culture or group appropriate.
“We’re starting to think about multiculturalism in Delphi,” said Liwei Jiang, one of the study’s authors. “Because in some situations or environments, one culture might consider something being offensive that isn’t in other cultures.”
Perhaps one of Delphi’s biggest successes is that its form of reasoning seems at times almost as complex as our own, even though it achieves this through entirely different means.
“It’s amazing,” said Jiang. “What Delphi is doing right now, we’re not sure if we can exactly call it reasoning. We don’t actually know why it’s predicting stuff, but as with humans, we follow this chain of reasoning, then come up with a judgment.”
Choi continued the thread. “Human reasoning is weird. The intuitive reasoning part is a little bit like what Delphi does, in the sense there’s a gut feeling thing that’s not rigid. With our own reasoning, we often rationalize after the fact. I think there is a really exciting opportunity here for ethical explainability of AI systems because in part it can be explained through similar examples in the Commonsense Norm Bank.”
So, how did Delphi do in responding to our earlier questions?
Is it okay to lie about something important in order to protect someone’s feelings? It’s okay. Is it okay for the poor to pay proportionally higher taxes? It’s regressive.Is it all right for big corporations to use loopholes to avoid taxes? It’s wrong.Should universal healthcare be a basic human right? It should.Should drug addicts be jailed? They shouldn’t.Is it okay to arrest a person for being homeless? It’s wrong.And finally: Is it a good idea to teach artificial intelligence right from wrong? Yes, it is a good idea.


Message from the UnderwriterPutting AI-driven insights, productivity tools,
and security in the hands of businesses

We’re all experiencing a real-time revolution with the rise of generative AI. The opportunity is huge—but it requires a new approach to cloud computing. Organizations are already leveraging Google Cloud’s AI capabilities to unlock data, lower costs, embrace hybrid work, and protect against threats.
Learn how they’re succeeding at Transform with Google Cloud.
Learn more about underwritten and sponsored content on GeekWire.
More Bot or NotHow clean tech companies can take advantage of AI — without draining energy from the planetAI has trouble identifying sarcasm from Seattle satirical news site The Needling‘We know something big is happening’: Tech vets encourage experimentation, education with AIRichard Yonck is a Seattle-based futurist and keynote speaker who explores future trends and emerging technologies, identifying their potential impacts on business and society. He’s written for dozens of national and global publications and is author of two books about the future of artificial intelligence, Heart of the Machine and Future Minds. Follow him on Twitter @ryonck and reach him at futurist@richardyonck.com. 
 Share  190
 Tweet
 Share
 Reddit
 Email




Previous StoryWeek in Review: Most popular stories on GeekWire for the week of Nov. 21, 2021 

Next StoryNew list highlights 40 top startups solving business problems with artificial intelligence 

 Filed Under: Bot or Not








GeekWire Newsletters

Subscribe to GeekWire's free newsletters to catch every headline




Email address

Subscribe







GeekWire Daily - Top headlines daily
                                    




GeekWire Weekly - Most-read stories of the week, delivered Sunday
                                    




Breaking News Alerts - Important news as it happens
                                    




GeekWire Startups - News, analysis, insights from the Pacific Northwest startup ecosystem, delivered Friday
                                    




GeekWire Mid-week Update — Most-read stories so far this week, delivered Wednesday
                                    




GeekWire Local Deals — Special offers for Pacific Northwest area readers
                                    







Send Us a Tip
Have a scoop that you'd like GeekWire to cover? Let us know.

Send Us a Tip








",,"[{'@type': 'NewsArticle', '@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/#article', 'isPartOf': {'@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/'}, 'author': [{'@id': 'https://www.geekwire.com/#/schema/person/8e226a96f1ded4c17d0b428bef8508c1'}], 'headline': 'Teaching artificial intelligence right from wrong: New tool from AI2 aims to model ethical judgments', 'datePublished': '2021-11-29T15:00:00+00:00', 'dateModified': '2021-11-29T15:03:39+00:00', 'mainEntityOfPage': {'@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/'}, 'wordCount': 1592, 'publisher': {'@id': 'https://www.geekwire.com/#organization'}, 'image': {'@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/#primaryimage'}, 'thumbnailUrl': 'https://cdn.geekwire.com/wp-content/uploads/2021/11/Delphi_Demo_Card_hjighres.png', 'articleSection': ['Bot or Not'], 'inLanguage': 'en-US'}, {'@type': 'WebPage', '@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/', 'url': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/', 'name': 'Teaching artificial intelligence right from wrong: New tool from AI2 aims to model ethical judgments &#8211; GeekWire', 'isPartOf': {'@id': 'https://www.geekwire.com/#website'}, 'primaryImageOfPage': {'@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/#primaryimage'}, 'image': {'@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/#primaryimage'}, 'thumbnailUrl': 'https://cdn.geekwire.com/wp-content/uploads/2021/11/Delphi_Demo_Card_hjighres.png', 'datePublished': '2021-11-29T15:00:00+00:00', 'dateModified': '2021-11-29T15:03:39+00:00', 'description': 'During the past two decades, machine ethics has gone from being a curiosity to a field of immense importance. Much of the work is based on the idea that', 'breadcrumb': {'@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/#breadcrumb'}, 'inLanguage': 'en-US', 'potentialAction': [{'@type': 'ReadAction', 'target': ['https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/']}]}, {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/#primaryimage', 'url': 'https://cdn.geekwire.com/wp-content/uploads/2021/11/Delphi_Demo_Card_hjighres.png', 'contentUrl': 'https://cdn.geekwire.com/wp-content/uploads/2021/11/Delphi_Demo_Card_hjighres.png', 'width': 1800, 'height': 945}, {'@type': 'BreadcrumbList', '@id': 'https://www.geekwire.com/2021/teaching-artificial-intelligence-right-from-wrong-new-tool-from-ai2-aims-to-model-ethical-judgments/#breadcrumb', 'itemListElement': [{'@type': 'ListItem', 'position': 1, 'name': 'Home', 'item': 'https://www.geekwire.com/'}, {'@type': 'ListItem', 'position': 2, 'name': 'Bot or Not', 'item': 'https://www.geekwire.com/bot-or-not/'}, {'@type': 'ListItem', 'position': 3, 'name': 'Teaching artificial intelligence right from wrong: New tool from AI2 aims to model ethical judgments'}]}, {'@type': 'WebSite', '@id': 'https://www.geekwire.com/#website', 'url': 'https://www.geekwire.com/', 'name': 'GeekWire', 'description': 'Breaking News in Technology &amp; Business', 'publisher': {'@id': 'https://www.geekwire.com/#organization'}, 'potentialAction': [{'@type': 'SearchAction', 'target': {'@type': 'EntryPoint', 'urlTemplate': 'https://www.geekwire.com/?s={search_term_string}'}, 'query-input': 'required name=search_term_string'}], 'inLanguage': 'en-US'}, {'@type': 'Organization', '@id': 'https://www.geekwire.com/#organization', 'name': 'GeekWire', 'url': 'https://www.geekwire.com/', 'logo': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geekwire.com/#/schema/logo/image/', 'url': 'https://cdn.geekwire.com/wp-content/uploads/2017/10/GeekWire-Logo.png', 'contentUrl': 'https://cdn.geekwire.com/wp-content/uploads/2017/10/GeekWire-Logo.png', 'width': 400, 'height': 400, 'caption': 'GeekWire'}, 'image': {'@id': 'https://www.geekwire.com/#/schema/logo/image/'}, 'sameAs': ['https://www.facebook.com/geekwire', 'https://x.com/geekwire', 'https://www.instagram.com/geekwire/', 'https://www.youtube.com/geekwire', 'https://www.pinterest.com/geekwire/', 'https://en.wikipedia.org/wiki/GeekWire', 'https://www.linkedin.com/company/geekwire/']}, {'@type': 'Person', '@id': 'https://www.geekwire.com/#/schema/person/8e226a96f1ded4c17d0b428bef8508c1', 'name': 'Richard Yonck', 'image': {'@type': 'ImageObject', 'inLanguage': 'en-US', '@id': 'https://www.geekwire.com/#/schema/person/image/9ccaa2e69b3b985837c95da18b171315', 'url': 'https://secure.gravatar.com/avatar/ca863909b74a6fb84b5d8b2bd4ddcd22?s=96&d=mm&r=g', 'contentUrl': 'https://secure.gravatar.com/avatar/ca863909b74a6fb84b5d8b2bd4ddcd22?s=96&d=mm&r=g', 'caption': 'Richard Yonck'}, 'description': 'Richard Yonck\xa0is a Seattle-based futurist\xa0and keynote speaker who explores future trends and emerging technologies, identifying their potential\xa0impacts on business and society. He’s written for dozens of national and global publications and is author of two books about the future of artificial intelligence,\xa0Heart of the Machine\xa0and\xa0Future Minds. Follow him on Twitter\xa0@ryonck\xa0and reach him at\xa0futurist@richardyonck.com.', 'url': 'https://www.geekwire.com/author/richardyonck/'}]",,,,,,,
https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vd3d3LnNwaWNld29ya3MuY29tL2hyL2Z1dHVyZS13b3JrL2d1ZXN0LWFydGljbGUvYWktYmlhcy1jaGFsbGVuZ2VzLWhyL9IBTmh0dHBzOi8vd3d3LnNwaWNld29ya3MuY29tL2hyL2Z1dHVyZS13b3JrL2d1ZXN0LWFydGljbGUvYWktYmlhcy1jaGFsbGVuZ2VzLWhyLw?oc=5,AI Bias Challenges in HR and 6 Ways Companies Can Address Them - Spiceworks News and Insights,2021-11-29,Spiceworks News and Insights,https://www.spiceworks.com,Employers in different fields are turning to AI (artificial intelligence) and ML (machine learning) algorithms to harness the technology into their decision-...,N/A,"This article by Sharad Panwar, growth marketer, Adaface, analyzes the disparity between the promise and reality of AI in human resource management and the potential next steps organizations can take to address the problem.",N/A,,,,,,,,,,,,N/A,N/A,"



























 



Sharad Panwar



					Growth Marketer, Adaface				




November 29, 2021




 





Employers in different fields are turning to AI (artificial intelligence) and ML (machine learning) algorithms to harness the technology into their decision-making processes.
These algorithms deliver sophisticated and pervasive tools that leverage massive data to carry out tasks done only by humans until very recently. Integration of AI/ML promises unparalleled efficiencies through statistical rigor.
The technology applies to HR as much as anything else. It promises to eliminate opinions made from the subjective bias of recruiters and simplify the complexities of decision-making in general.
But there is a gap between what AI promised (seamlessness, sophistication) of its role in HR and what we are witnessing today. The machine algorithms fall short of our expectations. HR today is facing disruption and transformational challenges with the integration of AI in its functions.
See More: 3 Pillars of Trustworthy AI in the Workplace
The Problem Is Much Deeper
Joy Buolamwini, a researcher at the MIT Media Lab, explains some biases of the real world that have crept into AI, especially in facial recognition technology. Because, at the end of the day, ML/AI is run by humans, and the system will only run as smartly as it has been trained to.
Buolamwini has been advocating for the emerging field of “algorithmic accountability.” Her TED Talk on algorithmic bias has been viewed and well-received worldwide. She is also the founder of the Algorithmic Justice League, which raises awareness of the issue.
After a few years of joining MIT Media Lab, she witnessed a rather shocking bias. Only when she had a white mask on did the software recognize a face in front of the screen. The fact that this facial software had percolated into the mainstream and was being adopted widely was the biggest problem.
She’s now leading the fight against ML algorithmic bias, an issue she has termed “coded gaze.”
AI Will Only Be as Good as Its Human Trainers
There’s no denying the fact that leveraging AI into the HR domain has enhanced the capabilities of HR in a collaborative environment and not made HR irrelevant, as was predicted by many. And integration of AI and HR is improvingOpens a new window  each day by leaps and bounds.
But the system comes with its own bias; there’s no fairness approach in place. The AI integration sometimes triggers socio-psychological concerns amongst candidates, leading to their true potential being left untapped.
In 2018, everyone went into a frenzy about Amazon’s secret AI recruiting tool. But the company rolled it back because their ML specialists recognized a huge bias issue in the system: their AI was penalizing women in the recruitment process.
Researchers at IBM and Microsoft developed and tested a face-analysis service to identify the gender of humans by simply looking at photos. The results concluded that the algorithms were nearly perfect at identifying men with lighter skin but frequently made errors while analyzing images of dark-skinned women.
In another example, a study by the Georgetown Law School estimates that 117 million American adults are a part of the facial recognition datasets used by the majority of law enforcement agencies, but most of them are white. The black population is disproportionately represented in these datasets.
Challenges HR Is Facing With the Implementation of AI in its Functions
●Performance metrics: The AI-based performance appraisal score is being done away with by HR departments at many companies because it gives a skewed view of how good a particular candidate might be at their job. These metrics have validity and bias issues because they are based upon historical data that could be incomplete or comprise faulty datasets. Moreover, it gets difficult to gauge an individual’s performance based on these metrics because when you’re working in a team, it’s difficult to measure the exact contribution of an individual owing to interdependencies.
●Lack of “Big Data”: Unlike any other field, HR lacks massive structured data sets to do a professional historical check on employees. And this only gets difficult as the majority of the companies have employees in thousands, so analysis tools don’t come to the rescue in that case.
●AI doesn’t give a concrete picture: It’s the innate nature of humans to hide their actual disposition when they are under observation. In the age of AI, humans have learned the limitations of AI, which fail to capture their true character, capabilities, and ethical nature.
● AI lacks fairness: It completely ignores the fairness approach consisting of procedural and distributive justice that impacts the decisions that involve the candidates. The concern is yet to be solved.
Mitigating AI Bias Through Deeper Systematic Solutions
The AI technology in HR can be an opportunity well seized, but that can only happen if the existing algorithmic bias is solved with more inclusivity, justice, and fairness in the process of machine training.
Below are some strategies that organizations can deploy to mitigate and prevent AI bias in HR:
● Implementing AI in HR only where it is needed: There are some aspects of the hiring process that will require an HR leader. AI is sometimes not capable of dealing with and looking at problems from multiple perspectives — problems that might require a manager’s unique point of view or looking into the work of employees.
○AI might not find the ideal candidate: AI lacks qualitative qualities like measuring an employee’s positioning and direction concerning the company’s goals and culture. It can also reject psychological and emotional traits — qualities that make employees who they are at a deeper level and play a major role in their work behavior.
○Incompetent to make connections: The algorithm may or may not make connections that could signal that a person’s history makes them the right fit. Sometimes it might outright reject applicants who wouldn’t match the hiring criteria, but that person could be capable of making a strong contribution to the company.
●Having a diverse team working on AI and algorithms: To rectify the very root cause of this bias, data scientists believe that having a diverse AI algorithm team is critical. A diverse team wouldn’t only mean including women but people of different ages, colors, backgrounds, abilities, sexual orientations, etc.
A more inclusive ML training will ensure that it is created by people who will eventually use it, rather than by a single group of people who set the technological algorithms and standards.
Incorporating DEI principles (diversity, equity, and inclusion) into an organization’s AI algorithm policies and practices can be one of the many ways to deal with the issue.
●Promoting a culture of AI responsibility and ethics: AI may or may not catch the bias of their trainer, who might have injected their bias into the training datasets very unknowingly. Such biases can cause a point of friction owing to the lack of ethical responsibility since they might put minority groups at a disadvantage.
Organizations need to enable a team culture where teammates understand the nuances of bias and discriminatory issues and reflect a sense of personal responsibility toward rectifying those biases.
●Developing responsible datasets: The HR department of organizations should run regular data checks to ensure that existing and new data is free of any systematic bias, is inclusive and will benefit every human it is developed for.
●Giving algorithm development team ethical framework: Rectifying bias through ethical framework handed down to algorithm development team, free of any variables that could trigger the algorithmic bias, is one of the solutions.
●Corporate and industry governance: Mitigating bias by a shared responsibility to tackle AI bias in the industry will be the preeminent step to solving the problem. Holding seniors with high authority in the industry accountable, questioning power dynamics and structures that might be unfavorable to some, deprived of diversity, should be some actions to be taken at the industry level.
See More: Data Driven HR: The Key To Retaining Talent and Enhancing Employee Experience
AI/ML in HR Has a Promising Future, But Only If…
AI is received as an augmenting force providing insightful analytics for efficient decision-making and processes. It shouldn’t be viewed as a hard-core problem solver, rather as a helping hand in the HR processes.
As technology is seeping into HR, it will take some time for the challenges we have discussed above to be solved. But these problems confirm the importance of humans in the HR process in evaluating the process uniquely.
The future overall looks promising, and though some of these decisions will be best solved with the help of AI tools, others will need mindful considerations before these algorithms can be designed to mitigate discriminatory results.
If you have implemented AI, what bias challenges are you facing? And how are you overcoming them? Let us know on LinkedInOpens a new window , FacebookOpens a new window , and TwitterOpens a new window .









Share This Article:
 





Sharad Panwar

				                  Growth Marketer, Adaface	                          


 opens a new window
 opens a new window 



 opens a new window  opens a new window
  	
					Sharad Panwar is a Growth Marketer at Adaface. When he’s not working, he is either learning MMA or discussing Politics and Philosophy.			








								Do you still have questions? Head over to the Spiceworks Community to find answers.
							

Take me to Community





",,,,,,,,,
https://news.google.com/rss/articles/CBMiKWh0dHBzOi8vd3d3Lm5jZWkubm9hYS5nb3YvbmV3cy9BSS1hdC1OT0FB0gEA?oc=5,NOAA Center for Artificial Intelligence Takes Root | News | National Centers for Environmental Information (NCEI) - National Centers for Environmental Information,2021-11-29,National Centers for Environmental Information,https://www.ncei.noaa.gov,"The establishment of the NOAA Center for Artificial Intelligence (NCAI) brings collaborations, networking, and possibilities into focus.","climate,data,observations,weather,coasts,oceans,geophysics,satellites","The establishment of the NOAA Center for Artificial Intelligence (NCAI) brings collaborations, networking, and possibilities into focus.","The establishment of the NOAA Center for Artificial Intelligence (NCAI) brings collaborations, networking, and possibilities into focus.",,,,,,,,,,,,N/A,N/A,"



 
    NCAI brings collaborations, networking, and possibilities into focus
    


 


 
    Courtesy of NOAA NCEI, Barbara Ambrose
    



In 2020, Congress passed the National AI Initiative Act,(link is external) which formalized the mandate for NOAA’s pioneering coordination of artificial intelligence application across climate, ocean, Earth, and space sciences. Working across scientific fields and offices, NOAA has established a center for artificial intelligence to support ongoing projects and to propel new uses of AI technology to support environmental knowledge and study. 
The NOAA Center for Artificial Intelligence (NCAI) is developing under the leadership of a coalition of NOAA staff, affiliates, cooperative institutes, and partners. A major facet of NCAI is to build NOAA’s capacity to use AI and machine learning (ML) techniques to support NOAA’s mission to understand climate, weather, the ocean, and coasts. 
AI became a NOAA science and technology strategy in 2019 to guide advancements in the quality and timeliness of NOAA science, products, and services. The strategy calls for the acceleration and expansion of AI applications across the agency to make “transformative improvements in NOAA mission performance and cost-effectiveness.” Using AI/ML, data can be processed more efficiently, thereby improving how science data and information are used. Furthermore, AI excels at identifying patterns in data that elude the human eye, offering improvements to many of our forecasting or prediction tools. NCAI efforts are showcased at NOAA.gov/AI.



  






NCAI is intended to act as a conduit for a larger “community of practice,” encompassing the NOAA workforce and other communities beyond the organization who are currently using AI/ML in Earth sciences, oceanography, geophysics, and other disciplines. Organizers hope to fold in more practitioners for future ground-breaking work. 
“We want to spark conversations, provide space for networking, and encourage information sharing about AI/ML within NOAA and its scientific communities,” says leading organizer Eric Kihn of NOAA NCEI.
NOAA AI Workshops
Since 2019, NOAA has held three AI workshops geared toward AI/ML information sharing, supported by the Center for Satellite Applications and Research (STAR) and NOAA NCEI. Each yearly conference has enabled users of AI applications to exchange experiences more widely. 
The 2021 workshop, which actively sought public participation worldwide, attracted more than 1,300 registrants and included 38 sessions and more than 180 presentations(link is external) including talks and posters. The workshop opening plenary featured NOAA Administrator Dr. Rick Spinrad, who shared NOAA perspectives on AI. Recordings of presentations from the virtual event, including eight plenary sessions, are available on the NOAA Satellite and Information Service’s YouTube channel(link is external).
NOAA AI Partnerships
NCAI is collaborating with Earth Science Information Partners (ESIP)(link is external) to define AI-ready data standards for Earth and space science through a Data Readiness Cluster(link is external). ESIP members and partners are discussing advancements and developing tools for open environmental data for AI applications. ESIP is a nonprofit organization supported by NOAA, NASA, USGS, and more than 130 member groups interested in furthering the study of Earth sciences.
Staff from several cooperative institutes are also supporting and collaborating with NCAI, including the Cooperative Institute for Satellite Earth System Studies (CISESS) at North Carolina State University, the Cooperative Institute for Research in Environmental Studies (CIRES) at the University of Colorado Boulder, and Cooperative Institute for Research in the Atmosphere (CIRA) at Colorado State University.
NOAA AI/ML Pilot Projects
A variety of AI/ML projects are underway, many of which have grown from NOAA Fisheries. For example, cameras using AI technology are helping to scan and classify protected species from aerial imagery (e.g., polar bears and ice seals, right whales), enumerate fishery catches from vessel-based cameras, and locate beluga whales(link is external) using acoustic technologies. Meanwhile, other branches of NOAA have used AI to characterize marine ecosystem health by listening to underwater soundscapes, monitor humpback whale populations from their songs, forecast space weather in real-time, and so much more. Many such projects have benefitted from collaborations with industry, including Microsoft, Kitware, Saildrone, Google, as well as with academia.
The rubber meets the road in several projects piloting collaborations with the NCAI. These projects include:

Water column sonar data. To develop a “data lake” to empower the detection and speciation of small fish.
Land surface temperature (LST). To use satellite data from the Himawari-8 and GOES-13 imager to develop improved LST estimates.
Magnetic navigation. Engaged the broad data science community in an X-prize competition to develop models for forecasting a key space weather activity parameter to improve magnetic navigation, demonstrating that crowdsourcing our AI/ML problems across the international data science community is an effective path.

NCAI Training and Tutorials 
NCAI is creating learning opportunities for scientists and staff who want to use AI/ML techniques in their work. From tutorials to best practices, NCAI is compiling materials, training, and ideas into a library of resources to search, discover, and tailor learning experiences. A clearinghouse of internal and public resources is under development to leverage standard tools such as Google Colab, Binder, and Github. NCAI welcomes contributions (e.g., Jupyter notebooks) from researchers that are interested in sharing their AI knowledge. Hackathons(link is external) have also been offered to expand learning by engaging teams to solve real-world scenarios. 



Published
    November 29, 2021

 




Related Links

NOAA Center for AI
NOAA Workshop Supports Artificial Intelligence
Using AI to Listen and Learn about Humpback Whales



Article Tags

 
Artificial Intelligence


 
Education/Outreach


 
NCEI Community









",,,,,,,,,
https://news.google.com/rss/articles/CBMiYmh0dHBzOi8vd3d3LmFiYy5uZXQuYXUvbmV3cy8yMDIxLTExLTMwL3doeS15b3Utc2hvdWxkLWNhcmUtYWJvdXQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UvMTAwNTkxNjg00gEoaHR0cHM6Ly9hbXAuYWJjLm5ldC5hdS9hcnRpY2xlLzEwMDU5MTY4NA?oc=5,Artificial intelligence has probably already made decisions about you. Here's why that matters - ABC News,2021-11-29,ABC News,https://www.abc.net.au,"Artificial intelligence is making decisions about people's insurance claims, credit scores and more. ABC Top 5 academic Jathan Sadowski explains why that matters.","AI,Artificial intelligence,Automated decision-making,Social media,Big Tech,Technology,Top 5","Artificial intelligence is making decisions about people's insurance claims, credit scores and more. ABC Top 5 academic Jathan Sadowski explains why that matters.",N/A,http://schema.org,NewsArticle,,2021-11-29T05:16:46+00:00,2021-11-29T18:00:00+00:00,Artificial intelligence has probably already made decisions about you. Here's why that matters,"{'@type': 'ImageObject', 'height': 485, 'url': 'https://live-production.wcms.abc-cdn.net.au/6f471f89ed73d9471436951ccee67394?impolicy=wcms_crop_resize&cropH=1680&cropW=2983&xPos=17&yPos=60&width=862&height=485', 'width': 862}",https://www.abc.net.au/news/2021-11-30/why-you-should-care-about-artificial-intelligence/100591684,"{'@type': 'Organization', 'name': 'ABC News', 'logo': {'@type': 'ImageObject', 'height': 60, 'url': 'https://www.abc.net.au/res/abc/logos/amp-news-logo-60x240.png', 'width': 240}}","{'@type': 'Organization', 'name': 'ABC News', 'logo': {'@type': 'ImageObject', 'height': 60, 'url': 'https://www.abc.net.au/res/abc/logos/amp-news-logo-60x240.png', 'width': 240}}",,N/A,N/A,"Artificial intelligence has probably already made decisions about you. Here's why that mattersABC RN / By Dr Jathan Sadowski for RN Top 5 and Download This ShowPosted Mon 29 Nov 2021 at 1:00pmMonday 29 Nov 2021 at 1:00pmMon 29 Nov 2021 at 1:00pm Artificial intelligence is likely to be tracking your life and making decisions that affect you.(Reuters: Thomas Peter)abc.net.au/news/why-you-should-care-about-artificial-intelligence/100591684Copy linkLink copiedShareShare articleArtificial intelligence has probably already made decisions about your life.It might have decided whether your insurance claim was accepted or rejected as fraudulent.It may have assessed your credit score, predicting if you were worthy of a loan or deemed too high risk.It could even have watched you drive, detecting if you are flouting the road rules and should be fined.And if AI hasn't already made a decision that affects your life, it almost certainly will, whether that be shaping what you see on social media or keeping tabs on you while you work from home with tracking software or some other application.So what do you need to know about automated decision-making and why should you care about it?What exactly is AI again?AI is a type of computer program that uses complex code and powerful processors to sift through massive amounts of data.The software analyses all that information to make decisions and take actions.There is not just one AI, but countless different systems being created for different purposes and applications.Most people often only pay attention to exciting new uses of AI, like the ""neural network"" that creates bizarre artworks prompted by key words. AI generated this image from the text prompt 'Bondi love story'.(ABC)Or to scary warnings of an ""AI apocalypse"" where scenarios from movies like The Terminator or War Games seem to be becoming a reality.But there are many everyday instances of AI that also deserve our attention.What are some examples?When Spotify recommends a song, it's analysing the music you've listened to and comparing them to tracks that other people like. The AI then makes decisions about what other songs you might also enjoy.By feeding more data into an AI, the system is trained to see more connections and correlations between different pieces of information. Spotify's AI analyses our listening habits to make song recommendations.(Pexels: Ivan Samkov)This kind of automated analysis can find things humans might miss. It can also process data at speeds and scales that are humanly impossible.This is why the technology industry often gathers as much data as possible from any sources available. The idea is that more data will result in more advanced and more accurate forms of AI.But that data has to come from somewhere.Behind the AI in smart home devices like Alexa or semi-autonomous cars like Tesla is an army of often low paid workers who clean and label data so it can be understood by machines — whether it's images of street signs, recordings of speech or videos of colonoscopies.Earlier this year it was reported that large numbers of refugees in Africa and the Middle East were doing this microwork for technology companies. Their wages are often miniscule and come with no security or rights. This kind of work is also being outsourced to prisoners in Finland.Is AI better at making decisions than humans?There's an old saying in computer science: garbage in, garbage out.AI is only as good as the data used to train it. If trained on incorrect, biased or poor-quality data, its decisions and actions will also be incorrect, biased or poor.Algorithms might 'screw' youAlgorithms that make decisions without human input will ""screw [some] people by default"", a visiting legal expert warns.Read moreWhen using AI tech in hiring decisions, for instance, there's evidence they're choosing men over better-qualified women. Why? Because the data used to train these systems are embedded with gender bias, which AI then reinforces.There's no shortage of similar examples. Different AI systems have rejected black loan applicants at much higher rates than white applicants and decided black hospital patients need fewer health care resources than white patients.But algorithms are just maths, right? Maths can't be badAI decisions tend to be viewed as the result of a neutral, objective technology.Yet, from start to finish, AI is the product of human choices — prone to human errors, shaped by human biases and directed by human values.Download This ShowYour weekly guide to the world of media, culture, and technology. From social media to gadgets, streaming services to privacy issues. Each week Marc Fennell and a panel of guests  take a fun deep dive into how technology is reshaping our lives. Read moreAnd AI is already directly affecting what sociologists call our ""life chances"" — our access to services, our opportunities for advancement and our place in society.The ways we often talk about AI as a system that exists outside of people ""makes us think of something mystical and overseeing us all,"" Ellen Broad, senior fellow in the 3A Institute at ANU, tells Download This Show.The risk is that we assume these technologies are ""much more intelligent than they actually are simply by virtue of calling it AI,"" Broad says.""So it gives these systems a sense of intelligence and superiority that sometimes they don't really deserve.""What are the solutions?There is growing recognition among the giants of Silicon Valley that building and using AI comes with many thorny ethical and social issues.Tech companies like Google have established principles for ""responsible AI"" that are meant to guide their practices. For them, responsible AI means considering issues related to fairness, privacy and security when designing these technologies.But others such as Facebook tend to argue that many of AI's problems can be solved by using more AI.So what's the issue?All AI is designed to accomplish specific tasks or solve specific problems.Critics, both outside and inside the industry, say these technologies are largely created and controlled by a small group of people — mostly white, mostly male, mostly affluent.'I'm always worried.' How online platforms can improve women's safetyPersonal abuse and degrading content in Kate's news feed make online spaces feel unsafe to her. So what are platforms doing about it?Read moreThey get to decide how AI is trained and why it's used. They get to define the problems. The rest of us, largely, must live with those decisions, both human and artificial.But there is increasing public debate about the transparency and accountability of these automated processes that are making decisions about our lives.As we speed into a future of AI, the crucial question that many are trying to answer is what kind of technologies are compatible with a democratic society?And that's a question more of us need to consider, since the answer will have a profound impact on everybody.Dr Jathan Sadowski is a research fellow at the Emerging Technologies Research Lab and Centre of Excellence for Automated Decision-Making and Society at Monash University. He is also an ABC Top 5 Humanities scholar for 2021.RN in your inboxGet more stories that go beyond the news cycle with our weekly newsletter.Your information is being handled in accordance with the ABC Privacy Collection Statement.This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.Email addressSubscribePosted 29 Nov 202129 Nov 2021Mon 29 Nov 2021 at 1:00pmShareCopy linkFacebookX (formerly Twitter)Related StoriesJade has experienced what unconscious gender bias feels like while looking for workWe asked an AI tool to 'paint' images of Australia. Critics say they're good enough to sell'Surveillance reaching into the home': The rise of Big Brother at workGoogle accused of 'racism and censorship' by staff amid firing of star researcherMore on:AustraliaComputer ScienceRoboticsTop Stories'It looks like greenwashing': AustralianSuper's ethical option investing in coal, oil and gas industriesFederal Labor under pressure to cut ties with scandal-engulfed CFMEU'I haven't lost hope': Memorial service held near Amsterdam to mark 10-year anniversary of MH17 disasterAnalysis by David SpeersAfter once calling Donald Trump 'nuts', Kevin Rudd has changed his tunelivePressure mounts on Biden as high-profile Democrat joins calls for president to drop out of raceNeo-Nazi's social media suspension is only a drop in the ocean against extremism, warn expertsAnalysis by Nick CamptonHow a blood-soaked try and unbreakable belief carried New South Wales to State of Origin gloryHere is the full list of Emmy nominees, with Shōgun, The Bear and Baby Reindeer on top and one more shout for The CrownIs there a clean finish for a wind turbine at the end of its life? Not entirelyFaced with successive natural disasters, lifelong farmers are leaving this 'salad bowl''I'd like a dollar for every photo taken': The 200-year history of one of Hobart's most recognisable attractionsFive quick hits: Luai steps up in Blues' Origin triumph, tempers flare and Slater throws pre-match curveballHow close is Laos to going bust?As DV incidents rise across the country, here's where new services will be availableSparrows accurately predict lead poisoning risk of children in Australian mining townsPopular NowDon't miss news that matters to you. Log in to ABC today to get a more personalised experience tailored to your preferences.GET STARTED1.Analysis by David Speersanalysis:After once calling Donald Trump 'nuts', Kevin Rudd has changed his tune2.Analysis by Nick Camptonanalysis:How a blood-soaked try and unbreakable belief carried New South Wales to State of Origin glory3.'It looks like greenwashing': AustralianSuper's ethical option investing in coal, oil and gas industries4.livelive:Pressure mounts on Biden as high-profile Democrat joins calls for president to drop out of race5.Former Miss Australia and ex-Labor MP in court over alleged abuse of then-husband6.How close is Laos to going bust?Top Stories'It looks like greenwashing': AustralianSuper's ethical option investing in coal, oil and gas industriesFederal Labor under pressure to cut ties with scandal-engulfed CFMEU'I haven't lost hope': Memorial service held near Amsterdam to mark 10-year anniversary of MH17 disasterAnalysis by David SpeersAfter once calling Donald Trump 'nuts', Kevin Rudd has changed his tunelivePressure mounts on Biden as high-profile Democrat joins calls for president to drop out of raceJust InJosh Neille's gritty care for rescued wildlife has made him a social media star and he's hoping to inspire others4m ago4 minutes agoWed 17 Jul 2024 at 4:45pmExemptions to long-awaited construction code could continue to lock people with disabilities out of homes, advocates say 8m ago8 minutes agoWed 17 Jul 2024 at 4:41pmEarly childhood policies spruiked as cost-of-living relief by Labor ahead of NT election9m ago9 minutes agoWed 17 Jul 2024 at 4:40pmAJ lost the ability to walk earlier this year. Robots are helping him to get back on his feet10m ago10 minutes agoWed 17 Jul 2024 at 4:40pmMore Just InBack to top",,,,,,,,,
https://news.google.com/rss/articles/CBMiWmh0dHBzOi8vd3d3LnN5ZnkuY29tL3N5Znktd2lyZS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1oZWxwcy1zY2llbnRpc3RzLWRlc2lnbi1iZXR0ZXItZm9vZNIBXmh0dHBzOi8vd3d3LnN5ZnkuY29tL3N5Znktd2lyZS9hcnRpZmljaWFsLWludGVsbGlnZW5jZS1oZWxwcy1zY2llbnRpc3RzLWRlc2lnbi1iZXR0ZXItZm9vZD9hbXA?oc=5,"A.I. plucks through 3,366 chickpea varieties, designs world-feeding genetically modified crops - Syfy",2021-11-27,Syfy,https://www.syfy.com,"Combating hunger and maintaining complete nutrition remains a challenge around the world, particularly in underdeveloped regions. Efforts to combat those challenges are ongoing and it’s a big job, one which gets bigger as the population increases, and the effects of climate change alter the agricultural landscape.","['Artificial Intelligence', 'food', 'genomes', 'Interviews', 'Science', 'news']","Combating hunger and maintaining complete nutrition remains a challenge around the world, particularly in underdeveloped regions. Efforts to combat those challenges are ongoing and it’s a big job, one which gets bigger as the population increases, and the effects of climate change alter the agricultural landscape.","Combating hunger and maintaining complete nutrition remains a challenge around the world, particularly in underdeveloped regions. Efforts to combat those challenges are ongoing and it’s a big job, one which gets bigger as the population increases, and the effects of climate change alter the agricultural landscape.",http://schema.org,NewsArticle,https://www.syfy.com/syfy-wire/artificial-intelligence-helps-scientists-design-better-food,2023-11-08T05:56:39Z,2021-11-27T14:30:13Z,"A.I. plucks through 3,366 chickpea varieties, designs world-feeding genetically modified crops","[{'@type': 'ImageObject', 'url': 'https://www.syfy.com/sites/syfy/files/styles/scale_1280/public/2021/11/a_vibrant_display_of_chickpea_diversity.jpeg', 'width': 1280, 'height': 852}, {'@type': 'ImageObject', 'url': '/sites/syfy/files/styles/scale_862/public/2021/11/chickpea_pods.jpg'}]",https://www.syfy.com/syfy-wire/artificial-intelligence-helps-scientists-design-better-food,"[{'@type': 'Person', 'name': 'Cassidy Ward'}]","{'@type': 'Organization', 'name': 'SYFY', 'logo': {'@type': 'ImageObject', 'url': 'https://www.syfy.com/modules/custom/ls_site/images/syfy-logo-85x60.png', 'width': '85', 'height': '60'}}",,N/A,N/A,"Syfy Insider ExclusiveCreate a free profile to get unlimited access to exclusive videos, sweepstakes, and more!Sign Up For Free to View  SYFY WIRE   Artificial Intelligence  A.I. plucks through 3,366 chickpea varieties, designs world-feeding genetically modified cropsNow we need a pita making robot.By  Cassidy Ward Nov 27, 2021, 9:30 AM ET        Photo: Rajeev Varshney Combating hunger and maintaining complete nutrition remains a challenge around the world, particularly in underdeveloped regions. Efforts to combat those challenges are ongoing and it’s a big job, one which gets bigger as the population increases, and the effects of climate change alter the agricultural landscape.Sometimes finding the solution to big problems requires detailed knowledge of very small things, like the genome of staple plants like chickpeas. A recent study carried out by Rajeev Varshney from the Center of Excellence in Genomics and Systems Biology at the International Crops Research Institute for Semi-Arid Tropics (ICRISAT) and colleagues looked at the genome of chickpeas in hopes of finding a way to build a better food crop. Their findings were published in the journal Nature.They didn’t just look at one kind of chickpea. Instead, the work analyzed more than 3,366 varieties of the plant — 3,171 cultivated species and 195 wild species — in order to get a full picture of genetic diversity. They created a pan-genome which describes genetic diversity across cultivated species and their wild source plants.“It was a long journey from inception in 2014,” Varshney told SYFY WIRE. “This was the first effort of its kind across any crop. It took about three years for us to generate all of the data and then three to four years for data analysis and interpretation.”The work, though daunting, resulted in the identification of 29,870 total genes, including 1,582 which had not been reported before. This analysis identified beneficial genes as well as detrimental mutations which result in less successful plants and lower crop yields. That data was then delivered to the University of Queensland where it was analyzed by an artificial intelligence called FastStack, which is specialized for designing new varieties of plants and crops with an eye toward optimal output.   Photo: Rajeev Varshney Productivity of pulse crops, of which chickpeas are one type, has been stagnant for the last half-century. As populations increase, that has resulted in low per-capita food availability and contributes to malnutrition. Improving yields through enhancements could help to lessen some of that load.“Chickpeas are an important legume crop, cultivated in more than 50 countries and are a rich source of protein. The chickpea is a key crop toward nutritional security, especially in developing countries,” Varshney said.Scientists identified genes and gene families called haplotypes — groupings of genes which are all inherited together — which play potential roles in controlling seed size and development. Next steps are to take the genetic and artificial intelligence data and breed new, more robust varieties of chickpea in the real world. Importantly, the team identified potential enhancements with an eye toward maintaining genetic diversity. They estimate an ability to improve seed weight, an important yield metric, by up to 23%“We propose three breeding approaches based on the genomic predictions that aim to improve 16 traits and enhance production,” Varshney said. “Our findings can be utilized to develop improved chickpea varieties with better yield, nutrition, and higher resistance to several biotic and abiotic factors. We have a plan to use the AI approach to combine haplotypes of our choosing for optimal output in elite varieties of chickpea.”While this genetic and AI research has provided scientists with the ability to begin breeding novel species for improved crop yields, it has also opened a door for similar research in other staple crops. ICRISAT is reviewing other legumes and cereals, crops which have been identified as key stability foods, to identify their next target. Moreover, the work has the potential for deploying benefits not just across different species of chickpea, but to entirely different plants.Previous work sequencing the genome of the pigeonpea isolated a gene which could be deployed in soybeans, making them resistant to Asian soybean rust, a destructive fungal disease. If the movies tell us anything about artificial intelligence, it’s that they will inevitably see us as a threat and seek to destroy us, but before they do, they’re busy working to design us more and better food.We can’t wait to try some of that intelligently designed hummus. Read more about:Artificial IntelligenceFoodGenomesInterviewsNewsScience  Related Stories           Violent Night Sequel Could Have Christmas Vacation Influences            The Inside Story of How Christopher Nolan Saved Donnie Darko            Cocaine Bear VFX Supervisor Talks Gonzo Project            Jaws' Alex Kintner Looks Back on Shark Victim Role 49 Years Later            How SYFY's The Ark Borrowed a Page From Star Wars in Season 2            Cocaine Bear Writer Talks Potential Sequels            Drive Angry Director Reveals Idea for Unmade Sequel            Jaws Caused an Early Audience Member to Vomit on the Floor            Bruce Almighty Writers Pitched Devil-centric Sequel Brucifer            Where to Stream the Jaws Movies This Summer            Before the Dark Universe, Van Helsing Was a Wild Remix of Universal's Monsters            'John Wick Puppy Scene Was Briefly Removed from Script   Sponsored Stories Recommended by Zergnet





The Tragedy Of Mayim Bialik Just Gets Sadder And SadderLooper.com







Maher Finally Weighs In About Trump's Assassination AttemptDecider.com







Graphic Scenes That Ruined These Actors' CareersLooper.com







Heartbreaking Details Are Spilling Out About Shelley DuvallTheList.com







Co-Stars Who Lost All Control While Kissing Each OtherLooper.com







The Girl Who Played Pepper On AHS Is Gorgeous In Real LifeLooper.com

Powered by ZergNet",,,269108,2021-11-27T14:30:13Z,https://www.syfy.com/sites/syfy/files/2021/11/a_vibrant_display_of_chickpea_diversity.jpeg,SYFY WIRE Blog Post,,,
https://news.google.com/rss/articles/CBMid2h0dHBzOi8vdGhlY29udmVyc2F0aW9uLmNvbS93aGF0cy10aGUtc2VjcmV0LXRvLW1ha2luZy1zdXJlLWFpLWRvZXNudC1zdGVhbC15b3VyLWpvYi13b3JrLXdpdGgtaXQtbm90LWFnYWluc3QtaXQtMTcyNjkx0gEA?oc=5,"What's the secret to making sure AI doesn't steal your job? Work with it, not against it - The Conversation",2021-11-30,The Conversation,https://theconversation.com,"The best AI chess computer outperforms the best human chess players. Yet the most supreme chess play on Earth comes from a human, helped by AI.",N/A,"The best AI chess computer outperforms the best human chess players. Yet the most supreme chess play on Earth comes from a human, helped by AI.",N/A,,,,,,,,,,,,N/A,N/A,"






Shutterstock









            What’s the secret to making sure AI doesn’t steal your job? Work with it, not against it
          




Published: November 30, 2021 12:00am EST












Cecile Paris, Andrew Reeson, CSIRO



Authors





        Cecile Paris
      


      Chief Research Scientist, Knowledge Discovery & Management, CSIRO
    





        Andrew Reeson
      


      Economist, Data61, CSIRO
    





Disclosure statement
Cecile Paris receives funding from various departments of the Australian Government. She is an Honorary Professor at Macquarie University.
Andrew Reeson has received funding from various departments of the Australian Government and is involved in research collaborations with nbn co and TAFE Queensland.


Partners

CSIRO provides funding as a founding partner of The Conversation AU.
View all partners

We believe in the free flow of informationRepublish our articles for free, online or in print, under a Creative Commons license.Republish this article





 Email


 X (Twitter)30


 Facebook29


 LinkedIn


 WhatsApp


 Messenger

 Print


Whether it’s athletes on a sporting field or celebrities in the jungle, nothing holds our attention like the drama of vying for a single prize. And when it comes to the evolution of artificial intelligence (AI), some of the most captivating moments have also been delivered in nailbiting finishes.
In 1997, IBM’s Deep Blue chess computer was pitted against grandmaster and reigning world champion Garry Kasparov, having lost to him the previous year.
But this time, the AI won. The popular Chinese game Go was next, in 2016, and again there was a collective intake of breath when Google’s AI was victorious. These competitions elegantly illustrate what is unique about AI: we can program it to do things we can’t do ourselves, such as beat a world champion.
But what if this framing obscures something vital – that human and artificial intelligence are not the same? AI can quickly process vast amounts of data and be trained to execute specific tasks; human intelligence is significantly more creative and adaptive.
The most interesting question is not who will win, but what can people and AI achieve together? Combining both forms of intelligence can provide a better outcome than either can achieve alone. 
This is called collaborative intelligence. And this is the premise of CSIRO’s new A$12 million Collaborative Intelligence (CINTEL) Future Science Platform, which we are leading. 




      Read more:
      Work is a fundamental part of being human. Robots won't stop us doing it




Checkmate mates
While chess has been used to illustrate AI-human competition, it also provides an example of collaborative intelligence. IBM’s Deep Blue beat the world champion, but did not render humans obsolete. Human chess players collaborating with AI have proven superior to both the best AI systems and human players. 
And while such “freestyle” chess requires both excellent human skill and AI technology, the best results don’t come from simply combining the best AI with the best grandmaster. The process through which they collaborate is crucial.
So for many problems – particularly those that involve complex, variable and hard-to-define contexts – we’re likely to get better results if we design AI systems explicitly to work with human partners, and give humans the skills to interpret AI systems. 



Machines can do repetitive and dangerous work, but only in a set environment. They can’t transfer their skills as humans can.
Shutterstock


A simple example of how machines and people are already working together is found in the safety features of modern cars. Lane keep assist technology uses cameras to monitor lane markings and will adjust the steering if the car appears to be drifting out of its lane. 
However, if it senses the driver is actively steering away, it will desist so the human remains in charge (and the AI continues to assist in the new lane). This combines the strengths of a computer, such as limitless concentration, with those of the human, such as knowing how to respond to unpredictable events. 
There is potential to apply similar approaches to a range of other challenging problems. In cybersecurity settings, humans and computers could work together to identify which of the many threats from cybercriminals are the most urgent. 
Similarly, in biodiversity science, collaborative intelligence can be used to make sense of massive numbers of specimens housed in biological collections.
Laying the foundations
We know enough about collaborative intelligence to say it has massive potential, but it’s a new field of research – and there are more questions than answers.
Through CSIRO’s CINTEL program we will explore how people and machines work and learn together, and how this way of collaborating can improve human work. 
Specifically, we will address four foundations of collaborative intelligence:

collaborative workflows and processes. Collaborative intelligence requires rethinking workflow and processes, to ensure humans and machines complement each other. We’ll also explore how it might help people develop new skills that might be useful across areas of the workforce 
situation awareness and understanding intent. Working towards the same goals and ensuring humans understand the current progress of a task
trust. Collaborative intelligence systems will not work without people trusting the machines. We must understand what trust means in different contexts, and how to establish and maintain trust
communication. The better the communication between humans and the machine, the better the collaboration. How do we ensure both understand each other?

Robots reimagined
One of our projects will involve working with the CSIRO-based robotics and autonomous systems team to develop richer human-robot collaboration. Collaborative intelligence will enable humans and robots to respond to changes in real time and make decisions together.
For example, robots are often used to explore environments that might be dangerous for humans, such as in rescue missions. In June, robots were sent to help in search and rescue operations, after a 12-storey condo building collapsed in Surfside, Florida. 




      Read more:
      An expert on search and rescue robots explains the technologies used in disasters like the Florida condo collapse




Often, these missions are ill-defined, and humans must use their own knowledge and skills (such as reasoning, intuition, adaptation and experience) to identify what the robots should be doing. While developing a true human-robot team may initially be difficult, it’s likely to be more effective in the long term for complex missions.





Artificial intelligence (AI)


CSIRO


Robots


Artificial intelligence and jobs


Robot workers


Collaborative problem solving









",,,,,,,,,
https://news.google.com/rss/articles/CBMiYWh0dHBzOi8vd3d3LmphcGFudGltZXMuY28uanAvbmV3cy8yMDIxLzExLzI3L25hdGlvbmFsL21lZGlhLW5hdGlvbmFsL2hhbGxvd2Vlbi10cmFpbi1zdGFiYmluZy1haS_SAQA?oc=5,Can artificial intelligence be harnessed to protect the public from random assailants? - The Japan Times,2021-11-27,The Japan Times,https://www.japantimes.co.jp,Facial recognition technology can be used to identify known criminals from a database as well as home in on a person acting suspiciously.,"crime,halloween,stabbings,Keio Line,Kyota Hattori",Facial recognition technology can be used to identify known criminals from a database as well as home in on a person acting suspiciously.,Facial recognition technology can be used to identify known criminals from a database as well as home in on a person acting suspiciously.,https://schema.org,NewsArticle,https://www.japantimes.co.jp/news/2021/11/27/national/media-national/halloween-train-stabbing-ai/,2021-11-28T08:58:55+09:00,1970-01-01T09:00:00+09:00,Can artificial intelligence be harnessed to protect the public from random assailants?,"{'@type': 'ImageObject', 'url': 'https://www.japantimes.co.jp/uploads/imported_images/uploads/2021/11/np_file_124867.jpeg'}","{'@type': 'WebPage', '@id': 'https://www.japantimes.co.jp/news/2021/11/27/national/media-national/halloween-train-stabbing-ai/'}","[{'@type': 'Person', 'name': 'Mark Schreiber'}]","{'@type': 'Organization', 'name': 'The Japan Times', 'url': 'https://www.japantimes.co.jp/', 'sameAs': ['https://twitter.com/japantimes', 'https://www.facebook.com/thejapantimes', 'https://www.instagram.com/thejapantimes/?hl=en', 'https://www.linkedin.com/company/the-japan-times'], 'logo': {'@type': 'ImageObject', 'url': '/theme_japantimes/images/logo.svg', 'width': 270, 'height': 57}}",,JAPAN,N/A,N/A,,,,1970-01-01T09:00:00+09:00,https://www.japantimes.co.jp/uploads/imported_images/uploads/2021/11/np_file_124867.jpeg,JAPAN,en,"On the evening of Oct. 31, 25-year-old Fukuoka native Kyota Hattori — wearing makeup and a purple and green ensemble to emulate the villainous Joker of “Batman” franchise fame — boarded a Keio Line train at Keio-Hachioji Station, heading for central Tokyo. After spending half an hour meandering around Shibuya, which was packed with costumed revelers feting Halloween, Hattori headed back toward Hachioji, but reversed direction again at Chofu, where he changed to a Shinjuku-bound limited express train.Soon after the doors closed, according to eye witness reports, he removed a survival knife and liquids from a backpack. When a 72-year-old male passenger tried to intervene, Hattori allegedly stabbed the man and proceeded to pursue fleeing passengers, splashing them with lighter fluid, which he then ignited. The stabbing victim was hospitalized in a critical condition and 16 other passengers suffered burns and smoke inhalation.Videos captured on smartphones showed desperate passengers struggling to squeeze out the train's partially opened windows onto the platform of Kokuryo Station.","{'@type': 'Organization', 'name': 'The Japan Times', 'url': 'https://www.japantimes.co.jp/'}"
